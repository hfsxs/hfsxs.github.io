<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Prometheus监控告警组件Alertmanager配置高可用集群</title>
    <link href="/linux/PrometheusAlertmanagerHA/"/>
    <url>/linux/PrometheusAlertmanagerHA/</url>
    
    <content type="html"><![CDATA[<p>Alertmanager，Prometheus的告警管理组件，官方为防止单点故障，设计了基于Gossip（去中心化的信息传播机制，类似于流言扩散，即每个节点定期将自身的状态数据随机同步给集群内的其他节点，而收到数据的节点再继续转发给其他节点，最终在短时间内实现全集群数据一致）协议的集群构建模式</p><h1 id="1-配置主节点"><a href="#1-配置主节点" class="headerlink" title="1.配置主节点"></a>1.配置主节点</h1><pre><code class="hljs">sudo vi /lib/systemd/system/alertmanager.service [Unit]Description=Alertmanager ServerDocumentation=https://github.com/prometheus/alertmanagerAfter=network.target[Service]Type=simpleUser=rootGroup=rootWorkingDirectory=/usr/local/prometheusExecStart=/usr/local/prometheus/alertmanager --config.file=/usr/local/prometheus/alertmanager.yml --storage.path=/usr/local/prometheus/data --cluster.listen-address=0.0.0.0:9094ExecReload=/bin/kill -s HUP $MAINPIDExecStop=/bin/kill -s QUIT $MAINPIDRestart=on-failure[Install]WantedBy=multi-user.target</code></pre><h1 id="2-配置其余节点"><a href="#2-配置其余节点" class="headerlink" title="2.配置其余节点"></a>2.配置其余节点</h1><pre><code class="hljs">sudo vi /lib/systemd/system/alertmanager.service[Unit]Description=Alertmanager ServerDocumentation=https://github.com/prometheus/alertmanagerAfter=network.target[Service]Type=simpleUser=rootGroup=rootWorkingDirectory=/usr/local/prometheusExecStart=/usr/local/prometheus/alertmanager --config.file=/usr/local/prometheus/alertmanager.yml --storage.path=/usr/local/prometheus/data --cluster.listen-address=0.0.0.0:9094 --cluster.peer=172.16.100.201:9094ExecReload=/bin/kill -s HUP $MAINPIDExecStop=/bin/kill -s QUIT $MAINPIDRestart=on-failure[Install]WantedBy=multi-user.target</code></pre><h1 id="3-启动Alertmanager"><a href="#3-启动Alertmanager" class="headerlink" title="3.启动Alertmanager"></a>3.启动Alertmanager</h1><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start alertmanager.servicesudo systemctl enable alertmanager.service</code></pre><h1 id="4-配置Prometheus"><a href="#4-配置Prometheus" class="headerlink" title="4.配置Prometheus"></a>4.配置Prometheus</h1><pre><code class="hljs">global:  scrape_interval: 15s   evaluation_interval: 15s   scrape_timeout: 10salerting:  alertmanagers:    - static_configs:        - targets:            - 172.16.100.201:9093            - 172.16.100.202:9093</code></pre><h1 id="5-重载Prometheus"><a href="#5-重载Prometheus" class="headerlink" title="5.重载Prometheus"></a>5.重载Prometheus</h1><pre><code class="hljs">curl -XPOST http://127.0.0.1:9090/-/reload</code></pre><h1 id="6-验证告警高可用"><a href="#6-验证告警高可用" class="headerlink" title="6.验证告警高可用"></a>6.验证告警高可用</h1><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.cnblogs.com/yinzhengjie/p/18547193">https://www.cnblogs.com/yinzhengjie/p/18547193</a></li><li><a href="https://blog.csdn.net/qq_50247813/article/details/151292374">https://blog.csdn.net/qq_50247813/article/details/151292374</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Prometheus</tag>
      
      <tag>监控告警</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PostgreSQL数据库管理命令详解</title>
    <link href="/linux/PostgreSQLManageSQL/"/>
    <url>/linux/PostgreSQLManageSQL/</url>
    
    <content type="html"><![CDATA[<h1 id="1-登录数据库"><a href="#1-登录数据库" class="headerlink" title="1.登录数据库"></a>1.登录数据库</h1>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>SQL</tag>
      
      <tag>数据库</tag>
      
      <tag>PostgreSQL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prometheus监控基于PromAI构建自动化巡检平台</title>
    <link href="/linux/PrometheusPromAI/"/>
    <url>/linux/PrometheusPromAI/</url>
    
    <content type="html"><![CDATA[<p>PromAI，Go语言编写的基于Prometheus监控系统的自动化巡检平台，自动收集Prometheus指标并分析系统健康状态，最后生成可视化的HTML报告。此外，还支持发送多种渠道告警通知，如钉钉、企业微信、邮件等</p><h1 id="1-配置Go语言环境"><a href="#1-配置Go语言环境" class="headerlink" title="1.配置Go语言环境"></a>1.配置Go语言环境</h1><h1 id="2-安装PromAI"><a href="#2-安装PromAI" class="headerlink" title="2.安装PromAI"></a>2.安装PromAI</h1><h2 id="2-1-下载PromAI"><a href="#2-1-下载PromAI" class="headerlink" title="2.1 下载PromAI"></a>2.1 下载PromAI</h2><pre><code class="hljs">cd /optgit clone https://github.com/kubehan/PromAI</code></pre><h2 id="2-2-编译PromAI"><a href="#2-2-编译PromAI" class="headerlink" title="2.2 编译PromAI"></a>2.2 编译PromAI</h2><pre><code class="hljs">cd PromAIgo mod downloadgo build -o PromAI main.go</code></pre><h1 id="3-创建配置文件"><a href="#3-创建配置文件" class="headerlink" title="3.创建配置文件"></a>3.创建配置文件</h1><pre><code class="hljs">cp config/config.yaml .</code></pre><h1 id="4-创建启动脚本"><a href="#4-创建启动脚本" class="headerlink" title="4.创建启动脚本"></a>4.创建启动脚本</h1><pre><code class="hljs">sudo vi /lib/systemd/system/promai.service[Unit]Description=PromAIDocumentation=https://github.com/kubehan/PromAIAfter=network.target[Service]Type=simpleUser=rootGroup=rootWorkingDirectory=/opt/prometheus/PromAIExecStart=/opt/PromAI/PromAI -config /opt/PromAI/config.yaml -port :8091ExecReload=/bin/kill -s HUP $MAINPIDExecStop=/bin/kill -s QUIT $MAINPIDRestart=on-failure[Install]WantedBy=multi-user.target</code></pre><h1 id="5-启动PromAI"><a href="#5-启动PromAI" class="headerlink" title="5.启动PromAI"></a>5.启动PromAI</h1><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start promai.servicesudo systemctl enable promai.service</code></pre><h1 id="6-验证PromAI"><a href="#6-验证PromAI" class="headerlink" title="6.验证PromAI"></a>6.验证PromAI</h1><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://mp.weixin.qq.com/s/GtizYq5Tvs0BHtfKiUeCvQ">https://mp.weixin.qq.com/s/GtizYq5Tvs0BHtfKiUeCvQ</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Prometheus</tag>
      
      <tag>监控告警</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prometheus监控MinIO对象存储</title>
    <link href="/linux/Prometheus-MinIO/"/>
    <url>/linux/Prometheus-MinIO/</url>
    
    <content type="html"><![CDATA[<h1 id="1-创建Prometheus认证token"><a href="#1-创建Prometheus认证token" class="headerlink" title="1.创建Prometheus认证token"></a>1.创建Prometheus认证token</h1><pre><code class="hljs">mc admin prometheus generate opsscrape_configs:- job_name: minio-job  bearer_token: eyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJwcm9tZXRoZXVzIiwic3ViIjoiNTFDc0VpSEpYWFU5SkNnYnB0NUEiLCJleHAiOjQ5MTY2MjU2NzF9.3t1YZ51afGkAnYGZj3VCJpE0LPXhT6Q4qBKT3Hv5U3JQNJWVOHxcHPnI-0VSOfJdVJOgotjJGGinZi2jmPuRoQ  metrics_path: /minio/v2/metrics/cluster  scheme: http  static_configs:  - targets: [minio.ops.org]</code></pre><h1 id="2-配置Prometheus"><a href="#2-配置Prometheus" class="headerlink" title="2.配置Prometheus"></a>2.配置Prometheus</h1><h2 id="2-1-配置MinIO监控实例"><a href="#2-1-配置MinIO监控实例" class="headerlink" title="2.1 配置MinIO监控实例"></a>2.1 配置MinIO监控实例</h2><pre><code class="hljs">sudo vi /usr/local/prometheus/prometheus.ymlglobal:  scrape_interval: 15s   evaluation_interval: 15s   scrape_timeout: 10s scrape_configs:  - job_name: &quot;prometheus&quot;    static_configs:      - targets: [&quot;localhost:9090&quot;]  - job_name: minio-job    bearer_token: eyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJwcm9tZXRoZXVzIiwic3ViIjoiNTFDc0VpSEpYWFU5SkNnYnB0NUEiLCJleHAiOjQ5MTY2MjU2NzF9.3t1YZ51afGkAnYGZj3VCJpE0LPXhT6Q4qBKT3Hv5U3JQNJWVOHxcHPnI-0VSOfJdVJOgotjJGGinZi2jmPuRoQ    metrics_path: /minio/v2/metrics/cluster    scheme: http    static_configs:    - targets:      - 172.16.100.101:9000      - 172.16.100.102:9000      - 172.16.100.103:9000</code></pre><h2 id="2-2-重载Prometheus"><a href="#2-2-重载Prometheus" class="headerlink" title="2.2 重载Prometheus"></a>2.2 重载Prometheus</h2><pre><code class="hljs">curl -XPOST http://127.0.0.1:9090/-/reload</code></pre><h1 id="3-Grafana导入监控模版"><a href="#3-Grafana导入监控模版" class="headerlink" title="3.Grafana导入监控模版"></a>3.Grafana导入监控模版</h1><p>Dashboards —&gt; New —&gt; Import —&gt; 模版ID：13502</p><p><img src="/img/wiki/prometheus/MinIO.jpg" alt="MinIO"></p><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/u013810234/article/details/143995607">https://blog.csdn.net/u013810234/article/details/143995607</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>存储</tag>
      
      <tag>分布式存储</tag>
      
      <tag>云原生</tag>
      
      <tag>Prometheus</tag>
      
      <tag>监控告警</tag>
      
      <tag>对象存储</tag>
      
      <tag>MinIO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MinIO客户端工具mc详解</title>
    <link href="/linux/MinIO-MC/"/>
    <url>/linux/MinIO-MC/</url>
    
    <content type="html"><![CDATA[<p>mc，MinIO强大的命令行客户端工具，且兼容任何Amazon S3的存储服务进行交互。mc提供了一系列类Unix操作命令，如ls、cp、rm等，以便用户能够轻松地管理和传输数据</p><h1 id="1-下载mc"><a href="#1-下载mc" class="headerlink" title="1.下载mc"></a>1.下载mc</h1><pre><code class="hljs">wget https://dl.minio.io/client/mc/release/linux-amd64/mc</code></pre><h1 id="2-安装mc"><a href="#2-安装mc" class="headerlink" title="2.安装mc"></a>2.安装mc</h1><pre><code class="hljs">mv mc /usr/local/bin/mc &amp;&amp; chmod +x /usr/local/bin/mcmc --autocompletion</code></pre><h1 id="3-MinIO创建AK-x2F-SK"><a href="#3-MinIO创建AK-x2F-SK" class="headerlink" title="3.MinIO创建AK&#x2F;SK"></a>3.MinIO创建AK&#x2F;SK</h1><h1 id="4-配置MinIO集群连接"><a href="#4-配置MinIO集群连接" class="headerlink" title="4.配置MinIO集群连接"></a>4.配置MinIO集群连接</h1><pre><code class="hljs">mc alias set &#39;ops&#39; &#39;http://minio.ops.org&#39;</code></pre><ul><li>注：ops表示MinIO存储服务端点的别名，最后将提示输入用户的AK&#x2F;SK</li></ul><h1 id="5-验证集群连接"><a href="#5-验证集群连接" class="headerlink" title="5.验证集群连接"></a>5.验证集群连接</h1><pre><code class="hljs">mc admin info ops●  10.1.11.201:9000   Uptime: 6 hours    Version: 2025-04-22T22:12:26Z   Network: 2/2 OK    Drives: 1/1 OK    Pool: 1●  10.1.11.202:9000   Uptime: 6 hours    Version: 2025-04-22T22:12:26Z   Network: 2/2 OK    Drives: 1/1 OK    Pool: 1┌──────┬──────────────────────┬─────────────────────┬──────────────┐│ Pool │ Drives Usage         │ Erasure stripe size │ Erasure sets ││ 1st  │ 0.2% (total: 20 GiB) │ 2                   │ 1            │└──────┴──────────────────────┴─────────────────────┴──────────────┘204 KiB Used, 1 Bucket, 3 Objects2 drives online, 0 drives offline, EC:1</code></pre><h1 id="6-常用操作"><a href="#6-常用操作" class="headerlink" title="6.常用操作"></a>6.常用操作</h1><h2 id="6-1-查看存储桶"><a href="#6-1-查看存储桶" class="headerlink" title="6.1 查看存储桶"></a>6.1 查看存储桶</h2><pre><code class="hljs">mc ls ops</code></pre><h2 id="6-2-创建存储桶"><a href="#6-2-创建存储桶" class="headerlink" title="6.2 创建存储桶"></a>6.2 创建存储桶</h2><pre><code class="hljs">mc mb ops/test</code></pre><h2 id="6-3-文件或目录上传"><a href="#6-3-文件或目录上传" class="headerlink" title="6.3 文件或目录上传"></a>6.3 文件或目录上传</h2><pre><code class="hljs"># 上传文件mc cp /root/config.yml ops/test# 上传目录mc cp -r /root/projects/ ops/test/projects</code></pre><h2 id="6-4-查看存储桶文件"><a href="#6-4-查看存储桶文件" class="headerlink" title="6.4 查看存储桶文件"></a>6.4 查看存储桶文件</h2><pre><code class="hljs">mc ls ops/test/config.yml</code></pre><h2 id="6-5-文件或目录下载"><a href="#6-5-文件或目录下载" class="headerlink" title="6.5 文件或目录下载"></a>6.5 文件或目录下载</h2><pre><code class="hljs"> # 下载文件 mc get ops/test/_config.yml /tmp # 下载目录 mc cp -r ops/test/projects /tmp</code></pre><h2 id="6-6-移动文件或目录"><a href="#6-6-移动文件或目录" class="headerlink" title="6.6 移动文件或目录"></a>6.6 移动文件或目录</h2><pre><code class="hljs"># 移动文件mc mv ops/test/config.yml ops/test/projects# 移动目录mc mv -r ops/test/projects ops/test/01</code></pre><h2 id="6-7-删除文件或目录"><a href="#6-7-删除文件或目录" class="headerlink" title="6.7 删除文件或目录"></a>6.7 删除文件或目录</h2><pre><code class="hljs"># 删除文件mc rm ops/test/01/config.yml# 删除目录mc rm ops/test/01 --recursive --force</code></pre><h2 id="6-8-删除存储桶"><a href="#6-8-删除存储桶" class="headerlink" title="6.8 删除存储桶"></a>6.8 删除存储桶</h2><pre><code class="hljs"># 删除空存储桶mc rb ops/001# 删除非空存储桶mc rb ops/test --force</code></pre><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.cnblogs.com/itzgr/p/18410023">https://www.cnblogs.com/itzgr/p/18410023</a></li><li><a href="https://blog.csdn.net/aaalk1001/article/details/143139038">https://blog.csdn.net/aaalk1001/article/details/143139038</a></li><li><a href="https://blog.csdn.net/codelearning/article/details/142151585">https://blog.csdn.net/codelearning/article/details/142151585</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>存储</tag>
      
      <tag>分布式存储</tag>
      
      <tag>对象存储</tag>
      
      <tag>MinIO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kafka集群部署可视化工具Kafbat</title>
    <link href="/linux/Kafbat/"/>
    <url>/linux/Kafbat/</url>
    
    <content type="html"><![CDATA[<h1 id="1-配置Java环境"><a href="#1-配置Java环境" class="headerlink" title="1.配置Java环境"></a>1.配置Java环境</h1><pre><code class="hljs">cd /optwget https://download.oracle.com/java/21/latest/jdk-21_linux-x64_bin.tar.gztar -xzvf jdk-21_linux-x64_bin.tar.gz</code></pre><h1 id="2-下载Kafbat"><a href="#2-下载Kafbat" class="headerlink" title="2.下载Kafbat"></a>2.下载Kafbat</h1><pre><code class="hljs">mkdir /opt/kafbat &amp;&amp; cd /opt/kafbatwget https://github.com/kafbat/kafka-ui/releases/download/v1.2.0/api-v1.2.0.jarmv api-v1.2.0.jar kafbat.jar</code></pre><h1 id="3-配置Kafka"><a href="#3-配置Kafka" class="headerlink" title="3.配置Kafka"></a>3.配置Kafka</h1><h2 id="3-1-开启JMX监控"><a href="#3-1-开启JMX监控" class="headerlink" title="3.1 开启JMX监控"></a>3.1 开启JMX监控</h2><pre><code class="hljs">sudo vi /opt/kafka/bin/kafka-server-start.shif [ $# -lt 1 ];then        echo &quot;USAGE: $0 [-daemon] server.properties [--override property=value]*&quot;        exit 1fibase_dir=$(dirname $0)if [ &quot;x$KAFKA_LOG4J_OPTS&quot; = &quot;x&quot; ]; then    export KAFKA_LOG4J_OPTS=&quot;-Dlog4j.configuration=file:$base_dir/../config/log4j.properties&quot;fiif [ &quot;x$KAFKA_HEAP_OPTS&quot; = &quot;x&quot; ]; then    export KAFKA_HEAP_OPTS=&quot;-Xmx1G -Xms1G&quot;    # 设置JMX监控端口    export JMX_PORT=&quot;9999&quot;fiEXTRA_ARGS=$&#123;EXTRA_ARGS-&#39;-name kafkaServer -loggc&#39;&#125;COMMAND=$1case $COMMAND in  -daemon)    EXTRA_ARGS=&quot;-daemon &quot;$EXTRA_ARGS    shift    ;;  *)    ;;esacexec $base_dir/kafka-run-class.sh $EXTRA_ARGS kafka.Kafka &quot;$@&quot;</code></pre><h2 id="3-2-重启Kafka"><a href="#3-2-重启Kafka" class="headerlink" title="3.2 重启Kafka"></a>3.2 重启Kafka</h2><pre><code class="hljs">sudo systemctl restart kafka.service</code></pre><h1 id="4-创建配置文件"><a href="#4-创建配置文件" class="headerlink" title="4.创建配置文件"></a>4.创建配置文件</h1><pre><code class="hljs">vi /opt/kafbat/application.ymlserver:  port: 8080# 启用JMX监控spring:  jmx:    enabled: true  security:    # 设置认证密码    user:      name: admin      password: Kafka_uikafka:  clusters:    - name: kraft-cluster      bootstrapServers: 172.100.100.101:9092,172.100.100.102:9092,172.100.100.103:9092      metrics:        type: JMX        # 设置JMX端口，与Kafka设置一致        port: 9999        jmx:          extraMappings:            - pattern: &#39;kafka.controller:type=KafkaController,name=*&#39;               attribute: &#39;Value&#39;      properties:        security.protocol:  PLAINTEXT        sasl.mechanism:  PLAIN auth:  type: LOGIN_FORMlogging:  level:    root: INFO    io.kafbat.ui:  DEBUG    reactor.netty.http.server.AccessLog:  INFO</code></pre><h1 id="5-创建启动脚本"><a href="#5-创建启动脚本" class="headerlink" title="5.创建启动脚本"></a>5.创建启动脚本</h1><pre><code class="hljs">sudo vi /lib/systemd/system/kafbat.service[Unit]Description=KafbatDocumentation=https://github.com/kafbat/kafka-uiAfter=network.target[Service]Type=simpleUser=rootExecStart=/opt/jdk-21.0.9/bin/java -Dspring.config.additional-location=/opt/kafbat/application.yml --add-opens java.rmi/javax.rmi.ssl=ALL-UNNAMED -jar /opt/kafbat/kafbat.jarRestart=on-failure[Install]WantedBy=multi-user.target</code></pre><h1 id="6-启动Kafbat"><a href="#6-启动Kafbat" class="headerlink" title="6.启动Kafbat"></a>6.启动Kafbat</h1><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start kafbat.servicesudo systemctl enable kafbat.service</code></pre><h1 id="7-验证kafbat"><a href="#7-验证kafbat" class="headerlink" title="7.验证kafbat"></a>7.验证kafbat</h1><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/Vector97/article/details/128539970">https://blog.csdn.net/Vector97/article/details/128539970</a></li><li><a href="https://blog.csdn.net/u011424614/article/details/151195306">https://blog.csdn.net/u011424614/article/details/151195306</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kafka</tag>
      
      <tag>MQ</tag>
      
      <tag>中间件</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prometheus监控Kafka集群</title>
    <link href="/linux/Prometheus-Kafka/"/>
    <url>/linux/Prometheus-Kafka/</url>
    
    <content type="html"><![CDATA[<h1 id="1-下载kafka-exporter"><a href="#1-下载kafka-exporter" class="headerlink" title="1.下载kafka_exporter"></a>1.下载kafka_exporter</h1><pre><code class="hljs">wget https://github.com/danielqsj/kafka_exporter/releases/download/v1.9.0/kafka_exporter-1.9.0.linux-amd64.tar.gz</code></pre><h1 id="2-安装kafka-exporter"><a href="#2-安装kafka-exporter" class="headerlink" title="2.安装kafka_exporter"></a>2.安装kafka_exporter</h1><pre><code class="hljs">tar -xzvf kafka_exporter-1.9.0.linux-amd64.tar.gzsudo cp kafka_exporter-1.9.0.linux-amd64/kafka_exporter /usr/local/bin</code></pre><h1 id="3-创建启动脚本"><a href="#3-创建启动脚本" class="headerlink" title="3.创建启动脚本"></a>3.创建启动脚本</h1><pre><code class="hljs">sudo vi /lib/systemd/system/kafka_exporter.service [Unit]Description=Kafka-ExporterAfter=network.target[Service]User=rootGroup=rootExecStart=/usr/local/bin/kafka_exporter --kafka.server=172.100.100.101:9092 Restart=always[Install]WantedBy=multi-user.target</code></pre><ul><li>注：其余kafka节点只需IP不同即可</li></ul><h1 id="4-启动kafka-exporter"><a href="#4-启动kafka-exporter" class="headerlink" title="4.启动kafka_exporter"></a>4.启动kafka_exporter</h1><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start kafka_exporter.servicesudo systemctl enable kafka_exporter.service</code></pre><h1 id="5-配置Prometheus"><a href="#5-配置Prometheus" class="headerlink" title="5.配置Prometheus"></a>5.配置Prometheus</h1><h2 id="5-1-配置监控实例"><a href="#5-1-配置监控实例" class="headerlink" title="5.1 配置监控实例"></a>5.1 配置监控实例</h2><pre><code class="hljs">sudo vi /usr/local/prometheus/prometheus.ymlglobal:  scrape_interval: 15s   evaluation_interval: 15s   scrape_timeout: 10s scrape_configs:  - job_name: &quot;prometheus&quot;    static_configs:      - targets: [&quot;localhost:9090&quot;]  - job_name: kafka    static_configs:      - targets:        - 172.100.100.101:9308        - 172.100.100.102:9308        - 172.100.100.103:9308        labels:          clusters: kafka</code></pre><h2 id="5-2-重载Prometheus"><a href="#5-2-重载Prometheus" class="headerlink" title="5.2 重载Prometheus"></a>5.2 重载Prometheus</h2><pre><code class="hljs">curl -XPOST http://127.0.0.1:9090/-/reload</code></pre><h1 id="6-Grafana导入监控模版"><a href="#6-Grafana导入监控模版" class="headerlink" title="6.Grafana导入监控模版"></a>6.Grafana导入监控模版</h1><p>Dashboards —&gt; New —&gt; Import —&gt; 模版ID：21078</p><p><img src="/img/wiki/prometheus/Kafka.jpg" alt="Kafka"></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/qq_18138507/article/details/142816739">https://blog.csdn.net/qq_18138507/article/details/142816739</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kafka</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>云原生</tag>
      
      <tag>Prometheus</tag>
      
      <tag>监控告警</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kafka集群运维命令详解</title>
    <link href="/linux/KafkaOps/"/>
    <url>/linux/KafkaOps/</url>
    
    <content type="html"><![CDATA[<h1 id="1-查看topic"><a href="#1-查看topic" class="headerlink" title="1.查看topic"></a>1.查看topic</h1><pre><code class="hljs">/usr/local/kafka/bin/kafka-topics.sh --list --bootstrap-server localhost:9092</code></pre><h1 id="2-创建topic"><a href="#2-创建topic" class="headerlink" title="2.创建topic"></a>2.创建topic</h1><pre><code class="hljs">/usr/local/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic test --partitions 3 --replication-factor 3</code></pre><h1 id="3-查看topic元数据信息"><a href="#3-查看topic元数据信息" class="headerlink" title="3.查看topic元数据信息"></a>3.查看topic元数据信息</h1><pre><code class="hljs">/usr/local/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic test</code></pre><h1 id="4-topic写入消息"><a href="#4-topic写入消息" class="headerlink" title="4.topic写入消息"></a>4.topic写入消息</h1><pre><code class="hljs">/usr/local/kafka/bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic test</code></pre><h1 id="5-查看topic消息数据"><a href="#5-查看topic消息数据" class="headerlink" title="5.查看topic消息数据"></a>5.查看topic消息数据</h1><pre><code class="hljs">/usr/local/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning --max-messages 50</code></pre><h1 id="6-删除topic"><a href="#6-删除topic" class="headerlink" title="6.删除topic"></a>6.删除topic</h1><pre><code class="hljs">/usr/local/kafka/bin/kafka-topics.sh --delete --topic test --bootstrap-server localhost:9092</code></pre>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kafka</tag>
      
      <tag>MQ</tag>
      
      <tag>中间件</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CloudStack集群配置高可用管理节点</title>
    <link href="/linux/CloudStackHA/"/>
    <url>/linux/CloudStackHA/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>虚拟化</tag>
      
      <tag>私有云</tag>
      
      <tag>公有云</tag>
      
      <tag>CloudStack</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PostgreSQL数据库配置文件详解</title>
    <link href="/linux/cmfercs030000pmpvcgojcg54/"/>
    <url>/linux/cmfercs030000pmpvcgojcg54/</url>
    
    <content type="html"><![CDATA[<p>PostgreSQL数据库通过3个配置文件控制整个数据库系统的管理与运行，即主配置文件postgresql.conf、访问策略配置文件pg_hba.conf和系统用户映射配置文件pg_ident.conf</p><h1 id="1-主配置文件"><a href="#1-主配置文件" class="headerlink" title="1.主配置文件"></a>1.主配置文件</h1><p>postgresql.conf，PostgreSQL数据库的主配置文件，定义了数据库服务器运行的各种参数，如性能参数、资源限制、日志设置、网络设置等，修改需重启PostgreSQL服务器或使用pg_ctl reload命令才能生效</p><p>General: 通用设置，如数据目录的位置、监听的端口等。<br>Connection and Authentication: 连接和认证相关的设置，如最大连接数、超时设置等。<br>Performance: 性能相关设置，如缓存大小、工作内存等。<br>Logging and Replication: 日志记录和复制相关的设置，如日志级别、归档模式等。<br>Security: 安全相关的设置，如密码复杂度要求、SSL 设置等</p><h1 id="2-访问策略配置文件"><a href="#2-访问策略配置文件" class="headerlink" title="2.访问策略配置文件"></a>2.访问策略配置文件</h1><p>pg_hba.conf，PostgreSQL数据库的网络访问策略配置文件，定义了允许访问数据库的客户端及其认证方式，如连接类型、目标数据库、用户、地址、认证方法等。该文件每行代表一个访问规则，且定义的规则按照顺序依次匹配，匹配到则直接应用于连接</p><h1 id="3-系统用户映射配置文件"><a href="#3-系统用户映射配置文件" class="headerlink" title="3.系统用户映射配置文件"></a>3.系统用户映射配置文件</h1><p>pg_ident.conf，PostgreSQL数据库的操作系统用户映射到数据库用户的配置文件，即映射名称、系统用户名和数据库用户名。该文件同样每行定义一个映射规则，若不是特别复杂的映射股则，建议忽略此文件，使用默认行为即可</p><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://cloud.tencent.com/developer/article/2449739">https://cloud.tencent.com/developer/article/2449739</a></li><li><a href="https://blog.csdn.net/mingongge/article/details/131485525">https://blog.csdn.net/mingongge/article/details/131485525</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>SQL</tag>
      
      <tag>数据库</tag>
      
      <tag>PostgreSQL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux系统基于PandaWiki构建AI知识库</title>
    <link href="/linux/AI-Wiki/"/>
    <url>/linux/AI-Wiki/</url>
    
    <content type="html"><![CDATA[<p>PandaWiki，AI大模型驱动的国内开源知识库系统，用于快速地构建产品文档、技术文档、FAQ和博客平台，并由AI整合辅助写作、智能问答和语义搜索等功能。PandaWiki部署简单，支持多种渠道和文件格式内容的导入、编辑与导出，具备很强的易用性。此外，还支持钉钉、飞书等第三方集成及API调用，能满足大多数企业内部和个人知识库的管理需求</p><h1 id="1-配置yum源"><a href="#1-配置yum源" class="headerlink" title="1.配置yum源"></a>1.配置yum源</h1><h1 id="2-安装Docker"><a href="#2-安装Docker" class="headerlink" title="2.安装Docker"></a>2.安装Docker</h1><pre><code class="hljs">curl -fsSL https://get.docker.com -o get-docker.sh &amp;&amp; sh get-docker.sh</code></pre><h1 id="3-部署PandaWiki"><a href="#3-部署PandaWiki" class="headerlink" title="3.部署PandaWiki"></a>3.部署PandaWiki</h1><pre><code class="hljs">mkdir -p /web/wiki &amp;&amp; cd /web/wikibash -c &quot;$(curl -fsSLk https://release.baizhi.cloud/panda-wiki/manager.sh)&quot;</code></pre><h1 id="4-配置AI大模型"><a href="#4-配置AI大模型" class="headerlink" title="4.配置AI大模型"></a>4.配置AI大模型</h1><h1 id="5-上传知识库文件"><a href="#5-上传知识库文件" class="headerlink" title="5.上传知识库文件"></a>5.上传知识库文件</h1><h1 id="6-验证知识库问答"><a href="#6-验证知识库问答" class="headerlink" title="6.验证知识库问答"></a>6.验证知识库问答</h1><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Docker</tag>
      
      <tag>容器</tag>
      
      <tag>AI</tag>
      
      <tag>大模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux系统开启回收站功能</title>
    <link href="/linux/Trash/"/>
    <url>/linux/Trash/</url>
    
    <content type="html"><![CDATA[<h1 id="1-安装trash"><a href="#1-安装trash" class="headerlink" title="1.安装trash"></a>1.安装trash</h1><pre><code class="hljs">sudo apt install -y trash-clisudo mkdir -p /home/backup/.trashsudo chmod 777 -R /home/backup/.trash</code></pre><h1 id="2-配置回收站"><a href="#2-配置回收站" class="headerlink" title="2.配置回收站"></a>2.配置回收站</h1><h2 id="2-1-创建回收站脚本"><a href="#2-1-创建回收站脚本" class="headerlink" title="2.1 创建回收站脚本"></a>2.1 创建回收站脚本</h2><pre><code class="hljs">sudo vi /usr/local/rm#!/bin/bashif ! command -v trash-put &amp;&gt; /dev/null; then  echo &quot;Error: &#39;trash-put&#39; command not found. Please install trash-cli.&quot;  exit 1fi# 设置回收站目录env XDG_DATA_HOME=&quot;/home/backup/.trash&quot; trash-put &quot;$@&quot;</code></pre><h2 id="2-2-回收站命令赋执行权限"><a href="#2-2-回收站命令赋执行权限" class="headerlink" title="2.2 回收站命令赋执行权限"></a>2.2 回收站命令赋执行权限</h2><pre><code class="hljs">sudo chmod +x /usr/local/rm</code></pre><h2 id="2-3-备份原rm命令"><a href="#2-3-备份原rm命令" class="headerlink" title="2.3 备份原rm命令"></a>2.3 备份原rm命令</h2><pre><code class="hljs">sudo mv /bin/rm /usr/bin/rm-real</code></pre><h1 id="3-验证回收站功能"><a href="#3-验证回收站功能" class="headerlink" title="3.验证回收站功能"></a>3.验证回收站功能</h1><h2 id="3-1-删除文件，验证回收站功能"><a href="#3-1-删除文件，验证回收站功能" class="headerlink" title="3.1 删除文件，验证回收站功能"></a>3.1 删除文件，验证回收站功能</h2><pre><code class="hljs">sudo rm -rf ~test/*trash-listll /home/backup/.trash/Trash/files</code></pre><h2 id="3-2-还原回收站文件"><a href="#3-2-还原回收站文件" class="headerlink" title="3.2 还原回收站文件"></a>3.2 还原回收站文件</h2><pre><code class="hljs">sudo su -trash-restore</code></pre><h2 id="3-3-清空回收站"><a href="#3-3-清空回收站" class="headerlink" title="3.3 清空回收站"></a>3.3 清空回收站</h2><pre><code class="hljs">trash-empty</code></pre><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/zfw_666666/article/details/124670544">https://blog.csdn.net/zfw_666666/article/details/124670544</a></li><li><a href="https://blog.csdn.net/qq_74397635/article/details/151857733">https://blog.csdn.net/qq_74397635/article/details/151857733</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>存储</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CloudStack集群部署</title>
    <link href="/linux/CloudStack/"/>
    <url>/linux/CloudStack/</url>
    
    <content type="html"><![CDATA[<p>CloudStack，基于Java的开源云计算管理平台，作为IaaS（基础设施即服务）管理和部署大规模的虚拟化环境，支持市面上大部分主流的hypervisors，如KVM、XenServer、VMware、OracleVM和Xen等，以其简洁的架构、良好的扩展性广泛应用于私有云、公共云和混合云场景，如韩国电信、英国天空广播、阿尔卡特-朗讯等企业。相比于OpenStack，CloudStack架构与配置都简单得多，安装、配置与维护十分友好，特别适用于小型的私有化项目。但其生态发展远不如OpenStack，商业支持较少也导致创新动力不足，开发进度缓慢</p><h1 id="体系架构"><a href="#体系架构" class="headerlink" title="体系架构"></a>体系架构</h1><h1 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h1><ul><li>172.100.100.160 manager</li><li>172.100.100.161 cloud01</li><li>172.100.100.162 cloud02</li></ul><h1 id="1-配置系统环境"><a href="#1-配置系统环境" class="headerlink" title="1.配置系统环境"></a>1.配置系统环境</h1><h2 id="1-1-关闭防火墙"><a href="#1-1-关闭防火墙" class="headerlink" title="1.1 关闭防火墙"></a>1.1 关闭防火墙</h2><h2 id="1-2-关闭Selinux"><a href="#1-2-关闭Selinux" class="headerlink" title="1.2 关闭Selinux"></a>1.2 关闭Selinux</h2><h2 id="1-3-配置时间同步"><a href="#1-3-配置时间同步" class="headerlink" title="1.3 配置时间同步"></a>1.3 配置时间同步</h2><h1 id="2-配置yum仓库"><a href="#2-配置yum仓库" class="headerlink" title="2.配置yum仓库"></a>2.配置yum仓库</h1><pre><code class="hljs">vi /etc/yum.repos.d/CloudStack.repo [cloudstack]name=cloudstackbaseurl=http://download.cloudstack.org/centos/$releasever/4.18/enabled=1gpgcheck=0</code></pre><h2 id="3-部署NFS"><a href="#3-部署NFS" class="headerlink" title="3.部署NFS"></a>3.部署NFS</h2><h1 id="4-部署管理节点"><a href="#4-部署管理节点" class="headerlink" title="4.部署管理节点"></a>4.部署管理节点</h1><h2 id="4-1-部署MySQL数据库"><a href="#4-1-部署MySQL数据库" class="headerlink" title="4.1 部署MySQL数据库"></a>4.1 部署MySQL数据库</h2><h2 id="4-2-安装管理服务"><a href="#4-2-安装管理服务" class="headerlink" title="4.2 安装管理服务"></a>4.2 安装管理服务</h2><pre><code class="hljs">yum update -yyum install -y epel-release cloudstack-management</code></pre><h2 id="4-3-初始化数据库"><a href="#4-3-初始化数据库" class="headerlink" title="4.3 初始化数据库"></a>4.3 初始化数据库</h2><pre><code class="hljs">cloudstack-setup-databases cloud:CloudStack_2025@localhost --deploy-as=root:CloudStack_2025</code></pre><h2 id="4-4-启动管理服务"><a href="#4-4-启动管理服务" class="headerlink" title="4.4 启动管理服务"></a>4.4 启动管理服务</h2><pre><code class="hljs">cloudstack-setup-management</code></pre><h1 id="5-部署计算节点"><a href="#5-部署计算节点" class="headerlink" title="5.部署计算节点"></a>5.部署计算节点</h1><h2 id="5-1-安装计算服务"><a href="#5-1-安装计算服务" class="headerlink" title="5.1 安装计算服务"></a>5.1 安装计算服务</h2><pre><code class="hljs">yum install -y cloudstack-agent</code></pre><h2 id="5-2-配置桥接网卡"><a href="#5-2-配置桥接网卡" class="headerlink" title="5.2 配置桥接网卡"></a>5.2 配置桥接网卡</h2><h2 id="5-3-配置libvirt服务"><a href="#5-3-配置libvirt服务" class="headerlink" title="5.3 配置libvirt服务"></a>5.3 配置libvirt服务</h2><h3 id="5-3-1-配置vnc默认监听地址"><a href="#5-3-1-配置vnc默认监听地址" class="headerlink" title="5.3.1 配置vnc默认监听地址"></a>5.3.1 配置vnc默认监听地址</h3><pre><code class="hljs">sed -i &#39;s/^#vnc_listen =.*/vnc_listen = &quot;0.0.0.0&quot;/g&#39; /etc/libvirt/qemu.conf</code></pre><h3 id="5-3-2-配置管理节点IP"><a href="#5-3-2-配置管理节点IP" class="headerlink" title="5.3.2 配置管理节点IP"></a>5.3.2 配置管理节点IP</h3><pre><code class="hljs">sed -i &quot;s@host=.*@host=172.16.100.160@g&quot; /etc/cloudstack/agent/agent.properties</code></pre><h3 id="5-3-3-重启libvirt"><a href="#5-3-3-重启libvirt" class="headerlink" title="5.3.3 重启libvirt"></a>5.3.3 重启libvirt</h3><pre><code class="hljs">systemctl restart libvirtd</code></pre><h1 id="6-验证CloudStack"><a href="#6-验证CloudStack" class="headerlink" title="6.验证CloudStack"></a>6.验证CloudStack</h1><p><img src="/img/wiki/CloudStack/CloudStack.jpg" alt="CloudStack"></p><ul><li>注：默认账号密码为admin&#x2F;password</li></ul><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.cnblogs.com/xull0651/p/15884218.html">https://www.cnblogs.com/xull0651/p/15884218.html</a></li><li><a href="https://blog.csdn.net/u012124304/article/details/80960504">https://blog.csdn.net/u012124304/article/details/80960504</a></li><li><a href="https://blog.csdn.net/weixin_51065082/article/details/132639500">https://blog.csdn.net/weixin_51065082/article/details/132639500</a></li><li><a href="https://docs.cloudstack.apache.org/en/4.18.0.0/installguide/management-server/index.html">https://docs.cloudstack.apache.org/en/4.18.0.0/installguide/management-server/index.html</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>虚拟化</tag>
      
      <tag>私有云</tag>
      
      <tag>公有云</tag>
      
      <tag>CloudStack</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes集群Event持久化存储与告警</title>
    <link href="/linux/KubernetesEvent/"/>
    <url>/linux/KubernetesEvent/</url>
    
    <content type="html"><![CDATA[<p>Events，即事件，用于记录Kubernetes集群中资源对象的状态变化，如Node的上线与下线、Deployment&#x2F;Pod&#x2F;PV的创建与销毁等，由集群组件API-Server生成并写入Etcd集群。Kubernetes集群的事件记录了集群各种操作的时间、类型、对象、原因以及描述等信息，反映了集群的实时和历史状态及行为轨迹，可作为调试故障、监控性能的核心依据。但由于事件频繁生成且数据量较大，Kubernetes集群基于Etcd压力的考量，默认情况下只保留1个小时的事件信息，很多时候来不及进行排查。因为，事件的持久化存储应运而生</p><p>Kubernetes集群的事件分为两种，即Warning事件和Normal事件，前者由非预期操作产生，也即异常或错误事件，如Pod调度失败、容器崩溃或重启及Node节点异常等</p><h1 id="1-下载kube-eventer"><a href="#1-下载kube-eventer" class="headerlink" title="1.下载kube-eventer"></a>1.下载kube-eventer</h1><pre><code class="hljs">git clone https://github.com/AliyunContainerService/kube-eventer.git </code></pre><h1 id="2-配置钉钉告警"><a href="#2-配置钉钉告警" class="headerlink" title="2.配置钉钉告警"></a>2.配置钉钉告警</h1><h2 id="2-1-创建钉钉集群机器人"><a href="#2-1-创建钉钉集群机器人" class="headerlink" title="2.1 创建钉钉集群机器人"></a>2.1 创建钉钉集群机器人</h2><h2 id="2-2-配置事件告警"><a href="#2-2-配置事件告警" class="headerlink" title="2.2 配置事件告警"></a>2.2 配置事件告警</h2><pre><code class="hljs">cd kube-eventer/deployvi deploy.yaml- --sink=dingtalk:https://oapi.dingtalk.com/robot/send?access_token=xxxxxxxxxxx&amp;level=Warning&amp;sign=xxxxxxxxxxx</code></pre><ul><li>注：level，需要收集的事件的级别；sign，钉钉群机器人的加签密钥</li></ul><h2 id="2-3-部署kube-eventer"><a href="#2-3-部署kube-eventer" class="headerlink" title="2.3 部署kube-eventer"></a>2.3 部署kube-eventer</h2><pre><code class="hljs">kubectl apply -f deploy.yaml</code></pre><h2 id="2-4-验证事件告警"><a href="#2-4-验证事件告警" class="headerlink" title="2.4 验证事件告警"></a>2.4 验证事件告警</h2><p><img src="/img/wiki/kubernetes/EventsAlert.jpg" alt="EventsAlert"></p><h1 id="3-配置事件持久化到MySQL数据库"><a href="#3-配置事件持久化到MySQL数据库" class="headerlink" title="3.配置事件持久化到MySQL数据库"></a>3.配置事件持久化到MySQL数据库</h1><h2 id="3-1-安装MySQL数据库"><a href="#3-1-安装MySQL数据库" class="headerlink" title="3.1 安装MySQL数据库"></a>3.1 安装MySQL数据库</h2><h2 id="3-2-创建事件存储数据库"><a href="#3-2-创建事件存储数据库" class="headerlink" title="3.2 创建事件存储数据库"></a>3.2 创建事件存储数据库</h2><pre><code class="hljs">MariaDB [(none)]&gt; create table k8s_event;MariaDB [(none)]&gt; use k8s_event;MariaDB [(none)]&gt; create table k8s_event (   id bigint(20)           not null auto_increment primary key comment &#39;event primary key&#39;,  name                    varchar(64)  not null default &#39;&#39; comment &#39;event name&#39;,  namespace               varchar(64)  not null default &#39;&#39; comment &#39;event namespace&#39;,  event_id                varchar(64)  not null default &#39;&#39; comment &#39;event_id&#39;,  type                    varchar(64)  not null default &#39;&#39; comment &#39;event type Warning or Normal&#39;,     reason                  varchar(64)  not null default &#39;&#39; comment &#39;event reason&#39;,  message                 text  not null  comment &#39;event message&#39; ,  kind                    varchar(64)  not null default &#39;&#39; comment &#39;event kind&#39; ,  first_occurrence_time   varchar(64)    not null default &#39;&#39; comment &#39;event first occurrence time&#39;,     last_occurrence_time    varchar(64)    not null default &#39;&#39; comment &#39;event last occurrence time&#39; )  ENGINE = InnoDB default CHARSET = utf8 comment =&#39;Event info tables&#39;;MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON kube_eventer.* TO &#39;kube_eventer&#39;@&#39;172.16.100.%&#39; IDENTIFIED BY &#39;kube_eventer&#39;;MariaDB [(none)]&gt; FLUSH PRIVILEGES;</code></pre><h2 id="3-3-配置事件持久化"><a href="#3-3-配置事件持久化" class="headerlink" title="3.3 配置事件持久化"></a>3.3 配置事件持久化</h2><pre><code class="hljs">cd kube-eventer/deployvi deploy.yaml- --sink=mysql:?kube_eventer:kube_eventer@tcp(172.16.100.200:3306)/kube_eventer?charset=utf8&amp;table=k8s_event&amp;level=Warning</code></pre><h2 id="3-4-部署kube-eventer"><a href="#3-4-部署kube-eventer" class="headerlink" title="3.4 部署kube-eventer"></a>3.4 部署kube-eventer</h2><pre><code class="hljs">kubectl apply -f deploy.yaml</code></pre><h2 id="3-5-配置Grafana，验证数据持久化"><a href="#3-5-配置Grafana，验证数据持久化" class="headerlink" title="3.5 配置Grafana，验证数据持久化"></a>3.5 配置Grafana，验证数据持久化</h2><p><img src="/img/wiki/kubernetes/EventsMySQL.jpg" alt="EventsMySQL"></p><p>注：计公式为：SELECT first_occurrence_time,name,namespace,reason,message FROM kube_eventer.k8s_event WHERE type &#x3D; ‘Warning’ LIMIT 50</p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.modb.pro/db/1777517834276507648">https://www.modb.pro/db/1777517834276507648</a></li><li><a href="https://www.cnblogs.com/dai-zhe/p/14735893.html">https://www.cnblogs.com/dai-zhe/p/14735893.html</a></li><li><a href="https://github.com/AliyunContainerService/kube-eventer">https://github.com/AliyunContainerService/kube-eventer</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux系统配置命令行终端美化工具OhMyZsh</title>
    <link href="/geek/OhMyZsh/"/>
    <url>/geek/OhMyZsh/</url>
    
    <content type="html"><![CDATA[<p>OhMyZsh，基于ZShell的开源命令行终端管理工具，内置了丰富、酷炫的终端主题和插件，弥补了Linux默认终端不够美观这一缺陷</p><h1 id="1-安装zsh"><a href="#1-安装zsh" class="headerlink" title="1.安装zsh"></a>1.安装zsh</h1><pre><code class="hljs"># MacOSbrew install zshsudo apt -y install zsh</code></pre><h1 id="2-切换设置默认终端"><a href="#2-切换设置默认终端" class="headerlink" title="2.切换设置默认终端"></a>2.切换设置默认终端</h1><pre><code class="hljs"># 查看当前Shellecho $SHELL# 切换为Zshellchsh -s /bin/zsh</code></pre><h1 id="3-安装oh-my-zsh"><a href="#3-安装oh-my-zsh" class="headerlink" title="3.安装oh-my-zsh"></a>3.安装oh-my-zsh</h1><h2 id="3-1-安装Git"><a href="#3-1-安装Git" class="headerlink" title="3.1 安装Git"></a>3.1 安装Git</h2><pre><code class="hljs"># MacOSbrew install gitsudo apt -y install git</code></pre><h2 id="3-2-安装oh-my-zsh"><a href="#3-2-安装oh-my-zsh" class="headerlink" title="3.2 安装oh-my-zsh"></a>3.2 安装oh-my-zsh</h2><pre><code class="hljs">sh -c &quot;$(curl -fsSL https://gitee.com/shmhlsy/oh-my-zsh-install.sh/raw/master/install.sh)&quot;</code></pre><h1 id="4-配置zsh"><a href="#4-配置zsh" class="headerlink" title="4.配置zsh"></a>4.配置zsh</h1><h2 id="4-1-配置主题"><a href="#4-1-配置主题" class="headerlink" title="4.1 配置主题"></a>4.1 配置主题</h2><p>oh-my-zsh内置了大量的主题以供使用，可将主题设为random，即每次登录随机分配</p><pre><code class="hljs">cd ~ &amp;&amp; vi .zshrcZSH_THEME=&quot;random&quot;ZSH_THEME=&quot;&quot;</code></pre><h2 id="4-2-配置字体"><a href="#4-2-配置字体" class="headerlink" title="4.2 配置字体"></a>4.2 配置字体</h2><h2 id="4-3-配置代码高亮"><a href="#4-3-配置代码高亮" class="headerlink" title="4.3 配置代码高亮"></a>4.3 配置代码高亮</h2><pre><code class="hljs">git clone https://github.com/zsh-users/zsh-syntax-highlighting.git $&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;/plugins/zsh-syntax-highlightingecho &#39;plugins=(zsh-syntax-highlighting zsh-autosuggestions)&#39; &gt;&gt;~/.zshrc</code></pre><h1 id="5-生效配置"><a href="#5-生效配置" class="headerlink" title="5.生效配置"></a>5.生效配置</h1><pre><code class="hljs">source .zshrc</code></pre><h1 id="6-验证Oh-My-Zsh"><a href="#6-验证Oh-My-Zsh" class="headerlink" title="6.验证Oh-My-Zsh"></a>6.验证Oh-My-Zsh</h1><p><img src="/img/wiki/linux/Oh-My-Zsh.jpg" alt="Oh-My-Zsh"></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.cnblogs.com/little-lunatic/p/17447723.html">https://www.cnblogs.com/little-lunatic/p/17447723.html</a></li><li><a href="https://blog.csdn.net/Xiang__Qian/article/details/139871268">https://blog.csdn.net/Xiang__Qian/article/details/139871268</a></li><li><a href="https://blog.csdn.net/weixin_46106427/article/details/147518326">https://blog.csdn.net/weixin_46106427/article/details/147518326</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>极客</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>极客</tag>
      
      <tag>Git</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ansible自动化运维工具生成Linux系统巡检报告</title>
    <link href="/linux/AnsibleReport/"/>
    <url>/linux/AnsibleReport/</url>
    
    <content type="html"><![CDATA[<p>AnsibleHealthCheck，基于Ansible的自动化系统巡检工具，通过Jinja2模板将收集到的服务器关键信息，如操作系统、CPU、内存、磁盘、网络连接等，渲染成详细的HTML巡检报告。此外，巡检报告还支持通过邮件发送，以便相关负责人实时了解整个系统的运行状况</p><h1 id="1-安装Ansible、Git"><a href="#1-安装Ansible、Git" class="headerlink" title="1.安装Ansible、Git"></a>1.安装Ansible、Git</h1><h1 id="2-下载安装包"><a href="#2-下载安装包" class="headerlink" title="2.下载安装包"></a>2.下载安装包</h1><pre><code class="hljs">cd /etc/ansiblegit clone https://github.com/liushiju/ansible-HealthCheck.gitmv ansible-HealthCheck HealthCheck &amp;&amp; cd HealthCheck</code></pre><h1 id="3-配置巡检组件"><a href="#3-配置巡检组件" class="headerlink" title="3.配置巡检组件"></a>3.配置巡检组件</h1><h2 id="3-1-配置过滤器插件"><a href="#3-1-配置过滤器插件" class="headerlink" title="3.1 配置过滤器插件"></a>3.1 配置过滤器插件</h2><pre><code class="hljs">vi ansible.cfgfilter_plugins     = /etc/ansible/HealthCheck/filter_plugins</code></pre><h2 id="3-2-配置资源清单"><a href="#3-2-配置资源清单" class="headerlink" title="3.2 配置资源清单"></a>3.2 配置资源清单</h2><pre><code class="hljs">vi hosts[all]opsmasternode01node02ceph01ceph02ceph03</code></pre><h2 id="3-3-配置邮箱"><a href="#3-3-配置邮箱" class="headerlink" title="3.3 配置邮箱"></a>3.3 配置邮箱</h2><pre><code class="hljs">cat os-check/defaults/main.yaml ---check_day: &quot;&#123;&#123; '%Y-%m-%d' | strftime &#125;&#125;&quot;check_report_path: /tmpcheck_report_file_suffix: &quot;-&#123;&#123; check_day &#125;&#125;&quot;check_mail_host: &quot;&quot;check_mail_port: &quot;&quot;check_mail_username: &quot;&quot;check_mail_password: &quot;&quot;check_mail_to: []check_mail_subject: &quot;System Check Report [&#123;&#123; check_day &#125;&#125;]&quot;</code></pre><h2 id="4-执行巡检任务"><a href="#4-执行巡检任务" class="headerlink" title="4.执行巡检任务"></a>4.执行巡检任务</h2><pre><code class="hljs">ansible-playbook -i hosts roles/os-check.yaml</code></pre><h2 id="5-配置定时任务"><a href="#5-配置定时任务" class="headerlink" title="5.配置定时任务"></a>5.配置定时任务</h2><pre><code class="hljs">crontab -l30 10 * * * ansible-playbook -i /etc/ansible/HealthCheck/hosts /etc/ansible/HealthCheck/roles/os-check.yaml </code></pre><h2 id="6-验证巡检报告"><a href="#6-验证巡检报告" class="headerlink" title="6.验证巡检报告"></a>6.验证巡检报告</h2><p><img src="/img/wiki/Ansible/HealthCheck.jpg" alt="HealthCheck"></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://github.com/liushiju/ansible-HealthCheck">https://github.com/liushiju/ansible-HealthCheck</a></li><li><a href="https://mp.weixin.qq.com/s/l-QXHTBwZURWzUQmOwmn7Q">https://mp.weixin.qq.com/s/l-QXHTBwZURWzUQmOwmn7Q</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Ansible</tag>
      
      <tag>自动化运维</tag>
      
      <tag>云计算</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>KVM虚拟机基于Ceph存储系统实现热迁移</title>
    <link href="/linux/KVM-Ceph/"/>
    <url>/linux/KVM-Ceph/</url>
    
    <content type="html"><![CDATA[<h1 id="1-Ceph集群创建KVM存储池"><a href="#1-Ceph集群创建KVM存储池" class="headerlink" title="1.Ceph集群创建KVM存储池"></a>1.Ceph集群创建KVM存储池</h1><pre><code class="hljs">ceph osd pool create kvm 16 16rbd pool init kvm</code></pre><h1 id="2-Ceph集群创建KVM用户"><a href="#2-Ceph集群创建KVM用户" class="headerlink" title="2.Ceph集群创建KVM用户"></a>2.Ceph集群创建KVM用户</h1><pre><code class="hljs">ceph auth get-or-create client.kvm mon &#39;profile rbd&#39; osd &#39;profile rbd pool=kvm&#39; -o /etc/ceph/ceph.client.kvm.keyringcat /etc/ceph/ceph.client.kvm.keyring[client.kvm]    key = AQAsf39ovuHNIxAAAQ8JnmPv3qZTCPmPiLadkg==</code></pre><h1 id="3-源宿主机配置"><a href="#3-源宿主机配置" class="headerlink" title="3.源宿主机配置"></a>3.源宿主机配置</h1><h2 id="3-1-创建Ceph集群KVM用户密钥"><a href="#3-1-创建Ceph集群KVM用户密钥" class="headerlink" title="3.1 创建Ceph集群KVM用户密钥"></a>3.1 创建Ceph集群KVM用户密钥</h2><h3 id="3-1-1-创建配置文件"><a href="#3-1-1-创建配置文件" class="headerlink" title="3.1.1 创建配置文件"></a>3.1.1 创建配置文件</h3><pre><code class="hljs">vi secret.xml&lt;secret ephemeral=&#39;no&#39; private=&#39;no&#39;&gt;  &lt;usage type=&#39;ceph&#39;&gt;    &lt;name&gt;client.kvm secret&lt;/name&gt;  &lt;/usage&gt;&lt;/secret&gt;</code></pre><h3 id="3-1-2-创建KVM加密密钥"><a href="#3-1-2-创建KVM加密密钥" class="headerlink" title="3.1.2 创建KVM加密密钥"></a>3.1.2 创建KVM加密密钥</h3><pre><code class="hljs">virsh secret-define --file secret.xml Secret 2f0003b4-d4d6-4b86-a654-2e849edbc6e8 created</code></pre><h3 id="3-1-3-创建KVM用户认证密钥"><a href="#3-1-3-创建KVM用户认证密钥" class="headerlink" title="3.1.3 创建KVM用户认证密钥"></a>3.1.3 创建KVM用户认证密钥</h3><pre><code class="hljs">virsh secret-set-value --secret 2f0003b4-d4d6-4b86-a654-2e849edbc6e8 --base64 AQAsf39ovuHNIxAAAQ8JnmPv3qZTCPmPiLadkg==</code></pre><h2 id="3-2-虚拟机磁盘文件导入Ceph"><a href="#3-2-虚拟机磁盘文件导入Ceph" class="headerlink" title="3.2 虚拟机磁盘文件导入Ceph"></a>3.2 虚拟机磁盘文件导入Ceph</h2><pre><code class="hljs">qemu-img convert -f qcow2 /home/kvm/ops.qcow2 -O raw rbd:kvm/ops.img</code></pre><h1 id="4-目标宿主机配置"><a href="#4-目标宿主机配置" class="headerlink" title="4.目标宿主机配置"></a>4.目标宿主机配置</h1><h2 id="4-1-导入源宿主机加密密钥"><a href="#4-1-导入源宿主机加密密钥" class="headerlink" title="4.1 导入源宿主机加密密钥"></a>4.1 导入源宿主机加密密钥</h2><h3 id="4-1-1-源宿主机导出加密密钥"><a href="#4-1-1-源宿主机导出加密密钥" class="headerlink" title="4.1.1 源宿主机导出加密密钥"></a>4.1.1 源宿主机导出加密密钥</h3><pre><code class="hljs">virsh secret-dumpxml 2f0003b4-d4d6-4b86-a654-2e849edbc6e8 &gt; secret-source.xml</code></pre><h3 id="4-1-2-目标宿主机导入加密密钥"><a href="#4-1-2-目标宿主机导入加密密钥" class="headerlink" title="4.1.2 目标宿主机导入加密密钥"></a>4.1.2 目标宿主机导入加密密钥</h3><pre><code class="hljs">virsh secret-define --file secret-source.xml</code></pre><h2 id="4-2-创建KVM用户认证密钥"><a href="#4-2-创建KVM用户认证密钥" class="headerlink" title="4.2 创建KVM用户认证密钥"></a>4.2 创建KVM用户认证密钥</h2><pre><code class="hljs">virsh secret-set-value --secret 2f0003b4-d4d6-4b86-a654-2e849edbc6e8 --base64 AQAsf39ovuHNIxAAAQ8JnmPv3qZTCPmPiLadkg==</code></pre><h1 id="5-迁移虚拟机"><a href="#5-迁移虚拟机" class="headerlink" title="5.迁移虚拟机"></a>5.迁移虚拟机</h1><pre><code class="hljs">virsh migrate --live --verbose ops qemu+ssh://192.168.100.200/system tcp://192.168.100.200 --unsafe --persistent</code></pre><h1 id="6-验证虚拟机迁移"><a href="#6-验证虚拟机迁移" class="headerlink" title="6.验证虚拟机迁移"></a>6.验证虚拟机迁移</h1><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.cnblogs.com/yinzhengjie/p/18134411">https://www.cnblogs.com/yinzhengjie/p/18134411</a></li><li><a href="https://blog.csdn.net/2501_91350469/article/details/148712782">https://blog.csdn.net/2501_91350469/article/details/148712782</a></li><li><a href="https://blog.csdn.net/weixin_44953658/article/details/140626237">https://blog.csdn.net/weixin_44953658/article/details/140626237</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Ceph</tag>
      
      <tag>存储</tag>
      
      <tag>KVM</tag>
      
      <tag>虚拟化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prometheus监控配置安全认证</title>
    <link href="/linux/PrometheusAuth/"/>
    <url>/linux/PrometheusAuth/</url>
    
    <content type="html"><![CDATA[<p>Prometheus监控系统默认没有配置安全加密机制，这可能会导致监控信息遭到泄漏，特别是对安全性要求较高的业务场景，很容易在合规检查中被扫描到安全漏洞。有鉴于此，Prometheus于2.24版本引入Basic Auth加密机制，以保障监控信息的安全性与完整性</p><h1 id="1-安装htpasswd"><a href="#1-安装htpasswd" class="headerlink" title="1.安装htpasswd"></a>1.安装htpasswd</h1><pre><code class="hljs">sudo yum -y install httpd-toolssudo apt -y install apache2-utils</code></pre><h1 id="2-创建身份认证文件"><a href="#2-创建身份认证文件" class="headerlink" title="2.创建身份认证文件"></a>2.创建身份认证文件</h1><h2 id="2-1-创建认证用户及密码"><a href="#2-1-创建认证用户及密码" class="headerlink" title="2.1 创建认证用户及密码"></a>2.1 创建认证用户及密码</h2><pre><code class="hljs">sudo htpasswd -nbB -C 10 prometheus Prometheus@2025prometheus:$2y$10$d70aiv0cdRPRSe8XYJt4fekhAnsNYtKfHyJKB8yy3gg2YzG80oNHW</code></pre><h2 id="2-2-创建认证文件"><a href="#2-2-创建认证文件" class="headerlink" title="2.2 创建认证文件"></a>2.2 创建认证文件</h2><pre><code class="hljs">sudo vi /usr/local/prometheus/auth.ymlbasic_auth_users:  prometheus: $2y$10$d70aiv0cdRPRSe8XYJt4fekhAnsNYtKfHyJKB8yy3gg2YzG80oNHW</code></pre><h1 id="3-配置Prometheus安全认证"><a href="#3-配置Prometheus安全认证" class="headerlink" title="3.配置Prometheus安全认证"></a>3.配置Prometheus安全认证</h1><h2 id="3-1-配置启动脚本"><a href="#3-1-配置启动脚本" class="headerlink" title="3.1 配置启动脚本"></a>3.1 配置启动脚本</h2><pre><code class="hljs">sudo vi /lib/systemd/system/prometheus.serviceExecStart=/usr/local/prometheus/prometheus --config.file=/usr/local/prometheus/prometheus.yml --web.listen-address=0.0.0.0:9090 --web.enable-lifecycle --web.config.file=/usr/local/prometheus/auth.yml</code></pre><h2 id="3-2-重启Prometheus"><a href="#3-2-重启Prometheus" class="headerlink" title="3.2 重启Prometheus"></a>3.2 重启Prometheus</h2><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl restart prometheus.service</code></pre><h1 id="4-配置Node-exporter安全认证"><a href="#4-配置Node-exporter安全认证" class="headerlink" title="4.配置Node_exporter安全认证"></a>4.配置Node_exporter安全认证</h1><h2 id="4-1-配置启动脚本"><a href="#4-1-配置启动脚本" class="headerlink" title="4.1 配置启动脚本"></a>4.1 配置启动脚本</h2><pre><code class="hljs">sudo vi /lib/systemd/system/node_exporter.serviceExecStart=/usr/local/bin/node_exporter --web.config.file=/usr/local/prometheus/auth.yml</code></pre><h2 id="4-2-重启Node-exporter"><a href="#4-2-重启Node-exporter" class="headerlink" title="4.2 重启Node_exporter"></a>4.2 重启Node_exporter</h2><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl restart node_exporter.service</code></pre><h2 id="4-3-配置Prometheus"><a href="#4-3-配置Prometheus" class="headerlink" title="4.3 配置Prometheus"></a>4.3 配置Prometheus</h2><h3 id="4-3-1-配置监控Job"><a href="#4-3-1-配置监控Job" class="headerlink" title="4.3.1 配置监控Job"></a>4.3.1 配置监控Job</h3><pre><code class="hljs">sudo vi /usr/local/prometheus/prometheus.yml scrape_configs:  - job_name: node    basic_auth:      username: prometheus      password: Prometheus@2025    file_sd_configs:    - files:      - /usr/local/prometheus/configs/*.yaml      refresh_interval: 2m</code></pre><h3 id="4-3-2-重载Prometheus"><a href="#4-3-2-重载Prometheus" class="headerlink" title="4.3.2 重载Prometheus"></a>4.3.2 重载Prometheus</h3><pre><code class="hljs">curl -XPOST http://127.0.0.1:9090/-/reload</code></pre><h1 id="5-配置Grafana"><a href="#5-配置Grafana" class="headerlink" title="5.配置Grafana"></a>5.配置Grafana</h1><p><img src="/img/wiki/prometheus/auth.jpg" alt="auth"></p><h1 id="6-验证认证机制"><a href="#6-验证认证机制" class="headerlink" title="6.验证认证机制"></a>6.验证认证机制</h1><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/gjjumin/article/details/143021107">https://blog.csdn.net/gjjumin/article/details/143021107</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>云原生</tag>
      
      <tag>Prometheus</tag>
      
      <tag>监控告警</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux系统部署CodeServer搭建云端开发环境</title>
    <link href="/linux/geek/CodeServer/"/>
    <url>/linux/geek/CodeServer/</url>
    
    <content type="html"><![CDATA[<p>code-server，开源的网页版VS Code编辑器，可部署于服务器以构建云端开发环境，使得开发者能够通过任何设备访问完整的开发环境</p><h1 id="1-安装CodeServer"><a href="#1-安装CodeServer" class="headerlink" title="1.安装CodeServer"></a>1.安装CodeServer</h1><pre><code class="hljs">sudo docker run -it -d --name code-server -p 8080:8080 \-v /root/.local:/root/.local -v /root/.config:/root/.config \-v /web:/home/coder/project -e DOCKER_USER=$USER code-server</code></pre><h1 id="2-查看登录密码"><a href="#2-查看登录密码" class="headerlink" title="2.查看登录密码"></a>2.查看登录密码</h1><pre><code class="hljs">cat /root/.config/code-server/config.yaml</code></pre><h1 id="3-配置Nginx代理"><a href="#3-配置Nginx代理" class="headerlink" title="3.配置Nginx代理"></a>3.配置Nginx代理</h1><pre><code class="hljs">sudo vi vscode.confserver &#123;  listen 80;  server_name localhost;  location / &#123;    proxy_pass http://127.0.0.1:8080;    # 设置WebSocket连接    proxy_http_version 1.1;    proxy_set_header Upgrade $http_upgrade;    proxy_set_header Connection &quot;upgrade&quot;;    proxy_set_header Host $host;    proxy_set_header X-Real-IP $remote_addr;    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;    proxy_set_header X-Forwarded-Proto $scheme;    proxy_connect_timeout 600s;    proxy_read_timeout 600s;    proxy_send_timeout 600s;  &#125;&#125;</code></pre><h1 id="4-登录CodeServer"><a href="#4-登录CodeServer" class="headerlink" title="4.登录CodeServer"></a>4.登录CodeServer</h1><p><img src="/img/wiki/linux/codeserver.jpg" alt="codeserver"></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/qq_35556658/article/details/147704987">https://blog.csdn.net/qq_35556658/article/details/147704987</a></li><li><a href="https://blog.csdn.net/gitblog_01038/article/details/148323626">https://blog.csdn.net/gitblog_01038/article/details/148323626</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
      <category>极客</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hadoop集群部署</title>
    <link href="/linux/Hadoop/"/>
    <url>/linux/Hadoop/</url>
    
    <content type="html"><![CDATA[<p>Hadoop，Apache基金会基于Java开发和维护的开源分布式计算与存储框架，核心功能是为庞大的计算机集群提供可靠的、统一且稳定的、可伸缩的存储和计算环境。Hadoop使用简单的编程模型跨计算机群集分布式处理大型数据集，其设计理念是将数据和计算任务分布到多个计算节点，通过分布式计算来提高处理效率，以解决单台机器处理大数据时的瓶颈问题</p><h1 id="体系架构"><a href="#体系架构" class="headerlink" title="体系架构"></a>体系架构</h1><p>Hadoop核心由三部分构成，即分布式文件系统HDFS (HadoopDFS)、分布式计算框架MapReduce和资源调度系统YARN，也即狭义上的Hadoop，广义上讲就是以这三个核心进行扩展的Hadoop生态圈</p><h2 id="1-HDFS"><a href="#1-HDFS" class="headerlink" title="1.HDFS"></a>1.HDFS</h2><p>HDFS，Hadoop Distributed File System，Hadoop分布式文件系统，工作原理是将文件分割为固定大小的数据块，并将这些块分布式存储在不同的节点上，从而实现大规模数据的存储和管理</p><h2 id="2-MapReduce"><a href="#2-MapReduce" class="headerlink" title="2.MapReduce"></a>2.MapReduce</h2><p>MapReduce，Hadoop分布式计算框架，工作原理是将计算任务分为Map和Reduce两个阶段，以这种分而治之的策略将复杂的任务分解成许多独立的小任务，从而实现大规模数据的分布式并行计算，极大地提高了数据处理的效率</p><h2 id="3-YARN"><a href="#3-YARN" class="headerlink" title="3.YARN"></a>3.YARN</h2><p>YARN，Yet Another Resource Negotiator，Hadoop资源管理系统，负责协调和分配集群的计算资源，支持多个数据处理框架（如 MapReduce、Spark）在同一集群上运行并共享资源，从而实现资源利用的最大化</p><h2 id="4-生态系统"><a href="#4-生态系统" class="headerlink" title="4.生态系统"></a>4.生态系统</h2><h3 id="4-1-Hive"><a href="#4-1-Hive" class="headerlink" title="4.1 Hive"></a>4.1 Hive</h3><p>Hive，Hadoop的数据仓库，为大数据分析提供了一种类SQL（HiveQL）的查询语言，使得数据工程师能够使用类似SQL的语法查询存储在 HDFS中的数据，而不需要编写复杂的MapReduce程序</p><h3 id="4-2-Pig"><a href="#4-2-Pig" class="headerlink" title="4.2 Pig"></a>4.2 Pig</h3><p>Pig，用于处理大规模数据的高级数据流语言，类似于Hive，Pig提供了一种简单的编程方式来对大数据进行处理，但与SQL类语言不同，Pig使用Pig Latin脚本语言，适用于更加灵活的、非结构化数据处理的场景</p><h2 id="4-3-HBase"><a href="#4-3-HBase" class="headerlink" title="4.3 HBase"></a>4.3 HBase</h2><p>HBase，基于HDFS的NoSQL数据库，用于存储非结构化和半结构化数据，能够处理大规模PB级的稀疏表格数据，通过列族组织数据，适合实时随机读写和处理海量的键值对数据，且可在大规模集群中进行水平扩展</p><h2 id="4-4-Sqoop"><a href="#4-4-Sqoop" class="headerlink" title="4.4 Sqoop"></a>4.4 Sqoop</h2><p>Sqoop，用于将关系型数据库（MySQL、PostgreSQL等）的数据导入到Hadoop生态系统（HDFS、Hive或HBase）以及将Hadoop集群的数据导出到关系型数据库。Sqoop提供了高效的数据传输能力，简化了数据在传统关系型数据库与Hadoop之间的传输</p><h2 id="4-5-Flume"><a href="#4-5-Flume" class="headerlink" title="4.5 Flume"></a>4.5 Flume</h2><p>Flume，分布式的、高可用的日志收集系统，可从多个不同的数据源（如应用日志、网络流量等）收集数据，并将数据传输到Hadoop集群（如HDFS或HBase）。Flume是一种流式数据收集工具，适合处理日志和流数据</p><h2 id="4-6-Oozie"><a href="#4-6-Oozie" class="headerlink" title="4.6 Oozie"></a>4.6 Oozie</h2><p>Oozie，用于管理Hadoop作业的工作流调度系统，允许用户定义复杂的工作流，并指定多个MapReduce、Hive、Pig等任务的依赖关系，支持基于时间的调度（如定时任务）和基于数据事件的调度（如新数据到达时触发任务）</p><h2 id="4-7-Zookeeper"><a href="#4-7-Zookeeper" class="headerlink" title="4.7 Zookeeper"></a>4.7 Zookeeper</h2><p>Zookeeper，分布式协调服务，用于管理Hadoop集群的协调问题，如配置管理、元数据同步和分布式锁</p><h2 id="4-8-Spark"><a href="#4-8-Spark" class="headerlink" title="4.8 Spark"></a>4.8 Spark</h2><p>Spark，快速的内存计算框架，作为Hadoop MapReduce的补充，提供了更加高效的内存计算能力，能够进行大规模数据处理。不用于MapReduce，Spark可更好地处理迭代计算和实时数据处理</p><h2 id="4-9-Mahout"><a href="#4-9-Mahout" class="headerlink" title="4.9 Mahout"></a>4.9 Mahout</h2><p>Mahout，Hadoop集群的机器学习库，帮助开发者构建大规模机器学习算法，提供了多个开箱即用的机器学习算法，如分类、聚类、协同过滤等</p><h1 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h1><h2 id="1-互联网行业"><a href="#1-互联网行业" class="headerlink" title="1.互联网行业"></a>1.互联网行业</h2><h3 id="1-1-日志分析"><a href="#1-1-日志分析" class="headerlink" title="1.1 日志分析"></a>1.1 日志分析</h3><p>互联网行业每天都会产生大量的日志文件，包括用户访问日志和服务器系统日志。用户访问日志记录了用户的登录时间、浏览页面、停留时长、点击链接等信息，通过Hadoop将这些日志数据收集存储在HDFS，再利用MapReduce或Spark等计算框架进行处理分析，以统计出用户的活跃度、最受欢迎的页面、用户在不同页面间的跳转路径等，为优化网站布局、提升用户体验、精准推送广告等提供依据，如电商网站根据用户行为分析来推荐符合其购买倾向的商品，提高商品转化率</p><p>服务器系统日志用于监测系统性能，主要记录了CPU使用率、内存占用、网络带宽消耗、请求响应时间等指标。通过Hadoop对这些日志进行分析，能及时发现系统性能瓶颈，排查故障隐患，如判断哪个时间段服务器负载过高、哪些服务频繁出现超时等情况，以便运维人员采取相应措施优化服务器配置和网络架构</p><h3 id="1-2-搜索引擎"><a href="#1-2-搜索引擎" class="headerlink" title="1.2 搜索引擎"></a>1.2 搜索引擎</h3><p>百度、谷歌这类搜索引擎需要抓取海量网页并对其内容进行索引处理，Hadoop可助力完成网页数据的存储（HDFS）及索引构建过程中的大规模数据并行计算（MapReduce等），通过对网页文本内容进行词频统计、倒排索引构建等操作，使得用户在搜索时能够快速定位到相关网页，提高搜索结果的准确性和检索速度</p><h2 id="2-金融行业"><a href="#2-金融行业" class="headerlink" title="2.金融行业"></a>2.金融行业</h2><h3 id="2-1-风险评估与信用分析"><a href="#2-1-风险评估与信用分析" class="headerlink" title="2.1 风险评估与信用分析"></a>2.1 风险评估与信用分析</h3><p>金融机构将收集到的客户的多维度数据，如个人基本信息、交易记录、借贷历史、消费行为等，存储到Hadoop集群，并通过数据挖掘算法（基于MapReduce或Spark等框架实现）进行特征提取和模型构建，进而分析客户的信用状况，预测违约风险，帮助银行等金融机构决定是否发放贷款、确定贷款额度及利率等，如分析一个中小企业的资金流水、上下游交易情况来评估其还款能力和信用等级</p><h3 id="2-2-金融市场数据分析"><a href="#2-2-金融市场数据分析" class="headerlink" title="2.2 金融市场数据分析"></a>2.2 金融市场数据分析</h3><p>证券交易所、投资银行等将海量的金融市场数据，如股票价格走势、成交量、宏观经济指标、行业数据等，存储到Hadoop集群，并结合数据分析工具对其进行相关性分析、趋势预测等，从而辅助投资者制定投资策略，如通过分析过去多年不同行业股票价格与宏观经济因素的关联，判断在当前经济形势下哪些行业更具投资潜力</p><h2 id="3-电信行业"><a href="#3-电信行业" class="headerlink" title="3.电信行业"></a>3.电信行业</h2><h3 id="3-1-客户关系管理"><a href="#3-1-客户关系管理" class="headerlink" title="3.1 客户关系管理"></a>3.1 客户关系管理</h3><p>电信运营商的庞大的客户群体积累了大量的数据，如用户通话记录、短信内容、上网流量使用情况、套餐订购历史等，可借助Hadoop进行数据的存储、挖掘和分析，以深入了解客户的使用习惯、需求偏好，进而实现精准营销，如针对流量使用大户推荐更合适的大流量套餐，对通话时长较长的客户推荐优惠的通话套餐等，同时还能及时处理客户投诉，提升客户满意度</p><h3 id="3-2-网络优化与运维"><a href="#3-2-网络优化与运维" class="headerlink" title="3.2 网络优化与运维"></a>3.2 网络优化与运维</h3><p>电信网络产生大量的设备运行日志、网络性能指标数据等，通过Hadoop收集并分析这些数据，能够发现网络中的故障点、拥塞区域，评估网络设备的健康状况，为网络的优化升级、基站的合理布局、资源的有效调配等提供决策支持，确保网络的稳定和高效运行，如根据不同区域基站的信号覆盖范围、用户接入数量等数据来决定是否需要增设基站或调整基站发射功率</p><h2 id="4-零售行业"><a href="#4-零售行业" class="headerlink" title="4.零售行业"></a>4.零售行业</h2><h3 id="4-1-销售数据分析"><a href="#4-1-销售数据分析" class="headerlink" title="4.1 销售数据分析"></a>4.1 销售数据分析</h3><p>零售商将海量的销售记录数据，如商品销售数量、销售额、库存变化、顾客购买时间、购买渠道等，存储到Hadoop集群并进行分析，从而的到不同商品的销售趋势、季节性波动、地区差异等数据，以帮助商家制定采购计划、调整商品定价、优化店铺布局，如大型连锁超市通过分析各门店不同季节商品的销售情况来提前备货、调配库存，避免积压或缺货现象</p><h3 id="4-2-供应链管理"><a href="#4-2-供应链管理" class="headerlink" title="4.2 供应链管理"></a>4.2 供应链管理</h3><p>供应商、物流、仓储及销售终端构成了整个零售行业的供应链体系，将这些环节产生的大量数据存储到Hadoop集群并进行整合分析，实现对供应链的可视化管理，优化物流配送路径、预测需求、降低库存成本等，如通过分析历史订单数据以及交通路况等信息来规划最优的物流配送路线，提高配送效率，减少运输损耗</p><h2 id="5-医疗行业"><a href="#5-医疗行业" class="headerlink" title="5.医疗行业"></a>5.医疗行业</h2><h3 id="5-1-医疗影像分析"><a href="#5-1-医疗影像分析" class="headerlink" title="5.1 医疗影像分析"></a>5.1 医疗影像分析</h3><p>医院会产生大量X光、CT、MRI等影像资料数据量极大且需要长期保存，Hadoop集群的HDFS可为此提供大容量的存储解决方案，且可结合机器学习算法（Spark等计算框架）对影像进行特征提取、疾病诊断辅助，如通过分析大量的肺部CT影像来辅助医生识别早期肺癌病变特征，提高诊断的准确性和效率</p><h3 id="5-2-临床数据研究"><a href="#5-2-临床数据研究" class="headerlink" title="5.2 临床数据研究"></a>5.2 临床数据研究</h3><p>基于Hadoop集群将医院患者的病历、症状、治疗方案、用药情况、基因数据等多方面临床数据进行存储和分析，以便于医学研究人员开展疾病的发病机制研究、药物疗效评估、个性化医疗方案制定等工作，如通过分析大量糖尿病患者的用药和血糖控制数据来评估不同药物的治疗效果，为研发更有效的治疗方法提供参考</p><h2 id="6-能源行业"><a href="#6-能源行业" class="headerlink" title="6.能源行业"></a>6.能源行业</h2><h3 id="6-1-电力系统监测与管理"><a href="#6-1-电力系统监测与管理" class="headerlink" title="6.1 电力系统监测与管理"></a>6.1 电力系统监测与管理</h3><p>电力公司基于Hadoop集群，将来自发电厂、变电站、输电线路等设备的运行数据，如发电量、电压、电流、设备温度等实时监测数据以及历史运维数据进行存储，并通过分析来预测设备故障、优化发电计划、调配电力资源等，如通过分析过往变压器故障前的各项运行参数变化来提前预警可能出现的故障，保障电力系统的安全稳定运行</p><h3 id="6-2-油气勘探与生产"><a href="#6-2-油气勘探与生产" class="headerlink" title="6.2 油气勘探与生产"></a>6.2 油气勘探与生产</h3><p>油气行业勘探阶段将海量的地质数据以及生产阶段的油井产量、压力、含水率等数据，存储到Hadoop并进行分析，以辅助确定油气资源的分布、优化油井开采方案、提高采收率等，如通过分析不同区域的地质构造和油井历史生产数据来制定更合理的油井压裂、注水等增产措施</p><h2 id="7-政府部门"><a href="#7-政府部门" class="headerlink" title="7.政府部门"></a>7.政府部门</h2><h3 id="7-1-智慧城市建设"><a href="#7-1-智慧城市建设" class="headerlink" title="7.1 智慧城市建设"></a>7.1 智慧城市建设</h3><p>智慧城市即是将交通、安防、环保、政务等多个领域的数据整合与分析，根据分析结果预判未来趋势，如交通管理部门集群Hadoop存储和分析道路监控摄像头采集的车辆流量、车速等交通数据，实现智能交通调度，缓解拥堵；安防部门通过分析监控视频、报警记录等数据来提升城市治安防控水平；环保部门处理空气质量监测、水污染监测等数据来制定更有效的环境治理策略等，全面提升城市的智能化管理水平</p><h3 id="7-2-人口普查与统计分析"><a href="#7-2-人口普查与统计分析" class="headerlink" title="7.2 人口普查与统计分析"></a>7.2 人口普查与统计分析</h3><p>人口普查产生的海量人口信息数据，如个人基本信息、家庭情况、就业情况等，基于Hadoop数据存储、挖掘与分析，以辅助政府制定相关政策，如分析不同地区的人口年龄结构、劳动力分布等来规划教育资源、就业培训、养老设施等方面的建设</p><h1 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h1><ul><li>172.16.100.101 hadoop01 NameNode&#x2F;DataNode</li><li>172.16.100.102 hadoop02 SecondaryNameNode&#x2F;DataNode</li><li>172.16.100.103 hadoop03 DataNode</li></ul><h1 id="1-配置集群SSH免密"><a href="#1-配置集群SSH免密" class="headerlink" title="1.配置集群SSH免密"></a>1.配置集群SSH免密</h1><pre><code class="hljs">useradd hadoop &amp;&amp; su - hadoop</code></pre><h1 id="2-配置集群Java环境"><a href="#2-配置集群Java环境" class="headerlink" title="2.配置集群Java环境"></a>2.配置集群Java环境</h1><pre><code class="hljs">yum install -y java-1.8.0-openjdk</code></pre><h1 id="3-配置环境变量"><a href="#3-配置环境变量" class="headerlink" title="3.配置环境变量"></a>3.配置环境变量</h1><h2 id="3-1-环境变量配置"><a href="#3-1-环境变量配置" class="headerlink" title="3.1 环境变量配置"></a>3.1 环境变量配置</h2><pre><code class="hljs">vi /etc/profile export HDFS_NAMENODE_USER=hadoopexport HDFS_DATANODE_USER=hadoopexport HDFS_SECONDARYNAMENODE_USER=hadoopexport YARN_RESOURCEMANAGER_USER=hadoopexport YARN_NODEMANAGER_USER=Hadoopexport HADOOP_HOME=/opt/hadoopexport HADOOP_COMMON_HOME=$HADOOP_HOMEexport HADOOP_HDFS_HOME=$HADOOP_HOMEexport HADOOP_YARN_HOME=$HADOOP_HOMEexport HADOOP_MAPRED_HOME=$HADOOP_HOMEexport JAVA_HOME=/usr/lib/jvm/jre-1.8.0-openjdkexport PATH=$PATH:$HADOOP_HOME/binexport PATH=$PATH:$HADOOP_HOME/sbin</code></pre><h2 id="3-2-环境变量生效"><a href="#3-2-环境变量生效" class="headerlink" title="3.2 环境变量生效"></a>3.2 环境变量生效</h2><pre><code class="hljs">source /etc/profile</code></pre><h1 id="4-部署HDFS集群"><a href="#4-部署HDFS集群" class="headerlink" title="4.部署HDFS集群"></a>4.部署HDFS集群</h1><pre><code class="hljs">tar -xzvf hadoop-3.2.4.tar.gz &amp;&amp; mv hadoop-3.2.4 /opt/hadoop mkdir -p /opt/hadoop/data &amp;&amp; chown -R hadoop.hadoop /opt/hadoop</code></pre><h2 id="4-1-HDFS集群配置"><a href="#4-1-HDFS集群配置" class="headerlink" title="4.1 HDFS集群配置"></a>4.1 HDFS集群配置</h2><h3 id="4-1-1-核心配置"><a href="#4-1-1-核心配置" class="headerlink" title="4.1.1 核心配置"></a>4.1.1 核心配置</h3><pre><code class="hljs">vi /opt/hadoop/etc/hadoop/core-site.xml&lt;configuration&gt;    &lt;!-- 设置NameNode地址 --&gt;    &lt;property&gt;        &lt;name&gt;fs.defaultFS&lt;/name&gt;        &lt;value&gt;hdfs://hadoop01:8020&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 设置Hadoop数据存储目录 --&gt;    &lt;property&gt;        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;        &lt;value&gt;/opt/hadoop/data&lt;/value&gt;    &lt;/property&gt;    &lt;!--设置HDFS网页登录用户 --&gt;    &lt;property&gt;      &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;      &lt;value&gt;hadoop&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><h3 id="4-1-2-NameNode配置"><a href="#4-1-2-NameNode配置" class="headerlink" title="4.1.2 NameNode配置"></a>4.1.2 NameNode配置</h3><pre><code class="hljs">vi /opt/hadoop/etc/hadoop/hdfs-site.xml&lt;configuration&gt;    &lt;!-- 设置NameNode控制界面访问地址 --&gt;    &lt;property&gt;      &lt;name&gt;dfs.namenode.http-address&lt;/name&gt;      &lt;value&gt;hadoop01:9870&lt;/value&gt;  &lt;/property&gt;    &lt;!-- 设置SecondaryNameNode控制界面访问地址 --&gt;  &lt;property&gt;      &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;      &lt;value&gt;hadoop02:9868&lt;/value&gt;  &lt;/property&gt;&lt;/configuration&gt;</code></pre><h2 id="4-2-HDFS集群初始化"><a href="#4-2-HDFS集群初始化" class="headerlink" title="4.2 HDFS集群初始化"></a>4.2 HDFS集群初始化</h2><pre><code class="hljs">su hadoop -c &quot;hdfs namenode -format&quot;</code></pre><h2 id="4-3-HDFS集群启动"><a href="#4-3-HDFS集群启动" class="headerlink" title="4.3 HDFS集群启动"></a>4.3 HDFS集群启动</h2><h3 id="4-3-1-启动NameNode"><a href="#4-3-1-启动NameNode" class="headerlink" title="4.3.1 启动NameNode"></a>4.3.1 启动NameNode</h3><pre><code class="hljs">su hadoop -c &quot;hdfs --daemon start namenode&quot;</code></pre><h3 id="4-3-2-启动SecondaryNameNode"><a href="#4-3-2-启动SecondaryNameNode" class="headerlink" title="4.3.2 启动SecondaryNameNode"></a>4.3.2 启动SecondaryNameNode</h3><pre><code class="hljs">su hadoop -c &quot;hdfs --daemon start secondarynamenode&quot;</code></pre><h3 id="4-3-3-启动DataNode"><a href="#4-3-3-启动DataNode" class="headerlink" title="4.3.3 启动DataNode"></a>4.3.3 启动DataNode</h3><pre><code class="hljs">su hadoop -c &quot;hdfs --daemon start datanode&quot;</code></pre><h2 id="4-4-验证集群"><a href="#4-4-验证集群" class="headerlink" title="4.4 验证集群"></a>4.4 验证集群</h2><h3 id="4-4-1-验证HDFS组件状态"><a href="#4-4-1-验证HDFS组件状态" class="headerlink" title="4.4.1 验证HDFS组件状态"></a>4.4.1 验证HDFS组件状态</h3><pre><code class="hljs">su hadoop -c &quot;hdfs dfsadmin -report&quot;</code></pre><h3 id="4-4-2-访问NameNode监控页面"><a href="#4-4-2-访问NameNode监控页面" class="headerlink" title="4.4.2 访问NameNode监控页面"></a>4.4.2 访问NameNode监控页面</h3><h2 id="4-5-文件操作"><a href="#4-5-文件操作" class="headerlink" title="4.5 文件操作"></a>4.5 文件操作</h2><h3 id="4-5-1-查看文件"><a href="#4-5-1-查看文件" class="headerlink" title="4.5.1 查看文件"></a>4.5.1 查看文件</h3><pre><code class="hljs">su hadoop -c &quot;hdfs dfs -ls /&quot;</code></pre><h3 id="4-5-2-创建目录"><a href="#4-5-2-创建目录" class="headerlink" title="4.5.2 创建目录"></a>4.5.2 创建目录</h3><pre><code class="hljs">su hadoop -c &quot;hdfs dfs -mkdir /test&quot;</code></pre><h3 id="4-5-3-上传文件"><a href="#4-5-3-上传文件" class="headerlink" title="4.5.3 上传文件"></a>4.5.3 上传文件</h3><pre><code class="hljs">su hadoop -c &quot;hdfs dfs -put /data/test.txt /test&quot;</code></pre><h3 id="4-5-4-复制文件"><a href="#4-5-4-复制文件" class="headerlink" title="4.5.4 复制文件"></a>4.5.4 复制文件</h3><pre><code class="hljs">su hadoop -c &quot;hdfs dfs -cp /test/test.txt test/test.txt.bak&quot;</code></pre><h3 id="4-5-5-移动文件"><a href="#4-5-5-移动文件" class="headerlink" title="4.5.5 移动文件"></a>4.5.5 移动文件</h3><pre><code class="hljs">su hadoop -c &quot;hdfs dfs -mv /test/test.txt.bak /test/test1.txt&quot;</code></pre><h3 id="4-5-6-下载文件"><a href="#4-5-6-下载文件" class="headerlink" title="4.5.6 下载文件"></a>4.5.6 下载文件</h3><pre><code class="hljs">su hadoop -c &quot;hdfs dfs -get /test/test.txt /tmp&quot;</code></pre><h3 id="4-5-7-查看文件"><a href="#4-5-7-查看文件" class="headerlink" title="4.5.7 查看文件"></a>4.5.7 查看文件</h3><pre><code class="hljs">su hadoop -c &quot;hdfs dfs -cat /test/test.txt&quot;</code></pre><h3 id="4-5-8-删除文件"><a href="#4-5-8-删除文件" class="headerlink" title="4.5.8 删除文件"></a>4.5.8 删除文件</h3><pre><code class="hljs">su hadoop -c &quot;hdfs dfs -rm /test/test1.txt&quot;</code></pre><h3 id="4-5-9-删除目录"><a href="#4-5-9-删除目录" class="headerlink" title="4.5.9 删除目录"></a>4.5.9 删除目录</h3><pre><code class="hljs">su hadoop -c &quot;hdfs dfs -rm -r /test&quot;</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.cnblogs.com/cxnph/articles/16803060.html">https://www.cnblogs.com/cxnph/articles/16803060.html</a></li><li><a href="https://blog.csdn.net/Rr0000_/article/details/148089175">https://blog.csdn.net/Rr0000_/article/details/148089175</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>存储</tag>
      
      <tag>大数据</tag>
      
      <tag>Hadoop</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MinIO对象存储集群部署</title>
    <link href="/linux/MinIO/"/>
    <url>/linux/MinIO/</url>
    
    <content type="html"><![CDATA[<p>MinIO，Go语言开发的高性能、分布式开源对象存储系统，兼容Amazon S3对象存储接口，并支持所有核心S3功能，非常适用于存储大容量非结构化的数据，如图片、视频、日志文件、备份数据和容器、虚拟机镜像文件等等，广泛应用于机器学习、大数据、私有云、混合云、数据分析、高性能应用负载、原生云等领域</p><h1 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h1><ul><li>172.16.100.101 minio01</li><li>172.16.100.102 minio02</li><li>172.16.100.103 minio03</li></ul><h1 id="1-下载MinIO"><a href="#1-下载MinIO" class="headerlink" title="1.下载MinIO"></a>1.下载MinIO</h1><pre><code class="hljs">wget https://dl.min.io/server/minio/release/linux-amd64/minio</code></pre><h1 id="2-集群节点挂载磁盘"><a href="#2-集群节点挂载磁盘" class="headerlink" title="2.集群节点挂载磁盘"></a>2.集群节点挂载磁盘</h1><h1 id="3-部署MinIO"><a href="#3-部署MinIO" class="headerlink" title="3.部署MinIO"></a>3.部署MinIO</h1><pre><code class="hljs">sudo mkdir -p /opt/minio/data &amp;&amp; sudo mv minio /opt/miniosudo vi /opt/minio/run.sh#!/bin/bashexport MINIO_ROOT_USER=adminexport MINIO_ROOT_PASSWORD=MinIO@2025/opt/minio/minio server --address :9000 --console-address :9001 \http://minio01:9000/mnt/minio01 http://minio01:9000/mnt/minio02 \http://minio02:9000/mnt/minio01 http://minio02:9000/mnt/minio02 \http://minio03:9000/mnt/minio01 http://minio03:9000/mnt/minio02</code></pre><h1 id="4-创建启动脚本"><a href="#4-创建启动脚本" class="headerlink" title="4.创建启动脚本"></a>4.创建启动脚本</h1><pre><code class="hljs">sudo vi /lib/systemd/system/minio.service[Unit]Description=Minio High Performance Object Storage serviceDocumentation=https://docs.min.io[Service]WorkingDirectory=/opt/minioExecStart=/opt/minio/run.shRestart=on-failureRestartSec=5[Install]WantedBy=muti-user.target</code></pre><h1 id="5-启动MinIO"><a href="#5-启动MinIO" class="headerlink" title="5.启动MinIO"></a>5.启动MinIO</h1><pre><code class="hljs">systemctl daemon-reloadsystemctl start minio.servicesystemctl enable minio.service</code></pre><h1 id="6-配置负载均衡"><a href="#6-配置负载均衡" class="headerlink" title="6.配置负载均衡"></a>6.配置负载均衡</h1><pre><code class="hljs">sudo vi /etc/nginx/conf.d/minio.confupstream minio_api &#123;  server 172.16.100.101:9000;  server 172.16.100.102:9000;  server 172.16.100.103:9000;&#125;upstream minio_console &#123;  server 172.16.100.101:9001;  server 172.16.100.102:9001;  server 172.16.100.103:9001;&#125;server &#123;  listen       19000;  server_name  172.16.100.101;  ignore_invalid_headers off;  client_max_body_size 0;  proxy_buffering off;  location / &#123;    proxy_set_header   X-Forwarded-Proto $scheme;    proxy_set_header   Host              $http_host;    proxy_set_header   X-Real-IP         $remote_addr;    proxy_connect_timeout 300;    proxy_http_version 1.1;    chunked_transfer_encoding off;    proxy_ignore_client_abort on;    access_log  /var/log/nginx/minio_api_access.log  main;    error_log  /var/log/nginx/minio_api_error.log;    proxy_pass http://minio_api;  &#125;&#125;server &#123;  listen       19001;  server_name  172.16.100.101;  ignore_invalid_headers off;  client_max_body_size 0;  proxy_buffering off;  location / &#123;    proxy_set_header   X-Forwarded-Proto $scheme;    proxy_set_header   Host              $http_host;    proxy_set_header   X-Real-IP         $remote_addr;    proxy_connect_timeout 300;    proxy_http_version 1.1;    chunked_transfer_encoding off;    proxy_ignore_client_abort on;    access_log  /var/log/nginx/minio_console_access.log  main;    error_log  /var/log/nginx/minio_console_error.log;    proxy_pass http://minio_console;  &#125;&#125;</code></pre><h1 id="7-验证MinIO"><a href="#7-验证MinIO" class="headerlink" title="7.验证MinIO"></a>7.验证MinIO</h1><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.minio.org.cn/">https://www.minio.org.cn</a></li><li><a href="https://www.modb.pro/db/634327">https://www.modb.pro/db/634327</a></li><li><a href="https://blog.csdn.net/m0_65477493/article/details/141926400">https://blog.csdn.net/m0_65477493/article/details/141926400</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>存储</tag>
      
      <tag>分布式存储</tag>
      
      <tag>对象存储</tag>
      
      <tag>MinIO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes集群离线部署工具Kubeasz</title>
    <link href="/linux/Kubernetes-Kubeasz/"/>
    <url>/linux/Kubernetes-Kubeasz/</url>
    
    <content type="html"><![CDATA[<p>Kubeasz，Kubernetes Easy Setup with Ansible，基于Ansible的自动化部署Kubernetes集群的开源项目，即通过Ansible预定义的Playbook和配置模板，以二进制的方式一键式全自动的完成Kubernetes高可用集群的部署，且可用于集群的后期维护，如添加或删除Master和Node节点、Etcd数据备份及恢复等，大大节省了运维成本和时间。此外，Kubeasz还可用于制作离线安装包，包括集群组件、基础镜像、网络插件及其余附属功能（监控、日志及镜像仓库等），特别适用于没有外网的局域网环境</p><h1 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h1><ul><li>172.16.100.100 manger</li><li>172.16.100.101 master01</li><li>172.16.100.102 master02</li><li>172.16.100.103 master03</li><li>172.16.100.201 worker01</li><li>172.16.100.202 worker02</li><li>172.16.100.203 worker03</li></ul><h1 id="1-下载Kubeasz"><a href="#1-下载Kubeasz" class="headerlink" title="1.下载Kubeasz"></a>1.下载Kubeasz</h1><pre><code class="hljs">wget https://github.com/easzlab/kubeasz/releases/download/3.6.5/ezdownchmod +x ezdown</code></pre><h1 id="2-创建离线安装包"><a href="#2-创建离线安装包" class="headerlink" title="2.创建离线安装包"></a>2.创建离线安装包</h1><h2 id="2-1-下载二进制包及镜像"><a href="#2-1-下载二进制包及镜像" class="headerlink" title="2.1 下载二进制包及镜像"></a>2.1 下载二进制包及镜像</h2><pre><code class="hljs"># -d，指定Docker版本；-k，指定Kubernetes版本./ezdown -D -k v.1.30.12</code></pre><ul><li>注：ezdown需要tar命令支持，需先行安装</li></ul><h2 id="2-2-下载Flannel网络插件"><a href="#2-2-下载Flannel网络插件" class="headerlink" title="2.2 下载Flannel网络插件"></a>2.2 下载Flannel网络插件</h2><pre><code class="hljs">./ezdown -X flannel</code></pre><h2 id="2-3-下载操作系统离线依赖包"><a href="#2-3-下载操作系统离线依赖包" class="headerlink" title="2.3 下载操作系统离线依赖包"></a>2.3 下载操作系统离线依赖包</h2><pre><code class="hljs">./ezdown -P centos_7</code></pre><ul><li>注：.&#x2F;ezdown -P –help命令可显示支持的操作系统，若不在此列，建议以Centos7为例查看所需依赖包的名称，再另行制备</li></ul><h2 id="2-4-下载Harbor离线安装包"><a href="#2-4-下载Harbor离线安装包" class="headerlink" title="2.4 下载Harbor离线安装包"></a>2.4 下载Harbor离线安装包</h2><pre><code class="hljs">./ezdown -R</code></pre><h2 id="2-5-制作离线依赖包"><a href="#2-5-制作离线依赖包" class="headerlink" title="2.5 制作离线依赖包"></a>2.5 制作离线依赖包</h2><pre><code class="hljs">cd /etc/kubeasztar -czvf kubeasz-v1.30.12.tar.gz kubeasz</code></pre><h1 id="3-配置管理节点"><a href="#3-配置管理节点" class="headerlink" title="3.配置管理节点"></a>3.配置管理节点</h1><p>将离线安装包发送到管理节点，所有操作都在此完成</p><h2 id="3-1-部署离线安装包"><a href="#3-1-部署离线安装包" class="headerlink" title="3.1 部署离线安装包"></a>3.1 部署离线安装包</h2><pre><code class="hljs">tar -xzvf kubeasz-v1.24.16.tar.gzmv kubeasz /etc &amp;&amp; cd /etc/kubeasz</code></pre><h3 id="3-1-1-加载离线包"><a href="#3-1-1-加载离线包" class="headerlink" title="3.1.1 加载离线包"></a>3.1.1 加载离线包</h3><pre><code class="hljs">./ezdown -D</code></pre><h3 id="3-1-2-加载flannel镜像"><a href="#3-1-2-加载flannel镜像" class="headerlink" title="3.1.2 加载flannel镜像"></a>3.1.2 加载flannel镜像</h3><pre><code class="hljs">./ezdown -X flannel</code></pre><h2 id="3-2-配置SSH免密登录"><a href="#3-2-配置SSH免密登录" class="headerlink" title="3.2 配置SSH免密登录"></a>3.2 配置SSH免密登录</h2><h2 id="3-3-启动容器化kubeasz"><a href="#3-3-启动容器化kubeasz" class="headerlink" title="3.3 启动容器化kubeasz"></a>3.3 启动容器化kubeasz</h2><pre><code class="hljs">./ezdown -S</code></pre><h2 id="3-4-创建集群"><a href="#3-4-创建集群" class="headerlink" title="3.4 创建集群"></a>3.4 创建集群</h2><pre><code class="hljs">ezctl new kubernetes</code></pre><h2 id="3-5-配置集群信息"><a href="#3-5-配置集群信息" class="headerlink" title="3.5 配置集群信息"></a>3.5 配置集群信息</h2><pre><code class="hljs">docker exec -it kubeasz /bin/bash</code></pre><h3 id="3-5-1-配置集群基础信息"><a href="#3-5-1-配置集群基础信息" class="headerlink" title="3.5.1 配置集群基础信息"></a>3.5.1 配置集群基础信息</h3><pre><code class="hljs">vi /etc/kubeasz/clusters/kubernetes/hosts# 设置Etcd节点[etcd]172.16.100.101172.16.100.102172.16.100.103# 设置Master节点[kube_master]172.16.100.101 k8s_nodename=&#39;master01&#39;172.16.100.102 k8s_nodename=&#39;master02&#39;172.16.100.103 k8s_nodename=&#39;master03&#39;# 设置Node节点[kube_node]172.16.100.201 k8s_nodename=&#39;worker01&#39;172.16.100.202 k8s_nodename=&#39;worker02&#39;172.16.100.202 k8s_nodename=&#39;worker03&#39;# 设置集群网络插件，默认为calicoCLUSTER_NETWORK=&quot;flannel&quot;</code></pre><h3 id="3-5-2-配置安装信息"><a href="#3-5-2-配置安装信息" class="headerlink" title="3.5.2 配置安装信息"></a>3.5.2 配置安装信息</h3><pre><code class="hljs">vi /etc/kubeasz/clusters/kubernetes/config.yml # 设置安装方式，默认为，online，表示在线；offline则表示离线# INSTALL_SOURCE: &quot;offline&quot;</code></pre><h1 id="4-离线部署Kubernetes集群"><a href="#4-离线部署Kubernetes集群" class="headerlink" title="4.离线部署Kubernetes集群"></a>4.离线部署Kubernetes集群</h1><pre><code class="hljs">docker exec -it kubeasz /bin/bash</code></pre><h2 id="4-1-配置集群环境"><a href="#4-1-配置集群环境" class="headerlink" title="4.1 配置集群环境"></a>4.1 配置集群环境</h2><pre><code class="hljs">ezctl setup kubernetes 01</code></pre><h2 id="4-2-部署Etcd"><a href="#4-2-部署Etcd" class="headerlink" title="4.2 部署Etcd"></a>4.2 部署Etcd</h2><pre><code class="hljs">ezctl setup kubernetes 02</code></pre><h2 id="4-3-部署容器运行时"><a href="#4-3-部署容器运行时" class="headerlink" title="4.3 部署容器运行时"></a>4.3 部署容器运行时</h2><pre><code class="hljs">ezctl setup kubernetes 03</code></pre><h2 id="4-4-部署Master节点"><a href="#4-4-部署Master节点" class="headerlink" title="4.4 部署Master节点"></a>4.4 部署Master节点</h2><pre><code class="hljs">ezctl setup kubernetes 04</code></pre><h2 id="4-5-部署Node节点"><a href="#4-5-部署Node节点" class="headerlink" title="4.5 部署Node节点"></a>4.5 部署Node节点</h2><pre><code class="hljs">ezctl setup kubernetes 05</code></pre><h2 id="4-6-部署网络插件"><a href="#4-6-部署网络插件" class="headerlink" title="4.6 部署网络插件"></a>4.6 部署网络插件</h2><pre><code class="hljs">ezctl setup kubernetes 06</code></pre><h2 id="4-6-部署集群插件"><a href="#4-6-部署集群插件" class="headerlink" title="4.6 部署集群插件"></a>4.6 部署集群插件</h2><pre><code class="hljs">ezctl setup kubernetes 07</code></pre><h1 id="5-部署Harbor镜像仓库"><a href="#5-部署Harbor镜像仓库" class="headerlink" title="5.部署Harbor镜像仓库"></a>5.部署Harbor镜像仓库</h1><h2 id="5-1-配置仓库地址"><a href="#5-1-配置仓库地址" class="headerlink" title="5.1 配置仓库地址"></a>5.1 配置仓库地址</h2><pre><code class="hljs">vi /etc/kubeasz/clusters/kubernetes/hosts[harbor]172.16.100.101 NEW_INSTALL=true</code></pre><h2 id="5-2-配置仓库信息"><a href="#5-2-配置仓库信息" class="headerlink" title="5.2 配置仓库信息"></a>5.2 配置仓库信息</h2><pre><code class="hljs">vi /etc/kubeasz/clusters/kubernetes/config.yml # 设置域名HARBOR_DOMAIN: &quot;reg.ops.com&quot;# 设置安装目录HARBOR_PATH: /var/data# 设置SSL端口号HARBOR_TLS_PORT: 443</code></pre><h2 id="5-3-部署Harbor"><a href="#5-3-部署Harbor" class="headerlink" title="5.3 部署Harbor"></a>5.3 部署Harbor</h2><pre><code class="hljs">ezctl setup kubernetes harbor</code></pre><ul><li>注：安装过程中将生成admin用户的随机密码</li></ul><h1 id="6-验证集群"><a href="#6-验证集群" class="headerlink" title="6.验证集群"></a>6.验证集群</h1><h2 id="6-1-配置Harbor仓库认证"><a href="#6-1-配置Harbor仓库认证" class="headerlink" title="6.1 配置Harbor仓库认证"></a>6.1 配置Harbor仓库认证</h2><pre><code class="hljs">sudo vi /etc/containerd/config.toml[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.configs]    [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.configs.&quot;reg.ops.com&quot;.tls]      insecure_skip_verify = true    [plugin.&quot;io.containerd.grpc.v1.cri&quot;.registry.configs.&quot;reg.ops.com&quot;.auth]      username = &quot;admin&quot;      password = &quot;Harbor12345&quot;</code></pre><h2 id="6-2-创建仓库认证Secret"><a href="#6-2-创建仓库认证Secret" class="headerlink" title="6.2 创建仓库认证Secret"></a>6.2 创建仓库认证Secret</h2><pre><code class="hljs">kubectl create secret docker-registry regsecret --docker-server=reg.ops.com --docker-username=admin --docker-password=Harbor12345</code></pre><h2 id="6-3-创建测试应用"><a href="#6-3-创建测试应用" class="headerlink" title="6.3 创建测试应用"></a>6.3 创建测试应用</h2><pre><code class="hljs"> vi test.yamlapiVersion: apps/v1kind: Deploymentmetadata:  name: nginxspec:  selector:    matchLabels:      app: nginx  replicas: 2  template:    metadata:      labels:        app: nginx    spec:      containers:        - name: nginx          image: reg.ops.com/test/nginx:v1.0          imagePullPolicy: IfNotPresent          ports:            - containerPort: 80              name: nginx              protocol: TCP          resources:            limits:              cpu: 200m              memory: 200Mi            requests:              cpu: 100m              memory: 100Mi      imagePullSecrets:      - name: regsecret</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://github.com/easzlab/kubeasz">https://github.com/easzlab/kubeasz</a></li><li><a href="https://www.cnblogs.com/Chen-Yi-jia/p/17814851.html">https://www.cnblogs.com/Chen-Yi-jia/p/17814851.html</a></li><li><a href="https://blog.csdn.net/u013527895/article/details/126672642">https://blog.csdn.net/u013527895/article/details/126672642</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes集群Etcd数据库故障处理与数据迁移</title>
    <link href="/linux/EtcdTroubleshooting/"/>
    <url>/linux/EtcdTroubleshooting/</url>
    
    <content type="html"><![CDATA[<p>Etcd作为Kubernetes的数据库，存储着整个集群的状态和配置信息，其性能直接决定着业务的稳定性。因此，Etcd集群的扩容、数据迁移及故障处理将是必然要考虑的问题，处理的大致流程为：将扩容的新节点或处理后的故障节点的配置文件ETCD_INITIAL_CLUSTER_STATE参数改为existing，随后再加入集群，重新同步集群数据</p><h1 id="1-查看集群节点"><a href="#1-查看集群节点" class="headerlink" title="1.查看集群节点"></a>1.查看集群节点</h1><pre><code class="hljs">/opt/etcd/bin/etcdctl --endpoints=https://10.1.11.180:2379,https://10.1.11.181:2379,https://10.1.11.182:2379 --cacert=/opt/etcd/ssl/ca.pem --cert=/opt/etcd/ssl/server.pem --key=/opt/etcd/ssl/server-key.pem member list --write-out=table</code></pre><h1 id="2-剔除节点"><a href="#2-剔除节点" class="headerlink" title="2.剔除节点"></a>2.剔除节点</h1><pre><code class="hljs">/opt/etcd/bin/etcdctl --endpoints=https://10.1.11.180:2379,https://10.1.11.181:2379,https://10.1.11.182:2379 --cacert=/opt/etcd/ssl/ca.pem --cert=/opt/etcd/ssl/server.pem --key=/opt/etcd/ssl/server-key.pem member remove bb109c41b2b0c75</code></pre><h1 id="3-导出KVM配置文件并关机"><a href="#3-导出KVM配置文件并关机" class="headerlink" title="3.导出KVM配置文件并关机"></a>3.导出KVM配置文件并关机</h1><h1 id="4-迁移KVM虚拟机磁盘及配置文件"><a href="#4-迁移KVM虚拟机磁盘及配置文件" class="headerlink" title="4.迁移KVM虚拟机磁盘及配置文件"></a>4.迁移KVM虚拟机磁盘及配置文件</h1><h1 id="5-加载KVM虚拟机文件并开机"><a href="#5-加载KVM虚拟机文件并开机" class="headerlink" title="5.加载KVM虚拟机文件并开机"></a>5.加载KVM虚拟机文件并开机</h1><ul><li>注：此步骤IP地址保持不变</li></ul><h1 id="6-重新同步集群数据"><a href="#6-重新同步集群数据" class="headerlink" title="6.重新同步集群数据"></a>6.重新同步集群数据</h1><h2 id="6-1-清空旧数据"><a href="#6-1-清空旧数据" class="headerlink" title="6.1 清空旧数据"></a>6.1 清空旧数据</h2><pre><code class="hljs">sudo mv /var/lib/etcd/member /var/lib/etcd/member.bak</code></pre><h2 id="6-2-修改配置文件"><a href="#6-2-修改配置文件" class="headerlink" title="6.2 修改配置文件"></a>6.2 修改配置文件</h2><pre><code class="hljs">sudo vi /opt/etcd/cfg/etcd.conf ETCD_INITIAL_CLUSTER_STATE=&quot;existing&quot;</code></pre><h2 id="6-3-迁移后的节点重新加入集群"><a href="#6-3-迁移后的节点重新加入集群" class="headerlink" title="6.3 迁移后的节点重新加入集群"></a>6.3 迁移后的节点重新加入集群</h2><pre><code class="hljs">/opt/etcd/bin/etcdctl --endpoints=https://10.1.11.180:2379,https://10.1.11.181:2379,https://10.1.11.182:2379 --cacert=/opt/etcd/ssl/ca.pem --cert=/opt/etcd/ssl/server.pem --key=/opt/etcd/ssl/server-key.pem  member add etcd03 --peer-urls=https://10.1.11.182:2380</code></pre><h2 id="6-4-重启Etcd，同步集群数据"><a href="#6-4-重启Etcd，同步集群数据" class="headerlink" title="6.4 重启Etcd，同步集群数据"></a>6.4 重启Etcd，同步集群数据</h2><pre><code class="hljs">sudo ystemctl daemon-reloadsudo systemctl enable etcd.servicesudo systemctl restart etcd.service</code></pre><h1 id="7-验证数据"><a href="#7-验证数据" class="headerlink" title="7.验证数据"></a>7.验证数据</h1><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.cnblogs.com/liweifeng888/p/17740486.html">https://www.cnblogs.com/liweifeng888/p/17740486.html</a></li><li><a href="https://blog.csdn.net/qq_44397993/article/details/115391463">https://blog.csdn.net/qq_44397993/article/details/115391463</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
      <tag>Etcd</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Containerd的安装与配置</title>
    <link href="/linux/Containerd/"/>
    <url>/linux/Containerd/</url>
    
    <content type="html"><![CDATA[<p>Containerd，工业标准的轻量级开源容器运行时，简洁、健壮、便携，作为操作系统的守护进程管理容器的核心功能，如镜像管理、容器完整的生命周期（启动、监控与销毁）及其存储和网络管理等。Containerd并不是直接面向终端用户的工具，而是为大规模系统（如kubernetes、Swarm、Mesos等）设计的通用的容器基础设施组件，以提供更加轻量级、稳定可靠、独立且可嵌入的容器运行时环境，从而在此基础上构建更高级别的容器化解决方案</p><h1 id="发展历程"><a href="#发展历程" class="headerlink" title="发展历程"></a>发展历程</h1><p>事实上，Containerd起源于Docker，且是其基础组件之一</p><p>起初，Docker公司在容器市场风头强劲，一直占据着领头羊的地位，甚至一度拒绝了谷歌公司协作开发的邀请。这迫使谷歌转向自身更有经验的技术市场，即大规模容器编排应用场景，并将内部系统Brog开源，最终诞生了Kubernetes。此后，谷歌以此为中心，带领RedHat、IBM等成立了CNCF（Cloud Native Computing Fundation）基金会，即云原生计算基金会，逐渐形成了现今风靡全球的云原生技术生态</p><p>与此同时，Docker公司推出意在一统Docker生态的具备大规模容器编排功能的Docker Swarm，剑指Kubernetes。但经过近一年左右的市场验证，始终无法抗衡kubernetes，最后被迫宣布原生支持Kubernetes。至此，大规模容器编排应用市场败局已定，但Docker不甘失败，将其核心依赖Containerd捐给CNCF，以此说明Docker依旧是一个PaaS平台</p><p>随着Kubernetes在全球技术市场的广泛应用，标准化容器运行时接口（CRI，Container Runntime Interface）应运而生，早期专为适配Docker接口而开发的shim被抛弃。2020年，CNCF基金会宣布Kubernetes 1.20版本将不再仅支持Docker容器管理工具，并于1.24版本开始弃用Docker作为容器运行时，以更加轻量级的Containerd取而代之</p><h1 id="体系架构"><a href="#体系架构" class="headerlink" title="体系架构"></a>体系架构</h1><p>Containerd是典型的C&#x2F;S架构，即服务端通过GRPC协议提供稳定的API，而客户端通过调用服务端的API进行高级操作。Containerd的设计大体上被分为三层，即Storage、Metadata和Runtime</p><h2 id="1-storage"><a href="#1-storage" class="headerlink" title="1.storage"></a>1.storage</h2><h3 id="1-1-Content"><a href="#1-1-Content" class="headerlink" title="1.1 Content"></a>1.1 Content</h3><p>Content，内容，存储容器镜像的实际数据，包括文件系统层和元数据，即用于创建和管理容器的基础文件系统</p><h3 id="1-2-Snapshot"><a href="#1-2-Snapshot" class="headerlink" title="1.2 Snapshot"></a>1.2 Snapshot</h3><p>Snapshot，快照，存储容器的快照数据，每个容器都可以有一个或多个快照，且共享文件功能，以提高效率</p><h3 id="1-3-Diff"><a href="#1-3-Diff" class="headerlink" title="1.3 Diff"></a>1.3 Diff</h3><p>Diff，差异，存储容器文件系统层之间的差异，即当容器运行时需要修改文件系统时，会在已有的文件系统层上创建一个差异层，以保存变更</p><h2 id="2-Metadata"><a href="#2-Metadata" class="headerlink" title="2.Metadata"></a>2.Metadata</h2><h3 id="2-1-Images"><a href="#2-1-Images" class="headerlink" title="2.1 Images"></a>2.1 Images</h3><p>Images，镜像，存储容器镜像的元数据，如镜像的标签、大小、创建时间等信息，负责镜像的管理和操作</p><h3 id="2-2-Containers"><a href="#2-2-Containers" class="headerlink" title="2.2 Containers"></a>2.2 Containers</h3><p>Containers，容器，存储容器的元数据，如容器的状态、配置信息、网络设置等，负责管理容器的生命周期</p><h2 id="3-Runtime"><a href="#3-Runtime" class="headerlink" title="3.Runtime"></a>3.Runtime</h2><h3 id="3-1-Tasks"><a href="#3-1-Tasks" class="headerlink" title="3.1 Tasks"></a>3.1 Tasks</h3><p>Tasks，任务，包含容器内的进程组，负责管理容器内的所有进程，并与Shim协同工作，维护容器的状态</p><h3 id="3-2-Events"><a href="#3-2-Events" class="headerlink" title="3.2 Events"></a>3.2 Events</h3><p>Events，事件，存储容器的各种事件，如容器的创建、启动、停止等，用于容器的运行状况监控与日志记录</p><h1 id="Containerd与Dcoker"><a href="#Containerd与Dcoker" class="headerlink" title="Containerd与Dcoker"></a>Containerd与Dcoker</h1><h2 id="1-架构"><a href="#1-架构" class="headerlink" title="1.架构"></a>1.架构</h2><p>Docker是一个完整的容器平台，包括镜像仓库、构建工具、管理工具等，而Containerd是一个轻量级的容器运行时，只负责容器的生命周期管理</p><h2 id="2-社区"><a href="#2-社区" class="headerlink" title="2.社区"></a>2.社区</h2><p>Docker是一个独立的开源项目，拥有庞大的社区和生态系统，而Containerd是一个CNCF项目，社区相对较小，但是与CNCF的其他项目有良好的协作</p><h2 id="3-功能"><a href="#3-功能" class="headerlink" title="3.功能"></a>3.功能</h2><p>Docker提供了更多的高级功能，如Swarm集群管理、Docker Compose应用编排等，而Containerd专注于容器的生命周期管理，不包括这些高级功能</p><h2 id="4-兼容性"><a href="#4-兼容性" class="headerlink" title="4.兼容性"></a>4.兼容性</h2><p>Docker和Containerd都支持OCI规范的容器和镜像格式，但是Docker在镜像格式方面有自己的扩展，导致Docker和Containerd在某些细节上不兼容。另外，Docker提供了更多的CLI命令和API接口，而Containerd则更注重与其他组件的整合</p><h2 id="5-性能"><a href="#5-性能" class="headerlink" title="5.性能"></a>5.性能</h2><p>Containerd的架构更简洁，功能上更为纯粹，因此更为轻量级，启动和运行速度更快，资源占用较少，性能表现更为优异</p><h1 id="1-下载安装包"><a href="#1-下载安装包" class="headerlink" title="1.下载安装包"></a>1.下载安装包</h1><pre><code class="hljs">wget https://github.com/containerd/containerd/releases/download/v1.6.36/containerd-1.6.36-linux-amd64.tar.gz</code></pre><h1 id="2-安装Containerd"><a href="#2-安装Containerd" class="headerlink" title="2.安装Containerd"></a>2.安装Containerd</h1><pre><code class="hljs">tar -xzvf /usr/local containerd-1.6.36-linux-amd64.tar.gzsudo cp bin/* /usr/local/bin</code></pre><h1 id="3-创建启动脚本"><a href="#3-创建启动脚本" class="headerlink" title="3.创建启动脚本"></a>3.创建启动脚本</h1><pre><code class="hljs">sudo vi /lib/systemd/system/containerd.service[Unit]Description=containerd container runtimeDocumentation=https://containerd.ioAfter=network.target local-fs.target[Service]#uncomment to enable the experimental sbservice (sandboxed) version of containerd/cri integration#Environment=&quot;ENABLE_CRI_SANDBOXES=sandboxed&quot;ExecStartPre=-/sbin/modprobe overlayExecStart=/usr/local/bin/containerdType=notifyDelegate=yesKillMode=processRestart=alwaysRestartSec=5# Having non-zero Limit*s causes performance problems due to accounting overhead# in the kernel. We recommend using cgroups to do container-local accounting.LimitNPROC=infinityLimitCORE=infinityLimitNOFILE=infinity# Comment TasksMax if your systemd version does not supports it.# Only systemd 226 and above support this version.TasksMax=infinityOOMScoreAdjust=-999[Install]WantedBy=multi-user.target</code></pre><h1 id="4-创建配置文件"><a href="#4-创建配置文件" class="headerlink" title="4.创建配置文件"></a>4.创建配置文件</h1><pre><code class="hljs">sudo mkdir -p /etc/containerdsudo containerd config default &gt; /etc/containerd/config.toml</code></pre><h2 id="4-1-配置国内镜像源"><a href="#4-1-配置国内镜像源" class="headerlink" title="4.1 配置国内镜像源"></a>4.1 配置国内镜像源</h2><h3 id="4-1-1-创建认证目录"><a href="#4-1-1-创建认证目录" class="headerlink" title="4.1.1 创建认证目录"></a>4.1.1 创建认证目录</h3><pre><code class="hljs">sudo mkdir -p /etc/containerd/certs.d</code></pre><h3 id="4-1-2-配置认证目录"><a href="#4-1-2-配置认证目录" class="headerlink" title="4.1.2 配置认证目录"></a>4.1.2 配置认证目录</h3><pre><code class="hljs">sudo vi /etc/containerd/certs.d/docker.io/hosts.tomlconfig_path = &quot;/etc/containerd/certs.d&quot;</code></pre><h3 id="4-1-3-创建认证文件"><a href="#4-1-3-创建认证文件" class="headerlink" title="4.1.3 创建认证文件"></a>4.1.3 创建认证文件</h3><pre><code class="hljs">sudo vi /etc/containerd/certs.d/docker.io/hosts.tomlserver = &quot;https://docker.io&quot;[host.&quot;http://hub-mirror.c.163.com&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.m.daocloud.io&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://registry.docker-cn.com&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;]</code></pre><h2 id="4-2-配置私有仓库镜像源"><a href="#4-2-配置私有仓库镜像源" class="headerlink" title="4.2 配置私有仓库镜像源"></a>4.2 配置私有仓库镜像源</h2><pre><code class="hljs">sudo vi /etc/containerd/config.toml[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.configs]    [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.configs.&quot;reg.ops.com&quot;.tls]      insecure_skip_verify = true    [plugin.&quot;io.containerd.grpc.v1.cri&quot;.registry.configs.&quot;reg.ops.com&quot;.auth]      username = &quot;admin&quot;      password = &quot;Harbor12345&quot;</code></pre><h1 id="5-启动Containerd"><a href="#5-启动Containerd" class="headerlink" title="5.启动Containerd"></a>5.启动Containerd</h1><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start containerd.servicesudo systemctl enable containerd.service</code></pre><h1 id="6-镜像管理"><a href="#6-镜像管理" class="headerlink" title="6.镜像管理"></a>6.镜像管理</h1><h2 id="6-1-镜像查看"><a href="#6-1-镜像查看" class="headerlink" title="6.1 镜像查看"></a>6.1 镜像查看</h2><pre><code class="hljs">ctr images ls</code></pre><h2 id="6-2-镜像拉取"><a href="#6-2-镜像拉取" class="headerlink" title="6.2 镜像拉取"></a>6.2 镜像拉取</h2><pre><code class="hljs">ctr images pull docker.io/library/nginx:latest --hosts-dir=/etc/containerd/certs.d</code></pre><h2 id="6-3-镜像打标签"><a href="#6-3-镜像打标签" class="headerlink" title="6.3 镜像打标签"></a>6.3 镜像打标签</h2><pre><code class="hljs">ctr images tag docker.io/library/nginx:latest reg.ops.com/library/nginx</code></pre><h2 id="6-4-镜像推送"><a href="#6-4-镜像推送" class="headerlink" title="6.4 镜像推送"></a>6.4 镜像推送</h2><pre><code class="hljs">ctr images pull --user admin:Harbor12345 -k reg.ops.com/library/nginx:latest</code></pre><h2 id="6-5-镜像导出"><a href="#6-5-镜像导出" class="headerlink" title="6.5 镜像导出"></a>6.5 镜像导出</h2><pre><code class="hljs"> ctr image export nginx.img reg.ops.com/library/nginx:latest</code></pre><h2 id="6-6-镜像导入"><a href="#6-6-镜像导入" class="headerlink" title="6.6 镜像导入"></a>6.6 镜像导入</h2><pre><code class="hljs"># 也可导入docker镜像ctr images import nginx.img</code></pre><h2 id="6-7-镜像删除"><a href="#6-7-镜像删除" class="headerlink" title="6.7 镜像删除"></a>6.7 镜像删除</h2><pre><code class="hljs">ctr mages rm docker.io/library/nginx:latest</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.51cto.com/article/718918.html">https://www.51cto.com/article/718918.html</a></li><li><a href="https://www.cnblogs.com/XY-Heruo/p/17638634.html">https://www.cnblogs.com/XY-Heruo/p/17638634.html</a></li><li><a href="https://blog.csdn.net/qq_41822345/article/details/126677121">https://blog.csdn.net/qq_41822345/article/details/126677121</a></li><li><a href="https://blog.csdn.net/weixin_45310323/article/details/130423510">https://blog.csdn.net/weixin_45310323/article/details/130423510</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>容器</tag>
      
      <tag>云原生</tag>
      
      <tag>Containerd</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prometheus监控Kubernetes集群证书</title>
    <link href="/linux/Prometheus-CertSSL/"/>
    <url>/linux/Prometheus-CertSSL/</url>
    
    <content type="html"><![CDATA[<p>509-Certificate-Exporter，Golang编写的轻量级SSL&#x2F;TLS证书监控工具，其核心是内置的crypto&#x2F;tls库所实现的高效的证书解析和导出机制，即快速安全地与远程服务器建立连接并解析SSL&#x2F;TLS证书的关键信息，如有效期、颁发者、SHA摘要、主题等多种证书属性，最后将这些信息转化为Prometheus可读取的指标并通过Exporter接口暴露出来</p><p>509-Certificate-Exporter特别适用于使用大量证书的Kubernetes集群, 如CA证书、kubelet、apiserver、proxy、etcd等组件的认证证书以及kubeconfig文件等，全面监控集群内部或外部网站的SSL&#x2F;TLS证书状态，确保安全合规，从而避免因证书过期导致的集群异常</p><h1 id="1-下载安装包"><a href="#1-下载安装包" class="headerlink" title="1.下载安装包"></a>1.下载安装包</h1><pre><code class="hljs">git clone https://github.com/enix/x509-certificate-exporter.git</code></pre><h1 id="2-编译x509-certificate-exporter"><a href="#2-编译x509-certificate-exporter" class="headerlink" title="2.编译x509-certificate-exporter"></a>2.编译x509-certificate-exporter</h1><pre><code class="hljs">cd x509-certificate-exportergo build ./cmd/x509-certificate-exportercp x509-certificate-exporter/x509-certificate-exporter /usr/local/bin/certificate_exporter</code></pre><h1 id="3-创建启动脚本"><a href="#3-创建启动脚本" class="headerlink" title="3.创建启动脚本"></a>3.创建启动脚本</h1><pre><code class="hljs">sudo vi /lib/systemd/system/certificate_exporter.service[Unit]Description=Prometheus exporter for X.509 certificatesDocumentation=https://github.com/enix/x509-certificate-exporterAfter=network.target[Service]Type=execRestart=on-failureExecStart=/usr/local/bin/certificate_exporter \--watch-dir=/var/lib/kubelet/pki \--watch-dir=/etc/kubernetes/pki \--watch-dir=/opt/etcd/ssl[Install]WantedBy=multi-user.target</code></pre><h1 id="4-启动x509-certificate-exporter"><a href="#4-启动x509-certificate-exporter" class="headerlink" title="4.启动x509-certificate-exporter"></a>4.启动x509-certificate-exporter</h1><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start certificate_exporter.servicesudo systemctl enable certificate_exporter.service</code></pre><h1 id="5-配置Prometheus"><a href="#5-配置Prometheus" class="headerlink" title="5.配置Prometheus"></a>5.配置Prometheus</h1><h2 id="5-1-配置监控实例"><a href="#5-1-配置监控实例" class="headerlink" title="5.1 配置监控实例"></a>5.1 配置监控实例</h2><pre><code class="hljs">sudo vi /usr/local/prometheus/prometheus.ymlglobal:  scrape_interval: 15s   evaluation_interval: 15s   scrape_timeout: 10s scrape_configs:  - job_name: prometheus    static_configs:      - targets:        - localhost:9090  - job_name: x509-certificate-exporter    static_configs:      - targets:        - 172.16.100.180:9793        - 172.16.100.181:9793        - 172.16.100.182:9793</code></pre><h2 id="5-2-重载Prometheus"><a href="#5-2-重载Prometheus" class="headerlink" title="5.2 重载Prometheus"></a>5.2 重载Prometheus</h2><pre><code class="hljs">curl -XPOST http://127.0.0.1:9090/-/reload</code></pre><h1 id="6-Grafana导入监控模版"><a href="#6-Grafana导入监控模版" class="headerlink" title="6.Grafana导入监控模版"></a>6.Grafana导入监控模版</h1><p>Dashboards —&gt; New —&gt; Import —&gt; 模版ID：13922</p><p><img src="/img/wiki/prometheus/x509-certificate.jpg" alt="x509-certificate"></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.jianshu.com/p/22623efffc95">https://www.jianshu.com/p/22623efffc95</a></li><li><a href="https://blog.csdn.net/gitblog_00981/article/details/141250527">https://blog.csdn.net/gitblog_00981/article/details/141250527</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>云原生</tag>
      
      <tag>Prometheus</tag>
      
      <tag>监控告警</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux系统基于Ollama部署本地AI大模型</title>
    <link href="/linux/AI-Ollama/"/>
    <url>/linux/AI-Ollama/</url>
    
    <content type="html"><![CDATA[<p>Ollama，管理和运维大语言模型（LLM，Large Language Model）的可扩展的轻量级开源框架，用于大语言模型的本地化部署、运行与管理，致力于简化大语言模型的运维流程和使用门槛。Ollama支持文本生成、翻译、代码编写、问答等多种自然语言处理任务，并以强大的扩展性支持多种开源语言模型文件的热加载，如Llama 3、Phi 3、Mistral、Gemma、Qwen及DeepSeek等</p><h1 id="1-安装Ollama"><a href="#1-安装Ollama" class="headerlink" title="1.安装Ollama"></a>1.安装Ollama</h1><h2 id="1-1-二进制安装"><a href="#1-1-二进制安装" class="headerlink" title="1.1 二进制安装"></a>1.1 二进制安装</h2><pre><code class="hljs">curl -fsSL https://ollama.com/install.sh | bash</code></pre><h2 id="1-2-Docker安装"><a href="#1-2-Docker安装" class="headerlink" title="1.2 Docker安装"></a>1.2 Docker安装</h2><pre><code class="hljs">docker run -it -d --name ollama -v /web/ollama:/root/.ollama -p 11434:11434 ollama/ollama</code></pre><h1 id="2-运行大模型"><a href="#2-运行大模型" class="headerlink" title="2.运行大模型"></a>2.运行大模型</h1><pre><code class="hljs">ollama run deepseek-r1:32b</code></pre><h1 id="3-安装AI界面"><a href="#3-安装AI界面" class="headerlink" title="3.安装AI界面"></a>3.安装AI界面</h1><h2 id="3-1-二进制安装"><a href="#3-1-二进制安装" class="headerlink" title="3.1 二进制安装"></a>3.1 二进制安装</h2><pre><code class="hljs">curl -LsSf https://astral.sh/uv/install.sh | sh</code></pre><h2 id="3-2-Docker安装"><a href="#3-2-Docker安装" class="headerlink" title="3.2 Docker安装"></a>3.2 Docker安装</h2><pre><code class="hljs">docker run -d -it -p 8080:8080 --name open-webui \--restart always -e OLLAMA_BASE_URL=http://172.16.100.200:11434 \-v /web/wiki:/app/backend/data ghcr.io/open-webui/open-webui</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://zhuanlan.zhihu.com/p/717086397">https://zhuanlan.zhihu.com/p/717086397</a></li><li><a href="https://mp.weixin.qq.com/s/vdSXxTy2WLLrdOVLaem_2g">https://mp.weixin.qq.com/s/vdSXxTy2WLLrdOVLaem_2g</a></li><li><a href="https://blog.csdn.net/python123456_/article/details/145747807">https://blog.csdn.net/python123456_/article/details/145747807</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>AI</tag>
      
      <tag>大模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prometheus监控Etcd集群</title>
    <link href="/linux/Prometheus-Etcd/"/>
    <url>/linux/Prometheus-Etcd/</url>
    
    <content type="html"><![CDATA[<p>Etcd默认在2379端口暴露监控metrics数据，且接口基于https协议，所以还涉及到SSL证书。若是部署于Kubernetes集群的Prometheus，则需要将证书挂载到集群内部，比较繁琐</p><h1 id="1-Prometheus服务器配置Etcd证书"><a href="#1-Prometheus服务器配置Etcd证书" class="headerlink" title="1.Prometheus服务器配置Etcd证书"></a>1.Prometheus服务器配置Etcd证书</h1><pre><code class="hljs">mkdir -p /usr/local/prometheus/etcdscp -r master:/opt/etcd/ssl /usr/local/prometheus/etcd</code></pre><h1 id="2-验证Etcd指标数据"><a href="#2-验证Etcd指标数据" class="headerlink" title="2.验证Etcd指标数据"></a>2.验证Etcd指标数据</h1><pre><code class="hljs">curl -Lk --cert /usr/local/prometheus/etcd/ssl/ca.pem --key /usr/local/prometheus/etcd/ssl/ca-key.pem https://172.18.100.100:2379/metrics</code></pre><h1 id="3-配置Prometheus"><a href="#3-配置Prometheus" class="headerlink" title="3.配置Prometheus"></a>3.配置Prometheus</h1><pre><code class="hljs">sudo vi /usr/local/prometheus/prometheus.yml- job_name: kubernetes-etcd  scheme: https  tls_config:    ca_file: /usr/local/prometheus/etcd/ssl/ca.pem    cert_file: /usr/local/prometheus/etcd/ssl/server.pem    key_file: /usr/local/prometheus/etcd/ssl/server-key.pem  scrape_interval: 5s  static_configs:  - targets:     - 172.18.100.100:2379    - 172.18.100.101:2379    - 172.18.100.102:2379</code></pre><h1 id="4-配置告警规则"><a href="#4-配置告警规则" class="headerlink" title="4.配置告警规则"></a>4.配置告警规则</h1><pre><code class="hljs">vi /usr/local/prometheus/rules/etcd.yamlgroups:- name: Etcd  rules:  - alert: EtcdInsufficientMembers    expr: count(etcd_server_id) % 2 == 0    for: 0m    labels:      severity: Critical      cluster: EtcdServer    annotations:      summary: Etcd insufficient Members (instance &#123;&#123; $labels.instance &#125;&#125;)      description: &quot;Etcd cluster should have an odd number of members\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;  - alert: EtcdNoLeader    expr: etcd_server_has_leader == 0    for: 0m    labels:      severity: Critical      cluster: EtcdServer    annotations:      summary: Etcd no Leader (instance &#123;&#123; $labels.instance &#125;&#125;)      description: &quot;Etcd cluster have no leader\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;  - alert: EtcdHighNumberOfLeaderChanges    expr: increase(etcd_server_leader_changes_seen_total[10m]) &gt; 2    for: 0m    labels:      severity: Warning      cluster: EtcdServer    annotations:      summary: Etcd high number of leader changes (instance &#123;&#123; $labels.instance &#125;&#125;)      description: &quot;Etcd leader changed more than 2 times during 10 minutes\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;  - alert: EtcdHighNumberOfFailedGrpcRequests    expr: sum(rate(grpc_server_handled_total&#123;grpc_code!=&quot;OK&quot;&#125;[1m])) BY (grpc_service, grpc_method) / sum(rate(grpc_server_handled_total[1m])) BY (grpc_service, grpc_method) &gt; 0.01    for: 2m    labels:      severity: Warning      cluster: EtcdServer    annotations:      summary: Etcd high number of failed GRPC requests (instance &#123;&#123; $labels.instance &#125;&#125;)      description: &quot;More than 1% GRPC request failure detected in Etcd\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;  - alert: EtcdHighNumberOfFailedGrpcRequests    expr: sum(rate(grpc_server_handled_total&#123;grpc_code!=&quot;OK&quot;&#125;[1m])) BY (grpc_service, grpc_method) / sum(rate(grpc_server_handled_total[1m])) BY (grpc_service, grpc_method) &gt; 0.05    for: 2m    labels:      severity: Critical      cluster: EtcdServer    annotations:      summary: Etcd high number of failed GRPC requests (instance &#123;&#123; $labels.instance &#125;&#125;)      description: &quot;More than 5% GRPC request failure detected in Etcd\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;  - alert: EtcdGrpcRequestsSlow    expr: histogram_quantile(0.99, sum(rate(grpc_server_handling_seconds_bucket&#123;grpc_type=&quot;unary&quot;&#125;[1m])) by (grpc_service, grpc_method, le)) &gt; 0.15    for: 2m    labels:      severity: Warning      cluster: EtcdServer    annotations:      summary: Etcd GRPC requests slow (instance &#123;&#123; $labels.instance &#125;&#125;)      description: &quot;GRPC requests slowing down, 99th percentile is over 0.15s\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;  - alert: EtcdHighNumberOfFailedHttpRequests    expr: sum(rate(etcd_http_failed_total[1m])) BY (method) / sum(rate(etcd_http_received_total[1m])) BY (method) &gt; 0.01    for: 2m    labels:      severity: Warning      cluster: EtcdServer    annotations:      summary: Etcd high number of failed HTTP requests (instance &#123;&#123; $labels.instance &#125;&#125;)      description: &quot;More than 1% HTTP failure detected in Etcd\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;  - alert: EtcdHttpRequestsSlow    expr: histogram_quantile(0.99, rate(etcd_http_successful_duration_seconds_bucket[1m])) &gt; 0.15    for: 2m    labels:      severity: Warning      cluster: EtcdServer    annotations:      summary: Etcd HTTP requests slow (instance &#123;&#123; $labels.instance &#125;&#125;)      description: &quot;HTTP requests slowing down, 99th percentile is over 0.15s\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;  - alert: EtcdMemberCommunicationSlow    expr: histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket[1m])) &gt; 0.15    for: 2m    labels:      severity: Warning      cluster: EtcdServer    annotations:      summary: Etcd member communication slow (instance &#123;&#123; $labels.instance &#125;&#125;)      description: &quot;Etcd member communication slowing down, 99th percentile is over 0.15s\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;  - alert: EtcdHighNumberOfFailedProposals    expr: increase(etcd_server_proposals_failed_total[1h]) &gt; 5    for: 2m    labels:      severity: Warning      cluster: EtcdServer    annotations:      summary: Etcd high number of failed proposals (instance &#123;&#123; $labels.instance &#125;&#125;)      description: &quot;Etcd server got more than 5 failed proposals past hour\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;  - alert: EtcdHighFsyncDurations    expr: histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket[1m])) &gt; 0.5    for: 2m    labels:      severity: Warning      cluster: EtcdServer    annotations:      summary: Etcd high fsync durations (instance &#123;&#123; $labels.instance &#125;&#125;)      description: &quot;Etcd WAL fsync duration increasing, 99th percentile is over 0.5s\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;  - alert: EtcdHighCommitDurations    expr: histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket[1m])) &gt; 0.25    for: 2m    labels:      severity: Warning      cluster: EtcdServer    annotations:      summary: Etcd high commit durations (instance &#123;&#123; $labels.instance &#125;&#125;)      description: &quot;Etcd commit duration increasing, 99th percentile is over 0.25s\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;</code></pre><h1 id="5-重载Prometheus，验证集群监控状态"><a href="#5-重载Prometheus，验证集群监控状态" class="headerlink" title="5.重载Prometheus，验证集群监控状态"></a>5.重载Prometheus，验证集群监控状态</h1><pre><code class="hljs">curl -XPOST http://127.0.0.1:9090/-/reload</code></pre><h1 id="6-Grafana导入监控模版"><a href="#6-Grafana导入监控模版" class="headerlink" title="6.Grafana导入监控模版"></a>6.Grafana导入监控模版</h1><p>Dashboards —&gt; Manage —&gt; Import —&gt; 模版ID：15308</p><p><img src="/img/wiki/prometheus/etcd.jpg" alt="etcd"></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.lixueduan.com/posts/etcd/17-monitor">https://www.lixueduan.com/posts/etcd/17-monitor</a></li><li><a href="https://blog.csdn.net/qq_34939308/article/details/123314719">https://blog.csdn.net/qq_34939308/article/details/123314719</a></li><li><a href="https://blog.csdn.net/qq_44930876/article/details/141565577">https://blog.csdn.net/qq_44930876/article/details/141565577</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>云原生</tag>
      
      <tag>Etcd</tag>
      
      <tag>Prometheus</tag>
      
      <tag>监控告警</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes集群证书过期解决方案</title>
    <link href="/linux/KubernetesCertExpiration/"/>
    <url>/linux/KubernetesCertExpiration/</url>
    
    <content type="html"><![CDATA[<p>Kubernetes集群所有组件之间的通信都需要通过SSL证书进行身份验证与安全传输，证书一旦过期则不能正常通信，导致应用程序无法使用。Kubernetes官方推荐的集群部署工具kubeadm所引导的集群的证书有效期都是1年，这其实是为了引导客户体验新版本，当然也可以避开集群的升级而只对证书进行续期，所设计的流程也是十分方便与快捷</p><h1 id="1-备份原证书文件"><a href="#1-备份原证书文件" class="headerlink" title="1.备份原证书文件"></a>1.备份原证书文件</h1><pre><code class="hljs">sudo cp -r /etc/kubernetes/pki/ /etc/kubernetes/pki_20250222</code></pre><h1 id="2-确认证书有效期"><a href="#2-确认证书有效期" class="headerlink" title="2.确认证书有效期"></a>2.确认证书有效期</h1><pre><code class="hljs">kubeadm certs check-expiration</code></pre><h1 id="3-更新集群所有证书"><a href="#3-更新集群所有证书" class="headerlink" title="3.更新集群所有证书"></a>3.更新集群所有证书</h1><pre><code class="hljs">kubeadm certs renew all</code></pre><h1 id="4-重启kubelet组件"><a href="#4-重启kubelet组件" class="headerlink" title="4.重启kubelet组件"></a>4.重启kubelet组件</h1><pre><code class="hljs">sudo systemctl restart kubelet</code></pre><h1 id="5-配置kubectl集群认证令牌"><a href="#5-配置kubectl集群认证令牌" class="headerlink" title="5.配置kubectl集群认证令牌"></a>5.配置kubectl集群认证令牌</h1><pre><code class="hljs">sudo rm -rf $HOME/.kube/*sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config</code></pre><h1 id="6-验证集群功能"><a href="#6-验证集群功能" class="headerlink" title="6.验证集群功能"></a>6.验证集群功能</h1><pre><code class="hljs">kubectl get nodes</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://juejin.cn/post/7403245452215320614">https://juejin.cn/post/7403245452215320614</a></li><li><a href="https://cloud.tencent.com/developer/article/1962891">https://cloud.tencent.com/developer/article/1962891</a></li><li><a href="https://blog.csdn.net/IT_ZRS/article/details/140191159">https://blog.csdn.net/IT_ZRS/article/details/140191159</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
      <tag>Etcd</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RabbitMQ配置高可用集群</title>
    <link href="/linux/RabbitMQ/"/>
    <url>/linux/RabbitMQ/</url>
    
    <content type="html"><![CDATA[<p>RabbitMQ，由Erlang语言开发的轻量级开源消息队列系统。RabbitMQ最初起源于金融系统，基于AMQP（高级消息队列协议，应用层协议的一个开放标准，为面向消息的中间件设计，基于此协议的客户端与消息中间件可传递消息，并不受产品、开发语言等条件的限制）传递消息，也可通过插件扩展的方式支持STOMP、MQTT、RabbitMQ Stream协议，广泛应用于分布式系统中存储、转发消息，在易用性、扩展性、高可用性等方面表现不俗</p><h1 id="体系架构"><a href="#体系架构" class="headerlink" title="体系架构"></a>体系架构</h1><p><img src="/img/wiki/rabbitmq/rabbitmq01.jpg" alt="rabbitmq"></p><h2 id="1-Message"><a href="#1-Message" class="headerlink" title="1.Message"></a>1.Message</h2><p>消息，由消息头和消息体组成，消息头则由一系列可选属性组成，包括routing-key（路由键）、priority（相对于其他消息的优先权）、delivery-mode（指出该消息可能需要持久性存储）等</p><h2 id="2-Publisher"><a href="#2-Publisher" class="headerlink" title="2.Publisher"></a>2.Publisher</h2><p>消息的生产者，也是一个向交换器发布消息的客户端应用程序</p><h2 id="3-Exchange"><a href="#3-Exchange" class="headerlink" title="3.Exchange"></a>3.Exchange</h2><p>交换机，用于接收生产者发送的消息并将这些消息路由给服务器中的队列，相当于消息的中转站</p><h2 id="4-Binding"><a href="#4-Binding" class="headerlink" title="4.Binding"></a>4.Binding</h2><p>绑定，用于联结消息队列和交换器，即基于路由键将交换器和消息队列连接起来的路由规则，这样就可以将交换器理解成一个由绑定构成的路由表</p><h2 id="5-Queue"><a href="#5-Queue" class="headerlink" title="5.Queue"></a>5.Queue</h2><p>消息队列，用于保存消息直到发送给消费者，即消息的容器，也是消息的终点。一个消息可投入一个或多个队列，其后将一直在队列里，等待消费者连接到这个队列并将其取走</p><h2 id="6-Connection"><a href="#6-Connection" class="headerlink" title="6.Connection"></a>6.Connection</h2><p>网络连接，类似于TCP连接</p><h2 id="7-Channel"><a href="#7-Channel" class="headerlink" title="7.Channel"></a>7.Channel</h2><p>信道，多路复用连接中的一条独立的双向数据流通道，是建立在真实的TCP连接内的虚拟连接，AMQP命令都通过其发出，如发布消息、订阅队列及接收消息。因为对于操作系统来说建立和销毁TCP都是非常昂贵的开销，所以引入了信道的概念，以复用一条TCP连接</p><h2 id="8-Consumer"><a href="#8-Consumer" class="headerlink" title="8.Consumer"></a>8.Consumer</h2><p>消息的消费者，表示一个从消息队列中取得消息的客户端应用程序</p><h2 id="9-Virtual-Host"><a href="#9-Virtual-Host" class="headerlink" title="9.Virtual Host"></a>9.Virtual Host</h2><p>虚拟主机，共享相同的身份认证和加密环境的独立服务器域，虚拟概念，表示一批交换器、消息队列和相关对象，可以看作拥有自己队列、交换器、绑定和权限机制的迷你版的RabbitMQ服务器，即权限控制组。连接RabbitMQ服务器时虚拟主机必须被指定，默认为 &#x2F;</p><h2 id="10-Broker"><a href="#10-Broker" class="headerlink" title="10.Broker"></a>10.Broker</h2><p>消息队列服务器实体，即RabbitMQ实例</p><h1 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h1><p><img src="/img/wiki/rabbitmq/rabbitmq02.jpg" alt="rabbitmq02"></p><h2 id="1-建立消息信道"><a href="#1-建立消息信道" class="headerlink" title="1.建立消息信道"></a>1.建立消息信道</h2><p>Producer与RabbitMQ物理节点Broker建立连接，并通过虚拟主机创建信道，且一个连接可以创建多个信道</p><h2 id="2-发送消息"><a href="#2-发送消息" class="headerlink" title="2.发送消息"></a>2.发送消息</h2><p>Producer发送消息到Broker，并根据Routing Key决定接收的Exchange</p><h2 id="3-路由转发"><a href="#3-路由转发" class="headerlink" title="3.路由转发"></a>3.路由转发</h2><p>Exchange收到消息后依据已经定义的Binding规则和ExchangeType，将消息路由到匹配的Queue</p><h2 id="4-消息接收"><a href="#4-消息接收" class="headerlink" title="4.消息接收"></a>4.消息接收</h2><p>Consumer监听相应的Queue，通过建立的消息Channel，接收和处理消息</p><h2 id="5-消息确认"><a href="#5-消息确认" class="headerlink" title="5.消息确认"></a>5.消息确认</h2><p>Consumer完成消息处理之后，将会发送确认（ACK）信息给对应的Queue。Queue收到ACK通知就将认为消息处理成<br>功，并将消息从Queue中移除。若对应的Channel断开后，Queue没有收到消息的ACK信息，该消息将被发送给另外的<br>Channel</p><h1 id="1-安装erlang"><a href="#1-安装erlang" class="headerlink" title="1.安装erlang"></a>1.安装erlang</h1><pre><code class="hljs">https://packagecloud.io/rabbitmq/erlang</code></pre><h1 id="2-安装rabbitmq"><a href="#2-安装rabbitmq" class="headerlink" title="2.安装rabbitmq"></a>2.安装rabbitmq</h1><pre><code class="hljs">wget https://github.com/rabbitmq/rabbitmq-server/releases/download/v3.8.25/rabbitmq-server-3.8.25-1.el7.noarch.rpmrpm -ivh rabbitmq-server-3.8.25-1.el7.noarch.rpm</code></pre><h1 id="3-启动Rabbitmq"><a href="#3-启动Rabbitmq" class="headerlink" title="3.启动Rabbitmq"></a>3.启动Rabbitmq</h1><h1 id="4-配置默认集群"><a href="#4-配置默认集群" class="headerlink" title="4.配置默认集群"></a>4.配置默认集群</h1><p>默认集群，即Rabbitmq内建集群，具体机制是将交换机、队列、虚拟主机等元数据信息在各个节点同步，而基于性能和存储空间的考虑，队列的消息数据只存储于队列的创建者节点，其余节点根据队列的元数据信息再路由到创建者节点进行读写。也即是说，一旦队列创建者节点宕机，则消息数据将会丢失，直到其恢复，并不具备真正的高可用性</p><p>Rabbitmq集群分为两种类型，即内存类型和磁盘类型：RAM Node，内存节点，消息存储于内存，优点是处理速度快，缺点是节点宕机将丢失数据，适用于实时的不需要消息持久化或对性能要求极高的场景；Disk Node，默认类型，数据存储于磁盘，能处理大量数据，且在节点故障后可从磁盘恢复，具备数据持久化和灾难恢复的能力。单节点只允许磁盘类型的节点，以防止重启RabbitMQ丢失数据。集群磁盘节点宕机仍然可以保持运行，但无法进行其他操作，如创建队列、交换器、绑定，添加用户、更改权限、添加和删除集群结点等，一直到其恢复正常，所以生产环境建议至少部署两个磁盘节点</p><p>Rabbitmq集群的主从概念作用于队列，而不是节点，队列的创建者即为其Master节点，其余为slave节点，若Master节点宕机，则最先加入的slave将会被选举为新Master节点。对于读写操作而言，无论客户端请求到达哪个节点，最终都是操作Master节点。请求到达Master节点则直接将消息返回给客户端，并由Master节点会通过GM（Guaranteed Multicast）协议将队列的最新状态广播到其余节点。这一过程将由GM保证广播消息的原子性，即要么都更新成功，要么都不更新；若请求到达slave节点，则将请求先重定向到Master节点进行进一步处理</p><h2 id="4-1-Slave节点关闭Rabbitmq"><a href="#4-1-Slave节点关闭Rabbitmq" class="headerlink" title="4.1 Slave节点关闭Rabbitmq"></a>4.1 Slave节点关闭Rabbitmq</h2><pre><code class="hljs">rabbitmqctl stop_app</code></pre><h2 id="4-2-Slave节点同步Master节点集群通信密钥文件"><a href="#4-2-Slave节点同步Master节点集群通信密钥文件" class="headerlink" title="4.2 Slave节点同步Master节点集群通信密钥文件"></a>4.2 Slave节点同步Master节点集群通信密钥文件</h2><pre><code class="hljs">scp master:/var/lib/rabbitmq/.erlang.cookie /var/lib/rabbitmq</code></pre><h2 id="4-3-Slave节点重置"><a href="#4-3-Slave节点重置" class="headerlink" title="4.3 Slave节点重置"></a>4.3 Slave节点重置</h2><pre><code class="hljs">rabbitmqctl reset</code></pre><h2 id="4-4-Slave节点加入集群"><a href="#4-4-Slave节点加入集群" class="headerlink" title="4.4 Slave节点加入集群"></a>4.4 Slave节点加入集群</h2><pre><code class="hljs">rabbitmqctl join_cluster rabbit@master</code></pre><ul><li>注：参数–ram表示内存节点，也可通过命令进行设置：rabbitmqctl set_node_type_ram node01</li></ul><h2 id="4-5-启动Rabbitmq"><a href="#4-5-启动Rabbitmq" class="headerlink" title="4.5 启动Rabbitmq"></a>4.5 启动Rabbitmq</h2><pre><code class="hljs">rabbitmqctl start_app</code></pre><h2 id="4-6-验证集群状态"><a href="#4-6-验证集群状态" class="headerlink" title="4.6 验证集群状态"></a>4.6 验证集群状态</h2><pre><code class="hljs">rabbitmqctl cluster_status</code></pre><h2 id="4-7-创建管理员用户"><a href="#4-7-创建管理员用户" class="headerlink" title="4.7 创建管理员用户"></a>4.7 创建管理员用户</h2><pre><code class="hljs"># 创建用户rabbitmqctl add_user admin 123456# 设置用户权限rabbitmqctl set_permissions admin &quot;.*&quot; &quot;.*&quot; &quot;.*&quot;# 设置用户角色rabbitmqctl set_user_tags admin administrator</code></pre><h2 id="4-8-集群节点启用Web控制台插件"><a href="#4-8-集群节点启用Web控制台插件" class="headerlink" title="4.8 集群节点启用Web控制台插件"></a>4.8 集群节点启用Web控制台插件</h2><pre><code class="hljs">rabbitmq-plugins enable rabbitmq_management</code></pre><h2 id="4-9-登录Web验证集群状态"><a href="#4-9-登录Web验证集群状态" class="headerlink" title="4.9 登录Web验证集群状态"></a>4.9 登录Web验证集群状态</h2><pre><code class="hljs">http://ip:15672</code></pre><h1 id="5-配置镜像集群"><a href="#5-配置镜像集群" class="headerlink" title="5.配置镜像集群"></a>5.配置镜像集群</h1><p>镜像集群，Rabbitmq内建集群的进阶版，即将队列镜像（同步）到集群其余节点，类似于分片式的多副本冗余。若集群节点宕机，队列将自动地切换到其余节点以保证服务的可用性，消息数据也不会丢失。也即是说，镜像集群的所有节点都将存储消息数据，以保障数据的安全性，缺点是资源耗费较大，数据存在冗余，性能也将受到影响</p><p>镜像集群由rabbitmqctl命令进行配置，具体命令为rabbitmqctl set_policy [-p Vhost] Name_Policy Pattern Definition [Priority]：-p Vhost，指定vhost；Name_Policy的名称，策略名称；Pattern，匹配队列的正则表达式；Definition，镜像定义，由三个参数定义，ha-mode用于设置镜像队列模式（all，表示在集群中所有节点进行镜像；exactly，表示在指定个数的节点进行镜像，节点的个数由ha-params指定；nodes，表示在指定的节点进行镜像，节点名称通过ha-params指定）、ha-params配合ha-mode参数使用以及设置队列同步方式的ha-sync-mode参数（可取值为automatic和manualha-sync-mode）；priority，策略的优先级</p><pre><code class="hljs">rabbitmqctl set_policy ha-all &quot;^&quot; &#39;&#123;&quot;ha-mode&quot;:&quot;all&quot;&#125;&#39; </code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.jianshu.com/p/79ca08116d57">https://www.jianshu.com/p/79ca08116d57</a></li><li><a href="https://blog.csdn.net/qq_46112274/article/details/143652840">https://blog.csdn.net/qq_46112274/article/details/143652840</a></li><li><a href="https://blog.csdn.net/Z1366566161664/article/details/129175045">https://blog.csdn.net/Z1366566161664/article/details/129175045</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>MQ</tag>
      
      <tag>中间件</tag>
      
      <tag>RabbitMQ</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prometheus监控告警数据持久化</title>
    <link href="/linux/Prometheus-Alertsnitch/"/>
    <url>/linux/Prometheus-Alertsnitch/</url>
    
    <content type="html"><![CDATA[<p>Prometheus监控系统的告警信息基于告警规则生成，并将生成的告警信息存储为时间序列，再发送给Alertmanager进行转发，整个过程一直专注于实时化告警的处理，并不会持久化存储。这一设计虽然简化了系统设计的复杂性，但大量的告警无疑将会影响故障排查和响应效率，也难以直观地区分告警的轻重缓急。此外，也不能对历史告警进行检索与统计，无法对业务系统的瓶颈与缺陷进行分析。Alertsnitch工具弥补了这一不足，其工作机制是将Alertmanager的告警信息捕获，并将其写入MySQL或PostgreSQL关系数据库，以便进行后续的审计或分析‌</p><h1 id="1-配置Go语言环境"><a href="#1-配置Go语言环境" class="headerlink" title="1.配置Go语言环境"></a>1.配置Go语言环境</h1><h2 id="1-1-安装Go语言程序"><a href="#1-1-安装Go语言程序" class="headerlink" title="1.1 安装Go语言程序"></a>1.1 安装Go语言程序</h2><pre><code class="hljs">wget https://golang.google.cn/dl/go1.24.0.linux-amd64.tar.gztar -xzvf go1.24.0.linux-amd64.tar.gz &amp;&amp; mv go /usr/local</code></pre><h2 id="1-2-配置环境变量"><a href="#1-2-配置环境变量" class="headerlink" title="1.2 配置环境变量"></a>1.2 配置环境变量</h2><pre><code class="hljs">sudo vi /etc/profileexport GO_HOME=/usr/local/goexport PATH=$PATH:$GO_HOME/binexport GOPATH=$GO_HOME/tagert</code></pre><h2 id="1-3-生效环境变量"><a href="#1-3-生效环境变量" class="headerlink" title="1.3 生效环境变量"></a>1.3 生效环境变量</h2><pre><code class="hljs">source /etc/profile</code></pre><h2 id="1-4-配置国内代理"><a href="#1-4-配置国内代理" class="headerlink" title="1.4 配置国内代理"></a>1.4 配置国内代理</h2><pre><code class="hljs">go env -w GOPROXY=https://goproxy.cn</code></pre><h1 id="2-安装Alertsnitch"><a href="#2-安装Alertsnitch" class="headerlink" title="2.安装Alertsnitch"></a>2.安装Alertsnitch</h1><h2 id="2-1-下载Alertsnitch安装包"><a href="#2-1-下载Alertsnitch安装包" class="headerlink" title="2.1 下载Alertsnitch安装包"></a>2.1 下载Alertsnitch安装包</h2><pre><code class="hljs">https://github.com/yakshaving-art/alertsnitch</code></pre><h2 id="2-2-编译Alertsnitch"><a href="#2-2-编译Alertsnitch" class="headerlink" title="2.2 编译Alertsnitch"></a>2.2 编译Alertsnitch</h2><pre><code class="hljs">tar -xzvf alertsnitch-0.2.1.tar.gz &amp;&amp; cd alertsnitch-0.2.1sudo go install &amp;&amp; ll /usr/local/go/tagert/binsudo mv /usr/local/go/tagert/bin/alertsnitch </code></pre><h2 id="2-3-创建启动脚本"><a href="#2-3-创建启动脚本" class="headerlink" title="2.3 创建启动脚本"></a>2.3 创建启动脚本</h2><pre><code class="hljs">sudo vi /lib/systemd/system/alertsnitch.service[Unit]Description=AlertsnitchDocumentation=https://github.com/yakshaving-art/alertsnitchAfter=network.target[Service]Type=simpleUser=rootGroup=rootEnvironment=&quot;ALERTSNITCH_DSN=alertsnitch:alertsnitch@tcp(10.1.11.188:3306)/alertsnitch&quot;ExecStart=/usr/local/bin/alertsnitchRestart=alwaysRestartSec=5[Install]WantedBy=multi-user.target</code></pre><h1 id="3-创建数据库"><a href="#3-创建数据库" class="headerlink" title="3.创建数据库"></a>3.创建数据库</h1><h2 id="3-1-修改数据库初始化脚本"><a href="#3-1-修改数据库初始化脚本" class="headerlink" title="3.1 修改数据库初始化脚本"></a>3.1 修改数据库初始化脚本</h2><pre><code class="hljs">cd alertsnitch-0.2.1/script.dvi bootstrap_mysql.sh #!/bin/bashset -EeufCo pipefailIFS=$&#39;\t\n&#39;echo &quot;Creating DB&quot;mysql --user=root --password=&quot;123456&quot; -e &quot;CREATE DATABASE IF NOT EXISTS alertsnitch;&quot;echo &quot;Creating bootstrapped model&quot;mysql --user=root --password=&quot;123456&quot; &quot;alertsnitch&quot; &lt; ../db.d/mysql/0.0.1-bootstrap.sqlecho &quot;Applying fingerprint model update&quot;mysql --user=root --password=&quot;123456&quot; &quot;alertsnitch&quot; &lt; ../db.d/mysql/0.1.0-fingerprint.sqlecho &quot;Done creating model&quot;</code></pre><h2 id="3-2-初始化数据库"><a href="#3-2-初始化数据库" class="headerlink" title="3.2 初始化数据库"></a>3.2 初始化数据库</h2><pre><code class="hljs">sh bootstrap_mysql.sh</code></pre><h2 id="3-3-创建数据账户并授权"><a href="#3-3-创建数据账户并授权" class="headerlink" title="3.3 创建数据账户并授权"></a>3.3 创建数据账户并授权</h2><pre><code class="hljs">MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON *.* TO &#39;alertsnitch&#39;@&#39;%&#39; IDENTIFIED BY &#39;alertsnitch&#39;;MariaDB [(none)]&gt; flush privileges;</code></pre><h2 id="4-启动Alertsnitch"><a href="#4-启动Alertsnitch" class="headerlink" title="4.启动Alertsnitch"></a>4.启动Alertsnitch</h2><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start alertsnitch.servicesudo systemctl enable alertsnitch.service</code></pre><h1 id="5-配置Alertmanager"><a href="#5-配置Alertmanager" class="headerlink" title="5.配置Alertmanager"></a>5.配置Alertmanager</h1><h2 id="5-1-配置Alertsnitch关联Alertmanager"><a href="#5-1-配置Alertsnitch关联Alertmanager" class="headerlink" title="5.1 配置Alertsnitch关联Alertmanager"></a>5.1 配置Alertsnitch关联Alertmanager</h2><pre><code class="hljs">sudo vi /usr/local/prometheus/alertmanager.yml route:  group_by: [&#39;alertname&#39;,&#39;cluster&#39;]  # 设置组内告警发送的等待时间，即组内收到第一个告警后的发送等待时间，目标是等待组内新增的告警以便同时合并发送，默认为30s  group_wait: 10s  # 设置组内不同批次告警发送的时间间隔，默认为5m  group_interval: 10s  # 设置告警未解决时重复发送的时间间隔，且此期间组内无新增告警，默认4h，建议根据告警严重程度进行设置  repeat_interval: 1h   # 设置默认告警接收者，即未被子路由的receivers.name选项匹配到的告警接收者  receiver: &#39;wechat&#39;  routes:  - receiver: &#39;wechat&#39;    continue: true  - receiver: &#39;alertsnitch&#39;    continue: truereceivers:  - name: &#39;wechat&#39;    webhook_configs:    - url: &#39;http://10.1.11.240:9095/qywechat&#39;      send_resolved: true  - name: &#39;alertsnitch&#39;    webhook_configs:      - url: &#39;http://10.1.11.182:9567/webhook&#39;        send_resolved: true</code></pre><h2 id="5-2-重启Alertmanager"><a href="#5-2-重启Alertmanager" class="headerlink" title="5.2 重启Alertmanager"></a>5.2 重启Alertmanager</h2><pre><code class="hljs">sudo systemctl restart alertmanager.service</code></pre><h1 id="6-配置Grafana"><a href="#6-配置Grafana" class="headerlink" title="6.配置Grafana"></a>6.配置Grafana</h1><h2 id="6-1-配置MySQL数据源"><a href="#6-1-配置MySQL数据源" class="headerlink" title="6.1 配置MySQL数据源"></a>6.1 配置MySQL数据源</h2><h2 id="6-2-导入监控模版"><a href="#6-2-导入监控模版" class="headerlink" title="6.2 导入监控模版"></a>6.2 导入监控模版</h2><p>Dashboards —&gt; Manage —&gt; Import —&gt; 模版ID：15833</p><p><img src="/img/wiki/prometheus/alertsnitch.jpg" alt="alertsnitch"></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.cnblogs.com/gered/p/17091907.html">https://www.cnblogs.com/gered/p/17091907.html</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>云原生</tag>
      
      <tag>Prometheus</tag>
      
      <tag>监控告警</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prometheus监控MySQL数据库</title>
    <link href="/linux/Prometheus-MySQL/"/>
    <url>/linux/Prometheus-MySQL/</url>
    
    <content type="html"><![CDATA[<h1 id="1-下载安装包"><a href="#1-下载安装包" class="headerlink" title="1.下载安装包"></a>1.下载安装包</h1><pre><code class="hljs">wget https://github.com/prometheus/mysqld_exporter/releases/download/v0.16.0/mysqld_exporter-0.16.0.linux-amd64.tar.gz</code></pre><h1 id="2-安装mysqld-exporter"><a href="#2-安装mysqld-exporter" class="headerlink" title="2.安装mysqld_exporter"></a>2.安装mysqld_exporter</h1><pre><code class="hljs">tar -xzvf mysqld_exporter-0.16.0.linux-amd64.tar.gzmv mysqld_exporter-0.15.0.linux-amd64 /usr/local/mysqld_exporter</code></pre><h1 id="3-创建MySQL监控账户并授权"><a href="#3-创建MySQL监控账户并授权" class="headerlink" title="3.创建MySQL监控账户并授权"></a>3.创建MySQL监控账户并授权</h1><pre><code class="hljs">MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON *.* TO &#39;prometheus&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;Prometheus_exporter&#39;; MariaDB [(none)]&gt; flush privileges;</code></pre><h1 id="4-创建配置文件"><a href="#4-创建配置文件" class="headerlink" title="4.创建配置文件"></a>4.创建配置文件</h1><pre><code class="hljs">sudo vi /usr/local/mysqld_exporter/mysqld_exporter.conf[client]user=prometheuspassword=Prometheus_exporterhost=172.16.100.240port=3306</code></pre><h1 id="5-创建启动脚本"><a href="#5-创建启动脚本" class="headerlink" title="5.创建启动脚本"></a>5.创建启动脚本</h1><pre><code class="hljs">sudo vi /lib/systemd/system/mysqld_exporter.service[Unit]Description=Prometheus MySQL ExporterAfter=network.target[Service]Type=simpleUser=rootGroup=rootRestart=alwaysExecStart=/usr/local/bin/mysqld_exporter \--config.my-cnf=/usr/local/mysqld_exporter/mysqld_exporter.conf \--collect.auto_increment.columns \--collect.info_schema.processlist \--collect.binlog_size \--collect.info_schema.tablestats \--collect.info_schema.innodb_metrics \--collect.info_schema.userstats \--collect.info_schema.tables \--collect.perf_schema.tablelocks \--collect.perf_schema.indexiowaits \--collect.perf_schema.tableiowaits \--collect.slave_status[Install]WantedBy=multi-user.target</code></pre><h1 id="6-启动mysqld-exporter"><a href="#6-启动mysqld-exporter" class="headerlink" title="6.启动mysqld_exporter"></a>6.启动mysqld_exporter</h1><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start mysqld_exporter.servicesudo systemctl enable mysqld_exporter.service</code></pre><h1 id="7-配置Prometheus"><a href="#7-配置Prometheus" class="headerlink" title="7.配置Prometheus"></a>7.配置Prometheus</h1><h2 id="7-1-配置监控实例"><a href="#7-1-配置监控实例" class="headerlink" title="7.1 配置监控实例"></a>7.1 配置监控实例</h2><pre><code class="hljs">sudo vi /usr/local/prometheus/prometheus.ymlglobal:  scrape_interval: 15s   evaluation_interval: 15s   scrape_timeout: 10s scrape_configs:  - job_name: &quot;prometheus&quot;    static_configs:      - targets: [&quot;localhost:9090&quot;]  - job_name: mysql    static_configs:    - targets:      - 172.16.100.240:9104 </code></pre><h2 id="7-2-重载Prometheus"><a href="#7-2-重载Prometheus" class="headerlink" title="7.2 重载Prometheus"></a>7.2 重载Prometheus</h2><pre><code class="hljs">curl -XPOST http://127.0.0.1:9090/-/reload</code></pre><h1 id="8-Grafana导入监控模版"><a href="#8-Grafana导入监控模版" class="headerlink" title="8.Grafana导入监控模版"></a>8.Grafana导入监控模版</h1><p>Dashboards —&gt; New —&gt; Import —&gt; 模版ID：7362</p><p><img src="/img/wiki/prometheus/mysql.jpg" alt="mysql"></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.cnblogs.com/yangmeichong/p/18156342">https://www.cnblogs.com/yangmeichong/p/18156342</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>MySQL</tag>
      
      <tag>云原生</tag>
      
      <tag>Prometheus</tag>
      
      <tag>监控告警</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prometheus监控Linux系统进程</title>
    <link href="/linux/Prometheus-Process/"/>
    <url>/linux/Prometheus-Process/</url>
    
    <content type="html"><![CDATA[<p>Prometheus监控Linux系统的process-exporter Exporter用于收集和暴露系统进程级别指标，即监控Linux系统各个进程的资源使用情况，如CPU、内存、I&#x2F;O以及上下文切换等，能够提供详尽的系统进程级别的运行数据，以便于优化应用性能、检测潜在的资源瓶颈和问题</p><h1 id="1-下载安装包"><a href="#1-下载安装包" class="headerlink" title="1.下载安装包"></a>1.下载安装包</h1><pre><code class="hljs">wget https://github.com/ncabatoff/process-exporter/releases/download/v0.8.5/process-exporter-0.8.5.linux-amd64.tar.gz</code></pre><h1 id="2-安装process-exporter"><a href="#2-安装process-exporter" class="headerlink" title="2.安装process_exporter"></a>2.安装process_exporter</h1><pre><code class="hljs">tar -xzvf process-exporter-0.8.5.linux-amd64.tar.gzcd process-exporter-0.8.5.linux-amd64 &amp;&amp; sudo cp process-exporter /usr/local/bin/process_exporter</code></pre><h1 id="3-创建配置文件"><a href="#3-创建配置文件" class="headerlink" title="3.创建配置文件"></a>3.创建配置文件</h1><pre><code class="hljs">sudo vi /etc/process_exporter.yamlprocess_names:  # 设置监控模板，即匹配出所要监控进程名称，支持正则匹配  - name: &quot;&#123;&#123;.Comm&#125;&#125;&quot;    cmdline:    # 匹配所有进程    - &#39;.+&#39;   - name: &quot;&#123;&#123;.Matches&#125;&#125;&quot;    cmdline:    # 匹配唯一进程    - &#39;nginx&#39;</code></pre><h1 id="4-创建启动脚本"><a href="#4-创建启动脚本" class="headerlink" title="4.创建启动脚本"></a>4.创建启动脚本</h1><pre><code class="hljs">sudo vi /lib/systemd/system/process_exporter.service[Unit]Description=process_exporterDocumentation=https://prometheus.ioAfter=network.target[Service]Type=simpleUser=rootExecStart=/usr/local/bin/process_exporter -config.path=/etc/process_exporter.yamlRestart=on-failure[Install]WantedBy=multi-user.target</code></pre><h1 id="5-启动process-exporter"><a href="#5-启动process-exporter" class="headerlink" title="5.启动process_exporter"></a>5.启动process_exporter</h1><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl enable process_exporter.servicesudo systemctl start process_exporter.service</code></pre><h1 id="6-配置Prometheus"><a href="#6-配置Prometheus" class="headerlink" title="6.配置Prometheus"></a>6.配置Prometheus</h1><h2 id="6-1-配置监控实例"><a href="#6-1-配置监控实例" class="headerlink" title="6.1 配置监控实例"></a>6.1 配置监控实例</h2><pre><code class="hljs">sudo vi /usr/local/prometheus/prometheus.ymlglobal:  scrape_interval: 15s   evaluation_interval: 15s   scrape_timeout: 10s scrape_configs:  - job_name: prometheus    static_configs:      - targets: [&quot;localhost:9090&quot;]  - job_name: process    static_configs:    - targets: 172.16.100.100:9256</code></pre><h2 id="6-2-创建告警规则"><a href="#6-2-创建告警规则" class="headerlink" title="6.2 创建告警规则"></a>6.2 创建告警规则</h2><pre><code class="hljs">sudo vi /opt/prometheus/rules/process.ymlgroups:- name: process  rules:  - alert: ProcessDown    expr: namedprocess_namegroup_num_threads == 0    for: 1m    labels:      severity: Warning      cluster: CloudServer    annotations:      summary: &quot;&#123;&#123; $labels.groupname &#125;&#125; 进程异常退出，请及时处理！&quot;      description: &quot;&#123;&#123; $labels.groupname &#125;&#125; 进程异常退出，当前状态为 &#123;&#123; $value &#125;&#125;&quot;</code></pre><h2 id="6-3-重载Prometheus"><a href="#6-3-重载Prometheus" class="headerlink" title="6.3 重载Prometheus"></a>6.3 重载Prometheus</h2><pre><code class="hljs">curl -XPOST http://127.0.0.1:9090/-/reload</code></pre><h1 id="7-Grafana导入监控模版"><a href="#7-Grafana导入监控模版" class="headerlink" title="7.Grafana导入监控模版"></a>7.Grafana导入监控模版</h1><p>Dashboards —&gt; New —&gt; Import —&gt; 模版ID：8378</p><p><img src="/img/wiki/prometheus/process.jpg" alt="process"></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.cnblogs.com/yangmeichong/p/18156518">https://www.cnblogs.com/yangmeichong/p/18156518</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>云原生</tag>
      
      <tag>Prometheus</tag>
      
      <tag>监控告警</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prometheus监控Elasticsearch</title>
    <link href="/linux/Prometheus-Elasticsearch/"/>
    <url>/linux/Prometheus-Elasticsearch/</url>
    
    <content type="html"><![CDATA[<h1 id="1-下载安装包"><a href="#1-下载安装包" class="headerlink" title="1.下载安装包"></a>1.下载安装包</h1><pre><code class="hljs">wget https://github.com/prometheus-community/elasticsearch_exporter/releases/download/v1.8.0/elasticsearch_exporter-1.8.0.linux-amd64.tar.gz</code></pre><h1 id="2-安装elasticsearch-exporter"><a href="#2-安装elasticsearch-exporter" class="headerlink" title="2.安装elasticsearch_exporter"></a>2.安装elasticsearch_exporter</h1><pre><code class="hljs">tar -xzvf elasticsearch_exporter-1.8.0.linux-amd64.tar.gzsudo mv elasticsearch_exporter-1.8.0.linux-amd64 /usr/local/bin/elasticsearch_exporter</code></pre><h1 id="3-创建启动脚本"><a href="#3-创建启动脚本" class="headerlink" title="3.创建启动脚本"></a>3.创建启动脚本</h1><pre><code class="hljs">sudo vi /lib/systemd/system/elasticsearch_exporter.service[Unit]Description=elasticsearch_exporterDocumentation=https://github.com/prometheus-community/elasticsearch_exporterAfter=network.target[Service]Type=simpleUser=rootExecStart=/usr/local/bin/elasticsearch_exporter --es.uri http://elastic:password@localhost:9200KillMode=mixedRestart=on-failure[Install]WantedBy=multi-user.target</code></pre><h1 id="4-启动elasticsearch-exporter"><a href="#4-启动elasticsearch-exporter" class="headerlink" title="4.启动elasticsearch_exporter"></a>4.启动elasticsearch_exporter</h1><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start elasticsearch_exporter.servicesudo systemctl enable elasticsearch_exporter.service</code></pre><h1 id="5-配置Prometheus"><a href="#5-配置Prometheus" class="headerlink" title="5.配置Prometheus"></a>5.配置Prometheus</h1><h2 id="5-1-配置监控实例"><a href="#5-1-配置监控实例" class="headerlink" title="5.1 配置监控实例"></a>5.1 配置监控实例</h2><pre><code class="hljs">sudo vi /usr/local/prometheus/prometheus.ymlglobal:  scrape_interval: 15s   evaluation_interval: 15s   scrape_timeout: 10s scrape_configs:  - job_name: &quot;prometheus&quot;    static_configs:      - targets: [&quot;localhost:9090&quot;]  - job_name: &#39;elasticsearch&#39;    static_configs:      - targets:        - localhost:9114</code></pre><h2 id="5-2-重载Prometheus"><a href="#5-2-重载Prometheus" class="headerlink" title="5.2 重载Prometheus"></a>5.2 重载Prometheus</h2><pre><code class="hljs">curl -XPOST http://127.0.0.1:9090/-/reload</code></pre><h1 id="6-Grafana导入监控模版"><a href="#6-Grafana导入监控模版" class="headerlink" title="6.Grafana导入监控模版"></a>6.Grafana导入监控模版</h1><p>Dashboards —&gt; New —&gt; Import —&gt; 模版ID：7259</p><p><img src="/img/wiki/prometheus/elasticsearch.jpg" alt="elasticsearch"></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/dl_11/article/details/141680508">https://blog.csdn.net/dl_11/article/details/141680508</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Elasticsearch</tag>
      
      <tag>云原生</tag>
      
      <tag>Prometheus</tag>
      
      <tag>监控告警</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Elasticsearch配置安全认证</title>
    <link href="/linux/ElasticsearchAuthentication/"/>
    <url>/linux/ElasticsearchAuthentication/</url>
    
    <content type="html"><![CDATA[<p>Elasticsearch作为分布式、可扩展的搜索和分析引擎，随着大数据时代的到来已经成为了许多企业和组织的重要数据处理工具。与此同时，数据安全问题也日益凸显，即确保数据的安全与合规，并保障数据资产不至泄露。Elasticsearch安全认证机制分为两种方式，即X-Pack安全插件和SSL证书认证</p><h1 id="1-X-Pack"><a href="#1-X-Pack" class="headerlink" title="1.X-Pack"></a>1.X-Pack</h1><p>X-Pack，Elastic 6.8版本之后开发的免费的安全扩展插件，并在7.0版本之后默认集成，无需再单独安装</p><h2 id="1-1-启用X-Pack"><a href="#1-1-启用X-Pack" class="headerlink" title="1.1 启用X-Pack"></a>1.1 启用X-Pack</h2><pre><code class="hljs">sudo vi /etc/elasticsearch/elasticsearch.ymlxpack.security.enabled: truexpack.license.self_generated.type: basicxpack.security.transport.ssl.enabled: true</code></pre><h2 id="1-2-重启Elasticsearch"><a href="#1-2-重启Elasticsearch" class="headerlink" title="1.2 重启Elasticsearch"></a>1.2 重启Elasticsearch</h2><pre><code class="hljs">sudo systemctl restart elasticsearch.service </code></pre><h2 id="1-3-设置内置账户密码"><a href="#1-3-设置内置账户密码" class="headerlink" title="1.3 设置内置账户密码"></a>1.3 设置内置账户密码</h2><pre><code class="hljs">/usr/share/elasticsearch/bin/elasticsearch-setup-passwords interactive******************************************************************************Note: The &#39;elasticsearch-setup-passwords&#39; tool has been deprecated. This       command will be removed in a future release.******************************************************************************Initiating the setup of passwords for reserved users elastic,apm_system,kibana,kibana_system,logstash_system,beats_system,remote_monitoring_user.You will be prompted to enter passwords as the process progresses.Please confirm that you would like to continue [y/N]yEnter password for [elastic]: Reenter password for [elastic]: Enter password for [apm_system]: Reenter password for [apm_system]: Enter password for [kibana_system]: Reenter password for [kibana_system]: Enter password for [logstash_system]: Reenter password for [logstash_system]: Enter password for [beats_system]: Reenter password for [beats_system]: Enter password for [remote_monitoring_user]: Reenter password for [remote_monitoring_user]: Changed password for user [apm_system]Changed password for user [kibana_system]Changed password for user [kibana]Changed password for user [logstash_system]Changed password for user [beats_system]Changed password for user [remote_monitoring_user]Changed password for user [elastic]</code></pre><h2 id="1-4-验证账户密码"><a href="#1-4-验证账户密码" class="headerlink" title="1.4 验证账户密码"></a>1.4 验证账户密码</h2><pre><code class="hljs">curl http://localhost:9200 -u elastic:password&#123;  &quot;name&quot; : &quot;node01&quot;,  &quot;cluster_name&quot; : &quot;es-node01&quot;,  &quot;cluster_uuid&quot; : &quot;XXXXXX1234567890&quot;,  &quot;version&quot; : &#123;    &quot;number&quot; : &quot;8.15.0&quot;,    &quot;build_flavor&quot; : &quot;default&quot;,    &quot;build_type&quot; : &quot;docker&quot;,    &quot;build_hash&quot; : &quot;1a77947f34deddb41af25e6f0ddb8e830159c179&quot;,    &quot;build_date&quot; : &quot;2024-12-06T16:05:34.233336849Z&quot;,    &quot;build_snapshot&quot; : false,    &quot;lucene_version&quot; : &quot;9.11.1&quot;,    &quot;minimum_wire_compatibility_version&quot; : &quot;7.17.0&quot;,    &quot;minimum_index_compatibility_version&quot; : &quot;7.0.0&quot;  &#125;,  &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125;</code></pre><h2 id="1-5-配置Kibana"><a href="#1-5-配置Kibana" class="headerlink" title="1.5 配置Kibana"></a>1.5 配置Kibana</h2><pre><code class="hljs">sudo vi /etc/kibana/kibana.ymlelasticsearch.username: &quot;kibana_system&quot;elasticsearch.password: &quot;password&quot;</code></pre><ul><li>注：重启Kibana生效配置</li></ul><h1 id="2-SSL证书认证"><a href="#2-SSL证书认证" class="headerlink" title="2.SSL证书认证"></a>2.SSL证书认证</h1><p>Elasticsearch最新版目前已对生产环境的集群强制启用SSL认证，以防用户凭据、个人身份信息这些敏感数据被恶意窃取，保障了数据传输过程的安全性</p><h2 id="2-1-创建CA证书"><a href="#2-1-创建CA证书" class="headerlink" title="2.1 创建CA证书"></a>2.1 创建CA证书</h2><pre><code class="hljs">/usr/share/elasticsearch/bin/elasticsearch-certutil ca</code></pre><h2 id="2-2-创建SSL证书"><a href="#2-2-创建SSL证书" class="headerlink" title="2.2 创建SSL证书"></a>2.2 创建SSL证书</h2><pre><code class="hljs">/usr/share/elasticsearch/bin/elasticsearch-certutil cert --ca elastic-stack-ca.p12</code></pre><h2 id="2-3-集群分发SSL证书"><a href="#2-3-集群分发SSL证书" class="headerlink" title="2.3 集群分发SSL证书"></a>2.3 集群分发SSL证书</h2><pre><code class="hljs">cp /usr/share/elasticsearch/*.p12 /etc/elasticsearchscp /usr/share/elasticsearch/*.p12 node02:/etc/elasticsearchscp /usr/share/elasticsearch/*.p12 node03:/etc/elasticsearch</code></pre><h2 id="2-4-配置SSL证书"><a href="#2-4-配置SSL证书" class="headerlink" title="2.4 配置SSL证书"></a>2.4 配置SSL证书</h2><pre><code class="hljs">vi /etc/elasticsearch/elasticsearch.ymlxpack.license.self_generated.type: basicxpack.security.transport.ssl.enabled: truexpack.security.transport.ssl.verification_mode: certificate xpack.security.transport.ssl.client_authentication: requiredxpack.security.transport.ssl.keystore.path: elastic-certificates.p12xpack.security.transport.ssl.truststore.path: elastic-certificates.p12</code></pre><h2 id="2-5-重启Elasticsearch集群"><a href="#2-5-重启Elasticsearch集群" class="headerlink" title="2.5 重启Elasticsearch集群"></a>2.5 重启Elasticsearch集群</h2><pre><code class="hljs">systemctl restart elasticsearch.service </code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/Death_Eric/article/details/105188968">https://blog.csdn.net/Death_Eric/article/details/105188968</a></li><li><a href="https://blog.csdn.net/qq_42263280/article/details/141438215">https://blog.csdn.net/qq_42263280/article/details/141438215</a></li><li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html">https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Elasticsearch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prometheus监控Nginx</title>
    <link href="/linux/Prometheus-Nginx/"/>
    <url>/linux/Prometheus-Nginx/</url>
    
    <content type="html"><![CDATA[<h1 id="1-nginx开启stub-status模块"><a href="#1-nginx开启stub-status模块" class="headerlink" title="1.nginx开启stub_status模块"></a>1.nginx开启stub_status模块</h1><h1 id="2-下载nginx-exporter"><a href="#2-下载nginx-exporter" class="headerlink" title="2.下载nginx_exporter"></a>2.下载nginx_exporter</h1><pre><code class="hljs">wget https://github.com/nginxinc/nginx-prometheus-exporter/releases/download/v1.4.0/nginx-prometheus-exporter_1.4.0_linux_amd64.tar.gz</code></pre><h1 id="3-安装nginx-exporter"><a href="#3-安装nginx-exporter" class="headerlink" title="3.安装nginx_exporter"></a>3.安装nginx_exporter</h1><pre><code class="hljs">tar -xzvf nginx-prometheus-exporter_1.4.0_linux_amd64.tar.gzsudo mv nginx-prometheus-exporter /usr/local/prometheus/nginx_exportersudo chmod +x /usr/local/prometheus/nginx_exporter</code></pre><h1 id="4-创建启动脚本"><a href="#4-创建启动脚本" class="headerlink" title="4.创建启动脚本"></a>4.创建启动脚本</h1><pre><code class="hljs">sudo vi /lib/systemd/system/nginx_exporter.service[Unit]Description=nginx_prometheus_exporterAfter=network.target[Service]Type=simpleUser=rootGroup=rootRestart=alwaysExecStart=/usr/local/prometheus/nginx_exporter --nginx.scrape-uri=http://10.1.11.180:20080/stub_status --web.listen-address=:9113[Install]WantedBy=multi-user.target</code></pre><h1 id="5-启动nginx-exporter"><a href="#5-启动nginx-exporter" class="headerlink" title="5.启动nginx_exporter"></a>5.启动nginx_exporter</h1><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl enable nginx_exporter.servicesudo systemctl start memcached_exporter.service</code></pre><h1 id="6-配置Prometheus"><a href="#6-配置Prometheus" class="headerlink" title="6.配置Prometheus"></a>6.配置Prometheus</h1><h2 id="6-1-配置监控实例"><a href="#6-1-配置监控实例" class="headerlink" title="6.1 配置监控实例"></a>6.1 配置监控实例</h2><pre><code class="hljs">sudo vi /usr/local/prometheus/prometheus.ymlscrape_configs:  - job_name: &quot;nginx&quot;    static_configs:      - targets: [&quot;10.1.11.180:9113&quot;]</code></pre><h2 id="6-2-创建告警规则"><a href="#6-2-创建告警规则" class="headerlink" title="6.2 创建告警规则"></a>6.2 创建告警规则</h2><pre><code class="hljs">sudo vi /usr/local/prometheus/rules/nginx.yml groups:- name: Nginx  rules:  - alert: NginxDown    expr: nginx_up == 0    for: 1m    labels:      severity: Critical      cluster: NginxServer    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; Nginx实例宕机,请及时处理!&quot;      description: &#39;&#123;&#123; $labels.instance &#125;&#125; Nginx实例宕机超过1分钟,当前状态为&#123;&#123; $value &#125;&#125;&#39;  - alert: NginxTooManyConnections    expr: nginx_connections_active&#123;job=&quot;nginx&quot;&#125; &gt; 5000    for: 5m    labels:      severity: Critical      cluster: NginxServer    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; Nginx实例客户端连接数过多,请及时处理!&quot;      description: &#39;&#123;&#123; $labels.instance &#125;&#125; Nginx实例客户端连接数超过5000,当前连接数为 &#123;&#123; $value &#125;&#125;&#39;</code></pre><h2 id="6-3-重载Prometheus"><a href="#6-3-重载Prometheus" class="headerlink" title="6.3 重载Prometheus"></a>6.3 重载Prometheus</h2><pre><code class="hljs">curl -XPOST http://127.0.0.1:9090/-/reload</code></pre><h1 id="7-Grafana导入监控模版"><a href="#7-Grafana导入监控模版" class="headerlink" title="7.Grafana导入监控模版"></a>7.Grafana导入监控模版</h1><p>Dashboards —&gt; New —&gt; Import —&gt; 模版ID：12708</p><p><img src="/img/wiki/prometheus/nginx.jpg" alt="nginx"></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/qq_18138507/article/details/142816442">https://blog.csdn.net/qq_18138507/article/details/142816442</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Nginx</tag>
      
      <tag>Prometheus</tag>
      
      <tag>监控告警</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prometheus监控Ingress-Controller</title>
    <link href="/linux/Prometheus-IngressController/"/>
    <url>/linux/Prometheus-IngressController/</url>
    
    <content type="html"><![CDATA[<h1 id="1-Ingress-nginx-Controller配置监控端口"><a href="#1-Ingress-nginx-Controller配置监控端口" class="headerlink" title="1.Ingress-nginx Controller配置监控端口"></a>1.Ingress-nginx Controller配置监控端口</h1><pre><code class="hljs">kubectl -n ingress-nginx edit svc ingress-nginx-controller- name: metrics  port: 10254  protocol: TCP  targetPort: 10254</code></pre><h1 id="2-配置Prometheus"><a href="#2-配置Prometheus" class="headerlink" title="2.配置Prometheus"></a>2.配置Prometheus</h1><h2 id="2-1-配置监控实例"><a href="#2-1-配置监控实例" class="headerlink" title="2.1 配置监控实例"></a>2.1 配置监控实例</h2><pre><code class="hljs">sudo vi /usr/local/prometheus/prometheus.yml- job_name: &quot;kubernetes-ingress-controller&quot;  scheme: http  kubernetes_sd_configs:  - api_server: https://10.1.11.180:6443    role: endpoints    bearer_token_file: /opt/prometheus/token.kubernetes    tls_config:      insecure_skip_verify: true  tls_config:    insecure_skip_verify: true  bearer_token_file: /opt/prometheus/token.kubernetes  relabel_configs:  - action: keep    regex: ingress-nginx;ingress-nginx-controller;metrics    source_labels:    - __meta_kubernetes_namespace    - __meta_kubernetes_service_name    - __meta_kubernetes_endpoint_port_name</code></pre><h2 id="2-2-重载Prometheus"><a href="#2-2-重载Prometheus" class="headerlink" title="2.2 重载Prometheus"></a>2.2 重载Prometheus</h2><pre><code class="hljs">curl -XPOST http://127.0.0.1:9090/-/reload</code></pre><h1 id="3-Grafana导入监控模版"><a href="#3-Grafana导入监控模版" class="headerlink" title="3.Grafana导入监控模版"></a>3.Grafana导入监控模版</h1><p>Dashboards —&gt; New —&gt; Import —&gt; 模版ID：20275</p><p><img src="/img/wiki/prometheus/ingress.jpg" alt="ingress"></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.jianshu.com/p/0e4981381507">https://www.jianshu.com/p/0e4981381507</a></li><li><a href="https://blog.csdn.net/qq_44930876/article/details/142848211">https://blog.csdn.net/qq_44930876/article/details/142848211</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
      <tag>Prometheus</tag>
      
      <tag>监控告警</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prometheus监控域名和站点的可用性</title>
    <link href="/linux/Prometheus-URL/"/>
    <url>/linux/Prometheus-URL/</url>
    
    <content type="html"><![CDATA[<p>Blackbox，即黑盒，Prometheus从外部发起探测的监控方式，用于探测url和端口的可用性及性能等指标。Blackbox不同于常见的exporter，即无须安装在被监控的目标端，只需能与Prometheus和被监控端通信即可，随后通过HTTP、HTTPS(URL&#x2F;API可用性检测)、DNS(域名解析)、TCP(端口存活检测)、ICMP(主机存活检测)等方式进行探测监控。此外，Blackbox还能用于探测SSL证书的过期时间</p><p>白盒监控是依据系统组件指标的性能趋势发现风险点，从而预测故障，而黑盒监控则以故障为导向进行故障通告，以便于迅速处理，两者结合以保障整个系统的稳定性和可用性</p><h1 id="1-部署blackbox-exporter"><a href="#1-部署blackbox-exporter" class="headerlink" title="1.部署blackbox_exporter"></a>1.部署blackbox_exporter</h1><h2 id="1-1-二进制包部署"><a href="#1-1-二进制包部署" class="headerlink" title="1.1 二进制包部署"></a>1.1 二进制包部署</h2><h3 id="1-1-1-下载安装包"><a href="#1-1-1-下载安装包" class="headerlink" title="1.1.1 下载安装包"></a>1.1.1 下载安装包</h3><pre><code class="hljs">wget https://github.com/prometheus/blackbox_exporter/releases/download/v0.25.0/blackbox_exporter-0.25.0.linux-amd64.tar.gztar -xzvf blackbox_exporter-0.25.0.linux-amd64.tar.gz &amp;&amp; sudo mv blackbox_exporter-0.25.0.linux-amd64/blackbox* /usr/local/prometheus</code></pre><h3 id="1-1-2-创建启动脚本"><a href="#1-1-2-创建启动脚本" class="headerlink" title="1.1.2 创建启动脚本"></a>1.1.2 创建启动脚本</h3><pre><code class="hljs">sudo vi /lib/systemd/system/blackbox_exporter.service[Unit]Description=blackbox_exporterAfter=network.target[Service]Type=simpleUser=rootGroup=rootExecStart=/usr/local/prometheus/blackbox_exporter --config.file /usr/local/prometheus/blackbox.yml --web.listen-address :9115Restart=on-failure[Install]WantedBy=multi-user.target</code></pre><h3 id="1-1-3-启动blackbox-exporter"><a href="#1-1-3-启动blackbox-exporter" class="headerlink" title="1.1.3 启动blackbox_exporter"></a>1.1.3 启动blackbox_exporter</h3><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl enable blackbox_exporter.servicesudo systemctl start blackbox_exporter.service</code></pre><h2 id="1-2-Kubernetes部署"><a href="#1-2-Kubernetes部署" class="headerlink" title="1.2 Kubernetes部署"></a>1.2 Kubernetes部署</h2><h3 id="1-2-1-创建配置文件ConfigMap"><a href="#1-2-1-创建配置文件ConfigMap" class="headerlink" title="1.2.1 创建配置文件ConfigMap"></a>1.2.1 创建配置文件ConfigMap</h3><pre><code class="hljs">vi blackbox-exporter-configuration.yamlapiVersion: v1kind: ConfigMapmetadata:  labels:    app: blackbox-exporter  name: blackbox-exporter  namespace: kube-systemdata:  blackbox.yml: |-    &quot;modules&quot;:      &quot;http_2xx&quot;:        &quot;http&quot;:          &quot;preferred_ip_protocol&quot;: &quot;ip4&quot;        &quot;prober&quot;: &quot;http&quot;      &quot;http_post_2xx&quot;:        &quot;http&quot;:          &quot;method&quot;: &quot;POST&quot;          &quot;preferred_ip_protocol&quot;: &quot;ip4&quot;        &quot;prober&quot;: &quot;http&quot;      &quot;irc_banner&quot;:        &quot;prober&quot;: &quot;tcp&quot;        &quot;tcp&quot;:          &quot;preferred_ip_protocol&quot;: &quot;ip4&quot;          &quot;query_response&quot;:          - &quot;send&quot;: &quot;NICK prober&quot;          - &quot;send&quot;: &quot;USER prober prober prober :prober&quot;          - &quot;expect&quot;: &quot;PING :([^ ]+)&quot;            &quot;send&quot;: &quot;PONG $&#123;1&#125;&quot;          - &quot;expect&quot;: &quot;^:[^ ]+ 001&quot;      &quot;pop3s_banner&quot;:        &quot;prober&quot;: &quot;tcp&quot;        &quot;tcp&quot;:          &quot;preferred_ip_protocol&quot;: &quot;ip4&quot;          &quot;query_response&quot;:          - &quot;expect&quot;: &quot;^+OK&quot;          &quot;tls&quot;: true          &quot;tls_config&quot;:            &quot;insecure_skip_verify&quot;: false      &quot;ssh_banner&quot;:        &quot;prober&quot;: &quot;tcp&quot;        &quot;tcp&quot;:          &quot;preferred_ip_protocol&quot;: &quot;ip4&quot;          &quot;query_response&quot;:          - &quot;expect&quot;: &quot;^SSH-2.0-&quot;      &quot;tcp_connect&quot;:        &quot;prober&quot;: &quot;tcp&quot;        &quot;tcp&quot;:          &quot;preferred_ip_protocol&quot;: &quot;ip4&quot;</code></pre><h3 id="1-2-2-创建Deployment"><a href="#1-2-2-创建Deployment" class="headerlink" title="1.2.2 创建Deployment"></a>1.2.2 创建Deployment</h3><pre><code class="hljs">vi blackbox-exporter-deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata:  name: blackbox-exporter  namespace: kube-system  labels:    app: blackbox-exporterspec:  replicas: 1  selector:    matchLabels:      app: blackbox-exporter  template:    metadata:      labels:        app: blackbox-exporter    spec:      volumes:      - name: config        configMap:          name: blackbox-exporter          defaultMode: 420      containers:      - name: blackbox-exporter        image: prom/blackbox-exporter:v0.25.0        imagePullPolicy: IfNotPresent        args:        - --config.file=/etc/blackbox_exporter/blackbox.yml        - --log.level=info        - --web.listen-address=:9115        ports:        - name: blackbox-port          containerPort: 9115          protocol: TCP        resources:          limits:            cpu: 200m            memory: 256Mi          requests:            cpu: 100m            memory: 50Mi        volumeMounts:        - name: config          mountPath: /etc/blackbox_exporter        readinessProbe:          tcpSocket:            port: 9115          initialDelaySeconds: 5          timeoutSeconds: 5          periodSeconds: 10          successThreshold: 1          failureThreshold: 3</code></pre><h3 id="1-2-3-创建Service"><a href="#1-2-3-创建Service" class="headerlink" title="1.2.3 创建Service"></a>1.2.3 创建Service</h3><pre><code class="hljs">vi blackbox-exporter-service.yamlapiVersion: v1kind: Servicemetadata:  labels:    app: blackbox-exporter  name: blackbox-exporter  namespace: kube-systemspec:  type: NodePort  ports:  - name: balckbox    port: 9115    protocol: TCP    targetPort: 9115    nodePort: 32115  selector:    app: blackbox-exporter</code></pre><h1 id="2-配置Prometheus"><a href="#2-配置Prometheus" class="headerlink" title="2.配置Prometheus"></a>2.配置Prometheus</h1><h1 id="2-1-url监控"><a href="#2-1-url监控" class="headerlink" title="2.1 url监控"></a>2.1 url监控</h1><pre><code class="hljs">- job_name: web-status  metrics_path: /probe  params:    module: [http_2xx]  scheme: http  static_configs:    - targets:      - https://hfsxs.github.io  relabel_configs:  - source_labels: [__address__]    target_label: __param_target  - source_labels: [__param_target]    target_label: instance  - target_label: __address__    replacement: 10.40.1.75:9115</code></pre><h1 id="2-2-Kubernetes集群Service监控"><a href="#2-2-Kubernetes集群Service监控" class="headerlink" title="2.2 Kubernetes集群Service监控"></a>2.2 Kubernetes集群Service监控</h1><pre><code class="hljs">- job_name: kubernetes-services  metrics_path: /probe  params:    module: [http_2xx]  scheme: http  kubernetes_sd_configs:  - api_server: https://10.1.11.180:6443    role: service    bearer_token_file: /opt/prometheus/token.kubernetes    tls_config:      insecure_skip_verify: true  bearer_token_file: /opt/prometheus/token.kubernetes  tls_config:    insecure_skip_verify: true  relabel_configs:  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]    action: keep    regex: true  - source_labels: [__address__]    target_label: __param_target  - target_label: __address__    replacement:  10.254.200.26:19115  - source_labels: [__param_target]    target_label: instance  - action: labelmap    regex: __meta_kubernetes_service_label_(.+)  - source_labels: [__meta_kubernetes_namespace]    target_label: kubernetes_namespace  - source_labels: [__meta_kubernetes_service_name]    target_label: kubernetes_name</code></pre><ul><li>注：被监控的Service需要添加注释：prometheus_io_probe: “true”</li></ul><h2 id="2-3-Kubernetes集群Pod端口监控"><a href="#2-3-Kubernetes集群Pod端口监控" class="headerlink" title="2.3 Kubernetes集群Pod端口监控"></a>2.3 Kubernetes集群Pod端口监控</h2><pre><code class="hljs">- job_name: kubernetes-tcp  metrics_path: /probe  params:    module: [tcp_connect]  scheme: http  kubernetes_sd_configs:  - api_server: https://10.1.11.180:6443    role: endpoints    bearer_token_file: /opt/prometheus/token.kubernetes    tls_config:      insecure_skip_verify: true  bearer_token_file: /opt/prometheus/token.kubernetes  tls_config:    insecure_skip_verify: true  relabel_configs:    - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]      action: keep      regex: true    - source_labels: [__address__]      target_label: __param_target    - source_labels: [__param_target]      target_label: instance    - source_labels: [__meta_kubernetes_pod_container_name]      target_label: app    - target_label: __address__      replacement: 10.254.200.26:19115</code></pre><h1 id="3-创建告警规则"><a href="#3-创建告警规则" class="headerlink" title="3.创建告警规则"></a>3.创建告警规则</h1><pre><code class="hljs">sudo vi /usr/local/prometheus/rules/blackbox.ymlgroups:- name: Blackbox  rules:  - alert: BlackboxProbeFailed    expr: probe_success == 0    for: 1m    labels:      severity: Critical    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例探测失败，请及时处理！&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例探测失败， 当前状态为 &#123;&#123; $value &#125;&#125;&quot;  - alert: BlackboxSlowProbe    expr: avg_over_time(probe_duration_seconds[1m]) &gt; 2    for: 2m    labels:      severity: Warning    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例探测响应时长超过1s，请及时处理！&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例探测响应时长超过1s，当前响应时长为&#123;&#123; $value &#125;&#125;&quot;  - alert: BlackboxProbeSlowHttp    expr: avg_over_time(probe_http_duration_seconds[1m]) &gt; 1    for: 1m    labels:      severity: Warning    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例HTTP探测响应时长超过1s，请及时处理！&quot;      description:  &quot;&#123;&#123; $labels.instance &#125;&#125; 实例HTTP探测响应时长超过1s，当前响应时长为&#123;&#123; $value &#125;&#125;&quot;  - alert: BlackboxProbeSlowPing    expr: avg_over_time(probe_icmp_duration_seconds[1m]) &gt; 1    for: 1m    labels:      severity: Warning    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例ping探测响应时长超过1s，请及时处理！&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例ping探测响应时长超过1s，当前响应时长为&#123;&#123; $value &#125;&#125;&quot;  - alert: BlackboxProbeHttpFailure    expr: probe_http_status_code &lt;= 199 OR probe_http_status_code &gt;= 400    for: 1m    labels:      severity: Critical    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例HTTP探测失败，请及时处理！&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例HTTP响应状态码不是200，当前状态为&#123;&#123; $value &#125;&#125;&quot;  - alert: BlackboxSslCertificateWillExpireSoon    expr: 3 &lt;= round((last_over_time(probe_ssl_earliest_cert_expiry[10m]) - time()) / 86400, 0.1) &lt; 7    for: 5m    labels:      severity: Warning    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例SSL证书有效期小于7天，请及时处理！&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例SSL证书有效期小于7天，当前有效期为&#123;&#123; $value &#125;&#125;&quot;  - alert: BlackboxSslCertificateWillExpireSoon    expr: 0 &lt;= round((last_over_time(probe_ssl_earliest_cert_expiry[10m]) - time()) / 86400, 0.1) &lt; 3    for: 0m    labels:      severity: Critical    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例SSL证书有效期小于3天，请及时处理！&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例SSL证书有效期小于3天，当前有效期为&#123;&#123; $value &#125;&#125;&quot;  - alert: BlackboxSslCertificateExpired    expr: round((last_over_time(probe_ssl_earliest_cert_expiry[10m]) - time()) / 86400, 0.1) &lt; 0    for: 0m    labels:      severity: Critical    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例SSL证书已过期，请及时处理！&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例SSL证书已过期，当前有效期为&#123;&#123; $value &#125;&#125;&quot;</code></pre><h1 id="4-重载Prometheus"><a href="#4-重载Prometheus" class="headerlink" title="4.重载Prometheus"></a>4.重载Prometheus</h1><pre><code class="hljs">curl -XPOST http://127.0.0.1:9090/-/reload</code></pre><h1 id="5-Grafana导入监控模版"><a href="#5-Grafana导入监控模版" class="headerlink" title="5.Grafana导入监控模版"></a>5.Grafana导入监控模版</h1><p>Dashboards —&gt; New —&gt; Import —&gt; 模版ID：18538、16124</p><p><img src="/img/wiki/prometheus/blackbox-18538.jpg" alt="blackbox-18538"></p><p><img src="/img/wiki/prometheus/blackbox-16124.jpg" alt="blackbox-16124"></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.jianshu.com/p/9d65e65444aa">https://www.jianshu.com/p/9d65e65444aa</a></li><li><a href="https://www.cnblogs.com/yangmeichong/p/18156586">https://www.cnblogs.com/yangmeichong/p/18156586</a></li><li><a href="https://www.cnblogs.com/normanlin/p/14505878.html">https://www.cnblogs.com/normanlin/p/14505878.html</a></li><li><a href="https://blog.csdn.net/liulunan_lln/article/details/144453682">https://blog.csdn.net/liulunan_lln/article/details/144453682</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Prometheus</tag>
      
      <tag>监控告警</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>JumpServer堡垒机的部署与配置</title>
    <link href="/linux/JumpServer/"/>
    <url>/linux/JumpServer/</url>
    
    <content type="html"><![CDATA[<p>JumpServer，由Python&#x2F;Django开发的分布式开源堡垒机系统，是符合4A规范的专业运维安全审计系统，用于完成认证、授权、审计、自动化等运维工作，现已支持管理SSH、 Telnet、RDP、VNC等协议资产、MySQL&#x2F;Redis&#x2F;MongoDB等数据库资产及Kubernetes集群云资产等等。JumpServer配备了业界领先的Web Terminal，交互界面美观而简洁。此外，JumpServer还支持多机房跨区域部署，即中心节点提供API，各机房部署登录节点，可方便的横向扩展，无并发访问限制</p><h1 id="1-下载离线安装包"><a href="#1-下载离线安装包" class="headerlink" title="1.下载离线安装包"></a>1.下载离线安装包</h1><pre><code class="hljs">tar -xzvf jumpserver-offline-installer-v3.10.12-amd64.tar.gzmv jumpserver-offline-installer-v3.10.12-amd64 /opt</code></pre><h1 id="2-安装MySQL、Redis"><a href="#2-安装MySQL、Redis" class="headerlink" title="2.安装MySQL、Redis"></a>2.安装MySQL、Redis</h1><h1 id="3-创建数据库"><a href="#3-创建数据库" class="headerlink" title="3.创建数据库"></a>3.创建数据库</h1><pre><code class="hljs">MariaDB [(none)]&gt; CREATE DATABASE jumpserver CHARACTER SET &#39;utf8mb4&#39;;MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON jumpserver.* TO &#39;jumpserver&#39;@&#39;%&#39; IDENTIFIED BY &#39;jumpserver&#39;; MariaDB [(none)]&gt; FLUSH PRIVILEGES;</code></pre><h1 id="4-安装JumpServer"><a href="#4-安装JumpServer" class="headerlink" title="4.安装JumpServer"></a>4.安装JumpServer</h1><h2 id="4-1-配置文件设置"><a href="#4-1-配置文件设置" class="headerlink" title="4.1 配置文件设置"></a>4.1 配置文件设置</h2><pre><code class="hljs">cd /opt/jumpserver-offline-installer-v3.10.12-amd64vi config-example.txt# 设置数据持久化目录, 存储录像、任务日志等文件VOLUME_DIR=/data/jumpserver# 设置MySQL连接信息DB_HOST=172.16.100.200DB_PORT=3306DB_USER=jumpserverDB_PASSWORD=jumpserverDB_NAME=jumpserver# 设置Redis连接信息REDIS_HOST=172.25.12.240REDIS_PORT=6379REDIS_PASSWORD=Redis@2020# 设置外部访问端口，默认为80HTTP_PORT=8088</code></pre><h2 id="4-2-安装JumpServer"><a href="#4-2-安装JumpServer" class="headerlink" title="4.2 安装JumpServer"></a>4.2 安装JumpServer</h2><pre><code class="hljs">./jmsctl.sh install</code></pre><p><img src="/img/wiki/jumpserver/install.jpg" alt="install"></p><ul><li>注：Jumpserver默认内置MySQL、Redis，无需额外配置</li></ul><h1 id="5-启动JumpServer"><a href="#5-启动JumpServer" class="headerlink" title="5.启动JumpServer"></a>5.启动JumpServer</h1><pre><code class="hljs">./jmsctl.sh start</code></pre><h1 id="6-登录JumpServer堡垒机"><a href="#6-登录JumpServer堡垒机" class="headerlink" title="6.登录JumpServer堡垒机"></a>6.登录JumpServer堡垒机</h1><p>默认账号密码为admin&#x2F;admin</p><p><img src="/img/wiki/jumpserver/jumpserver.jpg" alt="jumpserver"></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://docs.jumpserver.org/zh/v3">https://docs.jumpserver.org/zh/v3</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Docker</tag>
      
      <tag>堡垒机</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kafka集群Kraft模式部署</title>
    <link href="/linux/KafkaRaft/"/>
    <url>/linux/KafkaRaft/</url>
    
    <content type="html"><![CDATA[<p>Kafka基于Zookeeper服务管理集群的元数据信息以及集群的协调工作，性能必然受其制约与影响，也增加了运维难度。于是，2.8版本开始，Kafka引入KRaft集群架构模式，力图去除Zookeeper依赖。当然，KRaft集群模式最初并不如Zookeeper模式稳定，所以依然保持了兼容的状态。直到3.3.1版本，Kafka官方开始将KRaft标注为生产环境适用。之后，经过长期的多个版本的持续迭代，基本已经可以完全弃用Zookeeper</p><h1 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h1><p>KRaft模式集群基于分布式共识算法Raft管理集群，集群每个Broker节点都可参与，如分区分配、副本分配、元数据快照等</p><h1 id="1-配置Java环境"><a href="#1-配置Java环境" class="headerlink" title="1.配置Java环境"></a>1.配置Java环境</h1><h1 id="2-下载安装包"><a href="#2-下载安装包" class="headerlink" title="2.下载安装包"></a>2.下载安装包</h1><pre><code class="hljs">wget https://downloads.apache.org/kafka/3.6.2/kafka_2.12-3.6.2.tgztar -xzvf kafka_2.12-3.6.2.tgz &amp;&amp; sudo mv kafka_2.12-3.6.2 /usr/local/kafkasudo mkdir -p /usr/local/kafka</code></pre><h1 id="3-配置Kafka集群"><a href="#3-配置Kafka集群" class="headerlink" title="3.配置Kafka集群"></a>3.配置Kafka集群</h1><pre><code class="hljs">sudo vi /usr/local/kafka/config/kraft/server.properties# 设置节点ID，唯一性node.id = 1# 设置Broker节点角色，broker表示数据节点；controller表示控制器节点；broker,controller表示混合节点；为空则表示ZooKeeper模式process.roles = broker,controller# 设置数据存储目录log.dirs = /usr/local/kafka# 设置Controller选举节点，用于管理集群状态，建议配置所有节点controller.quorum.voters = 1@172.100.100.101:9093,2@172.100.100.102:9093,3@172.100.100.103:9093# 设置Broker服务协议别名inter.broker.listener.name = PLAINTEXT# 设置controller服务协议别名controller.listener.names = CONTROLLER# 设置Broker节点绑定的端口，数据节点端口为9092，控制节点端口为9093listeners = PLAINTEXT://:9092,CONTROLLER://:9093# 设置Broker节点外网访问地址，支持域名和IPadvertised.Listeners = PLAINTEXT://172.100.100.101:9092</code></pre><ul><li>注：三个节点node.id参数务必要保持其唯一性，且建议Broker节点角色单独部署，以免混合部署的服务器资源不足引发OOM等问题</li></ul><h1 id="4-格式化数据目录"><a href="#4-格式化数据目录" class="headerlink" title="4.格式化数据目录"></a>4.格式化数据目录</h1><h2 id="4-1-创建集群UUID"><a href="#4-1-创建集群UUID" class="headerlink" title="4.1 创建集群UUID"></a>4.1 创建集群UUID</h2><pre><code class="hljs">sudo /usr/local/kafka/bin/kafka-storage.sh random-uuid</code></pre><h2 id="4-2-格式化数据存储目录"><a href="#4-2-格式化数据存储目录" class="headerlink" title="4.2 格式化数据存储目录"></a>4.2 格式化数据存储目录</h2><pre><code class="hljs">sudo /usr/local/kafka/bin/kafka-storage.sh format -t 7NSm-xjRRGu3hj-4bA0qMw -c /usr/local/kafka/config/kraft/server.properties</code></pre><ul><li>注：所有节点均需执行</li></ul><h1 id="5-创建启动脚本"><a href="#5-创建启动脚本" class="headerlink" title="5.创建启动脚本"></a>5.创建启动脚本</h1><pre><code class="hljs">sudo vi /lib/systemd/system/kafka.service[Unit]Description=Apache KafkaDocumentation=https://kafka.apache.orgAfter=network.target[Service]ExecStart=/usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/kraft/server.propertiesExecStop=/usr/local/kafka/bin/kafka-server-stop.shRestart=on-failure[Install]WantedBy=multi-user.target</code></pre><h2 id="6-启动Kafka集群"><a href="#6-启动Kafka集群" class="headerlink" title="6.启动Kafka集群"></a>6.启动Kafka集群</h2><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start kafka.servicesudo systemctl enable kafka.service</code></pre><h2 id="7-验证集群"><a href="#7-验证集群" class="headerlink" title="7.验证集群"></a>7.验证集群</h2><h2 id="7-1-集群状态"><a href="#7-1-集群状态" class="headerlink" title="7.1 集群状态"></a>7.1 集群状态</h2><pre><code class="hljs">/usr/local/kafka/bin/kafka-metadata-quorum.sh --bootstrap-server localhost:9092 describe --status</code></pre><h2 id="7-2-控制器状态"><a href="#7-2-控制器状态" class="headerlink" title="7.2 控制器状态"></a>7.2 控制器状态</h2><pre><code class="hljs">/usr/local/kafka/bin/kafka-metadata-quorum.sh --bootstrap-server localhost:9092 describe --replication</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.cnblogs.com/iamxiaofu/p/17858962.html">https://www.cnblogs.com/iamxiaofu/p/17858962.html</a></li><li><a href="https://blog.csdn.net/qq_27740127/article/details/134725178">https://blog.csdn.net/qq_27740127/article/details/134725178</a></li><li><a href="https://blog.csdn.net/cold___play/article/details/132267259">https://blog.csdn.net/cold___play/article/details/132267259</a></li><li><a href="https://blog.csdn.net/ximenjianxue/article/details/132545093">https://blog.csdn.net/ximenjianxue/article/details/132545093</a></li><li><a href="https://blog.csdn.net/2201_75529678/article/details/128181202">https://blog.csdn.net/2201_75529678/article/details/128181202</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kafka</tag>
      
      <tag>MQ</tag>
      
      <tag>中间件</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kafka集群部署</title>
    <link href="/linux/Kafka/"/>
    <url>/linux/Kafka/</url>
    
    <content type="html"><![CDATA[<p>Kafka，由Scala和Java开发的开源的高可靠、高吞吐量、低延迟的流式数据平台，可用作分布式发布-订阅消息队列系统。Kafka基于ZooKeeper同步服务，将消息数据以分区的方式并行地存储于分布式集群的磁盘，并以多副本的方式构建数据冗余机制，适用于大数据实时数据流处理领域，如消费者在网站上所有动作流数据（网页浏览、点击、搜索以及其他用户行动）</p><h1 id="体系架构"><a href="#体系架构" class="headerlink" title="体系架构"></a>体系架构</h1><p><img src="/img/wiki/kafka/001%E9%9B%86%E7%BE%A4%E6%9E%B6%E6%9E%84.jpg" alt="集群架构"></p><p>Kafka是典型的分布式消息队列系统，由消息生产者Producer、消息消费者Consumer、消息服务器Broker和分布式协调服务器ZooKeeper组成</p><p>消息队列，Message Queue，简称为MQ，数据传输过程中的数据链表，是应用程序间异步数据处理机制，实质就是一个保存数据的队列的存储容器，工作流程为：消息的发布者将消息发布到MQ，消息使用者从MQ获取消息，而双方都无需关心传输的方式。基于消息队列设计的消息系统即是将数据以这种异步方式从一个应用程序传输到另外一个应用程序，数据的发送方与接收方不必考虑如何将数据共享与接收，也不必顾及数据的存储问题，只需专注于自身的业务逻辑即可。这样，应用程序之间通过消息系统以异步传输的方式进行数据传输，减少了系统组件之间的直接依赖，解耦了业务逻辑与数据传输逻辑，提高了数据传输的容错性，避免了流量高峰的冲击。消息系统有两种消息传递模式，即点对点方式和基于发布-订阅（publish-subscribe）方式</p><ul><li><p>点对点消息系统，将消息保留在队列中，一个或者多个消费者可以消耗队列中的消息，但是消息最多只能被一个消费者消费，且一旦有一个消费者将其消费掉，消息就从该队列中消失。注意，多个消费者可以同时工作，但最终能拿到该消息的只有其中一个，典型的例子就是订单处理系统，多个订单处理器可以同时工作，但是对于一个特定的订单，只有其中一个订单处理器可以拿到该订单进行处理</p></li><li><p>发布-订阅消息系统，将消息保留在主题中，不同于点对点系统，消费者可以订阅一个或多个主题并使用该主题中的所有消息，消息生产者称为发布者，消息使用者称为订阅者。典型的例子是Dish电视，只需发布不同的渠道，如运动，电影，音乐等，任何人都可通过主题订阅自己的频道集</p></li></ul><h2 id="1-Producer"><a href="#1-Producer" class="headerlink" title="1.Producer"></a>1.Producer</h2><p>Producer，生产者，即消息的创建者，将消息发布到Kafka</p><h2 id="2-Consumer"><a href="#2-Consumer" class="headerlink" title="2.Consumer"></a>2.Consumer</h2><p>Consumer，消费者，即消息的使用者，从Kafka获取消息，每个Consumer都从属于特定的ConsumerGroup（消费者组，可进行指定，若不指定则属于默认组）</p><h2 id="3-Broker"><a href="#3-Broker" class="headerlink" title="3.Broker"></a>3.Broker</h2><p>Broker，Kafka服务器节点，相当于MQ节点，用于存储消息数据，多个Broker节点即构成Kafka集群</p><h3 id="3-1-Topic"><a href="#3-1-Topic" class="headerlink" title="3.1 Topic"></a>3.1 Topic</h3><p>Topic，主题，Kafka所存储消息的逻辑上的类别，生产者和消费者的操作对象</p><h3 id="3-2-Partition"><a href="#3-2-Partition" class="headerlink" title="3.2 Partition"></a>3.2 Partition</h3><p>Partition，分区，Kafka数据存储的基本单元，是一个有序且不可变的记录序列，新纪录从末尾进行追加，且每个Topic都被分割为一个或多个物理上的Partition，并以文件形式分散存储于Broker节点。由于Kafka基于文件进行数据存储，文件内容必然会扩展到单个磁盘的上限，因此将文件分割为一个一个的Partition，一个Partition对应一个文件。这样，就将数据分割到不同的Broker进行存储，还实现了负载均衡</p><h3 id="3-3-Replication"><a href="#3-3-Replication" class="headerlink" title="3.3 Replication"></a>3.3 Replication</h3><p>Replication，副本，分区的备份，默认为1，最大值为集群Broker节点数，是Kafka的数据冗余机制。所有副本数据一致，但只有一个作为ReplicationLeader负责与生产者、消费者交互及数据同步，其余作为ReplicationFollower，只作为备份，并从replicationLeader同步数据，直到ReplicationLeader损坏或失效，再由ReplicationManager选举新的ReplicationLeader，完成副本状态切换</p><h3 id="3-4-Offset"><a href="#3-4-Offset" class="headerlink" title="3.4 Offset"></a>3.4 Offset</h3><p>Offset，偏移量，long型数字，消息的唯一序号，由于Partition文件是有序的记录，所以也标识了该消息在Partition中的位置</p><h3 id="3-5-LogSegment"><a href="#3-5-LogSegment" class="headerlink" title="3.5 LogSegment"></a>3.5 LogSegment</h3><p>LogSegment，日志段，Kafka日志对象分片的最小单位，每个分区被划分为多个LogSegment。LogSegment是逻辑概念，由一个日志文件（.log数据文件）和两个索引文件（.index和.timeindex，即表示偏移量索引文件和消息时间戳索引文件）组成</p><h2 id="4-ZooKeeper"><a href="#4-ZooKeeper" class="headerlink" title="4.ZooKeeper"></a>4.ZooKeeper</h2><p>ZooKeeper，开源的分布式应用程序协调服务，通常用作分布式架构的配置和同步服务，为分布式应用提供一致性服务，如配置维护、域名服务、分布式同步、组服务等。Kafka集群依赖于ZooKeeper共享信息与协调管理，如集群元数据存储（如Topic、Broker注册等）、协调Broker与Producer、Consumer（如Broker状态监控及Leader选举）</p><h1 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h1><p><img src="/img/wiki/kafka/002%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.jpg" alt="工作流程"></p><p>Kafka工作流程大致分为三步，生产消息，即生产者将消息发送到Kafka集群，并选择目标分区；存储消息，即Broker将消息持久化到磁盘，并通过副本机制保证数据的高可用性和容错性；消费消息，即消费者从Kafka集群拉取消息，并进行处理。同时，消费者需要定期提交消费进度，以确保消息的准确处理和故障恢复</p><h2 id="1-生产消息"><a href="#1-生产消息" class="headerlink" title="1.生产消息"></a>1.生产消息</h2><h3 id="1-1-创建生产者实例"><a href="#1-1-创建生产者实例" class="headerlink" title="1.1 创建生产者实例"></a>1.1 创建生产者实例</h3><p>生产者创建KafkaProducer实例，并配置必要的参数，如Kafka Broker的地址、序列化器（Serializer）等，然后与Kafka的Producer API建立连接</p><h3 id="1-2-构建消息"><a href="#1-2-构建消息" class="headerlink" title="1.2 构建消息"></a>1.2 构建消息</h3><p>生产者构建消息记录（ProducerRecord），包括目标主题、消息键（Key）和消息值（Value）</p><h3 id="1-3-发送消息"><a href="#1-3-发送消息" class="headerlink" title="1.3 发送消息"></a>1.3 发送消息</h3><p>生产者调用send()方法将消息发送到Kafka集群，发送方式有两种，即同步和异步</p><h3 id="1-4-选择分区"><a href="#1-4-选择分区" class="headerlink" title="1.4 选择分区"></a>1.4 选择分区</h3><p>Kafka根据消息键和分区策略（如轮询或哈希）选择目标分区，若消息键为空，则基于轮询策略将消息均匀分配到各个分区</p><h3 id="1-5-消息序列化"><a href="#1-5-消息序列化" class="headerlink" title="1.5 消息序列化"></a>1.5 消息序列化</h3><p>生产者将消息键和消息值序列化为字节数组，以便在网络上传输和存储</p><h3 id="1-6-消息发送"><a href="#1-6-消息发送" class="headerlink" title="1.6 消息发送"></a>1.6 消息发送</h3><p>生产者将序列化后的消息发送到目标分区的Leader Broker</p><h3 id="1-7-消息确认"><a href="#1-7-消息确认" class="headerlink" title="1.7 消息确认"></a>1.7 消息确认</h3><p>Leader Broker接收到消息后，将其写入本地日志文件，并向生产者发送确认（ACK）</p><h2 id="2-存储消息"><a href="#2-存储消息" class="headerlink" title="2.存储消息"></a>2.存储消息</h2><h3 id="2-1-接收消息"><a href="#2-1-接收消息" class="headerlink" title="2.1 接收消息"></a>2.1 接收消息</h3><p>kafka集群Leader Broker节点接收到生产者发送的消息后，将消息追加到对应主题的分区日志文件，属于顺序写磁盘，相比于随机写的吞吐量更高</p><h3 id="2-2-副本同步"><a href="#2-2-副本同步" class="headerlink" title="2.2 副本同步"></a>2.2 副本同步</h3><p>Leader Broker将消息同步到从副本（Follower）Broker，从副本将消息写入本地日志文件向Leader发送确认</p><h3 id="2-3-消息提交"><a href="#2-3-消息提交" class="headerlink" title="2.3 消息提交"></a>2.3 消息提交</h3><p>Leader Broker收到足够数量的从副本确认后，将消息标记为已提交（Committed），已提交的消息即对消费者可见</p><h3 id="2-4-日志管理"><a href="#2-4-日志管理" class="headerlink" title="2.4 日志管理"></a>2.4 日志管理</h3><p>Kafka集群定期清理过期的日志段（Log Segment），以释放磁盘空间。此外，日志压缩（Log Compaction）功能也能用于清理数据，以便保留每个键的最新消息</p><h2 id="3-消费消息"><a href="#3-消费消息" class="headerlink" title="3.消费消息"></a>3.消费消息</h2><h3 id="3-1-创建消费者实例"><a href="#3-1-创建消费者实例" class="headerlink" title="3.1 创建消费者实例"></a>3.1 创建消费者实例</h3><p>首先，消费者创建一个KafkaConsumer实例，并配置必要的参数，如Kafka Broker的地址、反序列化器（Deserializer）等</p><h3 id="3-2-订阅主题"><a href="#3-2-订阅主题" class="headerlink" title="3.2 订阅主题"></a>3.2 订阅主题</h3><p>消费者调用subscribe()方法订阅一个或多个主题，可以单独消费消息，也可以组成消费组（Consumer Group）共同消费</p><h3 id="3-3-拉取消息"><a href="#3-3-拉取消息" class="headerlink" title="3.3 拉取消息"></a>3.3 拉取消息</h3><p>消费者调用poll()方法从Kafka集群拉取消息，可根据需要设置拉取间隔和拉取数量</p><h3 id="3-4-消息反序列化"><a href="#3-4-消息反序列化" class="headerlink" title="3.4 消息反序列化"></a>3.4 消息反序列化</h3><p>消费者将消息键和消息值反序列化为原始数据类型，以便进行处理</p><h3 id="3-5-处理消息"><a href="#3-5-处理消息" class="headerlink" title="3.5 处理消息"></a>3.5 处理消息</h3><p>消费者处理拉取到的消息，执行业务逻辑</p><h3 id="3-6-提交偏移量"><a href="#3-6-提交偏移量" class="headerlink" title="3.6 提交偏移量"></a>3.6 提交偏移量</h3><p>消费者定期提交消费进度，即消费偏移量，以确保消息的准确处理和故障恢复，可选择自动提交或手动提交</p><h1 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h1><h2 id="1-消息系统"><a href="#1-消息系统" class="headerlink" title="1.消息系统"></a>1.消息系统</h2><p>消息系统通常应用与解耦生产者和消费者、缓存消息等场景</p><h2 id="2-监控指标"><a href="#2-监控指标" class="headerlink" title="2.监控指标"></a>2.监控指标</h2><p>即操作监控数据，涉及聚合来自分布式应用程序的统计信息，以产生操作数据的集中反馈</p><h2 id="3-日志聚合"><a href="#3-日志聚合" class="headerlink" title="3.日志聚合"></a>3.日志聚合</h2><p>用于跨组织从多个服务收集日志，并以标准格式提供给多个服务器</p><h2 id="4-流处理"><a href="#4-流处理" class="headerlink" title="4.流处理"></a>4.流处理</h2><p>流处理框架（Storm和Spark Streaming）从Kafka读取数据并对其进行处理，将处理后的数据写入Kafka后供用户和应用程序使用，如在线教育的课程与习题数据的计算与消费</p><hr><h1 id="1-配置Java环境"><a href="#1-配置Java环境" class="headerlink" title="1.配置Java环境"></a>1.配置Java环境</h1><h1 id="2-ZooKeeper集群部署"><a href="#2-ZooKeeper集群部署" class="headerlink" title="2.ZooKeeper集群部署"></a>2.ZooKeeper集群部署</h1><p>Kafka安装包实际上内置了ZooKeeper，但为了保障整个集群的性能，建议单独部署</p><h2 id="2-1-下载ZooKeeper"><a href="#2-1-下载ZooKeeper" class="headerlink" title="2.1 下载ZooKeeper"></a>2.1 下载ZooKeeper</h2><pre><code class="hljs">wget https://archive.apache.org/dist/zookeeper/zookeeper-3.6.4/apache-zookeeper-3.6.4-bin.tar.gztar -xzvf apache-zookeeper-3.6.4-bin.tar.gz &amp;&amp; sudo mv apache-zookeeper-3.6.4 /usr/local/zookeepersudo mkdir -p /usr/local/zookeeper/data</code></pre><h2 id="2-2-配置ZooKeeper集群"><a href="#2-2-配置ZooKeeper集群" class="headerlink" title="2.2 配置ZooKeeper集群"></a>2.2 配置ZooKeeper集群</h2><pre><code class="hljs">sudo cp /usr/local/zookeeper/conf/zoo_sample.cfg /usr/local/zookeeper/conf/zoo.cfgsudo vi /usr/local/zookeeper/conf/zoo.cfg# 设置Zookeeper服务端口clientPort = 2181# 设置Zookeeper服务器之间或与客户端之间的心跳时长，默认为2000，单位为毫秒，直接影响事件触发、回话超时和数据同步等tickTime = 2000# 设置集群Lollower节点和Leader节点初始连接的超时时长，其值为tickTime数，即n个tickTime时间，默认为10initLimit = 5# 设置集群Lollower节点和Leader节点初始化后请求与应答的超时时长，其值为tickTime数，即n个tickTime时间，默认为2syncLimit = 2# 设置Zookeeper本地数据目录，默认为/tmp/zookeeperdataDir = /usr/local/zookeeper/data# 设置Zookeeper集群节点server.1 = 172.16.100.101:2888:3888server.2 = 172.16.100.102:2888:3888server.3 = 172.16.100.103:2888:3888</code></pre><h2 id="2-3-配置节点myid文件"><a href="#2-3-配置节点myid文件" class="headerlink" title="2.3 配置节点myid文件"></a>2.3 配置节点myid文件</h2><pre><code class="hljs">echo 1 &gt; /usr/local/zookeeper/data/myidecho 2 &gt; /usr/local/zookeeper/data/myidecho 3 &gt; /usr/local/zookeeper/data/myid</code></pre><ul><li>注：myid文件是Zookeeper集群节点的唯一标识符，用于集群节点内部的互相识别、通信和协调，对应配置文件的server.x参数</li></ul><h2 id="2-4-创建启动脚本"><a href="#2-4-创建启动脚本" class="headerlink" title="2.4 创建启动脚本"></a>2.4 创建启动脚本</h2><pre><code class="hljs">sudo vi /lib/systemd/system/zookeeper.service[Unit]Description = Apache ZookeeperDocumentation = http://zookeeper.apache.org[Service]Type = forkingUser = rootGroup = rootExecStart = /usr/local/zookeeper/bin/zkServer.sh --config /usr/local/zookeeper/conf startExecStop = /usr/local/zookeeper/bin/zkServer.sh --config /usr/local/zookeeper/conf stopRestart = on-abnormal[Install]WantedBy = multi-user.target</code></pre><h2 id="2-5-启动集群"><a href="#2-5-启动集群" class="headerlink" title="2.5 启动集群"></a>2.5 启动集群</h2><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start zookeeper.servicesudo systemctl enable zookeeper.service</code></pre><h2 id="2-6-验证集群状态"><a href="#2-6-验证集群状态" class="headerlink" title="2.6 验证集群状态"></a>2.6 验证集群状态</h2><pre><code class="hljs">sudo /usr/local/zookeeper/bin/zkServer.sh status</code></pre><h1 id="3-Kafka集群部署"><a href="#3-Kafka集群部署" class="headerlink" title="3.Kafka集群部署"></a>3.Kafka集群部署</h1><h2 id="3-1-下载安装包"><a href="#3-1-下载安装包" class="headerlink" title="3.1 下载安装包"></a>3.1 下载安装包</h2><pre><code class="hljs">wget https://downloads.apache.org/kafka/3.6.2/kafka_2.12-3.6.2.tgztar -xzvf kafka_2.12-3.6.2.tgz &amp;&amp; sudo mv kafka_2.12-3.6.2 /usr/local/kafka</code></pre><h2 id="3-2-配置Kafka集群"><a href="#3-2-配置Kafka集群" class="headerlink" title="3.2 配置Kafka集群"></a>3.2 配置Kafka集群</h2><pre><code class="hljs">sudo cp /usr/local/kafka/config/server.properties /usr/local/kafka/config/server.properties.baksudo mkdir -p /usr/local/kafka/data sudo cp /usr/local/kafka/config/server.properties /usr/local/kafka/config/server.properties.baksudo vi /usr/local/kafka/config/server.properties# 设置监听端口port = 9092# 设置Broker节点全局唯一ID，不能重复，类似于Zookeeper的myidbroker.id = 1# 设置数据文件存储路径，默认为/tmplog.dirs = /usr/local/kafka/data# 设置Topic默认分区数，默认为1，建议对业务压测之后根据实际情况进行设置num.partitions = 3# 设置Zookeeper集群地址zookeeper.connect = 172.100.100.101:2181,172.100.100.102:2181,172.100.100.103:2181</code></pre><h2 id="3-3-创建启动脚本"><a href="#3-3-创建启动脚本" class="headerlink" title="3.3 创建启动脚本"></a>3.3 创建启动脚本</h2><pre><code class="hljs">sudo vi /lib/systemd/system/kafka.serviceDescription = Apache KafkaDocumentation = https://kafka.apache.orgAfter = network.target zookeeper.service[Service]Type = simpleUser = rootGroup = rootExecStart = /usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server.propertiesExecStop = /usr/local/kafka/bin/kafka-server-stop.shRestart = on-failure[Install]WantedBy = multi-user.target</code></pre><h2 id="3-4-启动Kafka集群"><a href="#3-4-启动Kafka集群" class="headerlink" title="3.4 启动Kafka集群"></a>3.4 启动Kafka集群</h2><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start kafka.servicesudo systemctl enable kafka.service</code></pre><h2 id="3-5-验证Kafka集群"><a href="#3-5-验证Kafka集群" class="headerlink" title="3.5 验证Kafka集群"></a>3.5 验证Kafka集群</h2><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/shenaohuoli132/article/details/137600814">https://blog.csdn.net/shenaohuoli132/article/details/137600814</a></li><li><a href="https://blog.csdn.net/wudidahuanggua/article/details/127086186">https://blog.csdn.net/wudidahuanggua/article/details/127086186</a></li><li><a href="https://blog.csdn.net/Blue_Pepsi_Cola/article/details/137432969">https://blog.csdn.net/Blue_Pepsi_Cola/article/details/137432969</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kafka</tag>
      
      <tag>MQ</tag>
      
      <tag>中间件</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes集群管理工具Rancher的安装与配置</title>
    <link href="/linux/Rancher/"/>
    <url>/linux/Rancher/</url>
    
    <content type="html"><![CDATA[<p>Rancher，开源的企业级一栈式多集群Kubernetes管理平台，专为依赖于容器技术的企业而打造，实现了Kubernetes集群在混合云和本地数据中心的集中部署与管理，简化了Kubernetes集群的运维与部署流程，从而加速企业数字化转型。Rancher基于集群的身份验证和角色访问控制（RBAC），将分散的多集群访问集中于一个平台，支持托管多个公有云服务服务商的集群、自动创建节点并安装的Kubernetes集群，或导入任何已经存在的Kubernetes集群。此外，Rancher还集成了几乎所有的云原生工具，如监控告警、日志分析、Helm应用、CI&#x2F;CD、服务网格、安全扫描等等，是Kubernetes多集群管理的利器</p><h1 id="1-安装Rancher"><a href="#1-安装Rancher" class="headerlink" title="1.安装Rancher"></a>1.安装Rancher</h1><pre><code class="hljs">sudo docker run -d -it \-p 80:80 -p 443:443 \--privileged --name=rancher \rancher/rancher</code></pre><h1 id="2-查看管理员密码"><a href="#2-查看管理员密码" class="headerlink" title="2.查看管理员密码"></a>2.查看管理员密码</h1><pre><code class="hljs">sudo docker logs rancher | grep &quot;Bootstrap Password:&quot;</code></pre><h1 id="3-登录Rancher平台"><a href="#3-登录Rancher平台" class="headerlink" title="3.登录Rancher平台"></a>3.登录Rancher平台</h1><pre><code class="hljs">https://ip</code></pre><h1 id="4-导入集群"><a href="#4-导入集群" class="headerlink" title="4.导入集群"></a>4.导入集群</h1><p><img src="/img/wiki/kubernetes/rancher001.jpg" alt="导入集群"></p><p><img src="/img/wiki/kubernetes/rancher002.jpg" alt="导入集群"></p><p><img src="/img/wiki/kubernetes/rancher003.jpg" alt="注册集群"></p><h1 id="5-验证集群状态"><a href="#5-验证集群状态" class="headerlink" title="5.验证集群状态"></a>5.验证集群状态</h1><p><img src="/img/wiki/kubernetes/rancher004.jpg" alt="集群状态验证"></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/MCB134/article/details/139611223">https://blog.csdn.net/MCB134/article/details/139611223</a></li><li><a href="https://blog.csdn.net/qq_35995514/article/details/130197353">https://blog.csdn.net/qq_35995514/article/details/130197353</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Redis缓存数据管理策略</title>
    <link href="/linux/RedisRefreshPolicy/"/>
    <url>/linux/RedisRefreshPolicy/</url>
    
    <content type="html"><![CDATA[<p>Redis通常用于高并发场景的缓存层，以减轻频繁数据库读写操所造成的非常耗时的磁盘IO压力，工作机制为：将即时性和数据一致性要求不高而访问量又特别巨大的热点数据（如电商系统的商品列表等）加载到处理速度更快的Redis内存数据库，直接供应用程序调用而不必再经过需要磁盘IO的数据库。若所调用的数据不存在，再到数据库去查询，并将之写入到Redis。同时，数据库有更新操作也应及时更新到Redis，以免造成数据不一致的问题。这样，就减少了磁盘IO的操作，提高了系统的整体性能与响应速度，由此改善了用户体验</p><p>Redis内存数据库的特性决定了不是所有数据都能进行缓存，所以在缓存层设计之初就要确认缓存数据及其读写、更新方式，以保障缓存与数据源的一致性。有鉴于此，Redis引入了一系列缓存策略，以便于快速有效地管理缓存数据</p><h1 id="1-缓存数据读写策略"><a href="#1-缓存数据读写策略" class="headerlink" title="1.缓存数据读写策略"></a>1.缓存数据读写策略</h1><h2 id="1-1-Cache-Aside"><a href="#1-1-Cache-Aside" class="headerlink" title="1.1 Cache-Aside"></a>1.1 Cache-Aside</h2><p>Cache-Aside，即旁路缓存，即将缓存作为独立的存储层，工作机制为：应用程序读取数据先查询Redis，若未命中则从数据库读取，再将数据写入Redis；写操作则是先更新数据库，再依据设置的策略更新或失效相应的缓存数据，以保持数据的一致性。该策略实现简单，灵活性高，数据一致性高，应用较为广泛，适用于读请求较多的场景。但会增加业务逻辑的复杂性，因为缓存数据的更新和失效都由其负责</p><h2 id="1-2-Read-x2F-Write-Through"><a href="#1-2-Read-x2F-Write-Through" class="headerlink" title="1.2 Read&#x2F;Write Through"></a>1.2 Read&#x2F;Write Through</h2><p>Read&#x2F;Write Through，即读写穿透，即将缓存与数据库紧密结合，读写操作都通过缓存层操作，工作机制为：应用程序读取数据只通过Redis，若未命中则由Redis负责从数据库读取数据并进行缓存；写入操作由应用程序写入Redis，再由Redis同步更新到数据库。该策略对应用层透明，无需关注缓存细节，数据一致性高。效率较高。但实现复杂，还会影响Redis性能，应用较少</p><h2 id="1-3-Write-Behind-Caching"><a href="#1-3-Write-Behind-Caching" class="headerlink" title="1.3 Write-Behind Caching"></a>1.3 Write-Behind Caching</h2><p>Write-Behind Caching，即异步写入，类似于读写穿透策略，都是由缓存复杂缓存层和数据库的读写，不同之处在于该策略不是同步更新缓存和数据库，而是只更新缓存，以异步批量方式更新数据库，工作机制为：读取机制与读写穿透策略一致，写操作则写入Redis后立即返回给应用程序，此后再定期或特定条件触发下将缓存数据批量同步到数据库，再依据设置的策略更新或失效相应的缓存数据，以保持数据的一致性。该策略写操作性能高，但数据丢失风险高，数据一致性较弱，适用于数据经常变化且数据一致性要求不高的场景，如浏览量、点赞量等</p><h1 id="2-缓存数据生成策略"><a href="#2-缓存数据生成策略" class="headerlink" title="2.缓存数据生成策略"></a>2.缓存数据生成策略</h1><h2 id="2-1-预加载"><a href="#2-1-预加载" class="headerlink" title="2.1 预加载"></a>2.1 预加载</h2><p>缓存预加载，又称缓存预热，是程序启动或缓存失效之后主动将热点数据加载到缓存的策略，即请求到达之前热点数据已存在于缓存之中，缺点就是需要提前预测热点数据，若是预测不准则会造成资源浪费</p><h2 id="2-2-定期生成"><a href="#2-2-定期生成" class="headerlink" title="2.2 定期生成"></a>2.2 定期生成</h2><p>定期生成策略即每隔⼀定的周期 (如每日&#x2F;周&#x2F;⽉) 对于访问的数据频次进⾏统计（如通过脚本的定时任务执行触发），从中挑选出访问频次最⾼的前N%数据加载到缓存以供调用，如搜索引擎的热点词数据。该策略实现简单，过程可控，排查问题也较为方便，但实时性不够，如突发性事件，很可能出现一个周期内统计不到而被忽略，下个周期又过于集中而造成冲击</p><h2 id="2-3-实时生成"><a href="#2-3-实时生成" class="headerlink" title="2.3 实时生成"></a>2.3 实时生成</h2><p>实时生成策略即先设置缓存容量的上限（通过Redis配置⽂件的maxmemory参数设定），再经过一段时间的数据库交互而达到动态平衡状态，Redis的key逐渐就变成了热点数据。该策略需要考虑项目的综合情况，如项目规模和系统资源、Redis的持久化策略、数据淘汰策略及内存交换等因素，实时监控Redis性能以便根据实际需求进行调整和优化</p><h1 id="3-缓存数据过期策略"><a href="#3-缓存数据过期策略" class="headerlink" title="3.缓存数据过期策略"></a>3.缓存数据过期策略</h1><p>Redis所存储的缓存数据通常都有其有效期，以确保缓存数据保持在最新状态，避免提供过时的数据，也消除了数据持续积累造成的内存耗尽。Redis过期策略即用于管理与清除过期数据，以释放内存、优化缓存管理及避免数据不一致问题，使得保持系统在最佳状态，确保系统的高效运行和稳定性。Redis缓存数据失效策略主要分为三种，即定时删除、惰性删除和内存淘汰</p><h2 id="3-1-定时清除"><a href="#3-1-定时清除" class="headerlink" title="3.1 定时清除"></a>3.1 定时清除</h2><p>定时删除，Expired by Time，Redis最为常用的过期策略，即为每个Key创建用于监控其过期时间定时器，并于预定过期时间自动删除。该策略及时性和数据一致性较强，但大量失效数据的清除操作将会加剧CPU负载，每个key的附加定时器也会增加资源的消耗</p><h2 id="3-2-惰性清除"><a href="#3-2-惰性清除" class="headerlink" title="3.2 惰性清除"></a>3.2 惰性清除</h2><p>惰性删除，Expired by Access，按需清除，即Redis仅在访问key时对其进行过期状态检查，若已过期则进行清除。该策略实现简单，易于理解和维护，也不会影响性能和浪费资源，但存在一定的延迟，过期数据存在堆积的风险，从而影响数据一致性和用户体验，可配合定期清除策略使用</p><h2 id="3-3-定期清除"><a href="#3-3-定期清除" class="headerlink" title="3.3 定期清除"></a>3.3 定期清除</h2><p>定期删除，Expired by Sampling，即定期循环扫描一部分设置了过期策略的Key以便于及时进行清除，还可设置过期数据的占比，并依此为依据进行循环扫描与清除。该策略灵活性较强，也不存在大量删除操作带来的性能压力，但及时性不好，很可能导致部分过期数据由于未被扫描到而产生堆积，从而影响数据一致性，可配合惰性清除策略使用</p><h2 id="4-缓存数据内存淘汰策略"><a href="#4-缓存数据内存淘汰策略" class="headerlink" title="4.缓存数据内存淘汰策略"></a>4.缓存数据内存淘汰策略</h2><p>Redis服务器内存不足以容纳新的数据时，将会根据预设的规则策略删除一些数据以释放空间。这就是内存淘汰策略，建议依据实际应用场景和需求进行选择</p><h2 id="4-1-LRU"><a href="#4-1-LRU" class="headerlink" title="4.1 LRU"></a>4.1 LRU</h2><p>LRU，Least Recently Used，最近最少使用，即从数据集中挑选最近最少被使用的数据进行淘汰，判断依据是访问频率和时间</p><h2 id="4-2-LFU"><a href="#4-2-LFU" class="headerlink" title="4.2 LFU"></a>4.2 LFU</h2><p>LFU，Least Frequently Used，最近最不常使用，类似于LRU，即从数据集中挑选最不经常使用的数据进行淘汰，只通过访问频率进行判断</p><h2 id="4-3-TTL"><a href="#4-3-TTL" class="headerlink" title="4.3 TTL"></a>4.3 TTL</h2><p>TTL，Time To Live，生存时间，即从已设置过期策略的数据集中选出剩余生存时间最短的数据进行淘汰</p><h2 id="4-4-Random"><a href="#4-4-Random" class="headerlink" title="4.4 Random"></a>4.4 Random</h2><p>Random，随机清除策略，分为两种，allkeys-random和volatile-random，前者从全部数据集随机淘汰，后者则从已设置过期策略的数据集随机淘汰</p><h1 id="5-缓存数据失效场景"><a href="#5-缓存数据失效场景" class="headerlink" title="5.缓存数据失效场景"></a>5.缓存数据失效场景</h1><p>Redis实际生产场景中，巨大的并发量和不合理的管理策略都可能引发缓存失效的问题，从而使得大量请求回源数据库，导致数据库服务器资源被占满，甚至引发数据库宕机的严重故障。缓存数据失效的具体场景分为三种，即缓存穿透、雪崩和缓存击穿</p><h2 id="5-1-缓存穿透"><a href="#5-1-缓存穿透" class="headerlink" title="5.1 缓存穿透"></a>5.1 缓存穿透</h2><p>缓存穿透，即客户端请求到达时Redis并没有所要访问的数据（未命中），继续访问数据库读取数据（回源）时也没有数据，后续的这种请求也将重复此过程，将会给数据库带来非常大的压力。该场景的根本原因是客户端访问了并不存在的资源，解决方案为：</p><ul><li><p>缓存空值，即将不存在的资源也写入Redis，但其值设为null（key&#x3D;-1000，value&#x3D;null）。这样就由Redis直接返回给客户端一个无效的空值，避免了回源到数据库造成的压力，实现简单，但将会导致额外的资源消耗</p></li><li><p>权限设置，即通过对Redis的实时监控对命中率下降问题进行排查，然后对访问数据进行权限控制，如设置黑名单、接口校验、访问拦截等等，直接不允许这些请求到达缓存和数据库</p></li><li><p>设置布隆过滤器，即以BitMap作为布隆过滤器，将目前所有可以访问到的资源通过简单的映射关系（哈希计算）放入其中，请求先经过布隆过滤器的判断，若存在则放行，否则直接拦截，但实现上较为复杂，还存在误判的可能</p></li></ul><h2 id="5-2-缓存雪崩"><a href="#5-2-缓存雪崩" class="headerlink" title="5.2 缓存雪崩"></a>5.2 缓存雪崩</h2><p>缓存雪崩，即大量缓存数据同一时间集中过期或缓存服务器宕机，大量请求回源数据库，给数据库服务器造成巨大压力，甚至引发数据库宕机的灾难。该场景的根本原因是缓存数据集中过期，解决方案为：</p><ul><li><p>构建高可用缓存集群，部署Redis Cluster或哨兵模式集群，确保缓存系统的高可用性，并进行实时监控，以提高缓存层的稳定性和可用性，但将会增加系统复杂度和维护成本</p></li><li><p>构建多级缓存，Redis缓存层之上再增加一层本地缓存，如Nginx&#x2F;Guava&#x2F;Spring&#x2F;JVM缓存，不同层使用不同的缓存，即请求先访问本地缓存，若未命中再访问Redis，但将会增加系统复杂度，还可能带来数据一致性问题</p></li><li><p>均匀分布过期时间，即设置缓存数据过期时间时加入一个随机值，避免大量缓存同时过期，简单有效，易于实现，但可能会发生缓存不一致的问题</p></li><li><p>配置熔断降级机制，即检测到缓存服务不可用时，暂时屏蔽部分非核心功能，只提供最基础服务，类似于Hystrix熔断框架，防止核心功能的崩溃，但会暂时降低用户体验</p></li></ul><h2 id="5-3-缓存击穿"><a href="#5-3-缓存击穿" class="headerlink" title="5.3 缓存击穿"></a>5.3 缓存击穿</h2><p>缓存击穿，即特定的高频热点Key在缓存过期的一刻同时有大量的请求到达，发现缓存过期之后同时回源数据库，导致数据库瞬间压力剧增，甚至引发数据库宕机。该场景根本原因是热点数据的过期，解决方案为：</p><ul><li><p>设置互斥锁，即只有一个到达Redis且未命中的请求获取互斥锁，其余的请求都处于等待状态，直到互斥锁的请求回源数据库并将数据写入Redis之后，再从Redis读取数据。简单有效，避免了大量请求回源数据库的压力，但可能会导致某些请求的等待时间较长</p></li><li><p>设置逻辑过期，即热点数据不设置过期时间，而是通过额外的业务逻辑定期异步更新，但实现复杂，且数据一致性也不强</p></li></ul><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/chenjiap/article/details/131630981">https://blog.csdn.net/chenjiap/article/details/131630981</a></li><li><a href="https://blog.csdn.net/CYK_byte/article/details/133029259">https://blog.csdn.net/CYK_byte/article/details/133029259</a></li><li><a href="https://blog.csdn.net/unravel_tom/article/details/140105493">https://blog.csdn.net/unravel_tom/article/details/140105493</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Redis</tag>
      
      <tag>中间件</tag>
      
      <tag>NoSQL</tag>
      
      <tag>缓存</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Redis数据持久化详解</title>
    <link href="/linux/Redis-RDB-AOF/"/>
    <url>/linux/Redis-RDB-AOF/</url>
    
    <content type="html"><![CDATA[<p>Redis是基于内存的NoSQL数据库，内存操作带来的优势是性能极高，但缺点就是服务关闭或主机宕机之后数据将会丢失。Redis持久化机制即是将内存数据写入到硬盘保存到文件，服务或主机重启之后再进行加载并将数据恢复到内存，从而避免数据的意外丢失。Redis数据持久化实现技术分为两种，即RDB和AOF</p><h1 id="1-RDB方式"><a href="#1-RDB方式" class="headerlink" title="1.RDB方式"></a>1.RDB方式</h1><p>RDB，Redis DataBase，内存快照，默认开启的持久化方式，即将内存某一时刻的数据快照全量写入到指定的rdb文件，服务或主机重启自动读取该文件，并将数据加载到内存，恢复到之前的数据库状态</p><h2 id="1-1-执行方式"><a href="#1-1-执行方式" class="headerlink" title="1.1 执行方式"></a>1.1 执行方式</h2><p>RDB持久化的执行方式有三种，即手动save命令、手动bgsave命令和自动触发命令</p><h3 id="1-1-1-save命令"><a href="#1-1-1-save命令" class="headerlink" title="1.1.1 save命令"></a>1.1.1 save命令</h3><p>redis-cli客户端执行save命令即可立即进行一次持久化保存，期间会阻塞redis-server进程，即在此期间Redis不能处理任何读写请求，无法对外提供服务，直到持久化过程完毕</p><h3 id="1-1-2-bgsave命令"><a href="#1-1-2-bgsave命令" class="headerlink" title="1.1.2 bgsave命令"></a>1.1.2 bgsave命令</h3><p>redis-cli客户端执行bgsave（background save）命令即可立即进行一次持久化保存，不同于save命令的是，bgsave命令使主进程redis-server生成一个子进程，由该子进程在后台完成持久化过程，并不会阻塞主进程对客户端读写请求的处理</p><h3 id="1-1-3-自动触发"><a href="#1-1-3-自动触发" class="headerlink" title="1.1.3 自动触发"></a>1.1.3 自动触发</h3><p>自动触发本质仍是bgsave命令的执行，但通过配置文件的相应设置，由Redis根据设置信息自动调用bgsave命令执行持久化过程</p><h2 id="1-2-工作原理"><a href="#1-2-工作原理" class="headerlink" title="1.2 工作原理"></a>1.2 工作原理</h2><p>Redis基于Linux系统写时复制（COW，即Copy On Write）机制实现RDB持久化，即Redis主进程redis-server由fork出来的子进程bgsave以异步方式，将继承自父进程的内存全量数据copy到磁盘的RDB临时文件，copy结束后再将该文件重命名为dump.rdb以替换掉原来的同名文件。在此过程中，redis-server主进程仍然可以处理写请求，并将数据修改的物理块基于写时复制技术copy出一个副本写入到不与子进程bgsave共享的内存区域，直到子进程的内存全量数据copy结束后再将此副本数据copy到RDB临时文件</p><h2 id="1-3-工作流程"><a href="#1-3-工作流程" class="headerlink" title="1.3 工作流程"></a>1.3 工作流程</h2><ul><li><p>1.redis客户端执行bgsave命令或自动触发bgsave命令</p></li><li><p>2.主进程判断当前是否已经存在正在执行的子进程，若存在则直接返回</p></li><li><p>3.若当前不存在正在执行的子进程，则fork一个新的子进程进行持久化数据，fork过程是阻塞的，fork操作完成后主进程即可执行其他操作</p></li><li><p>4.子进程将数据写入到临时的rdb文件，直到快照数据写入完成后再替换旧的rdb文件</p></li><li><p>5.同时发送信号给主进程，通知主进程rdb持久化完成，主进程完成相关的统计信息的更新</p></li></ul><h2 id="1-4-相关配置"><a href="#1-4-相关配置" class="headerlink" title="1.4 相关配置"></a>1.4 相关配置</h2><pre><code class="hljs"># 设置RDB和AOF持久化文件的存储目录，默认为Redis安装根目录dir &quot;/usr/local/redis/data&quot;# 设置RDB持久化文件名，默认为dump.rdbdbfilename &quot;dump.rdb&quot;# 设置RDB内存快照周期性出发的条件，为空表示关闭RDB持久化，格式为save &lt;seconds&gt; &lt;changes&gt;，表示指定时间内发生了指定次数的写操作，默认值为3600 1 300 100 60 10000，即3600/300/60秒内发生了1/100/10000次写操作save # 设置是否启用字符串LZF压缩算法，默认为yes，虽然会消耗系统资源，降低性能，但能大幅降低文件大小，方便保存到磁盘，以及加速主从集群中从节点的数据同步rdbcompression yes# 设置是否启动数据校验功能，默认为yes，CRC64校验和置于文件末尾，用于抵抗RDB文件的损坏，但性能会受到影响（约10%），追求极致性能的场景可关闭该功能rdbchecksum yes# 设置主从复制架构是否删除用于同步的从机上的RDB文件，默认为no，但只有从机的RDB和AOF持久化功能都未开启时才能生效rdb-del-sync-files no# 设置加载RDB文件或进行持久化时是否开启对zipList 、 listPack等数据的全面安全检测，默认为clients，用于降低命令处理时发生系统崩溃的可能，影响性能，可设置值为no，不检测；yes，总是检测，clients，只有当客户端连接时检测sanitize-dump-payload# 设置快照操作异常（如操作系统用户权限不够、磁盘空间写满等等）时主进程是否禁止写操作，默认为yes，即运行错误的告警，表示数据没有正确地保存到磁盘，故障被解决则将自动允许再次写入stop-write-on-bgsave-error yes</code></pre><h2 id="1-5-适用场景"><a href="#1-5-适用场景" class="headerlink" title="1.5 适用场景"></a>1.5 适用场景</h2><p>RDB持久化方式数据压缩与恢复速度快，压缩后的文件远小于内存数据的体积，但由于是非实时备份（即某一时刻的内存快照，快照中的数据将会少与或等于内存数据），所以安全性并不高，可能会导致数据小部分丢失，且fork子进程这种重量级操作频繁执行成本较高，导致实时性不够，无法做到秒级的持久化，降低整体性能。此外，RDB文件时二进制文件，没有可读性。所以，RDB持久化方式适用于数据安全性要求不高的环境，不适合实时持久化场景</p><h1 id="2-AOF方式"><a href="#2-AOF方式" class="headerlink" title="2.AOF方式"></a>2.AOF方式</h1><p>AOF，Append Only File，即将Redis每次写操作都以日志的形式追加记录到AOF文件，服务或内存重启加载该文件并将写操作重新执行，依此恢复到之前的内存数据状态</p><h2 id="2-1-工作原理"><a href="#2-1-工作原理" class="headerlink" title="2.1 工作原理"></a>2.1 工作原理</h2><p>Redis基于写后日志的方式记录AOF日志，即先执行命令将数据写入内存再记录日志，分为三步，即命令追加（append）、文件写入（write）及同步（sync），具体过程为：Redis执行完命令之后，以通讯协议格式将被执行的写命令以纯文本的方式追加到aof_buf缓冲区，最后按照设置的写回策略完成AOF日志的落盘。此外，随着AOF文件越来越大，必然会导致性能的降低，Redis为此引入了Rewrite机制对其进行压缩，即由主线程redis-server所fork的子线程bgrewriteaof对现有AOF文件进行计算（如剔除原文件的读命令、无效命令及过期数据的命令，多条更新命令合并写入），并将计算结果写入临时文件，最后再将该临时文件重命名为原AOF文件以覆盖原文件。在此过程中，主线程仍然可以对外提供读写服务，但将会消耗大量的系统资源，性能会有所下降，所以通常是通过配置文件设置一些触发条件（通常是AOF文件增长到指定的百分比），以规避对性能的影响</p><h2 id="2-2-工作流程"><a href="#2-2-工作流程" class="headerlink" title="2.2 工作流程"></a>2.2 工作流程</h2><ul><li><p>1.Redis将写命令按照通讯协议格式暂时添加到AOF缓冲区aof_buf</p></li><li><p>2.根据设置的数据同步策略，当同步条件满足时将缓冲区数据一次性写入磁盘的AOF文件，以减少磁盘IO次数，提高性能</p></li><li><p>3.当磁盘的AOF文件大小达到了rewrite条件时，redis-server主进程将会fork子进程bgrewriteaof，由该子进程完成rewrite过程</p></li><li><p>4.子进程bgrewriteaof先对该磁盘AOF文件进行rewrite计算，并将计算结果写入到临时文件，全部写入完毕后再重命名该临时文件为磁盘文件的原名称，覆盖原文件</p></li><li><p>5.若rewrite过程中又有写操作命令追加，那么这些数据会暂时写入aof_rewrite_buf缓冲区，直到全部rewrite计算结果写入临时文件后，再将aof_rewrite_buf缓冲区数据写入临时文件，最后为磁盘文件的原名称，覆盖原文件</p></li></ul><h2 id="2-3-相关配置"><a href="#2-3-相关配置" class="headerlink" title="2.3 相关配置"></a>2.3 相关配置</h2><pre><code class="hljs"># 设置是否启用AOF持久化，默认为noappendonly yes# 设置AOF持久化文件存储目录，默认为Redis安装根目录appenddirname# 设置AOF持久化文件名，默认为appendonly.aofappendfilename appendonly.aof# 设置AOF文件同步策略，默认为everysec，可取值为：always，命令执行完毕立即调用fsync()系统函数将缓冲区数据同步到磁盘文件，效率较低，但相对比较安全，不会丢失太多数据，最多即是刚刚执行过的写操作在尚未同步时出现宕机或重启造成数据丢失；no，命令执行完毕将日志写入aof_buf缓冲区后由操作系统调度磁盘同步，Linux系统默认同步周期为30秒，效率较高；everysec，命令执行完毕将日志写入缓冲区后每秒调用fsync()函数完成数据同步，兼顾性能与安全，折中方案appendfsync everysec# 设置数据重写期间是否不同步数据，默认为no，表示同步，数据量大时将会阻塞主进程对外服务，即存在延迟；yes表示不同步，相当于fsync设为no，存在30秒数据丢失的风险。建议高并发的写操作设为yes，读操作设为nono-appendfsync-on-rewrite no# 设置数据重写期间是否开启缓存控制，即rewrite计算结果的缓存累计到一定数量（默认为4M）时一次性写入临时文件，完成数据同步，以免由于单次刷盘量过大而引发长时间阻塞aof-rewrite-incremental-fsync yes# 设置AOF文件的数据不完整（突然宕机的情况最后的数据可能并没有完全同步）时能否启动Redis，yes表示将不完整的数据截断删除后加载，不影响Redis启动；no表示无法启动Redisaof-load-turncated yes# 设置是否开启AOF文件增加时间戳的显示功能，默认为no，不开启，虽然方便于按照时间对数据进行恢复，但可能会与当前AOF解析器不兼容aof-timestamp-enabled no</code></pre><h2 id="2-4-适用场景"><a href="#2-4-适用场景" class="headerlink" title="2.4 适用场景"></a>2.4 适用场景</h2><p>AOF持久化方式数据安全性高，AOF文件可读性高，在了解其结构的情况下可以手动修改或补全，但文件较大，数据恢复速度慢，日志写操作对性能有影响，官方并不推荐纯AOF方式，默认不开启</p><h1 id="3-混合方式"><a href="#3-混合方式" class="headerlink" title="3.混合方式"></a>3.混合方式</h1><p>Redis官方推荐RDB与AOF混合式持久化，以兼容两者的优点，即执行AOF重写时，将当前内存数据以RDB快照方式写入AOF文件的开头，之后的数据再以AOF格式追加到文件的末尾。数据恢复过程与AOF方式一致，只需将appendonly.aof文件存放到Redis的根目录即可自动加载并恢复。这样，AOF文件的开头部分是记录了某一时刻数据集的RDB快照，加快了恢复速度；后续部分是记录了数据集增量修改的AOF日志，降低了数据丢失的风险，缩小了文件体积（因为AOF日志并不是全量日志，而是自持久化开始到持久化结束这段时间发生的增量 AOF日志）。但混合持久化方式适合Redis 4.0之后的版本，此外由于AOF文件添加了RDB格式内容，可读性也随之变差</p><h2 id="3-1-工作流程"><a href="#3-1-工作流程" class="headerlink" title="3.1 工作流程"></a>3.1 工作流程</h2><ul><li>1.判断是否开启AOF持久化，若已开启则继续执行后续流程，否则执行加载RDB文件的流程</li><li>2.判断appendonly.aof文件是否存在，即是否已开启混合持久化方式，文件存在则执行后续流程</li><li>3.判断AOF文件开头是否为RDB格式, 若是则先加载RDB内容再加载剩余的AOF内容，否则直接以AOF格式加载整个文件</li></ul><h2 id="3-2-相关配置"><a href="#3-2-相关配置" class="headerlink" title="3.2 相关配置"></a>3.2 相关配置</h2><pre><code class="hljs"># 设置是否开启混合持久化，默认为yesaof-use-rdb-preamble yes</code></pre><h1 id="4-最佳实践"><a href="#4-最佳实践" class="headerlink" title="4.最佳实践"></a>4.最佳实践</h1><p>Redis官方建议基于实际业务出发，若数据丢失不敏感的场景则可关闭持久化，如仅用于缓存的环境。主从复制架构则建议负责写操作的主节点不开启持久化，而开启负责读操作的从节点持久化功能，以最大限度地降低持久化所损耗的IO性能。此外，由于Redis大部分时间都在做读写操作，所以更大的内存和更快的磁盘对其性能的提高非常有帮助，而对CPU的要求并不高</p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.modb.pro/db/126254">https://www.modb.pro/db/126254</a></li><li><a href="https://www.cnblogs.com/sxy-blog/p/17971886">https://www.cnblogs.com/sxy-blog/p/17971886</a></li><li><a href="https://blog.csdn.net/weixin_67596609/article/details/140417657">https://blog.csdn.net/weixin_67596609/article/details/140417657</a></li><li><a href="https://blog.csdn.net/weixin_43412762/article/details/134795648">https://blog.csdn.net/weixin_43412762/article/details/134795648</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Redis</tag>
      
      <tag>中间件</tag>
      
      <tag>NoSQL</tag>
      
      <tag>缓存</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Redis部署分片式集群</title>
    <link href="/linux/RedisCluster/"/>
    <url>/linux/RedisCluster/</url>
    
    <content type="html"><![CDATA[<p>Redis Cluster是Redis官方推出的无中心架构的分布式集群解决方案，集群所有节点都用于存储数据和集群状态，其具体机制为：基于主从复制模式将数据均匀分片到集群节点用于管理集群数据和集群状态的哈希槽，每个节点负责一部分数据，其中主节点负责读写操作和集群状态维护，从节点复制主节点的数据与状态信息，并承担读操作。节点之间通过gossip协议交换状态信息，失去连接或不可达将被标记为不可用，并从可用节点选举出新的主节点</p><h1 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h1><p>Redis Cluster基于虚拟哈希槽分区技术将数据划分为16384个Slots（槽），并将所有的Key依据哈希函数映射到这些槽位进行存储，即由集群每个节点负责维护一部分槽位信息及其所映射的K&#x2F;V数据，客户端请求到达后根据槽位配置信息直接定位到目标节点进行读写操作。Slots是Redis Cluster管理数据的基本单位，集群伸缩就是槽和数据在节点之间的移动，即扩容或缩容槽需要重新分配，数据也需要重新迁移，但服务不需要下线。由此，集群的扩展性大大增强，支持节点动态增删和数据分布动态调整，高可用性也有保障，且故障自动failover。但数据异步复制的特性决定了数据强一致性无法得到保证</p><h1 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h1><ul><li>172.100.100.180 master01&#x2F;slave01</li><li>172.100.100.181 master02&#x2F;slave02</li><li>172.100.100.183 master03&#x2F;slave03</li></ul><h1 id="1-安装redis"><a href="#1-安装redis" class="headerlink" title="1.安装redis"></a>1.安装redis</h1><h1 id="2-部署主节点"><a href="#2-部署主节点" class="headerlink" title="2.部署主节点"></a>2.部署主节点</h1><h2 id="2-1-创建配置文件"><a href="#2-1-创建配置文件" class="headerlink" title="2.1 创建配置文件"></a>2.1 创建配置文件</h2><pre><code class="hljs">sudo vi /etc/redis/master.confport 6379bind 0.0.0.0daemonize yesrequirepass redismasterauth redisprotected-mode yesmaxmemory 1024Mdbfilename master.rdbdir /usr/local/redis/datapidfile /var/run/redis-master.pidlogfile &quot;/usr/local/redis/logs/master.log&quot;# 设置启用集群功能cluster-enabled yes# 设置集群配置文件，由节点自动维护，用于存储集群节点信息及其状态信息cluster-config-file node-master.conf# 设置节点失联时间，即超时之后节点即从集群移除，若为主节点则触发主从切换cluster-node-timeout 15000</code></pre><ul><li>注：三个主节点配置文件相同</li></ul><h2 id="2-2-创建启动脚本"><a href="#2-2-创建启动脚本" class="headerlink" title="2.2 创建启动脚本"></a>2.2 创建启动脚本</h2><pre><code class="hljs">sudo vi /lib/systemd/system/redis_master.service[Unit]Description=RedisAfter=network.target[Service]Type=forkingExecStart=/usr/local/redis/bin/redis-server /etc/redis/master.confExecReload=/usr/local/redis/bin/redis-server -s reloadExecStop=/usr/local/redis/bin/redis-server -s stopPrivateTmp=true[Install]WantedBy=multi-user.target</code></pre><h2 id="2-3-启动主节点"><a href="#2-3-启动主节点" class="headerlink" title="2.3 启动主节点"></a>2.3 启动主节点</h2><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start redis_master.servicesudo systemctl enable redis_master.service</code></pre><h1 id="3-配置从节点"><a href="#3-配置从节点" class="headerlink" title="3.配置从节点"></a>3.配置从节点</h1><h2 id="3-1-创建配置文件"><a href="#3-1-创建配置文件" class="headerlink" title="3.1 创建配置文件"></a>3.1 创建配置文件</h2><pre><code class="hljs">sudo vi /etc/redis/slave.confport 6380bind 0.0.0.0daemonize yesrequirepass redismasterauth redisprotected-mode yesmaxmemory 1024Mdir /usr/local/redis/datapidfile /var/run/redis-slave.pidlogfile &quot;/usr/local/redis/logs/slave.log&quot;# 设置启用集群功能cluster-enabled yes# 设置集群配置文件，由节点自动维护，用于存储集群节点信息及其状态信息cluster-config-file node-slave.conf# 设置节点失联时间，即超时之后节点即从集群移除cluster-node-timeout 15000</code></pre><ul><li>注：三个从节点配置文件相同</li></ul><h2 id="2-2-创建启动脚本-1"><a href="#2-2-创建启动脚本-1" class="headerlink" title="2.2 创建启动脚本"></a>2.2 创建启动脚本</h2><pre><code class="hljs">sudo vi /lib/systemd/system/redis_slave.service[Unit]Description=RedisAfter=network.target[Service]Type=forkingExecStart=/usr/local/redis/bin/redis-server /etc/redis/slave.confExecReload=/usr/local/redis/bin/redis-server -s reloadExecStop=/usr/local/redis/bin/redis-server -s stopPrivateTmp=true[Install]WantedBy=multi-user.target</code></pre><h2 id="3-3-启动从节点"><a href="#3-3-启动从节点" class="headerlink" title="3.3 启动从节点"></a>3.3 启动从节点</h2><h2 id="2-3-启动主节点-1"><a href="#2-3-启动主节点-1" class="headerlink" title="2.3 启动主节点"></a>2.3 启动主节点</h2><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start redis_slave.servicesudo systemctl enable redis_slave.service</code></pre><h1 id="3-Redis集群初始化"><a href="#3-Redis集群初始化" class="headerlink" title="3.Redis集群初始化"></a>3.Redis集群初始化</h1><pre><code class="hljs">sudo /usr/local/redis/bin/redis-cli -a redis --cluster create --cluster-replicas 1 172.100.100.180:6379 172.100.100.180:6380 172.100.100.181:6379 172.100.100.181:6380 172.100.100.182:6379 172.100.100.182:6380</code></pre><h1 id="4-验证集群状态"><a href="#4-验证集群状态" class="headerlink" title="4.验证集群状态"></a>4.验证集群状态</h1><pre><code class="hljs">sudo /usr/local/redis/bin/redis-cli127.0.0.1:6379&gt; auth redisOK</code></pre><h2 id="4-1-集群信息"><a href="#4-1-集群信息" class="headerlink" title="4.1 集群信息"></a>4.1 集群信息</h2><pre><code class="hljs">127.0.0.1:6379&gt; cluster infocluster_state:okcluster_slots_assigned:16384cluster_slots_ok:16384cluster_slots_pfail:0cluster_slots_fail:0cluster_known_nodes:6cluster_size:3cluster_current_epoch:6cluster_my_epoch:5cluster_stats_messages_ping_sent:303cluster_stats_messages_pong_sent:304cluster_stats_messages_meet_sent:5cluster_stats_messages_sent:612cluster_stats_messages_ping_received:304cluster_stats_messages_pong_received:308cluster_stats_messages_received:612</code></pre><h2 id="4-2-节点主从关系"><a href="#4-2-节点主从关系" class="headerlink" title="4.2 节点主从关系"></a>4.2 节点主从关系</h2><pre><code class="hljs">127.0.0.1:6379&gt; info replication# Replicationrole:masterconnected_slaves:1slave0:ip=172.100.100.180,port=6380,state=online,offset=2842,lag=0master_replid:4593f8fefee93563e77de327cb17203c2ada6167master_replid2:0000000000000000000000000000000000000000master_repl_offset:2842master_repl_meaningful_offset:0second_repl_offset:-1repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:1repl_backlog_histlen:2842</code></pre><h2 id="4-3-集群节点关系"><a href="#4-3-集群节点关系" class="headerlink" title="4.3 集群节点关系"></a>4.3 集群节点关系</h2><pre><code class="hljs">127.0.0.1:6379&gt; cluster nodes9b3455aa02690fddb7fd04684f441a0cdb8a7e01 172.100.100.180:6379@16379 master - 0 1721630183744 1 connected 0-546099475e8b376d71fe51d7a52e413aab8b58ee6d82 172.100.100.181:6379@16379 master - 0 1721630186752 3 connected 5461-109225264f9b7db6512765c6c3406a14c03c9bd4979ed 172.100.100.182:6380@16380 slave 99475e8b376d71fe51d7a52e413aab8b58ee6d82 0 1721630185000 6 connectedcefbed9e3ece2a326bc8d88d2f1bddee526d8fe7 172.100.100.180:6380@16380 slave c1cf0836ef86f29c6786e523bc1b7f9e4cfef1c1 0 1721630186000 5 connected216177fbb99467606b945be756c35ca9dc4aaed8 172.100.100.181:6380@16380 slave 9b3455aa02690fddb7fd04684f441a0cdb8a7e01 0 1721630186000 4 connectedc1cf0836ef86f29c6786e523bc1b7f9e4cfef1c1 172.100.100.182:6379@16379 myself,master - 0 1721630184000 5 connected 10923-16383</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/m0_56754510/article/details/131942652">https://blog.csdn.net/m0_56754510/article/details/131942652</a></li><li><a href="https://blog.csdn.net/weixin_43888891/article/details/131208398">https://blog.csdn.net/weixin_43888891/article/details/131208398</a></li><li><a href="https://blog.csdn.net/suixinfeixiangfei/article/details/129356704">https://blog.csdn.net/suixinfeixiangfei/article/details/129356704</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Redis</tag>
      
      <tag>中间件</tag>
      
      <tag>高可用</tag>
      
      <tag>NoSQL</tag>
      
      <tag>缓存</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prometheus监控Redis数据库</title>
    <link href="/linux/Prometheus-Redis/"/>
    <url>/linux/Prometheus-Redis/</url>
    
    <content type="html"><![CDATA[<h1 id="1-下载redis-exporter"><a href="#1-下载redis-exporter" class="headerlink" title="1.下载redis_exporter"></a>1.下载redis_exporter</h1><pre><code class="hljs">wget https://github.com/oliver006/redis_exporter/releases/download/v1.61.0/redis_exporter-v1.61.0.linux-amd64.tar.gz</code></pre><h1 id="2-安装redis-exporter"><a href="#2-安装redis-exporter" class="headerlink" title="2.安装redis_exporter"></a>2.安装redis_exporter</h1><pre><code class="hljs">tar -xzvf redis_exporter-v1.61.0.linux-amd64.tar.gzsudo mv redis_exporter-v1.61.0.linux-amd64/redis_exporter /usr/local/redis/bin</code></pre><h1 id="3-创建启动脚本"><a href="#3-创建启动脚本" class="headerlink" title="3.创建启动脚本"></a>3.创建启动脚本</h1><pre><code class="hljs">sudo vi /lib/systemd/system/redis_exporter.service[Unit]Description=redis_exporterDocumentation=https://github.com/oliver006/redis_exporterAfter=network.targetBindsTo=redis.service[Service]Type=simpleUser=rootExecStart=/usr/local/redis/bin/redis_exporter -redis.password Redis@2020 -exclude-latency-histogram-metricsKillMode=mixedRestart=on-failure[Install]WantedBy=multi-user.target</code></pre><h1 id="4-启动redis-exporter"><a href="#4-启动redis-exporter" class="headerlink" title="4.启动redis_exporter"></a>4.启动redis_exporter</h1><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start redis_exporter.servicesudo systemctl enable redis_exporter.service</code></pre><h1 id="5-配置Prometheus"><a href="#5-配置Prometheus" class="headerlink" title="5.配置Prometheus"></a>5.配置Prometheus</h1><h2 id="5-1-配置监控实例"><a href="#5-1-配置监控实例" class="headerlink" title="5.1 配置监控实例"></a>5.1 配置监控实例</h2><pre><code class="hljs">sudo vi /usr/local/prometheus/prometheus.ymlglobal:  scrape_interval: 15s   evaluation_interval: 15s   scrape_timeout: 10s scrape_configs:  - job_name: &quot;prometheus&quot;    static_configs:      - targets: [&quot;localhost:9090&quot;]  - job_name: redis    static_configs:      - targets:        - ovirt.sword.org:9121        labels:          cluster: Redis    relabel_configs:      - source_labels: [ &#39;__address__&#39; ]        regex: &quot;(.*):(.*)&quot;        replacement: $1        target_label:  &#39;hostname&#39;</code></pre><h2 id="5-2-重载Prometheus"><a href="#5-2-重载Prometheus" class="headerlink" title="5.2 重载Prometheus"></a>5.2 重载Prometheus</h2><pre><code class="hljs">curl -XPOST http://127.0.0.1:9090/-/reload</code></pre><h1 id="6-Grafana导入监控模版"><a href="#6-Grafana导入监控模版" class="headerlink" title="6.Grafana导入监控模版"></a>6.Grafana导入监控模版</h1><p>Dashboards —&gt; Manage —&gt; Import —&gt; 模版ID：14091</p><p><img src="/img/wiki/prometheus/redis.jpg" alt="redis"></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://github.com/oliver006/redis_exporter">https://github.com/oliver006/redis_exporter</a></li><li><a href="https://www.cnblogs.com/miaocbin/p/12028105.html">https://www.cnblogs.com/miaocbin/p/12028105.html</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Redis</tag>
      
      <tag>云原生</tag>
      
      <tag>Prometheus</tag>
      
      <tag>监控告警</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux系统部署Spug自动化运维平台</title>
    <link href="/linux/SpugPlatform/"/>
    <url>/linux/SpugPlatform/</url>
    
    <content type="html"><![CDATA[<p>Spug，基于Python和JavaScript开发的轻量级无Agent的开源自动化运维管理平台，适用于中小型企业，UI基于Ant Design设计，整合了主机管理、主机批量执行、主机在线终端、应用发布部署、在线任务计划、配置中心及监控告警等一系列功能，且二次开发很方便，是一套十分全面的运维解决方案</p><h1 id="1-安装MySQL、Redis"><a href="#1-安装MySQL、Redis" class="headerlink" title="1.安装MySQL、Redis"></a>1.安装MySQL、Redis</h1><h1 id="2-安装Spug"><a href="#2-安装Spug" class="headerlink" title="2.安装Spug"></a>2.安装Spug</h1><pre><code class="hljs">sudo docker pull registry.aliyuncs.com/openspug/spugsudo docker tag registry.aliyuncs.com/openspug/spug spug</code></pre><h1 id="3-启动Spug"><a href="#3-启动Spug" class="headerlink" title="3.启动Spug"></a>3.启动Spug</h1><pre><code class="hljs">sudo sudo docker run -d -it --name=spug -p 80:80 -v /web:/data spug</code></pre><h1 id="4-创建数据库"><a href="#4-创建数据库" class="headerlink" title="4.创建数据库"></a>4.创建数据库</h1><pre><code class="hljs">sudo mysql -uroot -pEnter password: Welcome to the MariaDB monitor.  Commands end with ; or \g.Your MariaDB connection id is 376Server version: 10.5.6-MariaDB MariaDB ServerCopyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.Type &#39;help;&#39; or &#39;\h&#39; for help. Type &#39;\c&#39; to clear the current input statement.MariaDB [(none)]&gt; create database spug character set utf8mb4;MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON spug.* TO &#39;spug&#39;@&#39;%&#39; IDENTIFIED BY &#39;spug&#39;;MariaDB [(none)]&gt; flush privileges;</code></pre><h1 id="5-创建配置文件"><a href="#5-创建配置文件" class="headerlink" title="5.创建配置文件"></a>5.创建配置文件</h1><pre><code class="hljs">sudo vi /web/spug/spug_api/spug/overrides.pyDEBUG = FalseALLOWED_HOSTS = [&#39;127.0.0.1&#39;]SECRET_KEY = &#39;gyseWUT@ib45%PAbfE6ZZ@sX%2i.8z!pcNh5KpI7.Uu98ed84G&#39;DATABASES = &#123;    &#39;default&#39;: &#123;        &#39;ATOMIC_REQUESTS&#39;: True,        &#39;ENGINE&#39;: &#39;django.db.backends.mysql&#39;,        &#39;NAME&#39;: &#39;spug&#39;,        &#39;USER&#39;: &#39;spug&#39;,        &#39;PASSWORD&#39;: &#39;spug&#39;,        &#39;HOST&#39;: &#39;192.168.100.100&#39;,        &#39;OPTIONS&#39;: &#123;            &#39;charset&#39;: &#39;utf8mb4&#39;,            &#39;sql_mode&#39;: &#39;STRICT_TRANS_TABLES&#39;,        &#125;    &#125;&#125;CACHES = &#123;    &quot;default&quot;: &#123;        &quot;BACKEND&quot;: &quot;django_redis.cache.RedisCache&quot;,        &quot;LOCATION&quot;: &quot;redis://:redis@192.168.100.100:6379/3&quot;,        &quot;OPTIONS&quot;: &#123;            &quot;CLIENT_CLASS&quot;: &quot;django_redis.client.DefaultClient&quot;,        &#125;    &#125;&#125;CHANNEL_LAYERS = &#123;    &quot;default&quot;: &#123;        &quot;BACKEND&quot;: &quot;channels_redis.core.RedisChannelLayer&quot;,        &quot;CONFIG&quot;: &#123;            &quot;hosts&quot;: [&quot;redis://:redis@192.168.100.100:6379/3&quot;],        &#125;,    &#125;,&#125;</code></pre><h1 id="6-配置容器启动参数"><a href="#6-配置容器启动参数" class="headerlink" title="6.配置容器启动参数"></a>6.配置容器启动参数</h1><pre><code class="hljs">sudo docker exec -it spug /bin/bash[root@64c97dea4c11 /]# cp /etc/supervisord.d/spug.ini /etc/supervisord.d/.spug.ini[root@64c97dea4c11 /]# vi /etc/supervisord.d/spug.ini# 删除如下内容[program:mariadb]command = /usr/libexec/mysqld --user=mysqlautostart = true[program:redis]command = /usr/bin/redis-serverautostart = true</code></pre><h1 id="7-重启spug"><a href="#7-重启spug" class="headerlink" title="7.重启spug"></a>7.重启spug</h1><pre><code class="hljs">sudo docker restart spug</code></pre><h1 id="8-创建管理员账户"><a href="#8-创建管理员账户" class="headerlink" title="8.创建管理员账户"></a>8.创建管理员账户</h1><pre><code class="hljs">sudo docker exec spug init_spug admin spug.cc</code></pre><ul><li>注：管理员账户为admin，密码为spug.cc</li></ul><h1 id="9-验证spug"><a href="#9-验证spug" class="headerlink" title="9.验证spug"></a>9.验证spug</h1><p><a href="http://ip/">http://ip</a></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://spug.dev/docs/install-docker">https://spug.dev/docs/install-docker</a></li><li><a href="https://blog.csdn.net/ximenjianxue/article/details/122092843">https://blog.csdn.net/ximenjianxue/article/details/122092843</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>自动化运维</tag>
      
      <tag>云计算</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ansible自动化运维工具可视化界面Semaphore</title>
    <link href="/linux/Ansible-Semaphore/"/>
    <url>/linux/Ansible-Semaphore/</url>
    
    <content type="html"><![CDATA[<p>Semaphore，Ansible用于运行Playbook的响应式Web可视化工具，是由Go语言编写的开源项目，旨在为用户提供剧本管理和运行的直观的界面，简化其执行过程，并将之打造为简洁而高质量的代码库。此外，Semaphore还允许用户根据需求进行定制与扩展，以适应特定的环境和工作流程，文档与社区支持十分丰富</p><h1 id="1-下载安装包"><a href="#1-下载安装包" class="headerlink" title="1.下载安装包"></a>1.下载安装包</h1><pre><code class="hljs">sudo mkdir -p /usr/local/semaphore &amp;&amp; cd /usr/local/semaphorewget https://github.com/semaphoreui/semaphore/releases/download/v2.9.112/semaphore_2.9.112_linux_amd64.tar.gztar -xzvf semaphore_2.9.112_linux_amd64.tar.gz</code></pre><h1 id="2-安装MySQL数据库"><a href="#2-安装MySQL数据库" class="headerlink" title="2.安装MySQL数据库"></a>2.安装MySQL数据库</h1><h1 id="3-安装Semaphore"><a href="#3-安装Semaphore" class="headerlink" title="3.安装Semaphore"></a>3.安装Semaphore</h1><pre><code class="hljs">./semaphore setupHello! You will now be guided through a setup to:1. Set up configuration for a MySQL/MariaDB database2. Set up a path for your playbooks (auto-created)3. Run database Migrations4. Set up initial semaphore user &amp; passwordWhat database to use:   1 - MySQL   2 - BoltDB   3 - PostgreSQL(default 1): 1db Hostname (default 127.0.0.1:3306): db User (default root): db Password: 123456db Name (default semaphore): Playbook path (default /tmp/semaphore): /usr/local/semaphore/playbooks Public URL (optional, example: https://example.com/semaphore): Enable email alerts? (yes/no) (default no): Enable telegram alerts? (yes/no) (default no): Enable slack alerts? (yes/no) (default no): Enable Rocket.Chat alerts? (yes/no) (default no): Enable Microsoft Team Channel alerts? (yes/no) (default no): Enable LDAP authentication? (yes/no) (default no): Config output directory (default /usr/local/semaphore): Running: mkdir -p /usr/local/semaphore..Configuration written to /usr/local/semaphore/config.json..Loading configValidating config</code></pre><h1 id="4-创建启动脚本"><a href="#4-创建启动脚本" class="headerlink" title="4.创建启动脚本"></a>4.创建启动脚本</h1><pre><code class="hljs">sudo vi /lib/systemd/system/semaphore.service[Unit]Description=Semaphore AnsibleDocumentation=https://github.com/semaphoreui/semaphoreWants=network-online.targetAfter=network-online.target[Service]Type=simpleExecReload=/bin/kill -HUP $MAINPIDExecStart=/usr/local/semaphore/semaphore server --config=/usr/local/semaphore/config.jsonSyslogIdentifier=semaphoreRestart=alwaysRestartSec=10s[Install]WantedBy=multi-user.target</code></pre><h1 id="5-启动Semaphore"><a href="#5-启动Semaphore" class="headerlink" title="5.启动Semaphore"></a>5.启动Semaphore</h1><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start semaphore.servicesudo systemctl enable semaphore.service</code></pre><h1 id="6-访问Semaphore"><a href="#6-访问Semaphore" class="headerlink" title="6.访问Semaphore"></a>6.访问Semaphore</h1><p><a href="http://ip:3000/">http://ip:3000</a></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://docs.semaphoreui.com/administration-guide/installation">https://docs.semaphoreui.com/administration-guide/installation</a></li><li><a href="https://blog.csdn.net/weixin_73545275/article/details/134948388">https://blog.csdn.net/weixin_73545275/article/details/134948388</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Ansible</tag>
      
      <tag>自动化运维</tag>
      
      <tag>云计算</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux系统磁盘扩容方案</title>
    <link href="/linux/DiskExpansion/"/>
    <url>/linux/DiskExpansion/</url>
    
    <content type="html"><![CDATA[<p>Linux系统磁盘扩容分为两种方案，即新增磁盘和分区调整，前者是将新增的磁盘扩容为单独的分区，或者是将新磁盘加入到LVM的卷组再扩容到指定的逻辑分区，后者只能是基于LVM将空闲较多的逻辑分区扩容到容量紧张的逻辑分区</p><h1 id="1-新增磁盘"><a href="#1-新增磁盘" class="headerlink" title="1.新增磁盘"></a>1.新增磁盘</h1><p>新增一块100G的硬盘，其中80G挂载到&#x2F;data用于数据存储，20G用于扩容根分区</p><h1 id="1-1-查看当前所挂载磁盘"><a href="#1-1-查看当前所挂载磁盘" class="headerlink" title="1.1 查看当前所挂载磁盘"></a>1.1 查看当前所挂载磁盘</h1><pre><code class="hljs">lsblk NAME                    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTsr0                      11:0    1 1024M  0 rom  vda                     252:0    0  256G  0 disk ├─vda1                  252:1    0    1G  0 part /boot└─vda2                  252:2    0  255G  0 part   ├─centos_centos7-root 253:0    0   50G  0 lvm  /  ├─centos_centos7-swap 253:1    0    2G  0 lvm  [SWAP]  └─centos_centos7-home 253:2    0  203G  0 lvm  /homevdc                     252:32   0  100G  0 disk </code></pre><h1 id="1-2-数据分区"><a href="#1-2-数据分区" class="headerlink" title="1.2 数据分区"></a>1.2 数据分区</h1><h3 id="1-2-1-创建分区"><a href="#1-2-1-创建分区" class="headerlink" title="1.2.1 创建分区"></a>1.2.1 创建分区</h3><pre><code class="hljs">fdisk /dev/vdcWelcome to fdisk (util-linux 2.23.2).Changes will remain in memory only, until you decide to write them.Be careful before using the write command.Device does not contain a recognized partition tableBuilding a new DOS disklabel with disk identifier 0xe2af3ac8.Command (m for help): nPartition type:   p   primary (0 primary, 0 extended, 4 free)   e   extendedSelect (default p): Using default response pPartition number (1-4, default 1): First sector (2048-209715199, default 2048): Using default value 2048Last sector, +sectors or +size&#123;K,M,G&#125; (2048-209715199, default 209715199): +80GPartition 1 of type Linux and of size 80 GiB is setCommand (m for help): wThe partition table has been altered!Calling ioctl() to re-read partition table.Syncing disks.</code></pre><h3 id="1-2-2-验证磁盘分区"><a href="#1-2-2-验证磁盘分区" class="headerlink" title="1.2.2 验证磁盘分区"></a>1.2.2 验证磁盘分区</h3><pre><code class="hljs">fdisk -l /dev/vdc Disk /dev/vdc: 107.4 GB, 107374182400 bytes, 209715200 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0xe2af3ac8   Device Boot      Start         End      Blocks   Id  System/dev/vdc1            2048   167774207    83886080   83  Linux</code></pre><h3 id="1-2-3-格式化分区"><a href="#1-2-3-格式化分区" class="headerlink" title="1.2.3 格式化分区"></a>1.2.3 格式化分区</h3><pre><code class="hljs">mkfs.ext4 /dev/vdc1</code></pre><h3 id="1-2-4-挂载磁盘分区"><a href="#1-2-4-挂载磁盘分区" class="headerlink" title="1.2.4 挂载磁盘分区"></a>1.2.4 挂载磁盘分区</h3><pre><code class="hljs"># 新建挂载点mkdir -p /data# 挂载磁盘分区mount /dev/vdc1 /data</code></pre><h3 id="1-2-5-验证文件系统"><a href="#1-2-5-验证文件系统" class="headerlink" title="1.2.5 验证文件系统"></a>1.2.5 验证文件系统</h3><pre><code class="hljs">df -hFilesystem                       Size  Used Avail Use% Mounted ondevtmpfs                         908M     0  908M   0% /devtmpfs                            919M     0  919M   0% /dev/shmtmpfs                            919M  8.6M  911M   1% /runtmpfs                            919M     0  919M   0% /sys/fs/cgroup/dev/mapper/centos_centos7-root   50G  1.9G   49G   4% //dev/mapper/centos_centos7-home  203G   33M  203G   1% /home/dev/vda1                       1014M  236M  779M  24% /boottmpfs                             82M     0   82M   0% /run/user/1000/dev/vdc1                         79G   57M   75G   1% /data</code></pre><h3 id="1-2-6-配置分区开机自动挂载"><a href="#1-2-6-配置分区开机自动挂载" class="headerlink" title="1.2.6 配置分区开机自动挂载"></a>1.2.6 配置分区开机自动挂载</h3><h4 id="1-2-6-1-指定卷标，即盘符"><a href="#1-2-6-1-指定卷标，即盘符" class="headerlink" title="1.2.6.1 指定卷标，即盘符"></a>1.2.6.1 指定卷标，即盘符</h4><pre><code class="hljs">e2label /dev/vdc1 data</code></pre><h4 id="1-2-6-2-查找设备uuid"><a href="#1-2-6-2-查找设备uuid" class="headerlink" title="1.2.6.2 查找设备uuid"></a>1.2.6.2 查找设备uuid</h4><pre><code class="hljs">lsblk -fNAME                    FSTYPE      LABEL UUID                                   MOUNTPOINTsr0                                                                              vda                                                                              ├─vda1                  xfs               1d624794-ee47-4428-a34f-a887a3dcd04a   /boot└─vda2                  LVM2_member       sJdHkg-yBlF-j1Jb-vy26-aRqR-T2DU-oM3WSV   ├─centos_centos7-root xfs               af45f5b5-27fe-4157-b543-56d14d88a56d   /  ├─centos_centos7-swap swap              aa7f679b-71c8-4f03-9b81-95ae2749c284   [SWAP]  └─centos_centos7-home xfs               fd6c83c5-069c-49c5-aa4d-09979554ea3a   /homevdc                                                                              └─vdc1                  ext4        data  bec2696a-a272-447c-bb1c-ce42737144da   /data</code></pre><h4 id="1-2-6-3-配置分区自动挂载"><a href="#1-2-6-3-配置分区自动挂载" class="headerlink" title="1.2.6.3 配置分区自动挂载"></a>1.2.6.3 配置分区自动挂载</h4><pre><code class="hljs"># 也可用设备名称或卷标echo &quot;UUID=bec2696a-a272-447c-bb1c-ce42737144da  /data  ext4  defaults  1  2&quot; &gt;&gt; /etc/fstab</code></pre><h4 id="1-2-6-4-重启系统，验证磁盘自动挂载"><a href="#1-2-6-4-重启系统，验证磁盘自动挂载" class="headerlink" title="1.2.6.4 重启系统，验证磁盘自动挂载"></a>1.2.6.4 重启系统，验证磁盘自动挂载</h4><pre><code class="hljs">reboot</code></pre><h2 id="1-3-扩容分区"><a href="#1-3-扩容分区" class="headerlink" title="1.3 扩容分区"></a>1.3 扩容分区</h2><h3 id="1-3-1-创建分区"><a href="#1-3-1-创建分区" class="headerlink" title="1.3.1 创建分区"></a>1.3.1 创建分区</h3><pre><code class="hljs">fdisk /dev/vdc</code></pre><h3 id="1-3-2-验证分区"><a href="#1-3-2-验证分区" class="headerlink" title="1.3.2 验证分区"></a>1.3.2 验证分区</h3><pre><code class="hljs">fdisk -l /dev/vdc Disk /dev/vdc: 107.4 GB, 107374182400 bytes, 209715200 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0xe2af3ac8   Device Boot      Start         End      Blocks   Id  System/dev/vdc1            2048   167774207    83886080   83  Linux/dev/vdc2       167774208   209715199    20970496   83  Linux</code></pre><h3 id="1-3-2-创建物理卷"><a href="#1-3-2-创建物理卷" class="headerlink" title="1.3.2 创建物理卷"></a>1.3.2 创建物理卷</h3><pre><code class="hljs">pvcreate /dev/vdc2  Physical volume &quot;/dev/vdc2&quot; successfully created.</code></pre><h3 id="1-3-3-查看逻辑卷组名称"><a href="#1-3-3-查看逻辑卷组名称" class="headerlink" title="1.3.3 查看逻辑卷组名称"></a>1.3.3 查看逻辑卷组名称</h3><pre><code class="hljs">pvdisplay | grep VGVG Name               centos_centos7</code></pre><h3 id="1-3-4-将建物理卷加入逻辑卷组"><a href="#1-3-4-将建物理卷加入逻辑卷组" class="headerlink" title="1.3.4 将建物理卷加入逻辑卷组"></a>1.3.4 将建物理卷加入逻辑卷组</h3><pre><code class="hljs"> vgextend centos_centos7 /dev/vdc2  Volume group &quot;centos_centos7&quot; successfully extended</code></pre><h3 id="1-3-5-验证卷组空闲空间"><a href="#1-3-5-验证卷组空闲空间" class="headerlink" title="1.3.5 验证卷组空闲空间"></a>1.3.5 验证卷组空闲空间</h3><pre><code class="hljs">vgdisplay | grep Free Free  PE / Size       5120 / 20.00 GiB</code></pre><h3 id="1-3-6-查看要扩容的逻辑卷路径"><a href="#1-3-6-查看要扩容的逻辑卷路径" class="headerlink" title="1.3.6 查看要扩容的逻辑卷路径"></a>1.3.6 查看要扩容的逻辑卷路径</h3><pre><code class="hljs">lvdisplay | grep Path  LV Path                /dev/centos_centos7/swap  LV Path                /dev/centos_centos7/home  LV Path                /dev/centos_centos7/root</code></pre><h3 id="1-3-7-将空闲空间扩容到逻辑分区"><a href="#1-3-7-将空闲空间扩容到逻辑分区" class="headerlink" title="1.3.7 将空闲空间扩容到逻辑分区"></a>1.3.7 将空闲空间扩容到逻辑分区</h3><pre><code class="hljs">lvextend -L +19.9G /dev/centos_centos7/root  Rounding size to boundary between physical extents: 19.90 GiB.  Size of logical volume centos_centos7/root changed from 50.00 GiB (12800 extents) to 69.90 GiB (17895 extents).  Logical volume centos_centos7/root successfully resized.</code></pre><h3 id="1-3-8-重定义文件系统大小"><a href="#1-3-8-重定义文件系统大小" class="headerlink" title="1.3.8 重定义文件系统大小"></a>1.3.8 重定义文件系统大小</h3><pre><code class="hljs">resize2fs -p /dev/vg_oracle02/lv_var# xfs格式文件系统为xfs_growfs命令xfs_growfs /dev/centos_centos7/rootmeta-data=/dev/mapper/centos_centos7-root isize=512    agcount=4, agsize=3276800 blks     =                       sectsz=512   attr=2, projid32bit=1     =                       crc=1        finobt=0 spinodes=0data     =                       bsize=4096   blocks=13107200, imaxpct=25         =                       sunit=0      swidth=0 blksnaming   =version 2              bsize=4096   ascii-ci=0 ftype=1log      =internal               bsize=4096   blocks=6400, version=2         =                       sectsz=512   sunit=0 blks, lazy-count=1realtime =none                   extsz=4096   blocks=0, rtextents=0data blocks changed from 13107200 to 18324480</code></pre><h3 id="1-3-9-验证文件系统"><a href="#1-3-9-验证文件系统" class="headerlink" title="1.3.9 验证文件系统"></a>1.3.9 验证文件系统</h3><pre><code class="hljs">df -hFilesystem                       Size  Used Avail Use% Mounted ondevtmpfs                         908M     0  908M   0% /devtmpfs                            919M     0  919M   0% /dev/shmtmpfs                            919M  8.6M  911M   1% /runtmpfs                            919M     0  919M   0% /sys/fs/cgroup/dev/mapper/centos_centos7-root   70G  1.9G   69G   3% //dev/mapper/centos_centos7-home  203G   33M  203G   1% /home/dev/vda1                       1014M  236M  779M  24% /boottmpfs                             82M     0   82M   0% /run/user/1000/dev/vdc1                         79G   57M   75G   1% /data</code></pre><h1 id="2-扩容分区"><a href="#2-扩容分区" class="headerlink" title="2.扩容分区"></a>2.扩容分区</h1><p>将数据分区&#x2F;home部分空间扩容到分区&#x2F;root</p><h2 id="2-1-查看文件系统格式"><a href="#2-1-查看文件系统格式" class="headerlink" title="2.1 查看文件系统格式"></a>2.1 查看文件系统格式</h2><p>文件系统的扩缩容方式依赖于其格式，如Centos7系列默认的XFS文件系统就不支持在线缩容，用EXT4的方式进行扩缩容就会报错（Superblock错误）。因此，建议扩缩容操作之前先确认文件系统的格式</p><pre><code class="hljs">lsblk -fNAME                    FSTYPE      LABEL UUID                                   MOUNTPOINTsr0                                                                              vda                                                                              ├─vda1                  xfs               1d624794-ee47-4428-a34f-a887a3dcd04a   /boot└─vda2                  LVM2_member       sJdHkg-yBlF-j1Jb-vy26-aRqR-T2DU-oM3WSV   ├─centos_centos7-root xfs               af45f5b5-27fe-4157-b543-56d14d88a56d   /  ├─centos_centos7-swap swap              aa7f679b-71c8-4f03-9b81-95ae2749c284   [SWAP]  └─centos_centos7-home xfs               be669905-d291-487e-a52c-2d362ad47ec6   /home</code></pre><h2 id="2-2-卸载分区"><a href="#2-2-卸载分区" class="headerlink" title="2.2 卸载分区"></a>2.2 卸载分区</h2><h3 id="2-2-1-安装fuser工具"><a href="#2-2-1-安装fuser工具" class="headerlink" title="2.2.1 安装fuser工具"></a>2.2.1 安装fuser工具</h3><pre><code class="hljs">yum install -y psmisc</code></pre><h3 id="2-2-2-终止分区进程"><a href="#2-2-2-终止分区进程" class="headerlink" title="2.2.2 终止分区进程"></a>2.2.2 终止分区进程</h3><pre><code class="hljs">fuser -m -v -i -k /home                 用户     进程号 权限   命令/home:               root     kernel mount /home                 sword      7903 ..c.. bash杀死进程 7903 ? (y/N) y</code></pre><h3 id="2-2-3-卸载分区"><a href="#2-2-3-卸载分区" class="headerlink" title="2.2.3 卸载分区"></a>2.2.3 卸载分区</h3><pre><code class="hljs">umount /home</code></pre><h2 id="2-3-缩减文件系统"><a href="#2-3-缩减文件系统" class="headerlink" title="2.3 缩减文件系统"></a>2.3 缩减文件系统</h2><h3 id="2-3-1-XFS格式"><a href="#2-3-1-XFS格式" class="headerlink" title="2.3.1 XFS格式"></a>2.3.1 XFS格式</h3><h4 id="2-3-1-1-安装工具"><a href="#2-3-1-1-安装工具" class="headerlink" title="2.3.1.1 安装工具"></a>2.3.1.1 安装工具</h4><pre><code class="hljs">yum -y install xfsdump</code></pre><h4 id="2-3-1-2-缩减逻辑卷空间，使之成为空闲空间"><a href="#2-3-1-2-缩减逻辑卷空间，使之成为空闲空间" class="headerlink" title="2.3.1.2 缩减逻辑卷空间，使之成为空闲空间"></a>2.3.1.2 缩减逻辑卷空间，使之成为空闲空间</h4><pre><code class="hljs">lvresize -L 100G /dev/mapper/centos_centos7-home   WARNING: Reducing active logical volume to 100.00 GiB.  THIS MAY DESTROY YOUR DATA (filesystem etc.)Do you really want to reduce centos_centos7/home? [y/n]: y  Size of logical volume centos_centos7/home changed from 202.99 GiB (51966 extents) to 100.00 GiB (25600 extents).  Logical volume centos_centos7/home successfully resized.</code></pre><h4 id="2-3-1-3-重新格式化缩减的逻辑分区"><a href="#2-3-1-3-重新格式化缩减的逻辑分区" class="headerlink" title="2.3.1.3 重新格式化缩减的逻辑分区"></a>2.3.1.3 重新格式化缩减的逻辑分区</h4><pre><code class="hljs">mkfs.xfs -f /dev/mapper/centos_centos7-homemeta-data=/dev/mapper/centos_centos7-home isize=512    agcount=4, agsize=6553600 blks         =                       sectsz=512   attr=2, projid32bit=1         =                       crc=1        finobt=0, sparse=0data     =                       bsize=4096   blocks=26214400, imaxpct=25         =                       sunit=0      swidth=0 blksnaming   =version 2              bsize=4096   ascii-ci=0 ftype=1log      =internal log           bsize=4096   blocks=12800, version=2         =                       sectsz=512   sunit=0 blks, lazy-count=1realtime =none                   extsz=4096   blocks=0, rtextents=0resize2fs -f /dev/mapper/VolGroup-lv_data 15G</code></pre><h4 id="2-3-1-4-挂载缩减的分区"><a href="#2-3-1-4-挂载缩减的分区" class="headerlink" title="2.3.1.4 挂载缩减的分区"></a>2.3.1.4 挂载缩减的分区</h4><pre><code class="hljs"> mount /dev/mapper/centos_centos7-home /home</code></pre><h3 id="2-3-2-EXT4格式"><a href="#2-3-2-EXT4格式" class="headerlink" title="2.3.2 EXT4格式"></a>2.3.2 EXT4格式</h3><h4 id="2-3-2-1-缩减逻辑卷空间，使之成为空闲空间"><a href="#2-3-2-1-缩减逻辑卷空间，使之成为空闲空间" class="headerlink" title="2.3.2.1 缩减逻辑卷空间，使之成为空闲空间"></a>2.3.2.1 缩减逻辑卷空间，使之成为空闲空间</h4><pre><code class="hljs">lvreduce -L 100G /dev/mapper/centos_centos7-home</code></pre><h4 id="2-3-2-2-重新挂载数据分区"><a href="#2-3-2-2-重新挂载数据分区" class="headerlink" title="2.3.2.2 重新挂载数据分区"></a>2.3.2.2 重新挂载数据分区</h4><pre><code class="hljs">mount /dev/mapper/centos_centos7-home</code></pre><h2 id="2-4-验证空闲空间"><a href="#2-4-验证空闲空间" class="headerlink" title="2.4 验证空闲空间"></a>2.4 验证空闲空间</h2><pre><code class="hljs">vgdisplay | grep Free  Free  PE / Size       26367 / &lt;103.00 GiB</code></pre><h2 id="2-5-将空闲空间扩容到根分区"><a href="#2-5-将空闲空间扩容到根分区" class="headerlink" title="2.5 将空闲空间扩容到根分区"></a>2.5 将空闲空间扩容到根分区</h2><pre><code class="hljs">lvextend -L +103G /dev/mapper/centos_centos7-root</code></pre><h2 id="2-6-重定义文件系统大小"><a href="#2-6-重定义文件系统大小" class="headerlink" title="2.6 重定义文件系统大小"></a>2.6 重定义文件系统大小</h2><pre><code class="hljs"># XFS格式xfs_growfs /dev/mapper/centos_centos7-root# EXT4格式resize2fs -p /dev/mapper/centos_centos7-root</code></pre><h2 id="2-7-验证分区扩容"><a href="#2-7-验证分区扩容" class="headerlink" title="2.7 验证分区扩容"></a>2.7 验证分区扩容</h2><pre><code class="hljs">df -h文件系统                         容量  已用  可用 已用% 挂载点devtmpfs                         908M     0  908M    0% /devtmpfs                            919M     0  919M    0% /dev/shmtmpfs                            919M  8.5M  911M    1% /runtmpfs                            919M     0  919M    0% /sys/fs/cgroup/dev/mapper/centos_centos7-root  153G  1.6G  152G    2% //dev/vda1                       1014M  193M  822M   19% /boottmpfs                             82M     0   82M    0% /run/user/0/dev/mapper/centos_centos7-home  100G   33M  100G    1% /home</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://cloud.tencent.com/developer/article/2095805">https://cloud.tencent.com/developer/article/2095805</a></li><li><a href="https://blog.csdn.net/niechel/article/details/129217219">https://blog.csdn.net/niechel/article/details/129217219</a></li><li><a href="https://blog.csdn.net/weixin_53389944/article/details/129205588">https://blog.csdn.net/weixin_53389944/article/details/129205588</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>存储</tag>
      
      <tag>LVM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Openstack集群网络模型详解</title>
    <link href="/linux/OpenstackNeutron/"/>
    <url>/linux/OpenstackNeutron/</url>
    
    <content type="html"><![CDATA[<p>Neutron，基于SDN（Software Defined Networking，即软件定义网络）理念设计的网络虚拟化方案，Openstack H版本开始启用的网络组件，用于实现网络即服务（Networking-as-a-Service）功能。Neutron为复杂的多租户云环境提供了灵活的网络划分和独立的网络环境，方便、快速、动态地响应业务的网络需求，以保障网络的连通性和隔离性，如随时创建、修改和删除虚拟网络设备，无需再像传统的网络管理方式那样，很大程度依赖于手工配置和维护各种网络硬件设备</p><h1 id="功能概述"><a href="#功能概述" class="headerlink" title="功能概述"></a>功能概述</h1><p>Neutron为整个OpenStack环境提供网络支持，作为一个灵活的框架，通过配置调用开源或商业软件用于实现各种网络功能，如二层交换、三层路由、负载均衡、防火墙和VPN等</p><h2 id="1-二层交换Switching"><a href="#1-二层交换Switching" class="headerlink" title="1.二层交换Switching"></a>1.二层交换Switching</h2><p>OpenStack虚拟机实例通过虚拟交换机连接到虚拟二层网络，Neutron支持多种虚拟交换机，如Linux原生的Linux Bridge和Open vSwitch，除了可以创建传统的VLAN网络，还可以创建基于隧道技术的Overlay网络，如VxLAN和GRE（Linux Bridge 目前只支持 VxLAN）</p><h1 id="2-三层路由Routing"><a href="#2-三层路由Routing" class="headerlink" title="2.三层路由Routing"></a>2.三层路由Routing</h1><p>Neutron vrouter（虚拟路由器）是一个三层的（L3）的抽象，通过IP forwarding、iptables等技术实现路由和NAT，模拟物理路由器，为虚拟机实例提供跨网段的通信，以及外部网络之间的通信。此外，Neutron还支持物理路由器</p><h1 id="3-负载均衡LoadBalancing"><a href="#3-负载均衡LoadBalancing" class="headerlink" title="3.负载均衡LoadBalancing"></a>3.负载均衡LoadBalancing</h1><p>Openstack于Grizzly版本引入Load-Balancing-as-a-Service，LBaaS，即负载均衡即服务，提供了将负载分发到多个虚拟机实例的能力，并支持多种负载均衡产品和方案，不同的实现以Plugin的形式集成到Neutron，目前默认的Plugin为HAProxy</p><h1 id="4-防火墙Firewalling"><a href="#4-防火墙Firewalling" class="headerlink" title="4.防火墙Firewalling"></a>4.防火墙Firewalling</h1><p>Neutron通过Security Group和Firewall-as-a-Service两种方式保障虚拟机实例和网络的安全，两者都通过iptables规则进行实现，前者用于限制虚拟机实例网络包的进出，后者则用于限制虚拟路由器网络包的进出</p><h1 id="5-虚拟专用网VPN"><a href="#5-虚拟专用网VPN" class="headerlink" title="5.虚拟专用网VPN"></a>5.虚拟专用网VPN</h1><p>Neutron远程访问技术，即利用公用网络进行加密架设成专用网络，形成内网穿透隧道，从而构成局域网，提供了VPNaaS（VPN-as-a-Service），即VPN即服务</p><h1 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h1><h2 id="1-Network"><a href="#1-Network" class="headerlink" title="1.Network"></a>1.Network</h2><p>Network，网络，隔离的二层广播域，Neutron支持多种类型Network，如Local、Flat、 VLAN,、VxLAN和GRE</p><h3 id="1-1-Local"><a href="#1-1-Local" class="headerlink" title="1.1 Local"></a>1.1 Local</h3><p>Local网络与其他网络和节点隔离，其中的虚拟机实例只能与位于同一节点上同一网络的实例通信，主要用于单机测试</p><h3 id="1-2-Flat"><a href="#1-2-Flat" class="headerlink" title="1.2 Flat"></a>1.2 Flat</h3><p>Flat网络是无vlan tagging的网络，其中的虚拟机实例能与位于同一网络的实例通信，且可以跨多个节点</p><h3 id="1-3-VLAN"><a href="#1-3-VLAN" class="headerlink" title="1.3 VLAN"></a>1.3 VLAN</h3><p>VLAN网络是具有802.1q tagging的网络，是一个二层的广播域，同一VLAN中的虚拟机实例可以通信，不同VLAN之间需通过router通信，且可跨节点，是应用最广泛的网络类型</p><h3 id="1-4-VxLAN"><a href="#1-4-VxLAN" class="headerlink" title="1.4 VxLAN"></a>1.4 VxLAN</h3><p>VxLAN是基于隧道技术的overlay网络，通过唯一的segmentation ID（也称VNI）与其他VxLAN网络区分，其中的网络数据包通过VNI封装成UDP包进行传输。因为二层的包通过封装在三层传输，能够克服VLAN和物理网络基础设施的限制</p><h3 id="1-5-GRE"><a href="#1-5-GRE" class="headerlink" title="1.5 GRE"></a>1.5 GRE</h3><p>GRE网络类似于与VxLAN，也是overlay网络，主要区别在于使用IP包而非UDP进行封装</p><h2 id="2-Subnet"><a href="#2-Subnet" class="headerlink" title="2.Subnet"></a>2.Subnet</h2><p>Subnet，子网，IPv4或IPv6地址段，虚拟机实例的IP即从中分配，每个Subnet都需要定义IP地址范围和掩码</p><ul><li>Network与Subnet是一对多关系，即同一Network的Subnet可以是不同的IP段，但CIDR不能重叠</li><li>不同Network的Subnet的CIDR和IP都可以重叠，因为Neutron的router通过Linux Network Namespace实现。Network Namespace，网络隔离机制，通过网络命名空间的每个router都有各自独立的路由表，若两个subnet通过同一个router路由，根据router的配置，只有指定的一个subnet可被路由；若两个subnet通过不同router路由，因为router的路由表是独立的，所以两个subnet都可被路由</li></ul><h2 id="3-Port"><a href="#3-Port" class="headerlink" title="3.Port"></a>3.Port</h2><p>端口，即虚拟交换机的端口，其上定义了MAC地址和IP地址，并将MAC地址和IP地址分配给绑定某个端口的虚拟机实例的虚拟网卡VIF（Virtual Interface）</p><ul><li>注：Project、Network、Subnet、Port之间的关系：Project 1 : m Network 1 : m Subnet 1 : m Port 1 : 1 VIF m : 1 Instance</li></ul><h1 id="体系架构"><a href="#体系架构" class="headerlink" title="体系架构"></a>体系架构</h1><p>Neutron作为Openstack的核心组件，采用分布式架构，由多个组件共同对外提供网络服务</p><p><img src="/img/wiki/openstack/openstack011.jpg" alt="openstack011"></p><h2 id="1-Neutron-Server"><a href="#1-Neutron-Server" class="headerlink" title="1.Neutron Server"></a>1.Neutron Server</h2><p>运行于控制节点，对外提供OpenStack网络API的入口接收请求，并调用Plugin进行处理</p><h2 id="2-Neutron-Plugin"><a href="#2-Neutron-Plugin" class="headerlink" title="2.Neutron Plugin"></a>2.Neutron Plugin</h2><p>处理Neutron Server发来的请求，维护OpenStack逻辑网络状态，并调用Agent处理请求</p><h2 id="3-Neutron-Agent"><a href="#3-Neutron-Agent" class="headerlink" title="3.Neutron Agent"></a>3.Neutron Agent</h2><p>处理Plugin的请求，负责在network provider上真正实现各种网络功能</p><h2 id="4-Network-Provider"><a href="#4-Network-Provider" class="headerlink" title="4.Network Provider"></a>4.Network Provider</h2><p>提供网络服务的虚拟或物理网络设备，如Linux Bridge、OpenvSwitch或其他支持Neutron的物理交换机</p><h2 id="5-Queue"><a href="#5-Queue" class="headerlink" title="5.Queue"></a>5.Queue</h2><p>Neutron Server、Plugin和Agent之间通过Messaging Queue通信和调用</p><h2 id="6-Database"><a href="#6-Database" class="headerlink" title="6.Database"></a>6.Database</h2><p>OpenStack用于存储网络状态信息，如Network、Subnet、Port、Router等</p><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><hr><ul><li><a href="https://www.cnblogs.com/vincenshen/articles/6775236.html">https://www.cnblogs.com/vincenshen/articles/6775236.html</a></li><li><a href="https://blog.csdn.net/chenxiangui88/article/details/78093318">https://blog.csdn.net/chenxiangui88/article/details/78093318</a></li><li><a href="https://admin.dandelioncloud.cn/article/details/1435377339587969026">https://admin.dandelioncloud.cn/article/details/1435377339587969026</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>网络</tag>
      
      <tag>虚拟化</tag>
      
      <tag>私有云</tag>
      
      <tag>Openstack</tag>
      
      <tag>公有云</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux系统配置网卡bond</title>
    <link href="/linux/Bond/"/>
    <url>/linux/Bond/</url>
    
    <content type="html"><![CDATA[<p>Bond，网络绑定或链路聚合，即将多个网络接口绑定为一个逻辑网卡对外提供服务，从而实现网络流量的负载均衡，以及网卡级的冗余与扩容，提高网络总体可用性。Linux系统网卡绑定分为7种工作模式，其中mode0&#x2F;1&#x2F;6这三种模式最为常用</p><h1 id="1-mode-x3D-0"><a href="#1-mode-x3D-0" class="headerlink" title="1.mode&#x3D;0"></a>1.mode&#x3D;0</h1><p>balance-rr，Round-Robin Policy，即平衡轮训策略，工作机制是将网络数据包按照轮询方式依次地平均分配到所有被绑定的网络接口，链路故障自动切换。优点是具备负载均衡和容错能力，增加网络吞吐量，较为常用。但需要交换机配置端口静态聚合，且网络接口轮训的链路可能出现数据包无序到达而需要重新发送，从而影响网络吞吐量</p><h2 id="1-1-虚拟机新增网卡"><a href="#1-1-虚拟机新增网卡" class="headerlink" title="1.1 虚拟机新增网卡"></a>1.1 虚拟机新增网卡</h2><h2 id="1-2-配置网卡"><a href="#1-2-配置网卡" class="headerlink" title="1.2 配置网卡"></a>1.2 配置网卡</h2><h3 id="1-2-1-配置网卡ens2"><a href="#1-2-1-配置网卡ens2" class="headerlink" title="1.2.1 配置网卡ens2"></a>1.2.1 配置网卡ens2</h3><pre><code class="hljs">sudo cp /etc/sysconfig/network-scripts/ifcfg-ens2 /etc/sysconfig/network-scripts/ifcfg-ens2.baksudo vi /etc/sysconfig/network-scripts/ifcfg-ens2 TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noDEFROUTE=yesNAME=ens2UUID=2a8c33e3-34e6-4c1b-aaef-258efba38bf6DEVICE=ens2ONBOOT=yesBOOTPROTO=noneMASTER=bond0SLAVE=yesUSERCTL=no</code></pre><h3 id="1-2-2-配置网卡ens6"><a href="#1-2-2-配置网卡ens6" class="headerlink" title="1.2.2 配置网卡ens6"></a>1.2.2 配置网卡ens6</h3><pre><code class="hljs">sudo vi /etc/sysconfig/network-scripts/ifcfg-ens6TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noDEFROUTE=yesNAME=ens6UUID=2a8c33e3-34e6-4c1b-aaef-258efba38bf9DEVICE=ens6ONBOOT=yesBOOTPROTO=noneMASTER=bond0SLAVE=yesUSERCTL=no</code></pre><h2 id="1-3-配置网卡Bond"><a href="#1-3-配置网卡Bond" class="headerlink" title="1.3 配置网卡Bond"></a>1.3 配置网卡Bond</h2><pre><code class="hljs">sudo vi /etc/sysconfig/network-scripts/ifcfg-bond0TYPE=EthernetDEVICE=bond0ONBOOT=yesBOOTPROTO=staticIPADDR=192.168.100.120NETMASK=255.255.255.0GATEWAY=192.168.100.1DNS1=192.168.100.1DNS2=8.8.8.8BONDING_OPTS=&quot;miimon=100 mode=0 fail_over_mac=1&quot;</code></pre><h2 id="1-4-重启网卡以生效配置"><a href="#1-4-重启网卡以生效配置" class="headerlink" title="1.4 重启网卡以生效配置"></a>1.4 重启网卡以生效配置</h2><h1 id="2-mode-x3D-1"><a href="#2-mode-x3D-1" class="headerlink" title="2.mode&#x3D;1"></a>2.mode&#x3D;1</h1><p>active-backup，Active-Backup Policy，即主备策略，也就是主设备处于活动状态，备用设备只有在主设备故障才转换为主设备接管服务。优点是具备容错和冗余功能，保障了网络稳定性，较为常用。但资源利用率低，因为只有一个设备处于工作状态</p><h2 id="2-1-虚拟机新增网卡"><a href="#2-1-虚拟机新增网卡" class="headerlink" title="2.1 虚拟机新增网卡"></a>2.1 虚拟机新增网卡</h2><h2 id="2-2-配置网卡"><a href="#2-2-配置网卡" class="headerlink" title="2.2 配置网卡"></a>2.2 配置网卡</h2><pre><code class="hljs">sudo cp /etc/netplan/01-netcfg.yaml /etc/netplan/01-netcfg.yaml.baksudo vi /etc/netplan/01-netcfg.yaml# This file describes the network interfaces available on your system# For more information, see netplan(5).network:  version: 2  renderer: networkd  ethernets:    eth0:      dhcp4: no      dhcp6: no    eth1:      dhcp4: no      dhcp6: no  bonds:    bond1:      interfaces:        - eth0        - eth1      addresses: [192.168.100.120/24]      gateway4: 192.168.100.1      nameservers:        addresses: [192.168.100.1,8.8.8.8]      parameters:        mode: active-backup        mii-monitor-interval: 100</code></pre><h2 id="2-3-应用网卡配置"><a href="#2-3-应用网卡配置" class="headerlink" title="2.3 应用网卡配置"></a>2.3 应用网卡配置</h2><pre><code class="hljs">sudo netplan apply</code></pre><h1 id="3-mode-x3D-2"><a href="#3-mode-x3D-2" class="headerlink" title="3.mode&#x3D;2"></a>3.mode&#x3D;2</h1><p>balance-xor，XOR Policy，即平衡哈希策略，工作机制是将网络数据包基于HASH运算的方式平均分配到所有被绑定的网络接口，默认算法为：(源MAC地址 XOR 目标MAC地址) % slave数量。优点是具备负载均衡和容错能力，但需要交换机配置端口静态聚合，且交换机需要支持哈希分配，不常用</p><h1 id="4-mode-x3D-3"><a href="#4-mode-x3D-3" class="headerlink" title="4.mode&#x3D;3"></a>4.mode&#x3D;3</h1><p>broadcast，广播策略，即是将所有数据包发送到所有接口以实现广播传输，不提供负载均衡，只有冗余功能，资源浪费，且需要交换机配置端口静态聚合，不常用。但适用于高可靠性环境，如金融行业</p><h1 id="5-mode-x3D-4"><a href="#5-mode-x3D-4" class="headerlink" title="5.mode&#x3D;4"></a>5.mode&#x3D;4</h1><p>Dynamic Link aggregation Policy，即动态链接聚合策略，工作机制是基于IEEE 802.3ad规范创建共享速率和双工设定的网卡聚合组，外出网络流量的链路基于传输hash进行选举。优点是具备容错和冗余功能，但需要交换机支持动态链接聚合，且需配置LACP（Link Aggregation Control Protocol），不常用</p><h1 id="6-mode-x3D-5"><a href="#6-mode-x3D-5" class="headerlink" title="6.mode&#x3D;5"></a>6.mode&#x3D;5</h1><p>balance-tlb，Adaptive Transmit Load Balancing，即适配器传输负载均衡策略，工作机制是基于TLB算法分配外出网络流量，入口流量只有一个。优点是具备容错和冗余功能，但需要网络设备支持获取每个slave的速率，不常用</p><h1 id="7-mode-x3D-6"><a href="#7-mode-x3D-6" class="headerlink" title="7.mode&#x3D;6"></a>7.mode&#x3D;6</h1><p>balance-alb，Adaptive load balancing，即适配器适应性负载均衡策略，工作机制是基于ALB算法在传输端和接收端都进行负载均衡，也就是mode5方式的升级版，接收端负载均衡通过ARP协商实现，并使得不同的对端使用不同的硬件地址进行通信。该模式不需要特殊的交换机支持，网卡自动聚合。但不同于mode0的所有接口流量均衡，而是先占满第一个接口，再依次分配，所以会出现第一个接口流量非常高而其余接口只有小部分流量的情况，不均衡</p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/wangzongyu/article/details/127097986">https://blog.csdn.net/wangzongyu/article/details/127097986</a></li><li><a href="https://blog.csdn.net/weixin_44265455/article/details/139479821">https://blog.csdn.net/weixin_44265455/article/details/139479821</a></li><li><a href="https://blog.csdn.net/weixin_45548465/article/details/122625777">https://blog.csdn.net/weixin_45548465/article/details/122625777</a></li><li><a href="https://blog.csdn.net/hezuijiudexiaobai/article/details/131216840">https://blog.csdn.net/hezuijiudexiaobai/article/details/131216840</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prometheus监控Kubernetes集群</title>
    <link href="/linux/Prometheus-Kubernetes/"/>
    <url>/linux/Prometheus-Kubernetes/</url>
    
    <content type="html"><![CDATA[<p>Prometheus部署于Kubernetes集群内部将会增加集群的资源开销，也不利于监控系统的维护，因此建议独立部署于集群之外。Prometheus对Kubernetes集群的监控涉及到的指标分为三类，即Node节点性能指标、集群组件及资源状态指标和容器资源指标</p><h1 id="1-Node节点性能指标"><a href="#1-Node节点性能指标" class="headerlink" title="1.Node节点性能指标"></a>1.Node节点性能指标</h1><p>节点CPU、内存、磁盘空间及IO、TCP连接数和网络流量等，由节点上部署的node_exporter进行暴露。此外，也可使用textfile采集器自定义监控指标</p><h1 id="2-集群组件及资源状态指标"><a href="#2-集群组件及资源状态指标" class="headerlink" title="2.集群组件及资源状态指标"></a>2.集群组件及资源状态指标</h1><p>集群组件api-serverr、controller-manager、kube-scheduler、kube-proxy，以及集群资源对象Deployment&#x2F;Pod&#x2F;Daemonset&#x2F;Statefulset&#x2F;PV等状态指标，由kube-state-metrics组件进行暴露，该组件通过监听Kubernetes集群ApiServer生成不同资源的状态的Metrics数据，其中8080端口用于暴露Kubernetes集群指标数据，8081端口用于暴露kube-state-metrics服务的指标数据</p><h1 id="3-容器资源指标"><a href="#3-容器资源指标" class="headerlink" title="3.容器资源指标"></a>3.容器资源指标</h1><p>Node节点上运行的容器CPU、内存、文件系统和网络资源指标，由集群组件kubelet内置的cAdvisor进行暴露，通过kubelet&#x2F;metrics&#x2F;cadvisor接口进行采集</p><h1 id="1-创建集群外部访问凭证"><a href="#1-创建集群外部访问凭证" class="headerlink" title="1.创建集群外部访问凭证"></a>1.创建集群外部访问凭证</h1><h2 id="1-1-创建RBAC资源文件"><a href="#1-1-创建RBAC资源文件" class="headerlink" title="1.1 创建RBAC资源文件"></a>1.1 创建RBAC资源文件</h2><pre><code class="hljs">vi promethues-rbac.yamlapiVersion: v1kind: ServiceAccountmetadata:  name: prometheus  namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata:  name: prometheusrules:- apiGroups:  - &quot;&quot;  resources:  - nodes  - services  - endpoints  - pods  - nodes/proxy  verbs:  - get  - list  - watch- apiGroups:  - &quot;extensions&quot;  resources:    - ingresses  verbs:  - get  - list  - watch- apiGroups:  - &quot;&quot;  resources:  - configmaps  - nodes/metrics  verbs:  - get- nonResourceURLs:  - /metrics  verbs:  - get---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  name: prometheusroleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: prometheussubjects:- kind: ServiceAccount  name: prometheus  namespace: kube-system</code></pre><h2 id="1-2-创建RBAC"><a href="#1-2-创建RBAC" class="headerlink" title="1.2 创建RBAC"></a>1.2 创建RBAC</h2><pre><code class="hljs">kubectl apply -f promethues-rbac.yaml </code></pre><h2 id="1-3-创建外部访问凭证"><a href="#1-3-创建外部访问凭证" class="headerlink" title="1.3 创建外部访问凭证"></a>1.3 创建外部访问凭证</h2><pre><code class="hljs">kubectl -n kube-system describe secrets prometheus-token-5stl8 | grep tokenvi token.kuberneteseyJhbGciOiJSUzI1NiIsImtpZCI6InFhd0Q4b2VONWZaTV9fcXBUa1J6dGZZaE83WDhCNG44Uno0QWh3UnpkdTAifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJwcm9tZXRoZXVzLXRva2VuLWxuNXpiIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6InByb21ldGhldXMiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI5NjZlMjdhYy04MWZjLTQ0MTQtODVkOS0xMmJiOWQ0YTI3N2YiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06cHJvbWV0aGV1cyJ9.dWeO5OpjI4EewpDeicF6i-DwaYWPL8pwsmXEc8ZZm-OM15KjwZXzFpYIhnkvXPpA38HZOBpNg7gfQewTNZbQdhTrBCLX9UQgGYLzkEnxoxy8VAC0QHGs8tGsSC2dQVhVkwL_Xkgtse9V7ArNHDgfoG_78W3TNjEoDExRtrkUi0pEAFazWWnL9sGm-P1bF6S5iw0mFIydpz8ul4csMvHVYW51iegRHHoLdtdL4o5ys5QIvFGpfJal2JRayRn4IbtlvY-k7b4wSPEBrfZlX0v1_hpk7jPcYek7w9G665A_PeoqSN_OvgyCY9A_FZNDpFek2uH4Jm2gu5vPwSxJp1SJQA</code></pre><ul><li><p>注：Kubernetes集群1.23及之前的版本ServiceAccount会自动创建Secret，但之后的版本需自行创建secret，绑定SA，YAML文件为：</p><pre><code class="hljs">apiVersion: v1kind: Secrettype: kubernetes.io/service-account-tokenmetadata:  name: prometheus  namespace: kube-system  annotations:    kubernetes.io/service-account.name: &quot;prometheus&quot;</code></pre></li></ul><h2 id="1-4-将访问凭证发送到Prometheus服务器"><a href="#1-4-将访问凭证发送到Prometheus服务器" class="headerlink" title="1.4 将访问凭证发送到Prometheus服务器"></a>1.4 将访问凭证发送到Prometheus服务器</h2><pre><code class="hljs">scp token.kubernetes node01:/usr/local/prometheus</code></pre><h1 id="2-Kubernetes集群部署kube-state-metrics"><a href="#2-Kubernetes集群部署kube-state-metrics" class="headerlink" title="2.Kubernetes集群部署kube-state-metrics"></a>2.Kubernetes集群部署kube-state-metrics</h1><h2 id="2-1-部署kube-state-metrics"><a href="#2-1-部署kube-state-metrics" class="headerlink" title="2.1 部署kube-state-metrics"></a>2.1 部署kube-state-metrics</h2><pre><code class="hljs">curl -o kube-state-metrics-2.0.0.tar.gz https://github.com/kubernetes/kube-state-metrics/archive/refs/tags/v2.0.0.tar.gztar -xzvf kube-state-metrics-2.0.0.tar.gz &amp;&amp; cd kube-state-metrics-2.0.0/examples/standardkubectl apply -f .</code></pre><h1 id="3-Node节点监控配置"><a href="#3-Node节点监控配置" class="headerlink" title="3.Node节点监控配置"></a>3.Node节点监控配置</h1><p>node_exporter可安装在集群内或集群外，集群内即以DaemonSet方式进行部署</p><pre><code class="hljs">- job_name: kubernetes-nodes  scheme: http   tls_config:     insecure_skip_verify: true   bearer_token_file: /usr/local/prometheus/token.kubernetes  kubernetes_sd_configs:   - role: node     api_server: https://172.18.100.100:8443     tls_config:       insecure_skip_verify: true     bearer_token_file: /usr/local/prometheus/token.kubernetes  relabel_configs:     - source_labels: [__address__]       regex: (.*):10250      replacement: $&#123;1&#125;:9100      target_label: __address__       action: replace     - action: labelmap       regex: __meta_kubernetes_node_label_(.+)</code></pre><h1 id="4-集群组件监控配置"><a href="#4-集群组件监控配置" class="headerlink" title="4.集群组件监控配置"></a>4.集群组件监控配置</h1><h2 id="4-1-api-server监控配置"><a href="#4-1-api-server监控配置" class="headerlink" title="4.1 api-server监控配置"></a>4.1 api-server监控配置</h2><p>api-server的service部署于default命名空间，标签为component&#x3D;apiserver，访问方式为https，端口为6443，因此配置基于service的endpoints服务发现即可</p><pre><code class="hljs">- job_name: apiserver  kubernetes_sd_configs:   - role: endpoints    api_server: https://172.18.100.100:8443    tls_config:       insecure_skip_verify: true    bearer_token_file: /usr/local/prometheus/token.kubernetes  scheme: https   tls_config:     insecure_skip_verify: true   bearer_token_file: /usr/local/prometheus/token.kubernetes  relabel_configs:  - source_labels: [__meta_kubernetes_namespace,__meta_kubernetes_service_name]    action: keep    regex: default;kubernetes  - source_labels: [__meta_kubernetes_endpoints_name]    action: replace    target_label: endpoint  - source_labels: [__meta_kubernetes_service_name]    action: replace    target_label: service  - source_labels: [__meta_kubernetes_namespace]    action: replace    target_label: namespace</code></pre><h2 id="4-2-controller-manager监控配置"><a href="#4-2-controller-manager监控配置" class="headerlink" title="4.2 controller-manager监控配置"></a>4.2 controller-manager监控配置</h2><p>kube-controller-manager部署于kube-system命名空间，标签为component&#x3D;kube-controller-manager，默认没有配置service</p><h3 id="4-2-1-创建service"><a href="#4-2-1-创建service" class="headerlink" title="4.2.1 创建service"></a>4.2.1 创建service</h3><pre><code class="hljs">vi kube-controller-manager-service.yamlapiVersion: v1kind: Servicemetadata:  labels:    k8s-app: kube-controller-manager  name: kube-controller-manager  namespace: kube-systemspec:  selector:    component: kube-controller-manager  type: ClusterIP  clusterIP: None  ports:  - name: https-metrics    port: 10252    targetPort: 10252    protocol: TCP</code></pre><h3 id="4-2-2-配置监控端口"><a href="#4-2-2-配置监控端口" class="headerlink" title="4.2.2 配置监控端口"></a>4.2.2 配置监控端口</h3><pre><code class="hljs">sudo vi /etc/kubernetes/manifests/kube-controller-manager.yaml# 设置端口绑定，允许外部访问--bind-address=0.0.0.0# 设置监控端口，默认为0，表示开启https监控端口10257，此处配置文件http监控端口10252--port=10252</code></pre><h3 id="4-2-3-配置Prometheus"><a href="#4-2-3-配置Prometheus" class="headerlink" title="4.2.3 配置Prometheus"></a>4.2.3 配置Prometheus</h3><pre><code class="hljs">- job_name: kubernetes-controller-manager  kubernetes_sd_configs:  - role: pod    api_server: https://172.18.100.100:8443    tls_config:      insecure_skip_verify: true    bearer_token_file: /usr/local/prometheus/token.kubernetes  scheme: https  tls_config:    insecure_skip_verify: true  bearer_token_file: /usr/local/prometheus/token.kubernetes  relabel_configs:  - source_labels: [__meta_kubernetes_pod_label_component]    regex: kube-controller-manager    action: keep  - source_labels: [__meta_kubernetes_pod_ip]    regex: (.+)    target_label: __address__    replacement: $&#123;1&#125;:10252  - source_labels: [__meta_kubernetes_endpoints_name]    action: replace    target_label: endpoint  - source_labels: [__meta_kubernetes_pod_name]    action: replace    target_label: pod  - source_labels: [__meta_kubernetes_service_name]    action: replace    target_label: service  - source_labels: [__meta_kubernetes_namespace]    action: replace    target_label: namespace</code></pre><h2 id="4-3-scheduler监控配置"><a href="#4-3-scheduler监控配置" class="headerlink" title="4.3 scheduler监控配置"></a>4.3 scheduler监控配置</h2><p>kube-scheduler部署于kube-system命名空间，匹配Pod对象，标签为component&#x3D;kube-scheduler，默认没有配置service</p><h3 id="4-3-1-创建service"><a href="#4-3-1-创建service" class="headerlink" title="4.3.1 创建service"></a>4.3.1 创建service</h3><pre><code class="hljs">vi kube-scheduler.yamlapiVersion: v1kind: Servicemetadata:  namespace: kube-system  name: kube-scheduler  labels:    k8s-app: kube-schedulerspec:  selector:    component: kube-scheduler  type: ClusterIP  clusterIP: None  ports:  - name: http-metrics    port: 10251    targetPort: 10251    protocol: TCP</code></pre><h3 id="4-3-2-配置监控端口"><a href="#4-3-2-配置监控端口" class="headerlink" title="4.3.2 配置监控端口"></a>4.3.2 配置监控端口</h3><pre><code class="hljs">sudo vi /etc/kubernetes/manifests/kube-scheduler.yaml# 设置端口绑定，允许外部访问--bind-address=0.0.0.0# 设置监控端口，默认为0，表示开启https监控端口10259，此处配置文件http监控端口10251--port=10251</code></pre><h3 id="4-3-3-配置Prometheus"><a href="#4-3-3-配置Prometheus" class="headerlink" title="4.3.3 配置Prometheus"></a>4.3.3 配置Prometheus</h3><pre><code class="hljs">- job_name: kubernetes-scheduler  kubernetes_sd_configs:  - role: pod    api_server: https://172.18.100.100:8443    tls_config:      insecure_skip_verify: true    bearer_token_file: /usr/local/prometheus/token.kubernetes  scheme: https  tls_config:    insecure_skip_verify: true  bearer_token_file: /usr/local/prometheus/token.kubernetes  relabel_configs:  - source_labels: [__meta_kubernetes_pod_label_component]    regex: kube-scheduler    action: keep  - source_labels: [__meta_kubernetes_pod_ip]    regex: (.+)    target_label: __address__    replacement: $&#123;1&#125;:10251  - source_labels: [__meta_kubernetes_endpoints_name]    action: replace    target_label: endpoint  - source_labels: [__meta_kubernetes_pod_name]    action: replace    target_label: pod  - source_labels: [__meta_kubernetes_service_name]    action: replace    target_label: service  - source_labels: [__meta_kubernetes_namespace]    action: replace    target_label: namespace</code></pre><h2 id="4-4-kubelet监控配置"><a href="#4-4-kubelet监控配置" class="headerlink" title="4.4 kubelet监控配置"></a>4.4 kubelet监控配置</h2><p>kubelet组件集成cAdvisor，通过&#x2F;metrics&#x2F;cadvisor端点暴露容器性能指标，也可通过&#x2F;metrics端点暴露监控指标，端口为10250，role为node</p><pre><code class="hljs">- job_name: kubernetes-kubelet  metrics_path: /metrics/cadvisor  scheme: https  tls_config:    insecure_skip_verify: true  bearer_token_file: /usr/local/prometheus/token.kubernetes  kubernetes_sd_configs:  - role: node    api_server: https://172.18.100.100:8443    tls_config:      insecure_skip_verify: true    bearer_token_file: /usr/local/prometheus/token.kubernetes  relabel_configs:  - action: labelmap    regex: __meta_kubernetes_node_label_(.+)  - source_labels: [__meta_kubernetes_endpoints_name]    action: replace    target_label: endpoint  - source_labels: [__meta_kubernetes_pod_name]    action: replace    target_label: pod  - source_labels: [__meta_kubernetes_namespace]    action: replace    target_label: namespace  - source_labels: [__meta_kubernetes_node_address_Hostname]    action: replace    target_label: node</code></pre><h2 id="4-5-proxy监控配置"><a href="#4-5-proxy监控配置" class="headerlink" title="4.5 proxy监控配置"></a>4.5 proxy监控配置</h2><p>kube-proxy组件http监控端口为10249，通过&#x2F;metrics暴露监控指标，监控role为endpoints</p><h3 id="4-5-1-配置监控端口"><a href="#4-5-1-配置监控端口" class="headerlink" title="4.5.1 配置监控端口"></a>4.5.1 配置监控端口</h3><pre><code class="hljs">kubectl -n kube-system edit configmap kube-proxy# 设置监控端口，允许外部访问metricsBindAddress: 0.0.0.0:10249</code></pre><ul><li>注：需要重启组件才能生效，kubectl get pods -n kube-system | grep kube-proxy |awk ‘{print $1}’ | xargs kubectl delete pods -n kube-system</li></ul><h3 id="4-5-2-创建service"><a href="#4-5-2-创建service" class="headerlink" title="4.5.2 创建service"></a>4.5.2 创建service</h3><pre><code class="hljs">sudo vi kube-proxy-service.yamlapiVersion: v1kind: Servicemetadata:  labels:    k8s-app: kube-proxy  name: kube-proxy  namespace: kube-systemspec:  selector:    k8s-app: kube-proxy  type: ClusterIP  clusterIP: None  ports:  - name: https-metrics    port: 10249    targetPort: 10249    protocol: TCP</code></pre><h3 id="4-5-2-配置Prometheus"><a href="#4-5-2-配置Prometheus" class="headerlink" title="4.5.2 配置Prometheus"></a>4.5.2 配置Prometheus</h3><pre><code class="hljs">- job_name: kubernetes-proxy  kubernetes_sd_configs:  - role: endpoints    api_server: https://172.18.100.100:8443    tls_config:      insecure_skip_verify: true    bearer_token_file: /usr/local/prometheus/token.kubernetes  scheme: http  tls_config:    insecure_skip_verify: true  bearer_token_file: /usr/local/prometheus/token.kubernetes  relabel_configs:  - source_labels: [__meta_kubernetes_service_name]    regex: kube-proxy    action: keep  - action: labelmap    regex: __meta_kubernetes_pod_label_(.+)</code></pre><h1 id="5-集群资源监控配置"><a href="#5-集群资源监控配置" class="headerlink" title="5.集群资源监控配置"></a>5.集群资源监控配置</h1><h2 id="5-1-Pod及容器监控配置"><a href="#5-1-Pod及容器监控配置" class="headerlink" title="5.1 Pod及容器监控配置"></a>5.1 Pod及容器监控配置</h2><pre><code class="hljs">- job_name: kubernetes-pods   scheme: https  tls_config:    insecure_skip_verify: true  bearer_token_file: /usr/local/prometheus/token.kubernetes  kubernetes_sd_configs:   - role: pod     api_server: https://172.18.100.100:8443    tls_config:       insecure_skip_verify: true     bearer_token_file: /usr/local/prometheus/token.kubernetes  relabel_configs:   - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]    action: keep    regex: true  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]    action: replace    target_label: __metrics_path__    regex: (.+)  - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]    action: replace    regex: ([^:]+)(?::\d+)?;(\d+)    replacement: $1:$2    target_label: __address__  - action: labelmap    regex: __meta_kubernetes_pod_label_(.+)  - source_labels: [__meta_kubernetes_namespace]    action: replace    target_label: kubernetes_namespace  - source_labels: [__meta_kubernetes_pod_name]    action: replace    target_label: kubernetes_pod_name</code></pre><h2 id="5-2-Services后端Endpoint监控配置"><a href="#5-2-Services后端Endpoint监控配置" class="headerlink" title="5.2 Services后端Endpoint监控配置"></a>5.2 Services后端Endpoint监控配置</h2><pre><code class="hljs">- job_name: kubernetes-service-endpoints  kubernetes_sd_configs:  - role: endpoints    api_server: https://172.18.100.100:8443    bearer_token_file: /usr/local/prometheus/token.kubernetes    tls_config:      insecure_skip_verify: true  bearer_token_file: /usr/local/prometheus/token.kubernetes  tls_config:    insecure_skip_verify: true  relabel_configs:  - action: keep    regex: true    source_labels:    - __meta_kubernetes_service_annotation_prometheus_io_scrape  - action: replace    regex: (https?)    source_labels:    - __meta_kubernetes_service_annotation_prometheus_io_scheme    target_label: __scheme__  - action: replace    regex: (.+)    source_labels:    - __meta_kubernetes_service_annotation_prometheus_io_path    target_label: __metrics_path__  - action: replace    regex: ([^:]+)(?::\d+)?;(\d+)    replacement: $1:$2    source_labels:    - __address__    - __meta_kubernetes_service_annotation_prometheus_io_port    target_label: __address__  - action: labelmap    regex: __meta_kubernetes_service_label_(.+)  - action: replace    source_labels:    - __meta_kubernetes_namespace    target_label: kubernetes_namespace  - action: replace    source_labels:    - __meta_kubernetes_service_name    target_label: kubernetes_service_name</code></pre><h2 id="5-3-CoreDNS监控配置"><a href="#5-3-CoreDNS监控配置" class="headerlink" title="5.3 CoreDNS监控配置"></a>5.3 CoreDNS监控配置</h2><p>CoreDNS组件通过service暴露9153监控端口，监控接口为&#x2F;metrics</p><pre><code class="hljs">- job_name: kubernetes-coredns  kubernetes_sd_configs:  - role: endpoints    api_server: https://172.18.100.100:8443    tls_config:      insecure_skip_verify: true    bearer_token_file: /usr/local/prometheus/token.kubernetes  scheme: http  tls_config:    insecure_skip_verify: true  bearer_token_file: /usr/local/prometheus/token.kubernetes  relabel_configs:  - source_labels:      - __meta_kubernetes_service_label_k8s_app    regex: kube-dns    action: keep  - source_labels: [__meta_kubernetes_pod_ip]    regex: (.+)    target_label: __address__    replacement: $&#123;1&#125;:9153  - source_labels: [__meta_kubernetes_endpoints_name]    action: replace    target_label: endpoint  - source_labels: [__meta_kubernetes_pod_name]    action: replace    target_label: pod  - source_labels: [__meta_kubernetes_service_name]    action: replace    target_label: service  - source_labels: [__meta_kubernetes_namespace]    action: replace    target_label: namespace</code></pre><h2 id="5-4-Service监控配置"><a href="#5-4-Service监控配置" class="headerlink" title="5.4 Service监控配置"></a>5.4 Service监控配置</h2><pre><code class="hljs">- job_name: &quot;kubernetes-services&quot;  kubernetes_sd_configs:  - role: service  metrics_path: /probe  params:    module: [http_2xx]  relabel_configs:  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]    action: keep    regex: true  - source_labels: [__address__]    target_label: __param_target  - target_label: __address__    replacement: blackbox-exporter.example.com:9115  - source_labels: [__param_target]    target_label: instance  - action: labelmap    regex: __meta_kubernetes_service_label_(.+)  - source_labels: [__meta_kubernetes_namespace]    target_label: kubernetes_namespace  - source_labels: [__meta_kubernetes_service_name]    target_label: kubernetes_name</code></pre><h2 id="5-5-Ingress监控配置"><a href="#5-5-Ingress监控配置" class="headerlink" title="5.5 Ingress监控配置"></a>5.5 Ingress监控配置</h2><pre><code class="hljs">- job_name: &quot;kubernetes-ingresses&quot;  kubernetes_sd_configs:  - role: ingress  relabel_configs:  - source_labels: [__meta_kubernetes_ingress_annotation_prometheus_io_probe]    action: keep    regex: true  - source_labels: [__meta_kubernetes_ingress_scheme,__address__,__meta_kubernetes_ingress_path]    regex: (.+);(.+);(.+)    replacement: $&#123;1&#125;://$&#123;2&#125;$&#123;3&#125;    target_label: __param_target  - target_label: __address__    replacement: blackbox-exporter.example.com:9115  - source_labels: [__param_target]    target_label: instance  - action: labelmap    regex: __meta_kubernetes_ingress_label_(.+)  - source_labels: [__meta_kubernetes_namespace]    target_label: kubernetes_namespace  - source_labels: [__meta_kubernetes_ingress_name]    target_label: kubernetes_name</code></pre><h2 id="5-6-kube-state-metrics监控配置"><a href="#5-6-kube-state-metrics监控配置" class="headerlink" title="5.6 kube-state-metrics监控配置"></a>5.6 kube-state-metrics监控配置</h2><pre><code class="hljs">- job_name: kubernetes-state-metrics  scheme: http  kubernetes_sd_configs:  - api_server: https://172.18.100.100:8443    role: endpoints    namespaces:      names: kube-system    bearer_token_file: /usr/local/prometheus/token.kubernetes    tls_config:      insecure_skip_verify: true  tls_config:    insecure_skip_verify: true  bearer_token_file: /usr/local/prometheus/token.kubernetes  relabel_configs:  - action: keep    source_labels: [__meta_kubernetes_service_label_app_kubernetes_io_name]    regex: kube-state-metrics  - action: labelmap    regex: __meta_kubernetes_service_label_(.+)  - action: replace    source_labels: [__meta_kubernetes_namespace]    target_label: kubernetes_namespace  - action: replace    source_labels: [__meta_kubernetes_service_name]    target_label: kubernetes_service_name</code></pre><h1 id="6-配置告警规则"><a href="#6-配置告警规则" class="headerlink" title="6.配置告警规则"></a>6.配置告警规则</h1><pre><code class="hljs">sudo vi /usr/local/prometheus/rules/kubernetes.ymlgroups:- name: kubernetes-system  rules:  - alert: KubeVersionMismatch    annotations:      description: There are &#123;&#123; $value &#125;&#125; different semantic versions of Kubernetes components running.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubeversionmismatch      summary: Different semantic versions of Kubernetes components running.    expr: |      count(count by (git_version) (label_replace(kubernetes_build_info&#123;job!~&quot;kube-dns|coredns&quot;&#125;,&quot;git_version&quot;,&quot;$1&quot;,&quot;git_version&quot;,&quot;(v[0-9]*.[0-9]*).*&quot;))) &gt; 1    for: 15m    labels:      severity: Warning  - alert: KubeClientErrors    annotations:      description: Kubernetes API server client &#39;&#123;&#123; $labels.job &#125;&#125;/&#123;&#123; $labels.instance &#125;&#125;&#39; is experiencing &#123;&#123; $value | humanizePercentage &#125;&#125; errors.&#39;      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubeclienterrors      summary: Kubernetes API server client is experiencing errors.    expr: |      (sum(rate(rest_client_requests_total&#123;code=~&quot;5..&quot;&#125;[5m])) by (instance, job)      /      sum(rate(rest_client_requests_total[5m])) by (instance, job))      &gt; 0.01    for: 15m    labels:      severity: Warning- name: kubernetes-apiserver  rules:  - alert: KubeClientCertificateExpiration    annotations:      description: A client certificate used to authenticate to the apiserver is expiring in less than 7.0 days.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubeclientcertificateexpiration      summary: Client certificate is about to expire.    expr: apiserver_client_certificate_expiration_seconds_count&#123;job=&quot;apiserver&quot;&#125; &gt; 0 and on(job) histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket&#123;job=&quot;apiserver&quot;&#125;[5m]))) &lt; 604800    labels:      severity: Warning  - alert: KubeClientCertificateExpiration    annotations:      description: A client certificate used to authenticate to the apiserver is expiring in less than 24.0 hours.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubeclientcertificateexpiration      summary: Client certificate is about to expire.  expr: |    apiserver_client_certificate_expiration_seconds_count&#123;job=&quot;apiserver&quot;&#125; &gt; 0 and on(job) histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket&#123;job=&quot;apiserver&quot;&#125;[5m]))) &lt; 86400  labels:    severity: critical- alert: AggregatedAPIErrors  annotations:    description: An aggregated API &#123;&#123; $labels.name &#125;&#125;/&#123;&#123; $labels.namespace &#125;&#125; has reported errors. It has appeared unavailable &#123;&#123; $value | humanize &#125;&#125; times averaged over the past 10m.    runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/aggregatedapierrors    summary: An aggregated API has reported errors.  expr: |    sum by(name, namespace)(increase(aggregator_unavailable_apiservice_total[10m])) &gt; 4  labels:    severity: warning- alert: AggregatedAPIDown  annotations:    description: An aggregated API &#123;&#123; $labels.name &#125;&#125;/&#123;&#123; $labels.namespace &#125;&#125; has been only &#123;&#123; $value | humanize &#125;&#125;% available over the last 10m.    runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/aggregatedapidown    summary: An aggregated API is down.  expr: |    (1 - max by(name, namespace)(avg_over_time(aggregator_unavailable_apiservice[10m]))) * 100 &lt; 85  for: 5m  labels:    severity: warning- alert: KubeAPIDown  annotations:    description: KubeAPI has disappeared from Prometheus target discovery.    runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubeapidown    summary: Target disappeared from Prometheus target discovery.  expr: |    absent(up&#123;job=&quot;apiserver&quot;&#125; == 1)  for: 15m  labels:    severity: critical- alert: KubeAPITerminatedRequests  annotations:    description: The apiserver has terminated &#123;&#123; $value | humanizePercentage &#125;&#125; of its incoming requests.    runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubeapiterminatedrequests    summary: The apiserver has terminated &#123;&#123; $value | humanizePercentage &#125;&#125; of its incoming requests.  expr: |    sum(rate(apiserver_request_terminations_total&#123;job=&quot;apiserver&quot;&#125;[10m]))  / (  sum(rate(apiserver_request_total&#123;job=&quot;apiserver&quot;&#125;[10m])) + sum(rate(apiserver_request_terminations_total&#123;job=&quot;apiserver&quot;&#125;[10m])) ) &gt; 0.20  for: 5m  labels:    severity: warning- name: kube-apiserver-slos  rules:  - alert: KubeAPIErrorBudgetBurn    annotations:      description: The API server is burning too much error budget.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubeapierrorbudgetburn      summary: The API server is burning too much error budget.    expr: sum(apiserver_request:burnrate1h) &gt; (14.40 * 0.01000) and sum(apiserver_request:burnrate5m) &gt; (14.40 * 0.01000)    for: 2m    labels:      long: 1h      severity: critical      short: 5m  - alert: KubeAPIErrorBudgetBurn    annotations:      description: The API server is burning too much error budget.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubeapierrorbudgetburn      summary: The API server is burning too much error budget.    expr: sum(apiserver_request:burnrate6h) &gt; (6.00 * 0.01000) and sum(apiserver_request:burnrate30m) &gt; (6.00 * 0.01000)    for: 15m    labels:      long: 6h      severity: critical      short: 30m  - alert: KubeAPIErrorBudgetBurn    annotations:      description: The API server is burning too much error budget.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubeapierrorbudgetburn      summary: The API server is burning too much error budget.    expr: sum(apiserver_request:burnrate1d) &gt; (3.00 * 0.01000) and sum(apiserver_request:burnrate2h) &gt; (3.00 * 0.01000)    for: 1h    labels:      long: 1d      severity: warning      short: 2h  - alert: KubeAPIErrorBudgetBurn    annotations:      description: The API server is burning too much error budget.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubeapierrorbudgetburn      summary: The API server is burning too much error budget.    expr: sum(apiserver_request:burnrate3d) &gt; (1.00 * 0.01000) and sum(apiserver_request:burnrate6h) &gt; (1.00 * 0.01000)    for: 3h    labels:      long: 3d      severity: warning      short: 6h- name: kubernetes-scheduler  rules:  - alert: KubeSchedulerDown    annotations:      description: KubeScheduler has disappeared from Prometheus target discovery.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubeschedulerdown      summary: Target disappeared from Prometheus target discovery.    expr: absent(up&#123;job=&quot;kube-scheduler&quot;&#125; == 1)    for: 15m    labels:      severity: Critical- name: kubernetes-controller-manager  rules:  - alert: KubeControllerManagerDown    annotations:      description: KubeControllerManager has disappeared from Prometheus target discovery.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubecontrollermanagerdown      summary: Target disappeared from Prometheus target discovery.    expr: absent(up&#123;job=&quot;kube-controller-manager&quot;&#125; == 1)    for: 15m    labels:      severity: Critical- name: kubernetes-kubelet  rules:  - alert: KubeNodeNotReady    annotations:      description: &#39;&#123;&#123; $labels.node &#125;&#125; has been unready for more than 15 minutes.&#39;      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubenodenotready      summary: Node is not ready.    expr: kube_node_status_condition&#123;job=&quot;kube-state-metrics&quot;,condition=&quot;Ready&quot;,status=&quot;true&quot;&#125; == 0    for: 15m    labels:      severity: Warning  - alert: KubeNodeUnreachable    annotations:      description: &#39;&#123;&#123; $labels.node &#125;&#125; is unreachable and some workloads may be rescheduled.&#39;      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubenodeunreachable      summary: Node is unreachable.    expr: |      (kube_node_spec_taint&#123;job=&quot;kube-state-metrics&quot;,key=&quot;node.kubernetes.io/unreachable&quot;,effect=&quot;NoSchedule&quot;&#125; unless ignoring(key,value) kube_node_spec_taint&#123;job=&quot;kube-state-metrics&quot;,key=~&quot;ToBeDeletedByClusterAutoscaler|cloud.google.com/impending-node-termination|aws-node-termination-handler/spot-itn&quot;&#125;) == 1    for: 15m    labels:      severity: Warning  - alert: KubeletTooManyPods    annotations:      description: Kubelet &#39;&#123;&#123; $labels.node &#125;&#125;&#39; is running at &#123;&#123; $value | humanizePercentage &#125;&#125; of its Pod capacity.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubelettoomanypods      summary: Kubelet is running at capacity.    expr: |      count by(node) (        (kube_pod_status_phase&#123;job=&quot;kube-state-metrics&quot;,phase=&quot;Running&quot;&#125; == 1) * on(instance,pod,namespace,cluster) group_left(node) topk by(instance,pod,namespace,cluster) (1, kube_pod_info&#123;job=&quot;kube-state-metrics&quot;&#125;)      )      /      max by(node) (        kube_node_status_capacity&#123;job=&quot;kube-state-metrics&quot;,resource=&quot;pods&quot;&#125; != 1      ) &gt; 0.95    for: 15m    labels:      severity: Warning  - alert: KubeNodeReadinessFlapping    annotations:      description: The readiness status of node &#123;&#123; $labels.node &#125;&#125; has changed &#123;&#123; $value &#125;&#125; times in the last 15 minutes.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubenodereadinessflapping      summary: Node readiness status is flapping.    expr: sum(changes(kube_node_status_condition&#123;status=&quot;true&quot;,condition=&quot;Ready&quot;&#125;[15m])) by (node) &gt; 2    for: 15m    labels:      severity: Warning  - alert: KubeletPlegDurationHigh    annotations:      description: The Kubelet Pod Lifecycle Event Generator has a 99th percentile duration of &#123;&#123; $value &#125;&#125; seconds on node &#123;&#123; $labels.node &#125;&#125;.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubeletplegdurationhigh      summary: Kubelet Pod Lifecycle Event Generator is taking too long to relist.    expr: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile&#123;quantile=&quot;0.99&quot;&#125; &gt;= 10    for: 5m    labels:      severity: Warning  - alert: KubeletPodStartUpLatencyHigh    annotations:      description: Kubelet Pod startup 99th percentile latency is &#123;&#123; $value &#125;&#125; seconds on node &#123;&#123; $labels.node &#125;&#125;.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubeletpodstartuplatencyhigh      summary: Kubelet Pod startup latency is too high.    expr: |      histogram_quantile(0.99, sum(rate(kubelet_pod_worker_duration_seconds_bucket&#123;job=&quot;kubelet&quot;, metrics_path=&quot;/metrics&quot;&#125;[5m])) by (instance, le)) * on(instance) group_left(node) kubelet_node_name&#123;job=&quot;kubelet&quot;, metrics_path=&quot;/metrics&quot;&#125; &gt; 60    for: 15m    labels:      severity: Warning  - alert: KubeletClientCertificateExpiration    annotations:      description: Client certificate for Kubelet on node &#123;&#123; $labels.node &#125;&#125; expires in &#123;&#123; $value | humanizeDuration &#125;&#125;.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubeletclientcertificateexpiration      summary: Kubelet client certificate is about to expire.    expr: kubelet_certificate_manager_client_ttl_seconds &lt; 604800    labels:      severity: Warning  - alert: KubeletClientCertificateExpiration    annotations:      description: Client certificate for Kubelet on node &#123;&#123; $labels.node &#125;&#125; expires in &#123;&#123; $value | humanizeDuration &#125;&#125;.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubeletclientcertificateexpiration      summary: Kubelet client certificate is about to expire.    expr: kubelet_certificate_manager_client_ttl_seconds &lt; 86400    labels:      severity: Critical  - alert: KubeletServerCertificateExpiration    annotations:      description: Server certificate for Kubelet on node &#123;&#123; $labels.node &#125;&#125; expires in &#123;&#123; $value | humanizeDuration &#125;&#125;.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubeletservercertificateexpiration      summary: Kubelet server certificate is about to expire.    expr: kubelet_certificate_manager_server_ttl_seconds &lt; 604800    labels:      severity: Warning  - alert: KubeletServerCertificateExpiration    annotations:      description: Server certificate for Kubelet on node &#123;&#123; $labels.node &#125;&#125; expires in &#123;&#123; $value | humanizeDuration &#125;&#125;.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubeletservercertificateexpiration      summary: Kubelet server certificate is about to expire.    expr: kubelet_certificate_manager_server_ttl_seconds &lt; 86400    labels:      severity: Critical  - alert: KubeletClientCertificateRenewalErrors    annotations:      description: Kubelet on node &#123;&#123; $labels.node &#125;&#125; has failed to renew its client certificate (&#123;&#123; $value | humanize &#125;&#125; errors in the last 5 minutes).      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubeletclientcertificaterenewalerrors      summary: Kubelet has failed to renew its client certificate.    expr: increase(kubelet_certificate_manager_client_expiration_renew_errors[5m]) &gt; 0    for: 15m    labels:      severity: Warning  - alert: KubeletServerCertificateRenewalErrors    annotations:      description: Kubelet on node &#123;&#123; $labels.node &#125;&#125; has failed to renew its server certificate (&#123;&#123; $value | humanize &#125;&#125; errors in the last 5 minutes).      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubeletservercertificaterenewalerrors      summary: Kubelet has failed to renew its server certificate.    expr: increase(kubelet_server_expiration_renew_errors[5m]) &gt; 0    for: 15m    labels:      severity: Warning  - alert: KubeletDown    annotations:      description: Kubelet has disappeared from Prometheus target discovery.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubeletdown      summary: Target disappeared from Prometheus target discovery.    expr: absent(up&#123;job=&quot;kubelet&quot;, metrics_path=&quot;/metrics&quot;&#125; == 1)    for: 15m    labels:      severity: Critical- name: kubernetes-apps  rules:  - alert: KubeContainerWaiting    annotations:      description: Pod &#123;&#123; $labels.namespace &#125;&#125;/&#123;&#123; $labels.pod &#125;&#125; container &#123;&#123; $labels.container&#125;&#125; has been in waiting state for longer than 1 hour.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubecontainerwaiting      summary: Pod container waiting longer than 1 hour    expr: sum by (namespace, pod, container) (kube_pod_container_status_waiting_reason&#123;job=&quot;kube-state-metrics&quot;&#125;) &gt; 0    for: 1h    labels:      severity: warning  - alert: KubePodCrashLooping    annotations:      description: Pod &#123;&#123; $labels.namespace &#125;&#125;/&#123;&#123; $labels.pod &#125;&#125; (&#123;&#123; $labels.container &#125;&#125;) is restarting &#123;&#123; printf "%.2f" $value &#125;&#125; times / 10 minutes.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubepodcrashlooping      summary: Pod is crash looping.    expr: rate(kube_pod_container_status_restarts_total&#123;job=&quot;kube-state-metrics&quot;&#125;[10m]) * 60 * 5 &gt; 0    for: 15m    labels:      severity: Warning  - alert: KubePodNotReady    annotations:      description: Pod &#123;&#123; $labels.namespace &#125;&#125;/&#123;&#123; $labels.pod &#125;&#125; has been in a non-ready state for longer than 15 minutes.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubepodnotready      summary: Pod has been in a non-ready state for more than 15 minutes.    expr: sum by (namespace, pod) (max by(namespace, pod) (kube_pod_status_phase&#123;job=&quot;kube-state-metrics&quot;, phase=~&quot;Pending|Unknown&quot;&#125;) * on(namespace, pod) group_left(owner_kind) topk by(namespace, pod) (1, max by(namespace, pod, owner_kind) (kube_pod_owner&#123;owner_kind!=&quot;Job&quot;&#125;))) &gt; 0    for: 15m    labels:      severity: Warning  - alert: KubeDeploymentGenerationMismatch    annotations:      description: Deployment generation for &#123;&#123; $labels.namespace &#125;&#125;/&#123;&#123; $labels.deployment &#125;&#125; does not match, this indicates that the Deployment has failed but has not been rolled back.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubedeploymentgenerationmismatch      summary: Deployment generation mismatch due to possible roll-back    expr: kube_deployment_status_observed_generation&#123;job=&quot;kube-state-metrics&quot;&#125; != kube_deployment_metadata_generation&#123;job=&quot;kube-state-metrics&quot;&#125;    for: 15m    labels:      severity: Warning  - alert: KubeDeploymentReplicasMismatch    annotations:      description: Deployment &#123;&#123; $labels.namespace &#125;&#125;/&#123;&#123; $labels.deployment &#125;&#125; has not matched the expected number of replicas for longer than 15 minutes.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubedeploymentreplicasmismatch      summary: Deployment has not matched the expected number of replicas.    expr: (kube_deployment_spec_replicas&#123;job=&quot;kube-state-metrics&quot;&#125;!=kube_deployment_status_replicas_available&#123;job=&quot;kube-state-metrics&quot;&#125;) and (changes(kube_deployment_status_replicas_updated&#123;job=&quot;kube-state-metrics&quot;&#125;[10m])==0)    for: 15m    labels:      severity: Warning  - alert: KubeStatefulSetReplicasMismatch    annotations:      description: StatefulSet &#123;&#123; $labels.namespace &#125;&#125;/&#123;&#123; $labels.statefulset &#125;&#125; has not matched the expected number of replicas for longer than 15 minutes.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubestatefulsetreplicasmismatch      summary: Deployment has not matched the expected number of replicas.    expr: |      (        kube_statefulset_status_replicas_ready&#123;job=&quot;kube-state-metrics&quot;&#125;          !=        kube_statefulset_status_replicas&#123;job=&quot;kube-state-metrics&quot;&#125;      ) and (        changes(kube_statefulset_status_replicas_updated&#123;job=&quot;kube-state-metrics&quot;&#125;[10m])          ==        0      )    for: 15m    labels:      severity: Warning  - alert: KubeStatefulSetGenerationMismatch    annotations:      description: StatefulSet generation for &#123;&#123; $labels.namespace &#125;&#125;/&#123;&#123; $labels.statefulset &#125;&#125; does not match, this indicates that the StatefulSet has failed but has not been rolled back.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubestatefulsetgenerationmismatch      summary: StatefulSet generation mismatch due to possible roll-back    expr: kube_statefulset_status_observed_generation&#123;job=&quot;kube-state-metrics&quot;&#125; != kube_statefulset_metadata_generation&#123;job=&quot;kube-state-metrics&quot;&#125;    for: 15m    labels:      severity: Warning  - alert: KubeStatefulSetUpdateNotRolledOut    annotations:      description: StatefulSet &#123;&#123; $labels.namespace &#125;&#125;/&#123;&#123; $labels.statefulset &#125;&#125; update has not been rolled out.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubestatefulsetupdatenotrolledout      summary: StatefulSet update has not been rolled out.    expr: |      (        max without (revision) (          kube_statefulset_status_current_revision&#123;job=&quot;kube-state-metrics&quot;&#125;            unless          kube_statefulset_status_update_revision&#123;job=&quot;kube-state-metrics&quot;&#125;        )          *        (          kube_statefulset_replicas&#123;job=&quot;kube-state-metrics&quot;&#125;            !=          kube_statefulset_status_replicas_updated&#123;job=&quot;kube-state-metrics&quot;&#125;        )      )  and (        changes(kube_statefulset_status_replicas_updated&#123;job=&quot;kube-state-metrics&quot;&#125;[5m])          ==        0      )    for: 15m    labels:      severity: Warning  - alert: KubeDaemonSetRolloutStuck    annotations:      description: DaemonSet &#123;&#123; $labels.namespace &#125;&#125;/&#123;&#123; $labels.daemonset &#125;&#125; has not finished or progressed for at least 15 minutes.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubedaemonsetrolloutstuck      summary: DaemonSet rollout is stuck.    expr: |      (        (          kube_daemonset_status_current_number_scheduled&#123;job=&quot;kube-state-metrics&quot;&#125;           !=          kube_daemonset_status_desired_number_scheduled&#123;job=&quot;kube-state-metrics&quot;&#125;        ) or (          kube_daemonset_status_number_misscheduled&#123;job=&quot;kube-state-metrics&quot;&#125;           !=          0        ) or (          kube_daemonset_updated_number_scheduled&#123;job=&quot;kube-state-metrics&quot;&#125;           !=          kube_daemonset_status_desired_number_scheduled&#123;job=&quot;kube-state-metrics&quot;&#125;        ) or (          kube_daemonset_status_number_available&#123;job=&quot;kube-state-metrics&quot;&#125;           !=          kube_daemonset_status_desired_number_scheduled&#123;job=&quot;kube-state-metrics&quot;&#125;        )      ) and (        changes(kube_daemonset_updated_number_scheduled&#123;job=&quot;kube-state-metrics&quot;&#125;[5m])          ==        0      )    for: 15m    labels:      severity: Warning  - alert: KubeDaemonSetNotScheduled    annotations:      description: &#39;&#123;&#123; $value &#125;&#125; Pods of DaemonSet &#123;&#123; $labels.namespace &#125;&#125;/&#123;&#123; $labels.daemonset &#125;&#125; are not scheduled.&#39;      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubedaemonsetnotscheduled      summary: DaemonSet pods are not scheduled.    expr: kube_daemonset_status_desired_number_scheduled&#123;job=&quot;kube-state-metrics&quot;&#125; - kube_daemonset_status_current_number_scheduled&#123;job=&quot;kube-state-metrics&quot;&#125; &gt; 0    for: 10m    labels:      severity: Warning  - alert: KubeDaemonSetMisScheduled    annotations:      description: &#39;&#123;&#123; $value &#125;&#125; Pods of DaemonSet &#123;&#123; $labels.namespace &#125;&#125;/&#123;&#123; $labels.daemonset &#125;&#125; are running where they are not supposed to run.&#39;      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubedaemonsetmisscheduled      summary: DaemonSet pods are misscheduled.    expr: kube_daemonset_status_number_misscheduled&#123;job=&quot;kube-state-metrics&quot;&#125; &gt; 0    for: 15m    labels:      severity: Warning  - alert: KubeJobCompletion    annotations:      description: Job &#123;&#123; $labels.namespace &#125;&#125;/&#123;&#123; $labels.job_name &#125;&#125; is taking more than 12 hours to complete.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubejobcompletion      summary: Job did not complete in time    expr: kube_job_spec_completions&#123;job=&quot;kube-state-metrics&quot;&#125; - kube_job_status_succeeded&#123;job=&quot;kube-state-metrics&quot;&#125;  &gt; 0    for: 12h    labels:      severity: Warning  - alert: KubeJobFailed    annotations:      description: Job &#123;&#123; $labels.namespace &#125;&#125;/&#123;&#123; $labels.job_name &#125;&#125; failed to complete. Removing failed job after investigation should clear this alert.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubejobfailed      summary: Job failed to complete.    expr: kube_job_failed&#123;job=&quot;kube-state-metrics&quot;&#125;  &gt; 0    for: 15m    labels:      severity: Warning  - alert: KubeHpaReplicasMismatch    annotations:      description: HPA &#123;&#123; $labels.namespace &#125;&#125;/&#123;&#123; $labels.hpa &#125;&#125; has not matched the desired number of replicas for longer than 15 minutes.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubehpareplicasmismatch      summary: HPA has not matched descired number of replicas.    expr: |      (kube_hpa_status_desired_replicas&#123;job=&quot;kube-state-metrics&quot;&#125;        !=      kube_hpa_status_current_replicas&#123;job=&quot;kube-state-metrics&quot;&#125;)        and      (kube_hpa_status_current_replicas&#123;job=&quot;kube-state-metrics&quot;&#125;        &gt;      kube_hpa_spec_min_replicas&#123;job=&quot;kube-state-metrics&quot;&#125;)        and      (kube_hpa_status_current_replicas&#123;job=&quot;kube-state-metrics&quot;&#125;        &lt;      kube_hpa_spec_max_replicas&#123;job=&quot;kube-state-metrics&quot;&#125;)        and      changes(kube_hpa_status_current_replicas&#123;job=&quot;kube-state-metrics&quot;&#125;[15m]) == 0    for: 15m    labels:      severity: Warning  - alert: KubeHpaMaxedOut    annotations:      description: HPA &#123;&#123; $labels.namespace &#125;&#125;/&#123;&#123; $labels.hpa &#125;&#125; has been running at max replicas for longer than 15 minutes.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubehpamaxedout      summary: HPA is running at max replicas    expr: kube_hpa_status_current_replicas&#123;job=&quot;kube-state-metrics&quot;&#125; == kube_hpa_spec_max_replicas&#123;job=&quot;kube-state-metrics&quot;&#125;    for: 15m    labels:      severity: Warning- name: kubernetes-resources  rules:  - alert: KubeCPUOvercommit    annotations:      description: Cluster has overcommitted CPU resource requests for Pods and cannot tolerate node failure.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubecpuovercommit      summary: Cluster has overcommitted CPU resource requests.    expr: sum(namespace_cpu:kube_pod_container_resource_requests:sum&#123;&#125;) / sum(kube_node_status_allocatable&#123;resource=&quot;cpu&quot;&#125;)   &gt;      ((count(kube_node_status_allocatable&#123;resource=&quot;cpu&quot;&#125;) &gt; 1) - 1) / count(kube_node_status_allocatable&#123;resource=&quot;cpu&quot;&#125;)    for: 5m    labels:      severity: Warning  - alert: KubeMemoryOvercommit    annotations:      description: Cluster has overcommitted memory resource requests for Pods and cannot tolerate node failure.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubememoryovercommit      summary: Cluster has overcommitted memory resource requests.    expr: |      sum(namespace_memory:kube_pod_container_resource_requests:sum&#123;&#125;)      /      sum(kube_node_status_allocatable&#123;resource=&quot;memory&quot;&#125;)      &gt;      ((count(kube_node_status_allocatable&#123;resource=&quot;memory&quot;&#125;) &gt; 1) - 1)      /      count(kube_node_status_allocatable&#123;resource=&quot;memory&quot;&#125;)    for: 5m    labels:      severity: Warning    - alert: KubeCPUQuotaOvercommit    annotations:      description: Cluster has overcommitted CPU resource requests for Namespaces.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubecpuquotaovercommit      summary: Cluster has overcommitted CPU resource requests.    expr: sum(kube_resourcequota&#123;job=&quot;kube-state-metrics&quot;, type=&quot;hard&quot;, resource=&quot;cpu&quot;&#125;) / sum(kube_node_status_allocatable&#123;resource=&quot;cpu&quot;&#125;) &gt; 1.5    for: 5m    labels:      severity: Warning  - alert: KubeMemoryQuotaOvercommit    annotations:      description: Cluster has overcommitted memory resource requests for Namespaces.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubememoryquotaovercommit      summary: Cluster has overcommitted memory resource requests.    expr: sum(kube_resourcequota&#123;job=&quot;kube-state-metrics&quot;, type=&quot;hard&quot;, resource=&quot;memory&quot;&#125;) / sum(kube_node_status_allocatable&#123;resource=&quot;memory&quot;,job=&quot;kube-state-metrics&quot;&#125;) &gt; 1.5    for: 5m    labels:      severity: Warning  - alert: KubeQuotaAlmostFull    annotations:      description: Namespace &#123;&#123; $labels.namespace &#125;&#125; is using &#123;&#123; $value | humanizePercentage &#125;&#125; of its &#123;&#123; $labels.resource &#125;&#125; quota.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubequotaalmostfull      summary: Namespace quota is going to be full.    expr: kube_resourcequota&#123;job=&quot;kube-state-metrics&quot;, type=&quot;used&quot;&#125; / ignoring(instance, job, type) (kube_resourcequota&#123;job=&quot;kube-state-metrics&quot;, type=&quot;hard&quot;&#125; &gt; 0) &gt; 0.9 &lt; 1    for: 15m    labels:      severity: Info  - alert: KubeQuotaFullyUsed    annotations:      description: Namespace &#123;&#123; $labels.namespace &#125;&#125; is using &#123;&#123; $value | humanizePercentage &#125;&#125; of its &#123;&#123; $labels.resource &#125;&#125; quota.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubequotafullyused      summary: Namespace quota is fully used.    expr: kube_resourcequota&#123;job=&quot;kube-state-metrics&quot;, type=&quot;used&quot;&#125; / ignoring(instance, job, type)    (kube_resourcequota&#123;job=&quot;kube-state-metrics&quot;, type=&quot;hard&quot;&#125; &gt; 0) == 1    for: 15m    labels:      severity: Info  - alert: KubeQuotaExceeded    annotations:      description: Namespace &#123;&#123; $labels.namespace &#125;&#125; is using &#123;&#123; $value | humanizePercentage &#125;&#125; of its &#123;&#123; $labels.resource &#125;&#125; quota.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubequotaexceeded      summary: Namespace quota has exceeded the limits.    expr: kube_resourcequota&#123;job=&quot;kube-state-metrics&quot;, type=&quot;used&quot;&#125; / ignoring(instance, job, type)    (kube_resourcequota&#123;job=&quot;kube-state-metrics&quot;, type=&quot;hard&quot;&#125; &gt; 0) &gt; 1    for: 15m    labels:      severity: Warning  - alert: CPUThrottlingHigh    annotations:      description: &#39;&#123;&#123; $value | humanizePercentage &#125;&#125; throttling of CPU in namespace &#123;&#123; $labels.namespace &#125;&#125; for container &#123;&#123; $labels.container &#125;&#125; in pod &#123;&#123; $labels.pod &#125;&#125;.&#39;      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/cputhrottlinghigh      summary: Processes experience elevated CPU throttling.    expr: sum(increase(container_cpu_cfs_throttled_periods_total&#123;container!=&quot;&quot;, &#125;[5m])) by (container, pod, namespace) /    sum(increase(container_cpu_cfs_periods_total&#123;&#125;[5m])) by (container, pod, namespace) &gt; ( 25 / 100 )    for: 15m    labels:      severity: Info- name: kubernetes-storage  rules:  - alert: KubePersistentVolumeFillingUp    annotations:      description: The PersistentVolume claimed by &#123;&#123; $labels.persistentvolumeclaim &#125;&#125; in Namespace &#123;&#123; $labels.namespace &#125;&#125; is only &#123;&#123; $value | humanizePercentage &#125;&#125; free.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubepersistentvolumefillingup      summary: PersistentVolume is filling up.    expr: |      kubelet_volume_stats_available_bytes&#123;job=&quot;kubelet&quot;, metrics_path=&quot;/metrics&quot;&#125;      /      kubelet_volume_stats_capacity_bytes&#123;job=&quot;kubelet&quot;, metrics_path=&quot;/metrics&quot;&#125;      &lt; 0.03    for: 1m    labels:      severity: Critical  - alert: KubePersistentVolumeFillingUp    annotations:      description: Based on recent sampling, the PersistentVolume claimed by &#123;&#123; $labels.persistentvolumeclaim &#125;&#125; in Namespace &#123;&#123; $labels.namespace &#125;&#125; is expected to fill up within four days. Currently &#123;&#123; $value | humanizePercentage &#125;&#125; is available.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubepersistentvolumefillingup      summary: PersistentVolume is filling up.    expr: |      (      kubelet_volume_stats_available_bytes&#123;job=&quot;kubelet&quot;, metrics_path=&quot;/metrics&quot;&#125;      /      kubelet_volume_stats_capacity_bytes&#123;job=&quot;kubelet&quot;, metrics_path=&quot;/metrics&quot;&#125;      ) &lt; 0.15      and      predict_linear(kubelet_volume_stats_available_bytes&#123;job=&quot;kubelet&quot;, metrics_path=&quot;/metrics&quot;&#125;[6h], 4 * 24 * 3600) &lt; 0    for: 1h    labels:      severity: Warning  - alert: KubePersistentVolumeErrors    annotations:      description: The persistent volume &#123;&#123; $labels.persistentvolume &#125;&#125; has status &#123;&#123; $labels.phase &#125;&#125;.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/kubepersistentvolumeerrors      summary: PersistentVolume is having issues with provisioning.    expr: kube_persistentvolume_status_phase&#123;phase=~&quot;Failed|Pending&quot;,job=&quot;kube-state-metrics&quot;&#125; &gt; 0    for: 5m    labels:      severity: Critical- name: kubernetes-coredns  rules:  - alert: CoreDNSDown    annotations:      message: CoreDNS has disappeared from Prometheus target discovery.      runbook_url: https://github.com/povilasv/coredns-mixin/tree/master/runbook.md#alert-name-corednsdown    expr: absent(up&#123;job=&quot;kube-dns&quot;&#125; == 1)    for: 15m    labels:      severity: Critical  - alert: CoreDNSLatencyHigh    annotations:      message: CoreDNS has 99th percentile latency of &#123;&#123; $value &#125;&#125; seconds for server &#123;&#123; $labels.server &#125;&#125; zone &#123;&#123; $labels.zone &#125;&#125; .      runbook_url: https://github.com/povilasv/coredns-mixin/tree/master/runbook.md#alert-name-corednslatencyhigh    expr: histogram_quantile(0.99, sum(rate(coredns_dns_request_duration_seconds_bucket&#123;job=&quot;kube-dns&quot;&#125;[5m])) by(server, zone, le)) &gt; 4    for: 10m    labels:      severity: Critical  - alert: CoreDNSLatencyHigh    annotations:      message: CoreDNS has 99th percentile latency of &#123;&#123; $value &#125;&#125; seconds for server &#123;&#123; $labels.server &#125;&#125; zone &#123;&#123; $labels.zone &#125;&#125; .      runbook_url: https://github.com/povilasv/coredns-mixin/tree/master/runbook.md#alert-name-corednslatencyhigh    expr: histogram_quantile(0.99, sum(rate(coredns_dns_request_duration_seconds_bucket&#123;job=&quot;kube-dns&quot;&#125;[5m])) by(server, zone, le)) &gt; 4    for: 10m    labels:      severity: Critical  - alert: CoreDNSErrorsHigh    annotations:      message: CoreDNS is returning SERVFAIL for &#123;&#123; $value | humanizePercentage &#125;&#125; of requests.      runbook_url: https://github.com/povilasv/coredns-mixin/tree/master/runbook.md#alert-name-corednserrorshigh    expr: sum(rate(coredns_dns_responses_total&#123;job=&quot;kube-dns&quot;,rcode=&quot;SERVFAIL&quot;&#125;[5m])) / sum(rate(coredns_dns_responses_total&#123;job=&quot;kube-dns&quot;&#125;[5m])) &gt; 0.03    for: 10m    labels:      severity: Critical  - alert: CoreDNSErrorsHigh    annotations:      message: CoreDNS is returning SERVFAIL for &#123;&#123; $value | humanizePercentage &#125;&#125; of requests.      runbook_url: https://github.com/povilasv/coredns-mixin/tree/master/runbook.md#alert-name-corednserrorshigh    expr: sum(rate(coredns_dns_responses_total&#123;job=&quot;kube-dns&quot;,rcode=&quot;SERVFAIL&quot;&#125;[5m])) / sum(rate(coredns_dns_responses_total&#123;job=&quot;kube-dns&quot;&#125;[5m])) &gt; 0.01    for: 10m    labels:      severity: Critical  - alert: CoreDNSForwardLatencyHigh    annotations:      message: CoreDNS has 99th percentile latency of &#123;&#123; $value &#125;&#125; seconds forwarding requests to &#123;&#123; $labels.to &#125;&#125;.      runbook_url: https://github.com/povilasv/coredns-mixin/tree/master/runbook.md#alert-name-corednsforwardlatencyhigh    expr: histogram_quantile(0.99, sum(rate(coredns_forward_request_duration_seconds_bucket&#123;job=&quot;kube-dns&quot;&#125;[5m])) by(to, le)) &gt; 4    for: 10m    labels:      severity: Critical  - alert: CoreDNSForwardErrorsHigh    annotations:      message: CoreDNS is returning SERVFAIL for &#123;&#123; $value | humanizePercentage &#125;&#125; of forward requests to &#123;&#123; $labels.to &#125;&#125;.      runbook_url: https://github.com/povilasv/coredns-mixin/tree/master/runbook.md#alert-name-corednsforwarderrorshigh    expr: sum(rate(coredns_forward_responses_total&#123;job=&quot;kube-dns&quot;,rcode=&quot;SERVFAIL&quot;&#125;[5m])) / sum(rate(coredns_forward_responses_total&#123;job=&quot;kube-dns&quot;&#125;[5m])) &gt; 0.03    for: 10m    labels:      severity: Critical  - alert: CoreDNSForwardErrorsHigh    annotations:      message: CoreDNS is returning SERVFAIL for &#123;&#123; $value | humanizePercentage &#125;&#125; of forward requests to &#123;&#123; $labels.to &#125;&#125;.      runbook_url: https://github.com/povilasv/coredns-mixin/tree/master/runbook.md#alert-name-corednsforwarderrorshigh    expr: sum(rate(coredns_forward_responses_total&#123;job=&quot;kube-dns&quot;,rcode=&quot;SERVFAIL&quot;&#125;[5m])) / sum(rate(coredns_forward_responses_total&#123;job=&quot;kube-dns&quot;&#125;[5m])) &gt; 0.01    for: 10m    labels:      severity: Critical  - alert: CoreDNSForwardHealthcheckFailureCount    annotations:      message: CoreDNS health checks have failed to upstream server &#123;&#123; $labels.to &#125;&#125;.      runbook_url: https://github.com/povilasv/coredns-mixin/tree/master/runbook.md#alert-name-corednsforwardhealthcheckfailurecount    expr: sum(rate(coredns_forward_healthcheck_failures_total&#123;job=&quot;kube-dns&quot;&#125;[5m])) by (to) &gt; 0    for: 10m    labels:      severity: Warning  - alert: CoreDNSForwardHealthcheckBrokenCount    annotations:      message: CoreDNS health checks have failed for all upstream servers.      runbook_url: https://github.com/povilasv/coredns-mixin/tree/master/runbook.    expr: sum(rate(coredns_forward_healthcheck_broken_total&#123;job=&quot;kube-dns&quot;&#125;[5m])) &gt; 0    for: 10m    labels:      severity: Warning  - alert: CorednsPanicCount    expr: increase(coredns_panics_total[1m]) &gt; 0    for: 0m    labels:      severity: Critical    annotations:      summary: &quot;&#123;&#123;$labels.instance&#125;&#125; CoreDNS have Panics.&quot;      description: &quot;&#123;&#123;$labels.instance&#125;&#125; Number of CoreDNS panics encountered is &#123;&#123; $value &#125;&#125;&quot;</code></pre><h1 id="7-重载Prometheus，验证集群监控状态"><a href="#7-重载Prometheus，验证集群监控状态" class="headerlink" title="7.重载Prometheus，验证集群监控状态"></a>7.重载Prometheus，验证集群监控状态</h1><pre><code class="hljs">curl -XPOST http://127.0.0.1:9090/-/reload</code></pre><h1 id="8-导入grafana模版"><a href="#8-导入grafana模版" class="headerlink" title="8.导入grafana模版"></a>8.导入grafana模版</h1><p>Dashboards —&gt; Manage —&gt; Import —&gt; 模版ID：13105、16420</p><p><img src="/img/wiki/prometheus/kubernetes.jpg" alt="kubernetes"></p><p><img src="/img/wiki/prometheus/kubernetes-alerts.jpg" alt="kubernetes-alerts"></p><ul><li>注：修改计算公式，新增告警持续时长字段：time() - ALERTS_FOR_STATE and ignoring(alertstate) ALERTS</li></ul><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://zhuanlan.zhihu.com/p/671898732">https://zhuanlan.zhihu.com/p/671898732</a></li><li><a href="https://blog.51cto.com/u_16099314/9650104">https://blog.51cto.com/u_16099314/9650104</a></li><li><a href="https://www.cnblogs.com/suyj/p/16053993.html">https://www.cnblogs.com/suyj/p/16053993.html</a></li><li><a href="https://blog.csdn.net/MrFDd/article/details/134535787">https://blog.csdn.net/MrFDd/article/details/134535787</a></li><li><a href="https://blog.csdn.net/qq_33816243/article/details/126863790">https://blog.csdn.net/qq_33816243/article/details/126863790</a></li><li><a href="https://blog.csdn.net/zfw_666666/article/details/126134071">https://blog.csdn.net/zfw_666666/article/details/126134071</a></li><li><a href="https://docs.tianshu.org.cn/docs/setup/monitor-pod-indicator-information">https://docs.tianshu.org.cn/docs/setup/monitor-pod-indicator-information</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>云原生</tag>
      
      <tag>Prometheus</tag>
      
      <tag>监控告警</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prometheus监控配置Memcached监控实例</title>
    <link href="/linux/Prometheus-Memcached/"/>
    <url>/linux/Prometheus-Memcached/</url>
    
    <content type="html"><![CDATA[<h1 id="1-安装memcached-exporter"><a href="#1-安装memcached-exporter" class="headerlink" title="1.安装memcached_exporter"></a>1.安装memcached_exporter</h1><pre><code class="hljs">wget https://github.com/prometheus/memcached_exporter/releases/download/v0.14.3/memcached_exporter-0.14.3.linux-amd64.tar.gztar -xzvf memcached_exporter-0.14.3.linux-amd64.tar.gz &amp;&amp; sudo mv memcached_exporter /usr/local/bin</code></pre><h1 id="2-创建启动脚本"><a href="#2-创建启动脚本" class="headerlink" title="2.创建启动脚本"></a>2.创建启动脚本</h1><pre><code class="hljs">sudo vi /lib/systemd/system/memcached_exporter.service[Unit]Description = memcached_exporterDocumentation = https://github.com/prometheus/memcached_exporterAfter = network.target[Service]Type = simpleUser = rootExecStart = /usr/local/bin/memcached_exporterRestart = on-failure[Install]WantedBy = multi-user.target</code></pre><h1 id="3-启动memcached-exporter"><a href="#3-启动memcached-exporter" class="headerlink" title="3.启动memcached_exporter"></a>3.启动memcached_exporter</h1><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start memcached_exporter.servicesudo systemctl enable memcached_exporter.service</code></pre><h1 id="4-配置Prometheus"><a href="#4-配置Prometheus" class="headerlink" title="4.配置Prometheus"></a>4.配置Prometheus</h1><h2 id="4-1-配置监控实例"><a href="#4-1-配置监控实例" class="headerlink" title="4.1 配置监控实例"></a>4.1 配置监控实例</h2><pre><code class="hljs">sudo vi /usr/local/prometheus/prometheus.ymlglobal:  scrape_interval: 15s   evaluation_interval: 15s   scrape_timeout: 10s scrape_configs:  - job_name: &quot;prometheus&quot;    static_configs:      - targets: [&quot;localhost:9090&quot;]  - job_name: &quot;memcached&quot;    static_configs:      - targets: [&quot;192.168.100.180:9150&quot;]</code></pre><h2 id="4-2-创建告警规则"><a href="#4-2-创建告警规则" class="headerlink" title="4.2 创建告警规则"></a>4.2 创建告警规则</h2><pre><code class="hljs">sudo vi /usr/local/prometheus/rules/memcached.yml</code></pre><h1 id="5-重载Prometheus"><a href="#5-重载Prometheus" class="headerlink" title="5.重载Prometheus"></a>5.重载Prometheus</h1><pre><code class="hljs">curl -XPOST http://127.0.0.1:9090/-/reload</code></pre><h1 id="6-Grafana导入监控模版"><a href="#6-Grafana导入监控模版" class="headerlink" title="6.Grafana导入监控模版"></a>6.Grafana导入监控模版</h1><p>Dashboards —&gt; Manage —&gt; Import —&gt; 模版ID：37</p><p><img src="/img/wiki/prometheus/memcached.jpg" alt="memcached"></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.cnblogs.com/you-men/p/13206737.html">https://www.cnblogs.com/you-men/p/13206737.html</a></li><li><a href="https://github.com/prometheus/memcached_exporter">https://github.com/prometheus/memcached_exporter</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>云原生</tag>
      
      <tag>Prometheus</tag>
      
      <tag>监控告警</tag>
      
      <tag>Memcached</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prometheus监控配置Ceph集群监控</title>
    <link href="/linux/Prometheus-Ceph/"/>
    <url>/linux/Prometheus-Ceph/</url>
    
    <content type="html"><![CDATA[<p>Ceph集群Manager组件内部集成了Prometheus监控模块，并监听在每个Manager节点的9283端口，用于将采集到的信息通过http接口传送到Prometheus。当然，也可以通过ceph_exporter完成监控信息的采集</p><h1 id="1-Ceph集群启用Prometheus监控模块"><a href="#1-Ceph集群启用Prometheus监控模块" class="headerlink" title="1.Ceph集群启用Prometheus监控模块"></a>1.Ceph集群启用Prometheus监控模块</h1><pre><code class="hljs">ceph mgr module enable prometheus# 验证监控模块ceph mgr module ls</code></pre><h1 id="2-验证Prometheus监控指标"><a href="#2-验证Prometheus监控指标" class="headerlink" title="2.验证Prometheus监控指标"></a>2.验证Prometheus监控指标</h1><pre><code class="hljs">ceph mgr services&#123;&quot;dashboard&quot;: &quot;http://192.168.100.183:8080/&quot;,&quot;prometheus&quot;: &quot;http://192.168.100.183:9283/&quot;&#125;</code></pre><h1 id="3-Prometheus配置监控实例"><a href="#3-Prometheus配置监控实例" class="headerlink" title="3.Prometheus配置监控实例"></a>3.Prometheus配置监控实例</h1><pre><code class="hljs">sudo vi /usr/local/prometheus/prometheus.yml- job_name: ceph  static_configs:    - targets: [&quot;172.100.100.183:9283&quot;]      labels:        cluster: Ceph</code></pre><h1 id="4-重载Prometheus"><a href="#4-重载Prometheus" class="headerlink" title="4.重载Prometheus"></a>4.重载Prometheus</h1><pre><code class="hljs">curl -XPOST http://127.0.0.1:9090/-/reload</code></pre><h1 id="5-创建告警规则"><a href="#5-创建告警规则" class="headerlink" title="5.创建告警规则"></a>5.创建告警规则</h1><pre><code class="hljs">sudo vi /usr/local/prometheus/rules/ceph.ymlgroups:  - name: ceph    rules:    - alert: CephCluster      expr: ceph_health_status &gt; 0      for: 3m      labels:        severity: Critical      annotations:        summary: &quot;&#123;&#123;$labels.instance&#125;&#125;: Ceph集群状态异常&quot;        description: &quot;&#123;&#123;$labels.instance&#125;&#125;:Ceph集群状态异常，当前状态为&#123;&#123; $value &#125;&#125;&quot;    - alert: CephOSDDown      expr: count(ceph_osd_up&#123;&#125; == 0.0) &gt; 0      for: 3m      labels:        severity: Critical      annotations:        summary: &quot;&#123;&#123;$labels.instance&#125;&#125;: 有&#123;&#123; $value &#125;&#125;个OSD挂掉了&quot;        description: &quot;&#123;&#123;$labels.instance&#125;&#125;:&#123;&#123; $labels.osd &#125;&#125;当前状态为&#123;&#123; $labels.status &#125;&#125;&quot;    - alert: CephOSDOut      expr: count(ceph_osd_up&#123;&#125;) - count(ceph_osd_in&#123;&#125;) &gt; 0      for: 3m      labels:        severity: Critical      annotations:        summary: &quot;&#123;&#123;$labels.instance&#125;&#125;: 有&#123;&#123; $value &#125;&#125;个OSD Out&quot;        description: &quot;&#123;&#123;$labels.instance&#125;&#125;:&#123;&#123; $labels.osd &#125;&#125;当前状态为&#123;&#123; $labels.status &#125;&#125;&quot;    - alert: CephOverSpace      expr: ceph_cluster_total_used_bytes / ceph_cluster_total_bytes * 100 &gt; 80      for: 3m      labels:        severity: Critical      annotations:        summary: &quot;&#123;&#123;$labels.instance&#125;&#125;:集群空间不足&quot;        description: &quot;&#123;&#123;$labels.instance&#125;&#125;:当前空间使用率为&#123;&#123; $value &#125;&#125;&quot;    - alert: CephMonDown      expr: count(ceph_mon_quorum_status&#123;&#125;) &lt; 3      for: 3m      labels:        severity: Critical      annotations:        summary: &quot;&#123;&#123;$labels.instance&#125;&#125;:Mon进程异常&quot;        description: &quot;&#123;&#123;$labels.instance&#125;&#125;: Mon进程Down&quot;    - alert: CephMgrDown      expr: sum(ceph_mgr_status&#123;&#125;) &lt; 1.0      for: 3m      labels:        severity: Critical      annotations:        summary: &quot;&#123;&#123;$labels.instance&#125;&#125;:Mgr进程异常&quot;        description: &quot;&#123;&#123;$labels.instance&#125;&#125;: Mgr进程Down&quot;    - alert: CephMdsDown      expr: sum(ceph_mds_metadata&#123;&#125;) &lt; 3.0      for: 3m      labels:        severity: Warning      annotations:        summary: &quot;&#123;&#123;$labels.instance&#125;&#125;:Mds进程异常&quot;        description: &quot;&#123;&#123;$labels.instance&#125;&#125;: Mds进程Down&quot;    - alert: CephRgwDown      expr: sum(ceph_rgw_metadata&#123;&#125;) &lt; 2.0      for: 3m      labels:        severity: Warning      annotations:        summary: &quot;&#123;&#123;$labels.instance&#125;&#125;:Rgw进程异常&quot;        description: &quot;&#123;&#123;$labels.instance&#125;&#125;: Rgw进程Down&quot;    - alert: CephOsdOver      expr: sum(ceph_osd_stat_bytes_used / ceph_osd_stat_bytes &gt; 0.8) by (ceph_daemon) &gt; 0      for: 3m      labels:        severity: Warning      annotations:        summary: &quot;&#123;&#123;$labels.instance&#125;&#125;:High OSD Usage Alert&quot;        description: &quot;&#123;&#123;$labels.instance&#125;&#125;: Some OSDs have usage above 80%&quot;</code></pre><h1 id="6-Grafana导入监控模版"><a href="#6-Grafana导入监控模版" class="headerlink" title="6.Grafana导入监控模版"></a>6.Grafana导入监控模版</h1><p>Dashboards —&gt; Manage —&gt; Import —&gt; 模版ID：9966  </p><p><img src="/img/wiki/prometheus/ceph.jpg" alt="ceph"></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/xu710263124/article/details/135849819">https://blog.csdn.net/xu710263124/article/details/135849819</a></li><li><a href="https://blog.csdn.net/yangkang1122/article/details/88687944">https://blog.csdn.net/yangkang1122/article/details/88687944</a></li><li><a href="https://blog.csdn.net/zuoyang1990/article/details/132147203">https://blog.csdn.net/zuoyang1990/article/details/132147203</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Ceph</tag>
      
      <tag>存储</tag>
      
      <tag>分布式存储</tag>
      
      <tag>云存储</tag>
      
      <tag>Prometheus</tag>
      
      <tag>监控告警</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Openstack集群基于Ceph存储方案</title>
    <link href="/linux/Openstack-Ceph/"/>
    <url>/linux/Openstack-Ceph/</url>
    
    <content type="html"><![CDATA[<p>Ceph作为当前非常流行的开源分布式存储系统，以其高扩展性、高性能、高可靠性等优点，广泛应用于众多云计算厂商的公有云、私有云与混合云环境，也即是云计算超融合基础设施（HCI）解决方案。作为云原生技术引领者之一的Openstack，目前已将Ceph作为主流的后端存储，两者已融合为一种高性能、高可用性、高扩展性的成熟的解决方案，优点如下：</p><ul><li>Ceph代替了价格昂贵的商业存储设备，降低了整体成本</li><li>Openstack所有计算节点共享存储，迁移时不需要拷贝根磁盘，即使计算节点宕机，也能立即在其他计算节点启动虚拟机</li><li>Ceph RBD所具备的COW（Copy On Write）特性，使得创建虚拟机时只需基于镜像clone即可，不需要下载整个镜像，而clone操作基本是0开销，从而实现秒级、高并发的虚拟机启动</li><li>Ceph RBD支持thin provisioning，即按需分配空间，类似于Linux文件系统的sparse稀疏文件，即创建虚拟硬盘最初并不占用物理存储空间，只有当写入数据时，才按需分配存储空间</li><li>Openstack除了可将Ceph RBD作为Nova、Cinder、Glance这些组件的后端存储，还可将Swift、Manila组件分别对接Ceph RGW与CephFS。作为统一存储解决方案，Ceph有效降低了云环境的整体复杂性与运维成本，完全满足了对存储类型的需求</li></ul><hr><h1 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h1><ul><li>172.100.100.180 controller</li><li>172.100.100.181 compute001</li><li>172.100.100.182 compute002</li><li>172.100.100.183 storage001</li><li>172.100.100.184 storage002</li><li>172.100.100.185 storage003</li></ul><h1 id="1-部署Ceph集群"><a href="#1-部署Ceph集群" class="headerlink" title="1.部署Ceph集群"></a>1.部署Ceph集群</h1><h1 id="2-创建RBD存储池"><a href="#2-创建RBD存储池" class="headerlink" title="2.创建RBD存储池"></a>2.创建RBD存储池</h1><h2 id="2-1-创建数据卷存储池"><a href="#2-1-创建数据卷存储池" class="headerlink" title="2.1 创建数据卷存储池"></a>2.1 创建数据卷存储池</h2><pre><code class="hljs"># 用于存储Cinder组件的数据卷ceph osd pool create volumes 128</code></pre><h2 id="2-2-创建镜像存储池"><a href="#2-2-创建镜像存储池" class="headerlink" title="2.2 创建镜像存储池"></a>2.2 创建镜像存储池</h2><pre><code class="hljs"># 用于存储Glance组件的镜像ceph osd pool create images 128</code></pre><h2 id="2-3-创建虚拟机实例存储池"><a href="#2-3-创建虚拟机实例存储池" class="headerlink" title="2.3 创建虚拟机实例存储池"></a>2.3 创建虚拟机实例存储池</h2><pre><code class="hljs"># 用于存储Nova组件的实例启动镜像ceph osd pool create vms 128</code></pre><h1 id="3-创建客户端用户并授权"><a href="#3-创建客户端用户并授权" class="headerlink" title="3.创建客户端用户并授权"></a>3.创建客户端用户并授权</h1><h2 id="3-1-创建cinder用户"><a href="#3-1-创建cinder用户" class="headerlink" title="3.1 创建cinder用户"></a>3.1 创建cinder用户</h2><pre><code class="hljs"># 用于Cinder组件创建虚拟机实例与数据卷ceph auth get-or-create client.cinder mon &#39;allow r&#39; osd &#39;allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=vms, allow rx pool=images&#39; -o /etc/ceph/ceph.client.cinder.keyring</code></pre><h2 id="3-2-创建glance用户"><a href="#3-2-创建glance用户" class="headerlink" title="3.2 创建glance用户"></a>3.2 创建glance用户</h2><pre><code class="hljs"># 用于Clance组件创建镜像ceph auth get-or-create client.glance mon &#39;allow r&#39; osd &#39;allow class-read object_prefix rbd_children, allow rwx pool=images&#39; -o /etc/ceph/ceph.client.glance.keyring</code></pre><h2 id="3-3-将配置文件发送到Openstack节点"><a href="#3-3-将配置文件发送到Openstack节点" class="headerlink" title="3.3 将配置文件发送到Openstack节点"></a>3.3 将配置文件发送到Openstack节点</h2><pre><code class="hljs">scp -r /etc/ceph/ controller:/etcscp -r /etc/ceph/ compute001:/etcscp -r /etc/ceph/ compute002:/etc</code></pre><h1 id="4-Openstack控制节点配置"><a href="#4-Openstack控制节点配置" class="headerlink" title="4.Openstack控制节点配置"></a>4.Openstack控制节点配置</h1><h2 id="4-1-安装Ceph客户端"><a href="#4-1-安装Ceph客户端" class="headerlink" title="4.1 安装Ceph客户端"></a>4.1 安装Ceph客户端</h2><pre><code class="hljs">sudo apt install -y python3-rbd</code></pre><h2 id="4-2-设置Ceph密钥文件权限"><a href="#4-2-设置Ceph密钥文件权限" class="headerlink" title="4.2 设置Ceph密钥文件权限"></a>4.2 设置Ceph密钥文件权限</h2><pre><code class="hljs">sudo chown glance:glance /etc/ceph/ceph.client.glance.keyring</code></pre><h2 id="4-3-配置镜像服务"><a href="#4-3-配置镜像服务" class="headerlink" title="4.3 配置镜像服务"></a>4.3 配置镜像服务</h2><pre><code class="hljs">sudo cp /etc/glance/glance-api.conf /etc/glance/glance-api.conf.lvmsudo vi /etc/glance/glance-api.conf[default]# 启用镜像写时复制功能show_image_direct_url = True[glance_store]stores = rbddefault_store = rbdrbd_store_chunk_size = 8rbd_store_pool = imagesrbd_store_user = glancerbd_store_ceph_conf = /etc/ceph/ceph.conf</code></pre><h2 id="4-4-重启镜像服务"><a href="#4-4-重启镜像服务" class="headerlink" title="4.4 重启镜像服务"></a>4.4 重启镜像服务</h2><pre><code class="hljs"> sudo systemctl restart openstack-glance-api.service</code></pre><h2 id="4-5-验证镜像服务"><a href="#4-5-验证镜像服务" class="headerlink" title="4.5 验证镜像服务"></a>4.5 验证镜像服务</h2><h1 id="5-Openstack计算节点配置"><a href="#5-Openstack计算节点配置" class="headerlink" title="5.Openstack计算节点配置"></a>5.Openstack计算节点配置</h1><h2 id="5-1-安装Ceph客户端"><a href="#5-1-安装Ceph客户端" class="headerlink" title="5.1 安装Ceph客户端"></a>5.1 安装Ceph客户端</h2><pre><code class="hljs">sudo apt install -y ceph-common</code></pre><h2 id="设置Ceph密钥文件权限"><a href="#设置Ceph密钥文件权限" class="headerlink" title="设置Ceph密钥文件权限"></a>设置Ceph密钥文件权限</h2><pre><code class="hljs">sudo chown nova:nova /etc/ceph/ceph.client.cinder.keyring</code></pre><h2 id="5-2-配置libvirt"><a href="#5-2-配置libvirt" class="headerlink" title="5.2 配置libvirt"></a>5.2 配置libvirt</h2><p>由于libvirt进程在挂载或卸载一个由Cinder提供的Volume时需要访问Ceph集群，所以需要创建client.cinder用户的访问秘钥，再添加到libvirtd 守护进程，且每个计算节点都需添加，即用作cinder-volume调用凭证</p><h3 id="5-2-1-获取cinder密钥"><a href="#5-2-1-获取cinder密钥" class="headerlink" title="5.2.1 获取cinder密钥"></a>5.2.1 获取cinder密钥</h3><pre><code class="hljs">ceph auth get-key client.cinder &gt; client.cinder.key</code></pre><h3 id="5-2-2-生成uuid"><a href="#5-2-2-生成uuid" class="headerlink" title="5.2.2 生成uuid"></a>5.2.2 生成uuid</h3><pre><code class="hljs">uuidgen</code></pre><h3 id="5-2-2-创建libvirt密钥文件"><a href="#5-2-2-创建libvirt密钥文件" class="headerlink" title="5.2.2 创建libvirt密钥文件"></a>5.2.2 创建libvirt密钥文件</h3><pre><code class="hljs">vi secret.xml&lt;secret ephemeral=&#39;no&#39; private=&#39;no&#39;&gt;  &lt;uuid&gt;6421C80B-85FF-4D1E-A042-A23B99DB4689&lt;/uuid&gt;   &lt;usage type=&#39;ceph&#39;&gt;     &lt;name&gt;client.cinder secret&lt;/name&gt;   &lt;/usage&gt;&lt;/secret&gt;</code></pre><h3 id="5-2-3-创建libvirt密钥"><a href="#5-2-3-创建libvirt密钥" class="headerlink" title="5.2.3 创建libvirt密钥"></a>5.2.3 创建libvirt密钥</h3><pre><code class="hljs">virsh secret-define --file secret.xml</code></pre><h3 id="5-2-4-分配libvirt密钥"><a href="#5-2-4-分配libvirt密钥" class="headerlink" title="5.2.4 分配libvirt密钥"></a>5.2.4 分配libvirt密钥</h3><pre><code class="hljs">virsh secret-set-value --secret 6421C80B-85FF-4D1E-A042-A23B99DB4689 --base64 $(cat client.cinder.key) &amp;&amp; rm client.cinder.key secret.xml</code></pre><h3 id="5-3-启用RBD客户端缓存和管理Socket"><a href="#5-3-启用RBD客户端缓存和管理Socket" class="headerlink" title="5.3 启用RBD客户端缓存和管理Socket"></a>5.3 启用RBD客户端缓存和管理Socket</h3><pre><code class="hljs">sudo mkdir -p /var/run/ceph/guests/ /var/log/qemu/sudo chown -R libvirt-qemu:libvirt /var/run/ceph/guests /var/log/qemu/sudo vi /etc/ceph/ceph.conf[client]rbd cache = true# 启用RBD客户端缓存功能rbd cache writethrough until flush = trueadmin socket = /var/run/ceph/guests/$cluster-$type.$id.$pid.$cctid.asok# 设置rbd调用日志，用于故障排查log file = /var/log/qemu/qemu-guest-$pid.logrbd concurrent management ops = 20</code></pre><h2 id="5-4-配置计算服务"><a href="#5-4-配置计算服务" class="headerlink" title="5.4 配置计算服务"></a>5.4 配置计算服务</h2><pre><code class="hljs">sudo cp /etc/nova/nova.conf /etc/nova/nova.conf.lvmsudo vi /etc/nova/nova.conf[libvirt]virt_type = kvminject_password = falseinject_key = falseinject_partition = -2disk_cachemodes = &quot;network=writeback&quot;images_type = rbdimages_rbd_pool = vmsimages_rbd_ceph_conf = /etc/ceph/ceph.confhw_disk_discard = unmaprbd_user = cinderrbd_secret_uuid = 6421C80B-85FF-4D1E-A042-A23B99DB4689# 启用虚拟机热迁移功能live_migration_flag = &quot;VIR_MIGRATE_UNDEFINE_SOURCE,VIR_MIGRATE_PEER2PEER,VIR_MIGRATE_LIVE,VIR_MIGRATE_PERSIST_DEST,VIR_MIGRATE_TUNNELLED&quot;</code></pre><h2 id="5-5-重启计算服务"><a href="#5-5-重启计算服务" class="headerlink" title="5.5 重启计算服务"></a>5.5 重启计算服务</h2><pre><code class="hljs">sudo systemctl restart nova-compute.service</code></pre><ul><li>注：其余计算节点也都要按步骤执行，uuid保持一致即可</li></ul><h1 id="6-Openstack存储节点配置"><a href="#6-Openstack存储节点配置" class="headerlink" title="6.Openstack存储节点配置"></a>6.Openstack存储节点配置</h1><h2 id="6-1-安装Ceph客户端"><a href="#6-1-安装Ceph客户端" class="headerlink" title="6.1 安装Ceph客户端"></a>6.1 安装Ceph客户端</h2><pre><code class="hljs">sudo apt install -y ceph-common</code></pre><h2 id="6-2-设置Ceph密钥文件权限"><a href="#6-2-设置Ceph密钥文件权限" class="headerlink" title="6.2 设置Ceph密钥文件权限"></a>6.2 设置Ceph密钥文件权限</h2><pre><code class="hljs">sudo chown cinder:cinder /etc/ceph/ceph.client.cinder.keyring</code></pre><h2 id="6-3-配置存储服务"><a href="#6-3-配置存储服务" class="headerlink" title="6.3 配置存储服务"></a>6.3 配置存储服务</h2><pre><code class="hljs">sudo cp /etc/cinder/cinder.conf /etc/cinder/cinder.conf.lvmsudo vi /etc/cinder/cinder.conf[DEFAULT]enabled_backends = lvm,cephglance_api_version = 2[ceph]rbd_pool = volumesrbd_user = cinderrbd_ceph_conf = /etc/ceph/ceph.confrbd_flatten_volume_from_snapshot = falserbd_secret_uuid = 6421C80B-85FF-4D1E-A042-A23B99DB4689rbd_max_clone_depth = 5rbd_store_chunk_size = 4rados_connect_timeout = -1volume_driver = cinder.volume.drivers.rbd.RBDDrivervolume_backend_name = ceph</code></pre><h2 id="6-4-重启存储服务"><a href="#6-4-重启存储服务" class="headerlink" title="6.4 重启存储服务"></a>6.4 重启存储服务</h2><pre><code class="hljs">sudo systemctl restart openstack-cinder-api openstack-cinder-volume</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://docs.ceph.com/en/mimic/rbd/rbd-openstack">https://docs.ceph.com/en/mimic/rbd/rbd-openstack</a></li><li><a href="https://blog.csdn.net/qq_41786090/article/details/131574739">https://blog.csdn.net/qq_41786090/article/details/131574739</a></li><li><a href="https://blog.csdn.net/qq_37242520/article/details/106059132">https://blog.csdn.net/qq_37242520/article/details/106059132</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Ceph</tag>
      
      <tag>存储</tag>
      
      <tag>虚拟化</tag>
      
      <tag>私有云</tag>
      
      <tag>Openstack</tag>
      
      <tag>公有云</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prometheus监控RabbitMQ消息队列</title>
    <link href="/linux/Prometheus-RabbitMQ/"/>
    <url>/linux/Prometheus-RabbitMQ/</url>
    
    <content type="html"><![CDATA[<h1 id="1-RabbitMQ启用管理插件"><a href="#1-RabbitMQ启用管理插件" class="headerlink" title="1.RabbitMQ启用管理插件"></a>1.RabbitMQ启用管理插件</h1><pre><code class="hljs">sudo rabbitmq-plugins enable rabbitmq_management</code></pre><h1 id="2-RabbitMQ创建监控账号"><a href="#2-RabbitMQ创建监控账号" class="headerlink" title="2.RabbitMQ创建监控账号"></a>2.RabbitMQ创建监控账号</h1><pre><code class="hljs">sudo rabbitmqctl add_user prometheus prometheus_2024# 需要将用户赋予管理员权限: sudo rabbitmqctl set_user_tags prometheus administrator# 需要将用户赋予vhost权限: sudo rabbitmqctl set_permissions -p / prometheus &quot;.*&quot; &quot;.*&quot; &quot;.*&quot;</code></pre><h1 id="3-安装rabbitmq-exporter"><a href="#3-安装rabbitmq-exporter" class="headerlink" title="3.安装rabbitmq_exporter"></a>3.安装rabbitmq_exporter</h1><pre><code class="hljs">wget https://github.com/kbudde/rabbitmq_exporter/releases/download/v1.0.0/rabbitmq_exporter_1.0.0_linux_amd64.tar.gztar -xzvf rabbitmq_exporter_1.0.0_linux_amd64.tar.gz &amp;&amp; sudo mkdir -p /usr/local/rabbitmq_exportersudo mv rabbitmq_exporter /usr/local/rabbitmq_exporter</code></pre><h1 id="4-创建配置文件"><a href="#4-创建配置文件" class="headerlink" title="4.创建配置文件"></a>4.创建配置文件</h1><pre><code class="hljs"> sudo vi /usr/local/rabbitmq_exporter/config.json&#123;  &quot;rabbit_url&quot;: &quot;http://127.0.0.1:15672&quot;,   &quot;rabbit_user&quot;: &quot;prometheus&quot;,   &quot;rabbit_pass&quot;: &quot;prometheus_2024&quot;,   &quot;publish_port&quot;: &quot;9419&quot;,   &quot;publish_addr&quot;: &quot;&quot;,   &quot;output_format&quot;: &quot;TTY&quot;,   &quot;ca_file&quot;: &quot;ca.pem&quot;,   &quot;cert_file&quot;: &quot;client-cert.pem&quot;,   &quot;key_file&quot;: &quot;client-key.pem&quot;,   &quot;insecure_skip_verify&quot;: false,   &quot;exlude_metrics&quot;: [],   &quot;include_exchanges&quot;: &quot;.*&quot;,   &quot;skip_exchanges&quot;: &quot;^$&quot;,   &quot;include_queues&quot;: &quot;.*&quot;,   &quot;skip_queues&quot;: &quot;^$&quot;,   &quot;skip_vhost&quot;: &quot;^$&quot;,   &quot;include_vhost&quot;: &quot;.*&quot;,   &quot;rabbit_capabilities&quot;: &quot;no_sort,bert&quot;,   &quot;aliveness_vhost&quot;: &quot;/&quot;,   &quot;enabled_exporters&quot;: [     &quot;exchange&quot;,     &quot;node&quot;,     &quot;overview&quot;,     &quot;queue&quot;,     &quot;aliveness&quot;   ],   &quot;timeout&quot;: 30,   &quot;max_queues&quot;: 0</code></pre><p>  }</p><h1 id="5-创建启动脚本"><a href="#5-创建启动脚本" class="headerlink" title="5.创建启动脚本"></a>5.创建启动脚本</h1><pre><code class="hljs">sudo vi /lib/systemd/system/rabbitmq_exporter.service[Unit]Description=https://www.rabbitmq.com/prometheus.htmlAfter=network-online.target[Service]Restart=on-failureExecStart=/usr/local/bin/rabbitmq_exporter -config-file /usr/local/rabbitmq_exporter/config.json[Install]WantedBy=multi-user.target</code></pre><h1 id="6-启动rabbitmq-exporter"><a href="#6-启动rabbitmq-exporter" class="headerlink" title="6.启动rabbitmq_exporter"></a>6.启动rabbitmq_exporter</h1><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start rabbitmq_exporter.servicesudo systemctl enable rabbitmq_exporter.service</code></pre><h1 id="7-配置Prometheus"><a href="#7-配置Prometheus" class="headerlink" title="7.配置Prometheus"></a>7.配置Prometheus</h1><h2 id="7-1-配置监控实例"><a href="#7-1-配置监控实例" class="headerlink" title="7.1 配置监控实例"></a>7.1 配置监控实例</h2><pre><code class="hljs">sudo vi /usr/local/prometheus/prometheus.ymlglobal:  scrape_interval: 15s   evaluation_interval: 15s   scrape_timeout: 10s scrape_configs:  - job_name: &quot;prometheus&quot;    static_configs:      - targets: [&quot;localhost:9090&quot;]  - job_name: &quot;rabbitmq&quot;    static_configs:      - targets: [&quot;192.168.100.120:9414&quot;]</code></pre><h2 id="7-2-配置告警规则"><a href="#7-2-配置告警规则" class="headerlink" title="7.2 配置告警规则"></a>7.2 配置告警规则</h2><pre><code class="hljs">sudo vi /usr/local/prometheus/rules/rabbitmq.ymlgroups:- name: RabbitMQ  rules:  - alert: RabbitmqDown    expr: sum(rabbitmq_build_info) &lt; 1    for: 1m    labels:      severity: Critical    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125;RabbitMQ集群实例宕机，请尽快处理!&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125;RabbitMQ集群实例宕机超过1分钟，当前状态为&#123;&#123; $value &#125;&#125;&quot;         - alert: RabbitmqNodeNotDistributed    expr: erlang_vm_dist_node_state &lt; 3    for: 0m    labels:      severity: Warning    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125;RabbitMQ集群实例未启用分布式功能，请尽快处理!&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125;RabbitMQ集群实例未启用分布式功能，当前状态为&#123;&#123; $value &#125;&#125;&quot;                 - alert: RabbitmqMemoryHigh    expr: rabbitmq_process_resident_memory_bytes / rabbitmq_resident_memory_limit_bytes * 100 &gt; 90    for: 2m    labels:      severity: Warning    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125;RabbitMQ集群实例内存超限，请尽快处理!&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125;RabbitMQ集群实例内存超限90%，当前内存占用为&#123;&#123; $value &#125;&#125;&quot;  - alert: RabbitmqFileDescriptorsUsage    expr: rabbitmq_process_open_fds / rabbitmq_process_max_fds * 100 &gt; 90    for: 5m    labels:      severity: Warning    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125;RabbitMQ集群实例文件描述符占用超限，请尽快处理!&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125;RabbitMQ集群实例文件描述符占用超限90%，当前占用量为VALUE = &#123;&#123; $value &#125;&#125;&quot;   - alert: RabbitmqTooManyUnackMessages    expr: sum(rabbitmq_queue_messages_unacked) BY (queue) &gt; 1000    for: 1m    labels:      severity: Warning    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125;RabbitMQ集群实例消息队列未确认消息量超限，请尽快处理!&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125;RabbitMQ集群实例消息队列未确认消息量已超过1000，当前量为&#123;&#123; $value &#125;&#125;&quot;   - alert: RabbitmqUnroutableMessages    expr: increase(rabbitmq_channel_messages_unroutable_returned_total[5m]) &gt; 0 or increase(rabbitmq_channel_messages_unroutable_dropped_total[5m]) &gt; 0    for: 5m    labels:      severity: Warning    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125;RabbitMQ集群实例消息队存在不可路由消息，请尽快处理!&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125;RabbitMQ集群实例消息队存在不可路由消息，当前不可路由消息数量为&#123;&#123; $value &#125;&#125;&quot;  - alert: RabbitmqTooManyConnections    expr: rabbitmq_connections &gt; 1000    for: 2m    labels:      severity: Warning    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125;RabbitMQ集群实例连接数超限，请尽快处理!&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125;RabbitMQ集群实例连接数超过1000，当前连接数为&#123;&#123; $value &#125;&#125;&quot;                   - alert: RabbitmqNoQueueConsumer    expr: rabbitmq_queue_consumers &lt; 1    for: 1m    labels:      severity: Warning    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125;RabbitMQ集群实例消息队列无消费者，请尽快处理!&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125;RabbitMQ集群实例消息队列无消费者，当前消费者数为&#123;&#123; $value &#125;&#125;&quot;  - alert: RabbitmqInstancesDifferentVersions    expr: count(count(rabbitmq_build_info) by (rabbitmq_version)) &gt; 1    for: 5m    labels:      severity: Warning    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125;RabbitMQ集群实例存在不同版本，请尽快处理!&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125;RabbitMQ集群实例消息队列无消费者，当前状态为&#123;&#123; $value &#125;&#125;&quot;  - alert: RabbitmqClusterPartition    expr: rabbitmq_partitions &gt; 0    for: 5m    labels:      severity: Critical    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125;RabbitMQ集群实例存在网络分区，请尽快处理!&quot;      description:&quot;&#123;&#123; $labels.instance &#125;&#125;RabbitMQ集群实例存在网络分区，当前状态为&#123;&#123; $value &#125;&#125;&quot;</code></pre><h2 id="7-3-重载Prometheus"><a href="#7-3-重载Prometheus" class="headerlink" title="7.3 重载Prometheus"></a>7.3 重载Prometheus</h2><pre><code class="hljs">curl -XPOST http://127.0.0.1:9090/-/reload</code></pre><h1 id="8-Grafana导入监控模版"><a href="#8-Grafana导入监控模版" class="headerlink" title="8.Grafana导入监控模版"></a>8.Grafana导入监控模版</h1><p>Dashboards —&gt; Manage —&gt; Import —&gt; 模版ID：10120</p><p><img src="/img/wiki/prometheus/rabbitmq.jpg" alt="rabbitmq"></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.51cto.com/u_16213630/10089792">https://blog.51cto.com/u_16213630/10089792</a></li><li><a href="https://github.com/kbudde/rabbitmq_exporter">https://github.com/kbudde/rabbitmq_exporter</a></li><li><a href="https://blog.csdn.net/manba_24/article/details/134441710">https://blog.csdn.net/manba_24/article/details/134441710</a></li><li><a href="https://blog.csdn.net/wybaaaaaaaa/article/details/130887890">https://blog.csdn.net/wybaaaaaaaa/article/details/130887890</a></li><li><a href="https://blog.csdn.net/weixin_43845924/article/details/136167093">https://blog.csdn.net/weixin_43845924/article/details/136167093</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>云原生</tag>
      
      <tag>MQ</tag>
      
      <tag>中间件</tag>
      
      <tag>Prometheus</tag>
      
      <tag>监控告警</tag>
      
      <tag>RabbitMQ</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prometheus监控数据持久化存储方案</title>
    <link href="/linux/PrometheusTSDB/"/>
    <url>/linux/PrometheusTSDB/</url>
    
    <content type="html"><![CDATA[<p>Prometheus默认以tsdb时序数据库简单高效地将监控数据存储于本地，自2.0版本之后还将压缩数据的能力大大地提升（每个采样数据仅为3.5b左右），单节点即可满足大部分用户的监控需求。但本地存储也限制了其扩展性，带来了数据持久化等一系列问题。鉴于此，为解决单节点存储的限制，Prometheus提供了远程读写接口，用户可自行选择合适的时序数据库，如Influxdb、Thanos和VictoriaMetrics等等</p><h1 id="时序数据"><a href="#时序数据" class="headerlink" title="时序数据"></a>时序数据</h1><p>时序数据，即带有时间戳的数据，存储于时序数据库，目的是监测数据的前后差异，以便于做出相应的动作</p><h2 id="数据特点"><a href="#数据特点" class="headerlink" title="数据特点"></a>数据特点</h2><ul><li>一旦被存储就不会被修改，即新增数据只会被添加到系统，不会在将来的某个时段被修改为其他的值</li><li>由于时序数据通常用于监测指标的变化趋势，所以最近产生的数据其重要性超过旧数据</li><li>数据量巨大，由于每隔一个时间段就会新增一批数据，数据增速非常快，如5000数据点每秒采集一次数据，一小时的数据量就将超过18,000,000条</li><li>往往是时间间隔越小差异性越小，如某地的温度，如秒级监控，差异将会很小</li></ul><h2 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h2><h3 id="IOT"><a href="#IOT" class="headerlink" title="IOT"></a>IOT</h3><p>主要来源是传感器，如某点的温度、湿度、压力、电流、电压等</p><h3 id="金融和科学数据"><a href="#金融和科学数据" class="headerlink" title="金融和科学数据"></a>金融和科学数据</h3><p>如交易时段的证券价格、地震监控数据等</p><h3 id="IT基础架构"><a href="#IT基础架构" class="headerlink" title="IT基础架构"></a>IT基础架构</h3><p>即软硬件的监控数据</p><h1 id="时序数据库"><a href="#时序数据库" class="headerlink" title="时序数据库"></a>时序数据库</h1><p>时序数据的数据量通常都非常巨大，由此也将带来许多问题</p><h2 id="1-高速接收和存储数据"><a href="#1-高速接收和存储数据" class="headerlink" title="1.高速接收和存储数据"></a>1.高速接收和存储数据</h2><p>时序数据的数据点多，更新频率高，目前普遍使用LSM技术，而非B树来存储数据。LSM先在内存中积累数据然后再批量写入磁盘，而B树的优势在于读取而不是存储</p><h2 id="2-压缩数据"><a href="#2-压缩数据" class="headerlink" title="2.压缩数据"></a>2.压缩数据</h2><p>目前大部分时序数据库采用Facebook提出的Gorilla算法，简单说就是存储数据的差异，因为时序数据有频率高差异小的特点。如果直接使用传统的关系数据库来存储时序数据会需要极高的存储成本，相比传统的关系数据库，时序数据库在存储时序数据上能做到只需要1&#x2F;20甚至更低的存储空间需求</p><h2 id="3-巨大数据量的快速查询"><a href="#3-巨大数据量的快速查询" class="headerlink" title="3.巨大数据量的快速查询"></a>3.巨大数据量的快速查询</h2><p>很多时序数据库是列数据库，列数据库具有更好的分析数据的性能</p><h2 id="4-留存策略"><a href="#4-留存策略" class="headerlink" title="4.留存策略"></a>4.留存策略</h2><p>时序数据库有相应的处理策略，如几年前和最近几个月的数据处理方式和留存策略需要做不同的处理</p><h1 id="1-本地存储"><a href="#1-本地存储" class="headerlink" title="1.本地存储"></a>1.本地存储</h1><p>Prometheus默认采用tsdb时序数据库，将监控数据存储在本地磁盘，默认存储时长为15天时间较短，且不支持跨集群的聚合</p><h2 id="1-1-存储原理"><a href="#1-1-存储原理" class="headerlink" title="1.1 存储原理"></a>1.1 存储原理</h2><p>Prometheus按每2小时一个Block进行存储，每个block由一个目录组成，该目录包含：一个或多个chunk文件，每个chunk默认大小为512M，用于保存两小时的时间序列数据，而最新写入的数据保存在内存block中，两小时后再写入chunk文件，且后台会将block压缩成更大的block，并合并成更高level的block文件后删除低level的block文件。通过这样时间窗口形式所保存的所有的样本数据明显提高了Prometheus的查询效率，即查询一段时间范围内的所有样本数据只需简单的从落在该范围内的块中查询数据即可；一个metadata文件；一个index文件，通过metric name和labels查找时间序列数据在chunk块文件的位置</p><p>为防止程序崩溃导致数据丢失，Prometheus采用WAL（write-ahead-log）机制，启动时以写入日志(WAL)的方式实现重播，从而恢复数据</p><p>删除数据时，删除条目会记录在独立的tombstone文件，而不是立即从chunk文件删除</p><h2 id="1-2-磁盘大小计算"><a href="#1-2-磁盘大小计算" class="headerlink" title="1.2 磁盘大小计算"></a>1.2 磁盘大小计算</h2><p>磁盘大小计算方式：磁盘大小 &#x3D; 保留时间 * 每秒获取样本数 * 样本大小</p><p>保留时间(retention_time_seconds)和样本大小(bytes_per_sample)不变的情况下，如需减少本地磁盘的容量需求，只能通过减少每秒获取样本数(ingested_samples_per_second)的方式，因此有两种手段：即减少时间序列的数量，或是增加采集样本的时间间隔。由于Prometheus会对时间序列进行压缩，因此减少时间序列的数量效果更明显</p><h2 id="1-3-内存占用"><a href="#1-3-内存占用" class="headerlink" title="1.3 内存占用"></a>1.3 内存占用</h2><p>随着规模变大，Prometheus需要的CPU和内存都会升高，内存一般先达到瓶颈，这时要么加内存，要么集群分片减少单机指标。内存消耗主要由以下因素引起：</p><ul><li>Prometheus的内存消耗主要在于每2小时的Block数据落盘，落盘之前所有数据都在内存里面，因此和采集量有关</li><li>加载历史数据是从磁盘到内存，查询范围越大所占内存越大</li><li>一些不合理的查询条件也会加大内存，如Group或大范围Rate</li></ul><h2 id="1-4-数据存储方式"><a href="#1-4-数据存储方式" class="headerlink" title="1.4 数据存储方式"></a>1.4 数据存储方式</h2><p>内存中的block数据未写入磁盘时，block目录下面主要保存wal文件:</p><pre><code class="hljs">./data/01BKGV7JBM69T2G1BGBGM6KB12./data/01BKGV7JBM69T2G1BGBGM6KB12/meta.json./data/01BKGV7JBM69T2G1BGBGM6KB12/wal/000002./data/01BKGV7JBM69T2G1BGBGM6KB12/wal/000001</code></pre><p>持久化的block目录下wal文件被删除，时序数据保存在chunk文件，index用于索引timeseries在wal文件里的位置：</p><pre><code class="hljs">./data/01BKGV7JC0RY8A6MACW02A2PJD./data/01BKGV7JC0RY8A6MACW02A2PJD/meta.json./data/01BKGV7JC0RY8A6MACW02A2PJD/index./data/01BKGV7JC0RY8A6MACW02A2PJD/chunks./data/01BKGV7JC0RY8A6MACW02A2PJD/chunks/000001./data/01BKGV7JC0RY8A6MACW02A2PJD/tombstones</code></pre><h1 id="2-远程存储"><a href="#2-远程存储" class="headerlink" title="2.远程存储"></a>2.远程存储</h1><p>2017年，Prometheus集成了Remote Read&#x2F;Write API接口，支持将数据存储到远端和从远端读取数据的功能，也即是将数据通过接口保存到第三方存储服务。此后，社区涌现出大量长期存储的方案，如InfluxDB、Thanos、Grafana Cortex&#x2F;Mimir、VictoriaMetrics、Wavefront、Splunk、Sysdig、SignalFx、Graphite等</p><h2 id="2-1-工作机制"><a href="#2-1-工作机制" class="headerlink" title="2.1 工作机制"></a>2.1 工作机制</h2><p>Prometheus将采集到的指标数据通过HTTP的形式发送给适配器(Adaptor)，再由适配器进行数据的存入；remote_read特性则会向适配器发起查询请求，适配器根据请求条件从第三方存储服务中获取响应的数据</p><h3 id="2-1-1-Remote-Write"><a href="#2-1-1-Remote-Write" class="headerlink" title="2.1.1 Remote Write"></a>2.1.1 Remote Write</h3><p>Prometheus配置文件指定Remote Write(远程写)的URL地址，如指向influxdb，一旦设置了该配置项，Prometheus将样本数据通过HTTP的形式发送给适配器(Adaptor)，再由适配器对接任意的外部服务，如公有&#x2F;私有云的存储服务，或消息队列等任意形式</p><h3 id="2-1-2-Remote-Read"><a href="#2-1-2-Remote-Read" class="headerlink" title="2.1.2 Remote Read"></a>2.1.2 Remote Read</h3><p>Prometheus的查询请求由配置文件remote_read所指向的URL进行处理，Adaptor根据请求条件从第三方存储服务中获取响应的数据，同时将数据转换为Promethues的原始样本数据返回给Prometheus Server。之后，Promethues在本地使用PromQL对样本数据进行二次处理。即原始样本数据从远程存储获取，规则文件及Metadata API处理在本地完成</p><h2 id="2-2-配置文件"><a href="#2-2-配置文件" class="headerlink" title="2.2 配置文件"></a>2.2 配置文件</h2><pre><code class="hljs">remote_write:  - url: &quot;http://localhost:9201/write&quot;remote_read:  - url: &quot;http://localhost:9201/read&quot;</code></pre><h2 id="2-3-方案选择"><a href="#2-3-方案选择" class="headerlink" title="2.3 方案选择"></a>2.3 方案选择</h2><ul><li>InfluxDB，Go语言开发的开源时间序列数据库，行业标杆，最为流行，数据压缩能力极强</li><li>VictoriaMetrics，最新技术，简单易用，但有些重要功能没开源，如Downsampling降采样功能，跨较长时间范围的聚合及查询耗时较久</li><li>Thanos依赖对象存储，备份优势明显</li></ul><h1 id="3-InfluxDB"><a href="#3-InfluxDB" class="headerlink" title="3.InfluxDB"></a>3.InfluxDB</h1><h2 id="3-1-安装InfluxDB"><a href="#3-1-安装InfluxDB" class="headerlink" title="3.1 安装InfluxDB"></a>3.1 安装InfluxDB</h2><pre><code class="hljs">wget https://dl.influxdata.com/influxdb/releases/influxdb-1.8.10_linux_arm64.tar.gztar -xzvf influxdb-1.8.10_linux_arm64.tar.gz &amp;&amp; sudo mv influxdb-1.8.10-1 /usr/local/influxdbsudo mkdir -p /usr/local/influxdb/data/meta &amp;&amp; sudo mkdir -p /usr/local/influxdb/data/wal</code></pre><h2 id="3-2-创建配置文件"><a href="#3-2-创建配置文件" class="headerlink" title="3.2 创建配置文件"></a>3.2 创建配置文件</h2><pre><code class="hljs">sudo vi /usr/local/influxdb/etc/influxdb.conf[meta]  dir = &quot;/usr/local/influxdb/data/meta&quot;[data]  dir = &quot;/usr/local/influxdb/data&quot;  wal-dir = &quot;/usr/local/influxdb/data/wal&quot;</code></pre><h2 id="3-2-创建启动脚本"><a href="#3-2-创建启动脚本" class="headerlink" title="3.2 创建启动脚本"></a>3.2 创建启动脚本</h2><pre><code class="hljs">sudo vi /lib/systemd/system/influxd.service[Unit]Description=influxdDocumentation=https://docs.influxdata.com/influxdb/v1/install/?t=LinuxAfter=network.target[Service]ExecStart=/usr/local/influxdb/usr/bin/influxd -config /usr/local/influxdb/etc/influxdb.confRestart=on-failureRestartSec=20[Install]WantedBy=multi-user.target</code></pre><h2 id="3-3-启动InfluxDB"><a href="#3-3-启动InfluxDB" class="headerlink" title="3.3 启动InfluxDB"></a>3.3 启动InfluxDB</h2><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start influxd.servicesudo systemctl enable influxd.service</code></pre><h2 id="3-4-创建数据库"><a href="#3-4-创建数据库" class="headerlink" title="3.4 创建数据库"></a>3.4 创建数据库</h2><pre><code class="hljs">/usr/local/influxdb/usr/bin/influxConnected to http://localhost:8086 version 1.8.10InfluxDB shell version: 1.8.10&gt; create database prometheus;# 创建默认保留策略&gt; CREATE RETENTION POLICY &quot;prometheus&quot; ON &quot;prometheus&quot; DURATION 1h REPLICATION 1 DEFAULT# 创建用户&gt; CREATE USER prometheus WITH PASSWORD &#39;prometheus@2024&#39;# 用户授权&gt; GRANT ALL ON &quot;prometheus&quot; TO &quot;prometheus&quot;</code></pre><h2 id="3-5-配置Prometheus"><a href="#3-5-配置Prometheus" class="headerlink" title="3.5 配置Prometheus"></a>3.5 配置Prometheus</h2><pre><code class="hljs">sudo vi /usr/local/prometheus/prometheus.ymlglobal:  scrape_interval: 15s   evaluation_interval: 15s   scrape_timeout: 10s remote_write:  - url: &quot;http://127.0.0.1:8086/api/v1/prom/write?db=prometheus&amp;u=prometheus&amp;p=prometheus@2024.&quot;    remote_timeout: 30s    queue_config:      capacity: 100000      max_shards: 1000      max_samples_per_send: 1000      batch_send_deadline: 5s      min_backoff: 30ms      max_backoff: 100msremote_read:  - url: &quot;http://127.0.0.1:8086/api/v1/prom/read?db=prometheus&amp;u=prometheus&amp;p=prometheus@2024.&quot;    remote_timeout: 10s    read_recent: true</code></pre><h2 id="3-6-重载Prometheus"><a href="#3-6-重载Prometheus" class="headerlink" title="3.6 重载Prometheus"></a>3.6 重载Prometheus</h2><pre><code class="hljs">curl -XPOST http://127.0.0.1:9090/-/reload</code></pre><h2 id="3-7-验证InfluxDB数据"><a href="#3-7-验证InfluxDB数据" class="headerlink" title="3.7 验证InfluxDB数据"></a>3.7 验证InfluxDB数据</h2><pre><code class="hljs">/usr/local/influxdb/usr/bin/influxConnected to http://localhost:8086 version 1.8.10InfluxDB shell version: 1.8.10&gt; use prometheus&gt; show measurements&gt; select * from go_info;</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.cnblogs.com/liushiya/p/18009620">https://www.cnblogs.com/liushiya/p/18009620</a></li><li><a href="https://www.cnblogs.com/yangjianbo/articles/17391415.html">https://www.cnblogs.com/yangjianbo/articles/17391415.html</a></li><li><a href="https://blog.csdn.net/m0_64417032/article/details/125450979">https://blog.csdn.net/m0_64417032/article/details/125450979</a></li><li><a href="https://blog.csdn.net/weixin_43202160/article/details/134418698">https://blog.csdn.net/weixin_43202160/article/details/134418698</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>云原生</tag>
      
      <tag>Prometheus</tag>
      
      <tag>监控告警</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux系统服务管理工具Systemd详解</title>
    <link href="/linux/Systemd/"/>
    <url>/linux/Systemd/</url>
    
    <content type="html"><![CDATA[<p>Systemd，当前Linux发行版最为流行的init系统和服务管理工具，通过套接字激活机制，使得无论有无依赖关系的程序全部并行启动，且仅按照系统启动的需要启动相应的服务，最大化地加快启动速度。Systemd使用单个配置文件管理所有服务，并提供了丰富的命令行工具。此外，Systemd还支持动态加载和卸载服务，可以在系统运行时添加或删除服务</p><h1 id="1-init系统"><a href="#1-init系统" class="headerlink" title="1.init系统"></a>1.init系统</h1><p>init，即系统初始化进程，Linux系统启动的第一个进程，是其他所有进程的起点，pid为1。其发展大体上分为三个阶段，即sysvinit -&gt; upstart -&gt; systemd</p><h2 id="1-1-sysvinit"><a href="#1-1-sysvinit" class="headerlink" title="1.1 sysvinit"></a>1.1 sysvinit</h2><p>类Unix系统最初启用的init初始化系统，以脚本的形式管理系统服务，按照特定的顺序启动系统服务，并运行对应的启动脚本。由于启动方式为按照顺序逐个的串行启动，所以启动速度较慢，目前基本已弃用</p><h2 id="1-2-Upstart"><a href="#1-2-Upstart" class="headerlink" title="1.2 Upstart"></a>1.2 Upstart</h2><p>最初由Ubuntu8.04启用，以事件驱动，支持并行启动多个服务，启动速度大大加快，能够自动监测服务的状态，且服务崩溃或停止运行时支持自动重启</p><h2 id="1-3-Systemd"><a href="#1-3-Systemd" class="headerlink" title="1.3 Systemd"></a>1.3 Systemd</h2><p>当前最新的初始化系统和系统管理器，由于其高效的性能和简便的管理，大多数Linux发行版当前都以其取代传统的SysV，以完成初始化系统。Systemd，最初由Fedora15启用，以.service服务文件将所有守护进程加入到cgroups排序，而不是像SysVinit那样使用bash脚本，可通过&#x2F;cgroup&#x2F;systemd文件查看系统等级，兼容SysV和LSB初始化脚本。其管理命令为systemctl，是管理Systemd守护进程及服务的工具，如开启、重启、关闭、启用、禁用、重载和状态</p><h1 id="2-​Systemd架构"><a href="#2-​Systemd架构" class="headerlink" title="2.​Systemd架构"></a>2.​Systemd架构</h1><h2 id="2-1-第一层"><a href="#2-1-第一层" class="headerlink" title="2.1 第一层"></a>2.1 第一层</h2><p>​​内核层​​面依赖，如cgroup、autofs、kdbus</p><h2 id="2-2-第二层"><a href="#2-2-第二层" class="headerlink" title="2.2 第二层"></a>2.2 第二层</h2><p>libraries​，依赖库​</p><h2 id="2-3-第三层"><a href="#2-3-第三层" class="headerlink" title="2.3 第三层"></a>2.3 第三层</h2><p>​Systemd Core​​，systemd核心库</p><h2 id="2-4-第四层"><a href="#2-4-第四层" class="headerlink" title="2.4 第四层"></a>2.4 第四层</h2><p>​Systemd daemons​​及targets，自带的基本unit、target，类似于sysvinit自带的脚本</p><h2 id="2-5-第五层"><a href="#2-5-第五层" class="headerlink" title="2.5 第五层"></a>2.5 第五层</h2><p>命令行工具，如systemctl，用户通过其进行操作Systemd</p><h1 id="3-工作机制"><a href="#3-工作机制" class="headerlink" title="3.工作机制"></a>3.工作机制</h1><p>Unit，​Systemd管理和控制系统资源、服务或任务的基本配置单元，用于定义系统服务和其他资源，包括服务的启动顺序、依赖关系、部署状态等，每个unit都有名称、类型和配置文件，如docker.service、mysql.socket等。Systemd通过配置单元文件，来定义所有的管理工作</p><h2 id="3-1-单元类型"><a href="#3-1-单元类型" class="headerlink" title="3.1 单元类型"></a>3.1 单元类型</h2><h3 id="3-1-1-Service-unit"><a href="#3-1-1-Service-unit" class="headerlink" title="3.1.1 Service unit"></a>3.1.1 Service unit</h3><p>.service服务，用于封装后台服务进程的单元</p><h3 id="3-1-2-Target-unit"><a href="#3-1-2-Target-unit" class="headerlink" title="3.1.2 Target unit"></a>3.1.2 Target unit</h3><p>.target服务，为其他配置单元进行逻辑分组的单元，本身实际上并不做什么，只用于引用其他配置单元，从而对配置单元做一个统一的控制。此外，还可以实现进程运行级别，如系统图形化模式需要运行的许多服务和配置命令都由一个个配置单元表示，将所有这些配置单元组合为一个目标(target)，即表示将这些配置单元全部执行一遍（multi-user.target，相当于传统SysV系统的运行级别5）</p><h3 id="3-1-3-Device-Unit"><a href="#3-1-3-Device-Unit" class="headerlink" title="3.1.3 Device Unit"></a>3.1.3 Device Unit</h3><p>.device服务，封装Linux设备树中的某个设备的单元，每个使用udev规则标记的设备都将在Systemd中作为一个设备配置单元</p><h3 id="3-1-4-Mount-Unit"><a href="#3-1-4-Mount-Unit" class="headerlink" title="3.1.4 Mount Unit"></a>3.1.4 Mount Unit</h3><p>.mount服务，封装文件系统结构层次中挂载点的单元，以完成挂载点的监控和管理，如系统启动自动挂载、某些条件的自动卸载等等，即将&#x2F;etc&#x2F;fstab文件中的条目都转换为挂载点，并在开机时处理</p><h3 id="3-1-5-Automount-Unit"><a href="#3-1-5-Automount-Unit" class="headerlink" title="3.1.5 Automount Unit"></a>3.1.5 Automount Unit</h3><p>.automount服务，封装系统结构层次中自动挂载点的单元，该单元对应一个挂载配置单元，并于系统启动时被触发，从而执行挂载点定义的挂载操作</p><h3 id="3-1-6-Path-Unit"><a href="#3-1-6-Path-Unit" class="headerlink" title="3.1.6 Path Unit"></a>3.1.6 Path Unit</h3><p>.path服务，文件系统的文件或目录，用于监控指定目录或文件的变化，并触发其它Unit</p><h3 id="3-1-7-Scope-Unit"><a href="#3-1-7-Scope-Unit" class="headerlink" title="3.1.7 Scope Unit"></a>3.1.7 Scope Unit</h3><p>用于cgroups，表示Systemd外部创建的进程，即由Systemd运行产生的、不是由用户创建的文件，描述一些系统服务的分组信息</p><h3 id="3-1-8-Slice-Unit"><a href="#3-1-8-Slice-Unit" class="headerlink" title="3.1.8 Slice Unit"></a>3.1.8 Slice Unit</h3><p>.slice服务，进程组，用于cgroups，表示一组按层级排列的单位，并不包含进程，但会组建一个层级，并将scope和service都放置其中</p><h3 id="3-1-9-Snapshot-Unit"><a href="#3-1-9-Snapshot-Unit" class="headerlink" title="3.1.9 Snapshot Unit"></a>3.1.9 Snapshot Unit</h3><p>.snapshot服务，用于表示由Systemctl snapshot命令创建的单元运行状态的快照，类似于target，保存了系统当前的运行状态，是一组配置单元</p><h3 id="3-1-10-Socket-Unit"><a href="#3-1-10-Socket-Unit" class="headerlink" title="3.1.10 Socket Unit"></a>3.1.10 Socket Unit</h3><p>.socket服务，封装系统和互联网中的套接字，有一个相应的服务配置单元，相应的服务在第一个客户端连接进入套接字时就会启动，如nscd.socket在有新连接后便启动nscd.service，支持流式、数据报和连续包的AF_INET、AF_INET6、AF_UNIX socket</p><h3 id="3-1-11-Swap-Unit"><a href="#3-1-11-Swap-Unit" class="headerlink" title="3.1.11 Swap Unit"></a>3.1.11 Swap Unit</h3><p>.swap服务，类似于Mount单元，用于管理交换分区，使得交换分区在启动时被激活</p><h3 id="3-1-12-Timer-Unit"><a href="#3-1-12-Timer-Unit" class="headerlink" title="3.1.12 Timer Unit"></a>3.1.12 Timer Unit</h3><p>.timer服务，定时器配置单元，用于定时触发用户定义的操作，类似于atd、crontab等传统的定时服务</p><h2 id="3-2-配置文件目录"><a href="#3-2-配置文件目录" class="headerlink" title="3.2 配置文件目录"></a>3.2 配置文件目录</h2><ul><li>&#x2F;etc&#x2F;systemd&#x2F;system.conf，Systemd全局配置文件</li><li>&#x2F;etc&#x2F;systemd&#x2F;logind.conf，登录管理器的配置文件，用于管理用户登录时的会话</li><li>&#x2F;etc&#x2F;systemd&#x2F;systemd-journald.conf，journal日志配置文件，用于管理系统日志</li><li>&#x2F;etc&#x2F;systemd&#x2F;timesyncd.conf，timesyncd时间同步服务配置文件</li><li>&#x2F;etc&#x2F;systemd&#x2F;system&#x2F;，所有系统服务单元文件的配置文件，包括启动时运行的系统服务和用户服务</li><li>&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;，所有已安装软件的服务单元文件，包括系统服务和第三方软件服务</li><li>&#x2F;run&#x2F;systemd&#x2F;system&#x2F;，所有正在运行的服务单元文件，基于&#x2F;etc&#x2F;systemd&#x2F;system&#x2F;和&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;目录中的文件生成</li><li>&#x2F;etc&#x2F;systemd&#x2F;user&#x2F;，用户定义的服务单元文件，用户登录时自动启动</li><li>&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;systemd-sleep，系统睡眠模式相关的服务脚本</li></ul><h2 id="3-3-配置文件"><a href="#3-3-配置文件" class="headerlink" title="3.3 配置文件"></a>3.3 配置文件</h2><p>Unit配置文件由三部分组成，即Unit、Service和Install段</p><h3 id="3-3-1-Unit"><a href="#3-3-1-Unit" class="headerlink" title="3.3.1 Unit"></a>3.3.1 Unit</h3><p>Unit段所有Unit文件通用，用于定义Unit的元数据、配置及与其他Unit的关系</p><ul><li>Description，Unit描述信息</li><li>Documentation，Unit说明文档</li><li>Requires，强依赖关系，即所依赖的Unit未启动时当前Unit也不能启动</li><li>BindsTo，强依赖关系，即所依赖的Unit未启动时当前Unit也不能启动，且强依赖终止或重启时当前Unit也会跟随其状态</li><li>Wants，弱依赖关系，即所依赖的Unit未启动时当前Unit也可启动</li><li>After，先后依赖关系，即当前Unit需在其全部启动之后才能启动</li><li>Before：与After相反，即当前Unit启动之后才会启动其指定的Unit</li><li>RequiresOverridable，类似于Requires，但允许在启动时覆盖依赖的Unit，如在容器中运行</li><li>PartOf，从属关系，即当前Unit是其一部分，随其终止而终止</li><li>Conflicts，互斥关系，即当前Unit不能与其一起启动</li><li>OnFailure，当前Unit启动失败时，就将自动启动其指定的Unit</li></ul><h3 id="3-3-2-Service"><a href="#3-3-2-Service" class="headerlink" title="3.3.2 Service"></a>3.3.2 Service</h3><p>service段是服务（Service）类型Unit特有的字段，用于定义服务的具体管理和执行动作</p><h4 id="3-3-2-1-生命周期控制"><a href="#3-3-2-1-生命周期控制" class="headerlink" title="3.3.2.1 生命周期控制"></a>3.3.2.1 生命周期控制</h4><p>Type，定义启动进程行为</p><ul><li>simple，默认值，表示执行ExecStart指定的命令，启动主进程</li><li>forking，表示以fork方式从父进程创建子进程，创建后父进程立即退出</li><li>oneshot，表示启动一次性进程，当前服务退出后再继续执行</li><li>dbus，表示当前服务通过D-Bus启动</li><li>notify，表示当前服务启动完毕将会通知Systemd，再继续执行</li><li>idle，表示其他任务执行完毕才会运行当前服务</li><li>PrivateTmp，是否启用私有临时文件目录，设为true则会在&#x2F;tmp目录生成类似systemd-private-*-apache.service-RedVyu的文件夹，以提高文件的安全性，且该目录会随服务的重启而自动清理，无需再定义临时目录清理规则</li><li>RemainAfterExit，默认为false，配置为true表示Systemd只会负责启动服务进程，之后即便服务进程退出了，也仍然会认为这个服务还在运行中，主要用于启动注册后立即退出的非常驻内存，且需等待消息按需启动的特殊类型服务</li></ul><h4 id="3-3-2-2-服务启动控制"><a href="#3-3-2-2-服务启动控制" class="headerlink" title="3.3.2.2 服务启动控制"></a>3.3.2.2 服务启动控制</h4><ul><li>ExecStart，当前服务的启动命令</li><li>ExecStartPre，当前服务启动之前执行的命令</li><li>ExecStartPost，当前服务启动之后执行的命令</li><li>ExecReload，当前服务重启时执行的命令</li><li>ExecStop，当前服务停止时执行的命令</li><li>ExecStopPost，当前服务停止之后执行的命令</li><li>RestartSec，当前服务退出时自动重启间隔的秒数</li><li>Restart，当前服务退出时自动重启的模式，no，默认值，退出后不自动重启；on-success，正常退出即退出码为0时自动重启；on-failure，非正常退出即退出码不为0时自动重启，包括被信号终止或超时，建议守护进程设为此值；on-abnormal，被信号终止或超时才会自动重启；on-abort，收到没有捕捉到终止信号时才会自动重启；on-watchdog，超时退出时才会自动重启；always，无论是什么原因总是重启</li><li>TimeoutStartSec，启动服务等待时长，单位为秒，超时则将判断为启动失败，特别对于Docker容器而，由于第一次运行时可能需要下载镜像，很容易被误判为启动失败杀死，可设置为0，即关闭超时检测</li><li>TimeoutStopSec，停止服务时的等待秒数，超时则将通过SIGKILL信号强行终止服务进程</li><li>KillMode，当前服务停止的方式，control-group，默认值，表示当前控制组所有子进程都被kill；process，主进程被kill；mixed，主进程收到SIGTERM信号，子进程收到SIGKILL信号；none，没有进程会被杀掉，只是执行服务的stop命令</li></ul><h4 id="3-3-2-3-上下文配置"><a href="#3-3-2-3-上下文配置" class="headerlink" title="3.3.2.3 上下文配置"></a>3.3.2.3 上下文配置</h4><ul><li>Environment，设置服务的环境变量</li><li>EnvironmentFile，指定加载一个包含服务所需的环境变量的列表的文件，文件中的每一行都是一个环境变量的定义，该文件内部的key&#x3D;value键值对，可以用$key的形式，在当前配置文件中获取</li><li>Nice，设置服务的进程优先级，值越小优先级越高，默认为0，-20为最高优先级，19为最低优先级</li><li>WorkingDirectory，设置服务的工作目录</li><li>RootDirectory，指定服务进程的根目录，服务将无法访问指定目录以外的任何文件</li><li>User，设置运行服务的用户</li><li>Group，设置运行服务的用户组</li><li>MountFlags，设置服务的Mount Namespace，影响进程上下文中挂载点的信息，即服务是否会继承主机上已有挂载点，以及如果服务运行执行了挂载或卸载设备的操作，是否会真实地在主机上产生效果，shared，服务与主机共用一个Mount Namespace，继承主机挂载点，且服务挂载或卸载设备会真实地反映到主机上；slave，服务使用独立的Mount Namespace，能继承主机挂载点，但对挂载点的操作只有在自己的Namespace内生效，不会反映到主机上；private，服务使用独立的Mount Namespace，启动时没有任何任何挂载点，对挂载点的操作也不会反映到主机上</li><li>LimitCPU&#x2F;LimitSTACK&#x2F;LimitNOFILE&#x2F;LimitNPROC，限制服务占用的系统资源，如CPU、程序堆栈、文件句柄数量、子进程数量等</li></ul><h4 id="3-3-2-4-输出日志控制"><a href="#3-3-2-4-输出日志控制" class="headerlink" title="3.3.2.4 输出日志控制"></a>3.3.2.4 输出日志控制</h4><p>Systemd输出日志到journal，不指定则默认syslog</p><ul><li>StandardError&#x3D;journal</li><li>StandardOutput&#x3D;journal</li><li>StandardInput&#x3D;null</li></ul><h4 id="3-3-2-5-文件占位符"><a href="#3-3-2-5-文件占位符" class="headerlink" title="3.3.2.5 文件占位符"></a>3.3.2.5 文件占位符</h4><p>Unit文件可能会需要使用到一些与运行环境有关的信息，如节点ID、运行服务的用户等，这些信息由占位符表示，实际运行被动态地替换实际的值</p><ul><li>%n，完整的Unit文件名字，包括.service后缀名</li><li>%p，Unit模板文件名中@符号之前的部分，不包括符号@</li><li>%i，Unit模板文件名中@符号之后的部分，不包括符号@和.service后缀名</li><li>%t，存放系统运行文件的目录，通常为run</li><li>%u，运行服务的用户，不指定则默认为root</li><li>%U，运行服务的用户ID</li><li>%h，运行服务的用户家目录，即%{HOME}环境变量的值</li><li>%s，运行服务的用户默认Shell类型，即%{SHELL}环境变量的值</li><li>%m，实际运行节点的机器ID，用于标识服务的运行位置</li><li>%b，Boot ID，随机数，每个节点各不相同，并且每次节点重启时都会改变</li><li>%H，实际运行节点的主机名</li><li>%v，系统操作内核版本，即uname -r命令的输出</li><li>%%，Unit模板文件中表示一个普通的百分号</li></ul><h2 id="3-4-Install"><a href="#3-4-Install" class="headerlink" title="3.4 Install"></a>3.4 Install</h2><p>Install部分配置的目标模块通常是特定运行目标的.target 文件，使得服务开机自启</p><ul><li>WantedBy，类似于Unit部分的Wants，表示依赖当前Unit的模块，其值为一个或多个Target，当前Unit激活时（enable）符号链接将会被放入&#x2F;etc&#x2F;systemd&#x2F;system目录，并以&lt;Target 名&gt; + .wants后缀构成子目录，如&#x2F;etc&#x2F;systemd&#x2F;system&#x2F;multi-user.target.wants&#x2F;</li><li>RequiredBy，类似于Unit部分的WantedBy，当前Unit激活时，符号链接将会被放入&#x2F;etc&#x2F;systemd&#x2F;system，并以&lt;Target 名&gt; + .required后缀构成子目录</li><li>Also，当前Unit enable&#x2F;disable时，同时enable&#x2F;disable的其他Unit</li><li>Alias，当前Unit可用于启动的别名</li></ul><h2 id="3-5-单元状态"><a href="#3-5-单元状态" class="headerlink" title="3.5 单元状态"></a>3.5 单元状态</h2><ul><li>active (running)，正在运行</li><li>active (exited)，已完成运行并退出</li><li>inactive (dead)，Unit已经停止运行</li><li>activating (start)，正在启动</li><li>activating (auto-restart)，正在自动重启</li><li>deactivating (stop)，正在停止</li><li>deactivating (restart)，正在重启</li><li>failed，启动失败，或者在运行过程中突然停止</li></ul><h1 id="4-服务生命周期"><a href="#4-服务生命周期" class="headerlink" title="4.服务生命周期"></a>4.服务生命周期</h1><ul><li>加载阶段，Systemd加载服务的Unit文件，并根据配置启动或禁止服务</li><li>准备阶段，Systemd准备服务的启动环境，包括设置环境变量、创建所需目录、检查依赖关系等操作</li><li>启动阶段，Systemd启动服务，执行服务的启动命令或程序，并记录服务的PID</li><li>运行阶段，服务正常运行期间，Systemd监视服务的状态，并记录日志、处理信号等操作</li><li>停止阶段，服务需要停止时，Systemd发送stop信号给服务进程，并等待服务进程退出</li><li>停止后处理阶段，服务进程退出后，Systemd执行服务的停止后处理命令或程序，并记录服务的退出状态</li><li>卸载阶段，服务不再需要时，Systemd卸载服务的Unit文件，并清理相关的运行时文件和日志</li></ul><h1 id="5-配置实例"><a href="#5-配置实例" class="headerlink" title="5.配置实例"></a>5.配置实例</h1><pre><code class="hljs">sudo vi /lib/systemd/system/docker.service[Unit]Description=Docker Application Container EngineDocumentation=https://docs.docker.comBindsTo=containerd.serviceAfter=network-online.target firewalld.service containerd.serviceWants=network-online.targetRequires=docker.socket[Service]Type=notifyExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sockExecReload=/bin/kill -s HUP $MAINPIDTimeoutSec=0RestartSec=2Restart=alwaysStartLimitBurst=3StartLimitInterval=60sLimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinityTasksMax=infinityDelegate=yesKillMode=process[Install]WantedBy=multi-user.target</code></pre><h1 id="6-相关命令"><a href="#6-相关命令" class="headerlink" title="6.相关命令"></a>6.相关命令</h1><h2 id="6-1-状态查询"><a href="#6-1-状态查询" class="headerlink" title="6.1 状态查询"></a>6.1 状态查询</h2><pre><code class="hljs"># Systemd系统状态systemctl status# Unit状态sysystemctl status httpd.service# 远程主机Unit状态systemctl -H root@hostname.example.com status httpd.service# Unit是否正在运行systemctl is-active httpd.service# Unit是否处于启动失败状态systemctl is-failed httpd.service# Unit是否开机自启systemctl is-enabled httpd.service# 所有启动的Unitsystemctl list-units# 查询系统默认的targetsystemctl get-default# 设置默认Target sudo systemctl set-default multi-user.target# 切换Target默认不会关闭原来Target启动的进程，改命令即可达到这个效果sudo systemctl isolate multi-user.target# 查看target类型systemctl list-unit-files --type=target# target包含哪些unitsystemctl list-dependencies multi-user.target# 查看配置文件systemctl cat multi-user.target</code></pre><h1 id="6-2-服务管理"><a href="#6-2-服务管理" class="headerlink" title="6.2 服务管理"></a>6.2 服务管理</h1><pre><code class="hljs"># 启动服务sudo systemctl start apache.service# 停止服务sudo systemctl stop apache.service# 重启服务sudo systemctl restart apache.service# kill服务的所有子进程sudo systemctl kill apache.service# 重载服务配置文件sudo systemctl reload apache.service# 重载所有修改过的配置文件sudo systemctl daemon-reload# 设置开机自启，将在/etc/systemd/system/建立服务的符号链接，并指向/usr/lib/systemd/system/systemctl enable apache.service# 取消开机自启systemctl disable apache.service# 查看Unit的所有底层参数systemctl show httpd.service# 显示Unit指定属性的值systemctl show -p CPUShares httpd.service# 设置Unit的指定属性sudo systemctl set-property httpd.service CPUShares=500</code></pre><h1 id="6-3-日志管理"><a href="#6-3-日志管理" class="headerlink" title="6.3 日志管理"></a>6.3 日志管理</h1><pre><code class="hljs"># 查看sshd服务日志journalctl -u sshd# 查看sshd服务日志，并直接跳转到日志末尾journalctl -eu sshd# 查看sshd服务日志，并直接跳转到日志末尾，同时打印可用的服务描述journalctl -xeu sshd# 以实时更新的方式查看sshd服务日志journalctl -u sshd -f# 读取journal文件内容，一般存放于/run/log/journal/ journalctl --file $&#123;FILENAME&#125;</code></pre><h1 id="6-4-系统时间管理"><a href="#6-4-系统时间管理" class="headerlink" title="6.4 系统时间管理"></a>6.4 系统时间管理</h1><p>timedatectl用于管理系统时间</p><pre><code class="hljs"># 列出所有时区timedatectl list-timezones# 修改系统时区为Asia/Shanghaitimedatectl set-timezone Asia/Shanghai# 修改系统时间timedatectl set-time &quot;2022-2-20 12:00:00&quot;</code></pre><h1 id="6-5-主机信息管理"><a href="#6-5-主机信息管理" class="headerlink" title="6.5 主机信息管理"></a>6.5 主机信息管理</h1><p>hostnamectl用于管理当前主机的信息</p><pre><code class="hljs"># 显示主机信息，如主机名、主机类型、虚拟化技术、CPU架构、内核版本、操作系统等hostnamectl# 设置主机名，通过su -命令重新加载终端或重新登录系统即可生效hostnamectl set-hostname hostname.example.com</code></pre><h1 id="6-6-本地化设置管理"><a href="#6-6-本地化设置管理" class="headerlink" title="6.6 本地化设置管理"></a>6.6 本地化设置管理</h1><p>localectl用于查看本地化设置</p><pre><code class="hljs"># 显示本地化配置localectl# 设置本地语言，配置为英文localectl set-locale LANG=en_GB.utf8</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://juejin.cn/post/6992390303814516749">https://juejin.cn/post/6992390303814516749</a></li><li><a href="https://blog.csdn.net/qiqi_6666/article/details/131688840">https://blog.csdn.net/qiqi_6666/article/details/131688840</a></li><li><a href="https://blog.csdn.net/yuelai_217/article/details/130949299">https://blog.csdn.net/yuelai_217/article/details/130949299</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>操作系统</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MySQL数据库通过二进制日志恢复数据</title>
    <link href="/linux/MySQLBinLog/"/>
    <url>/linux/MySQLBinLog/</url>
    
    <content type="html"><![CDATA[<hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/qq_48391148/article/details/126311002">https://blog.csdn.net/qq_48391148/article/details/126311002</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>MySQL</tag>
      
      <tag>数据库</tag>
      
      <tag>数据灾备</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Openstack集群网络配置</title>
    <link href="/linux/OpenstackNetwork/"/>
    <url>/linux/OpenstackNetwork/</url>
    
    <content type="html"><![CDATA[<h1 id="1-配置外部网络"><a href="#1-配置外部网络" class="headerlink" title="1.配置外部网络"></a>1.配置外部网络</h1><h2 id="1-1-查看物理网络配置"><a href="#1-1-查看物理网络配置" class="headerlink" title="1.1 查看物理网络配置"></a>1.1 查看物理网络配置</h2><pre><code class="hljs">cat /etc/neutron/plugins/ml2/ml2_conf.ini[ml2]type_drivers = flat,vlan,vxlantenant_network_types = vxlanmechanism_drivers = linuxbridge,l2populationextension_drivers = port_security[ml2_type_flat]flat_networks = provider</code></pre><h2 id="1-2-创建外部网络"><a href="#1-2-创建外部网络" class="headerlink" title="1.2 创建外部网络"></a>1.2 创建外部网络</h2><p><img src="/img/wiki/openstack/openstack001.jpg" alt="openstack001"></p><p><img src="/img/wiki/openstack/openstack002.jpg" alt="openstack002"></p><p><img src="/img/wiki/openstack/openstack003.jpg" alt="openstack003"></p><h1 id="2-配置内部网络"><a href="#2-配置内部网络" class="headerlink" title="2.配置内部网络"></a>2.配置内部网络</h1><p><img src="/img/wiki/openstack/openstack004.jpg" alt="openstack004"></p><p><img src="/img/wiki/openstack/openstack005.jpg" alt="openstack005"></p><p><img src="/img/wiki/openstack/openstack006.jpg" alt="openstack006"></p><h1 id="3-配置路由器"><a href="#3-配置路由器" class="headerlink" title="3.配置路由器"></a>3.配置路由器</h1><p><img src="/img/wiki/openstack/openstack007.jpg" alt="openstack007"></p><p><img src="/img/wiki/openstack/openstack008.jpg" alt="openstack008"></p><p><img src="/img/wiki/openstack/openstack009.jpg" alt="openstack009"></p><h1 id="4-验证网络配置"><a href="#4-验证网络配置" class="headerlink" title="4.验证网络配置"></a>4.验证网络配置</h1><p><img src="/img/wiki/openstack/openstack010.jpg" alt="openstack010"></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/m0_57464618/article/details/129899280">https://blog.csdn.net/m0_57464618/article/details/129899280</a></li><li><a href="https://blog.csdn.net/qq_36073886/article/details/130849987">https://blog.csdn.net/qq_36073886/article/details/130849987</a></li><li><a href="https://blog.csdn.net/weixin_51202460/article/details/123985315">https://blog.csdn.net/weixin_51202460/article/details/123985315</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>网络</tag>
      
      <tag>虚拟化</tag>
      
      <tag>私有云</tag>
      
      <tag>Openstack</tag>
      
      <tag>公有云</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MySQL数据库性能调优</title>
    <link href="/linux/MySQLOptimization/"/>
    <url>/linux/MySQLOptimization/</url>
    
    <content type="html"><![CDATA[<p>MySQL数据库管理系统以其高效稳定的性能而著称，广泛应用于许多企业和应用程序。但实际的开发和运维过程中，也经常会遇到一些性能问题，如查询慢、请求堆积等，此时就需要对其进行优化来解决。调优的主要目的是找出系统的瓶颈，提高MySQL数据库的整体性能。此外，还可能需要对结构设计和参数进行调整，以提高用户的相应速度，同时尽可能的节约系统资源，以便让系统提供更大的负荷，从而更好地满足业务需求。MySQL调优是一个复杂的过程，涉及多个方面，并需要从多个角度进行衡量，包括硬件配置、参数配置、数据库结构、SQL优化等</p><h1 id="1-性能瓶颈"><a href="#1-性能瓶颈" class="headerlink" title="1.性能瓶颈"></a>1.性能瓶颈</h1><p>进行优化配置之前，需要先了解MySQL的性能瓶颈，从而有针对性地进行优化，主要包括以下几个方面：</p><h2 id="1-1-磁盘IO"><a href="#1-1-磁盘IO" class="headerlink" title="1.1 磁盘IO"></a>1.1 磁盘IO</h2><p>由于磁盘是MySQL的主要数据存储介质，因此磁盘IO是MySQL性能的瓶颈之一，主要受到磁盘类型、容量、转速、缓存等多种因素的影响</p><h2 id="1-2-内存占用"><a href="#1-2-内存占用" class="headerlink" title="1.2 内存占用"></a>1.2 内存占用</h2><p>MySQL内存不足可能会导致频繁的IO操作和性能下降，如缓存、临时表、排序等，进而影响查询效率和性能</p><h2 id="1-3-CPU"><a href="#1-3-CPU" class="headerlink" title="1.3 CPU"></a>1.3 CPU</h2><p>MySQL数据库服务器若是CPU负载过高，将会导致系统响应缓慢和请求堆积</p><h2 id="1-4-网络延迟"><a href="#1-4-网络延迟" class="headerlink" title="1.4 网络延迟"></a>1.4 网络延迟</h2><p>MySQL通常运行于分布式环境，网络延迟将会影响响应速度和并发处理能力</p><h2 id="1-5-SQL语句"><a href="#1-5-SQL语句" class="headerlink" title="1.5 SQL语句"></a>1.5 SQL语句</h2><p>SQL语句的效率直接影响MySQL的性能和查询速度，不合理的SQL语句可能导致索引失效、查询锁表等问题，进而降低MySQL的性能</p><hr><ul><li><a href="https://blog.csdn.net/u012581020/article/details/130686557">https://blog.csdn.net/u012581020/article/details/130686557</a></li><li><a href="https://blog.csdn.net/qq_40991313/article/details/131059110">https://blog.csdn.net/qq_40991313/article/details/131059110</a></li><li><a href="https://blog.csdn.net/lijuncheng963375877/article/details/120032348">https://blog.csdn.net/lijuncheng963375877/article/details/120032348</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>MySQL</tag>
      
      <tag>性能优化</tag>
      
      <tag>数据库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Openstack集群部署</title>
    <link href="/linux/Openstack/"/>
    <url>/linux/Openstack/</url>
    
    <content type="html"><![CDATA[<p>OpenStack，NASA（美国国家航空航天局）和Rackspace合作开发的开源云计算管理平台项目，一套通过各种互补服务组件提供公有云和私有云基础设施即服务（IaaS）的解决方案，提供实施简单、可大规模扩展、丰富、标准统一的弹性云计算服务。OpenStack并不是一个软件，而是一系列开源项目的集合</p><p>Openstack自诞生之日起，其发展历程即充满了挑战与机遇，最终以其开放的架构、强大的兼容性与扩展性、模块松耦合设计，现已发展成为仅次于Linux的开源项目，其社区拥有180多个国家的677家企业87426名会员，全球一半以上的500强企业都采用OpenStack进行云计算的构建，当之无愧的云计算的引领者</p><h3 id="体系架构"><a href="#体系架构" class="headerlink" title="体系架构"></a>体系架构</h3><p>OpenStack集群是典型的C&#x2F;S架构，由控制节点、计算节点、网络节点和存储节点组成</p><h4 id="1-控制节点"><a href="#1-控制节点" class="headerlink" title="1.控制节点"></a>1.控制节点</h4><p>Controller Node，控制节点，作为集群管理与调度中心，通过API服务(API Service)与其他服务组件之间进行通信和交互，从而完成云资源的管理与协调，即接收用户请求、管理资源分配、监控系统运行等，如虚拟机实例管理、网络及存储分配，为整个集群提供了基础管理服务（核心服务，如认证服务、镜像服务、计算服务、网络服务及UI等）、管理支持服务（底层服务，如数据库服务、消息队列服务和缓存服务等）和扩展管理服务（可选服务，如对象存储服务、计费服务和性能监控服务等）</p><h4 id="2-计算节点"><a href="#2-计算节点" class="headerlink" title="2.计算节点"></a>2.计算节点</h4><p>Compute Node，计算节点，作为集群计算资源的提供者，通过API接收控制节点的发送来的请求并执行相应的操作，如运行虚拟机实例、处理计算任务等</p><h4 id="3-网络节点"><a href="#3-网络节点" class="headerlink" title="3.网络节点"></a>3.网络节点</h4><p>Networking Node，网络节点，作为集群网络资源的提供者，负责虚拟机间的通信、网络隔离、网络安全等，如虚拟网络、子网、路由的管理与配置</p><h4 id="4-存储节点"><a href="#4-存储节点" class="headerlink" title="4.存储节点"></a>4.存储节点</h4><p>Storage Node，存储节点，作为集群存储资源的提供者，负责存储数据和镜像，即通过API接收控制节点发送来的存储请求并将数据存储在相应的存储后端，支持多种存储后端，如Cinder(块存储服务)、Swift(对象存储服务)等</p><h3 id="功能组件"><a href="#功能组件" class="headerlink" title="功能组件"></a>功能组件</h3><h4 id="1-Nova"><a href="#1-Nova" class="headerlink" title="1.Nova"></a>1.Nova</h4><p>计算模块，负责实例生命周期的管理，计算资源的单位，提供云计算、虚拟化服务，其本身并不支持虚拟化，而是通过管理、调度底层的虚拟化完成对Hypervisor的屏蔽，支持多种虚拟化技术，如KVM、XEN等，支持横向扩展</p><h4 id="2-Neutron"><a href="#2-Neutron" class="headerlink" title="2.Neutron"></a>2.Neutron</h4><p>网络模块，为虚拟机、计算和控制节点提供网络功能，负责虚拟网络的管理，为实例创建网络的拓扑结构，是面向租户的网络管理，可以自己定义自己的网络，各个租户之间互不影响</p><h4 id="3-Keystone"><a href="#3-Keystone" class="headerlink" title="3.Keystone"></a>3.Keystone</h4><p>认证模块，为所有用户、租户和角色及其余的服务组件进行认证与授权，提供身份认证服务，类似于LDAP服务，且支持多认证机制，即相当一个CA机构，提供多种方式进行身份认证，如密钥，哈希等</p><h4 id="4-Glance"><a href="#4-Glance" class="headerlink" title="4.Glance"></a>4.Glance</h4><p>镜像模块，提供虚拟机镜像模板的注册与管理，将做好的操作系统拷贝为镜像模板，创建虚拟机时可直接使用，并提供镜像的上传、下载功能，支持多种格式的镜像</p><h4 id="5-Horizon"><a href="#5-Horizon" class="headerlink" title="5.Horizon"></a>5.Horizon</h4><p>图形化用户管理模块，通过web控制台界面与OpenStack底层服务进行交互</p><h4 id="5-Cinder"><a href="#5-Cinder" class="headerlink" title="5.Cinder"></a>5.Cinder</h4><p>块存储模块，负责为运行的虚拟机实例提供持久的块存储设备，即为虚拟机提供磁盘空间，扩展方便，按需付费，支持多种后端存储</p><h4 id="6-Swift"><a href="#6-Swift" class="headerlink" title="6.Swift"></a>6.Swift</h4><p>对象存储模块，为OpenStack提供基于云的弹性存储，支持集群无单点故障</p><h4 id="7-Heat"><a href="#7-Heat" class="headerlink" title="7.Heat"></a>7.Heat</h4><p>编排模块，提供编排服务或功能。使用 Heat 管理平台可以轻松地将虚拟机作为堆栈，并且根据需要可以将虚拟机扩展或收缩</p><h4 id="8-Ceilometer"><a href="#8-Ceilometer" class="headerlink" title="8.Ceilometer"></a>8.Ceilometer</h4><p>计量模块，提供计量与监控功能，用于度量、监控和控制数据资源的集中来源，汇报数据，为OpenStack用户提供记账途径</p><h4 id="非核心组件"><a href="#非核心组件" class="headerlink" title="非核心组件"></a>非核心组件</h4><p>基础设施组件</p><ul><li>Collectd、Telegraf、InfluxDB、Prometheus和Grafana，用于性能监控</li><li>OpenSearch和OpenSearch仪表盘，用于搜索、分析和可视化日志信息</li><li>Etcd，分布式的可靠键值存储数据库</li><li>Fluentd，数据收集器，用于统一的日志层</li><li>Gnocchi，时间序列存储数据库</li><li>HAProxy和Keepalived，用于服务及其端点的高可用</li><li>MariaDB和Galera Cluster，用于高可用的MySQL数据库</li><li>Memcached，分布式内存对象缓存系统</li><li>Open vSwitch，Neutron一起使用的</li><li>RabbitMQ，作为服务间通信的消息传递后端</li><li>Redis，内存数据结构存储</li></ul><p>容器化组件  </p><ul><li>zun，集成支持容器，创建和管理Docker</li></ul><h3 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h3><h4 id="1-认证鉴权"><a href="#1-认证鉴权" class="headerlink" title="1.认证鉴权"></a>1.认证鉴权</h4><p>用户登录界面dashboard或以命令行CLI通过RESTful API向keystone获取认证信息，keystone通过用户请求认证信息，并生成auth-token返回给对应的认证请求，完成用户认证授权流程</p><h4 id="2-创建实例请求"><a href="#2-创建实例请求" class="headerlink" title="2.创建实例请求"></a>2.创建实例请求</h4><p>通过Horizon或Resetful API向nova-api发送创建虚拟机服务（boot instance）的请求，然后由nova-api向keystone发送认证请求，验证有效的用户及token，验证通过后与数据库通讯，并初始化新建虚拟机实例的数据库记录，同时调用rabbitmq，向nova-scheduler请求创建虚拟机的资源</p><h4 id="3-实例调度"><a href="#3-实例调度" class="headerlink" title="3.实例调度"></a>3.实例调度</h4><p>nova-scheduler进程监听rabbitmq消息队列，获取到nova-api发来的创建虚拟机实例的请求，之后通过查询nova数据库计算资源的情况，并由调度算法计算符合虚拟机创建需要的主机。对于有符合虚拟机创建的主机，nova-scheduler更新数据库中虚拟机对应的物理主机信息，并基于rpc调用rabbitmq，向nova-compute发送对应的创建虚拟机请求的消息。nova-compute通过rpc调用向nova-conductor请求获取虚拟机消息（确认虚拟机实例的规格，即Flavor），</p><p>虚拟机实例采用Glance提供镜像服务，然后使用Neutron为新建的虚拟机分配IP地址,并将其纳入虚拟网络中，之后在通过Cinder创建的卷为虚拟机挂载存储块,整个过程都在Ceilometer模块资源的监控下，Cinder产生的卷(Volume)和Glance提供的镜像(Image)可以通过Swift的对象存储机制进行保存</p><h1 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h1><ul><li>172.100.100.180&#x2F;192.168.100.180 controller</li><li>172.100.100.181 compute001</li><li>172.100.100.182 compute002</li></ul><h1 id="1-配置系统环境"><a href="#1-配置系统环境" class="headerlink" title="1.配置系统环境"></a>1.配置系统环境</h1><h2 id="1-1-配置hosts"><a href="#1-1-配置hosts" class="headerlink" title="1.1 配置hosts"></a>1.1 配置hosts</h2><pre><code class="hljs">vi /etc/hosts172.100.100.180 controller172.100.100.181 compute001172.100.100.182 compute002</code></pre><h2 id="1-2-关闭防火墙"><a href="#1-2-关闭防火墙" class="headerlink" title="1.2 关闭防火墙"></a>1.2 关闭防火墙</h2><h2 id="1-3-关闭selinux"><a href="#1-3-关闭selinux" class="headerlink" title="1.3 关闭selinux"></a>1.3 关闭selinux</h2><h2 id="1-4-配置免密登录"><a href="#1-4-配置免密登录" class="headerlink" title="1.4 配置免密登录"></a>1.4 配置免密登录</h2><h1 id="2-安装基础服务"><a href="#2-安装基础服务" class="headerlink" title="2.安装基础服务"></a>2.安装基础服务</h1><h2 id="2-1-安装时间同步服务"><a href="#2-1-安装时间同步服务" class="headerlink" title="2.1 安装时间同步服务"></a>2.1 安装时间同步服务</h2><pre><code class="hljs">yum install -y chrony</code></pre><h3 id="2-1-1-配置控制节点，允许其他节点连接"><a href="#2-1-1-配置控制节点，允许其他节点连接" class="headerlink" title="2.1.1 配置控制节点，允许其他节点连接"></a>2.1.1 配置控制节点，允许其他节点连接</h3><pre><code class="hljs">echo &#39;allow 172.100.100.0/24&#39; &gt;&gt; /etc/chrony.conf</code></pre><h3 id="2-1-2-配置计算节点，连接控制节点的chrony服务器"><a href="#2-1-2-配置计算节点，连接控制节点的chrony服务器" class="headerlink" title="2.1.2 配置计算节点，连接控制节点的chrony服务器"></a>2.1.2 配置计算节点，连接控制节点的chrony服务器</h3><pre><code class="hljs">sed -i &#39;/^server/d&#39; /etc/chrony.conf sed -i &#39;2aserver controller iburst&#39; /etc/chrony.conf</code></pre><h3 id="2-1-3-启动chronyd"><a href="#2-1-3-启动chronyd" class="headerlink" title="2.1.3 启动chronyd"></a>2.1.3 启动chronyd</h3><pre><code class="hljs">systemctl start chronyd.servicesystemctl enable chronyd.service</code></pre><h2 id="2-2-安装OpenStack库"><a href="#2-2-安装OpenStack库" class="headerlink" title="2.2 安装OpenStack库"></a>2.2 安装OpenStack库</h2><pre><code class="hljs">yum install -y openstack-release-wallabyyum clean all &amp;&amp; yum makecacheadd-apt-repository cloud-archive:wallabyapt update &amp;&amp; apt upgrade -y</code></pre><h2 id="2-3-控制节点安装OpenStack客户端"><a href="#2-3-控制节点安装OpenStack客户端" class="headerlink" title="2.3 控制节点安装OpenStack客户端"></a>2.3 控制节点安装OpenStack客户端</h2><pre><code class="hljs">yum install -y python3-openstackclientapt install -y python3-openstackclient</code></pre><h2 id="2-4-控制节点安装MariaDB"><a href="#2-4-控制节点安装MariaDB" class="headerlink" title="2.4 控制节点安装MariaDB"></a>2.4 控制节点安装MariaDB</h2><pre><code class="hljs">yum install -y mariadb mariadb-server python3-PyMySQLapt install -y mariadb-server python3-pymysql</code></pre><h3 id="2-4-1-修改配置文件"><a href="#2-4-1-修改配置文件" class="headerlink" title="2.4.1 修改配置文件"></a>2.4.1 修改配置文件</h3><pre><code class="hljs">vi /etc/my.cnf.d/openstack.cnf[mysqld]default-storage-engine = innodbinnodb_file_per_table = onmax_connections = 4096collation-server = utf8_general_cicharacter-set-server = utf8</code></pre><h3 id="2-4-2-启动MariaDB"><a href="#2-4-2-启动MariaDB" class="headerlink" title="2.4.2 启动MariaDB"></a>2.4.2 启动MariaDB</h3><pre><code class="hljs">systemctl start mariadb.servicesystemctl enable mariadb.service</code></pre><h3 id="2-4-3-进行数据库安全加固，设置root密码"><a href="#2-4-3-进行数据库安全加固，设置root密码" class="headerlink" title="2.4.3 进行数据库安全加固，设置root密码"></a>2.4.3 进行数据库安全加固，设置root密码</h3><pre><code class="hljs">mysql_secure_installation</code></pre><h2 id="2-5-控制节点安装消息队列RabbitMQ"><a href="#2-5-控制节点安装消息队列RabbitMQ" class="headerlink" title="2.5 控制节点安装消息队列RabbitMQ"></a>2.5 控制节点安装消息队列RabbitMQ</h2><pre><code class="hljs">yum install -y rabbitmq-serverapt install -y rabbitmq-server</code></pre><h3 id="2-5-1-启动RabbitMQ"><a href="#2-5-1-启动RabbitMQ" class="headerlink" title="2.5.1 启动RabbitMQ"></a>2.5.1 启动RabbitMQ</h3><pre><code class="hljs">systemctl start rabbitmq-server.servicesystemctl enable rabbitmq-server.service</code></pre><h3 id="2-5-2-创建用户"><a href="#2-5-2-创建用户" class="headerlink" title="2.5.2 创建用户"></a>2.5.2 创建用户</h3><pre><code class="hljs">rabbitmqctl add_user openstack Openstack_2024</code></pre><h3 id="2-5-3-用户授权"><a href="#2-5-3-用户授权" class="headerlink" title="2.5.3 用户授权"></a>2.5.3 用户授权</h3><pre><code class="hljs">rabbitmqctl set_permissions openstack &quot;.*&quot; &quot;.*&quot; &quot;.*&quot; </code></pre><h2 id="2-6-控制节点安装缓存服务器Memcached"><a href="#2-6-控制节点安装缓存服务器Memcached" class="headerlink" title="2.6 控制节点安装缓存服务器Memcached"></a>2.6 控制节点安装缓存服务器Memcached</h2><pre><code class="hljs">yum install -y memcached python3-memcachedapt install -y memcached python3-memcache</code></pre><h3 id="2-6-1-修改监听ip"><a href="#2-6-1-修改监听ip" class="headerlink" title="2.6.1 修改监听ip"></a>2.6.1 修改监听ip</h3><pre><code class="hljs">sed -i &#39;s/127.0.0.1/0.0.0.0/&#39; /etc/sysconfig/memcached </code></pre><h3 id="2-6-2-启动Memcache"><a href="#2-6-2-启动Memcache" class="headerlink" title="2.6.2 启动Memcache"></a>2.6.2 启动Memcache</h3><pre><code class="hljs">systemctl start memcached.servicesystemctl enable memcached.service</code></pre><h1 id="3-控制节点安装身份认证服务Keystone"><a href="#3-控制节点安装身份认证服务Keystone" class="headerlink" title="3.控制节点安装身份认证服务Keystone"></a>3.控制节点安装身份认证服务Keystone</h1><pre><code class="hljs">yum install -y openstack-keystone httpd mod_wsgiapt install -y keystone</code></pre><h2 id="3-1-创建keystone数据库"><a href="#3-1-创建keystone数据库" class="headerlink" title="3.1 创建keystone数据库"></a>3.1 创建keystone数据库</h2><pre><code class="hljs">mysql -u root -pMariaDB [(none)]&gt; create database keystone;MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON keystone.* TO &#39;keystone&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;Openstack_2024&#39;;MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON keystone.* TO &#39;keystone&#39;@&#39;%&#39; IDENTIFIED BY &#39;Openstack_2024&#39;;MariaDB [(none)]&gt; flush privileges;</code></pre><h2 id="3-2-配置身份认证服务"><a href="#3-2-配置身份认证服务" class="headerlink" title="3.2 配置身份认证服务"></a>3.2 配置身份认证服务</h2><h3 id="3-2-1-备份原配置文件"><a href="#3-2-1-备份原配置文件" class="headerlink" title="3.2.1 备份原配置文件"></a>3.2.1 备份原配置文件</h3><pre><code class="hljs">sed -i.default -e &#39;/^#/d&#39; -e &#39;/^$/d&#39; /etc/keystone/keystone.conf  </code></pre><h3 id="3-2-2-修改配置文件"><a href="#3-2-2-修改配置文件" class="headerlink" title="3.2.2 修改配置文件"></a>3.2.2 修改配置文件</h3><pre><code class="hljs">vi /etc/keystone/keystone.conf[database]connection = mysql+pymysql://keystone:Openstack_2024@controller/keystone[token]provider = fernet</code></pre><h2 id="3-3-同步数据，初始化身份认证服务数据库"><a href="#3-3-同步数据，初始化身份认证服务数据库" class="headerlink" title="3.3 同步数据，初始化身份认证服务数据库"></a>3.3 同步数据，初始化身份认证服务数据库</h2><pre><code class="hljs">su -s /bin/sh -c &quot;keystone-manage db_sync&quot; keystone</code></pre><h2 id="3-4-初始化Fernet密钥存储库"><a href="#3-4-初始化Fernet密钥存储库" class="headerlink" title="3.4 初始化Fernet密钥存储库"></a>3.4 初始化Fernet密钥存储库</h2><pre><code class="hljs">keystone-manage fernet_setup --keystone-user keystone --keystone-group keystonekeystone-manage credential_setup --keystone-user keystone --keystone-group keystone</code></pre><h2 id="3-5-创建keystone管理员，引导身份认证"><a href="#3-5-创建keystone管理员，引导身份认证" class="headerlink" title="3.5 创建keystone管理员，引导身份认证"></a>3.5 创建keystone管理员，引导身份认证</h2><pre><code class="hljs">keystone-manage bootstrap --bootstrap-password Openstack_2024 \  --bootstrap-admin-url http://controller:5000/v3/ \  --bootstrap-internal-url http://controller:5000/v3/ \  --bootstrap-public-url http://controller:5000/v3/ \  --bootstrap-region-id RegionOne</code></pre><h2 id="3-6-配置Apache-http服务器"><a href="#3-6-配置Apache-http服务器" class="headerlink" title="3.6 配置Apache http服务器"></a>3.6 配置Apache http服务器</h2><h3 id="3-6-1-配置ServerName"><a href="#3-6-1-配置ServerName" class="headerlink" title="3.6.1 配置ServerName"></a>3.6.1 配置ServerName</h3><pre><code class="hljs">sed -i &#39;/#ServerName/aServerName controller:80&#39; /etc/httpd/conf/httpd.conf </code></pre><h3 id="3-6-2-创建keystone配置文件"><a href="#3-6-2-创建keystone配置文件" class="headerlink" title="3.6.2 创建keystone配置文件"></a>3.6.2 创建keystone配置文件</h3><pre><code class="hljs">ln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/</code></pre><h3 id="3-6-3-启动Apache"><a href="#3-6-3-启动Apache" class="headerlink" title="3.6.3 启动Apache"></a>3.6.3 启动Apache</h3><pre><code class="hljs">systemctl start httpd.servicesystemctl enable httpd.service</code></pre><h2 id="3-7-配置管理员账号环境变量"><a href="#3-7-配置管理员账号环境变量" class="headerlink" title="3.7 配置管理员账号环境变量"></a>3.7 配置管理员账号环境变量</h2><pre><code class="hljs">export OS_USERNAME=adminexport OS_PASSWORD=Openstack_2024export OS_PROJECT_NAME=adminexport OS_USER_DOMAIN_NAME=Defaultexport OS_PROJECT_DOMAIN_NAME=Defaultexport OS_AUTH_URL=http://controller:5000/v3export OS_IDENTITY_API_VERSION=3</code></pre><h2 id="3-8-创建域、项目、用户和角色"><a href="#3-8-创建域、项目、用户和角色" class="headerlink" title="3.8 创建域、项目、用户和角色"></a>3.8 创建域、项目、用户和角色</h2><h3 id="3-8-1-创建域，也可不创建，Keystone已存在一个default域"><a href="#3-8-1-创建域，也可不创建，Keystone已存在一个default域" class="headerlink" title="3.8.1 创建域，也可不创建，Keystone已存在一个default域"></a>3.8.1 创建域，也可不创建，Keystone已存在一个default域</h3><pre><code class="hljs"># openstack domain create --description &quot;An Example Domain&quot; example</code></pre><h3 id="3-8-2-创建服务项目，供glance、placement、nova和neutron等组件使用"><a href="#3-8-2-创建服务项目，供glance、placement、nova和neutron等组件使用" class="headerlink" title="3.8.2 创建服务项目，供glance、placement、nova和neutron等组件使用"></a>3.8.2 创建服务项目，供glance、placement、nova和neutron等组件使用</h3><pre><code class="hljs">openstack project create --domain default --description &quot;Service Project&quot; service</code></pre><h3 id="3-8-3-创建普通项目与用户"><a href="#3-8-3-创建普通项目与用户" class="headerlink" title="3.8.3 创建普通项目与用户"></a>3.8.3 创建普通项目与用户</h3><pre><code class="hljs"># 创建swords项目# openstack project create --domain default --description &quot;Swords Project&quot; swords# 创建sword用户# openstack user create --domain default --password Openstack_2024 sword</code></pre><h3 id="3-8-4-创建角色"><a href="#3-8-4-创建角色" class="headerlink" title="3.8.4 创建角色"></a>3.8.4 创建角色</h3><pre><code class="hljs"># openstack role create user</code></pre><h3 id="3-8-5-将用户与角色添加到项目"><a href="#3-8-5-将用户与角色添加到项目" class="headerlink" title="3.8.5 将用户与角色添加到项目"></a>3.8.5 将用户与角色添加到项目</h3><pre><code class="hljs"># openstack role add --project swords --user sword user</code></pre><h2 id="3-9-验证Keystone服务"><a href="#3-9-验证Keystone服务" class="headerlink" title="3.9 验证Keystone服务"></a>3.9 验证Keystone服务</h2><h3 id="3-9-1-删除临时环境变量"><a href="#3-9-1-删除临时环境变量" class="headerlink" title="3.9.1 删除临时环境变量"></a>3.9.1 删除临时环境变量</h3><pre><code class="hljs">unset OS_AUTH_URL OS_PASSWORD</code></pre><h3 id="3-9-2-验证admin用户"><a href="#3-9-2-验证admin用户" class="headerlink" title="3.9.2 验证admin用户"></a>3.9.2 验证admin用户</h3><pre><code class="hljs">openstack --os-auth-url http://controller:5000/v3 \  --os-project-domain-name Default --os-user-domain-name Default \  --os-project-name admin --os-username admin token issue</code></pre><h3 id="3-9-3-验证sword用户"><a href="#3-9-3-验证sword用户" class="headerlink" title="3.9.3 验证sword用户"></a>3.9.3 验证sword用户</h3><pre><code class="hljs">openstack --os-auth-url http://controller:5000/v3 \  --os-project-domain-name Default --os-user-domain-name Default \  --os-project-name swords --os-username sword token issue</code></pre><h2 id="3-10-创建openstack客户端环境变量脚本"><a href="#3-10-创建openstack客户端环境变量脚本" class="headerlink" title="3.10 创建openstack客户端环境变量脚本"></a>3.10 创建openstack客户端环境变量脚本</h2><h3 id="3-10-1-创建admin用户环境变量脚本"><a href="#3-10-1-创建admin用户环境变量脚本" class="headerlink" title="3.10.1 创建admin用户环境变量脚本"></a>3.10.1 创建admin用户环境变量脚本</h3><pre><code class="hljs">cd /root &amp;&amp; vi admin-openrcexport OS_PROJECT_DOMAIN_NAME=Defaultexport OS_USER_DOMAIN_NAME=Defaultexport OS_PROJECT_NAME=adminexport OS_USERNAME=adminexport OS_PASSWORD=Openstack_2024export OS_AUTH_URL=http://controller:5000/v3export OS_IDENTITY_API_VERSION=3export OS_IMAGE_API_VERSION=2</code></pre><h3 id="3-10-2-创建普通用户sword环境变量脚本"><a href="#3-10-2-创建普通用户sword环境变量脚本" class="headerlink" title="3.10.2 创建普通用户sword环境变量脚本"></a>3.10.2 创建普通用户sword环境变量脚本</h3><pre><code class="hljs">vi sword-openrcexport OS_PROJECT_DOMAIN_NAME=Defaultexport OS_USER_DOMAIN_NAME=Defaultexport OS_PROJECT_NAME=swordsexport OS_USERNAME=swordexport OS_PASSWORD=Openstack_2024export OS_AUTH_URL=http://controller:5000/v3export OS_IDENTITY_API_VERSION=3export OS_IMAGE_API_VERSION=2</code></pre><h3 id="3-10-3-加载环境变量"><a href="#3-10-3-加载环境变量" class="headerlink" title="3.10.3 加载环境变量"></a>3.10.3 加载环境变量</h3><pre><code class="hljs">. admin-openrc. sword-openrc</code></pre><h3 id="3-10-4-请求验证token"><a href="#3-10-4-请求验证token" class="headerlink" title="3.10.4 请求验证token"></a>3.10.4 请求验证token</h3><pre><code class="hljs">openstack token issue</code></pre><h1 id="4-控制节点安装镜像服务Glance"><a href="#4-控制节点安装镜像服务Glance" class="headerlink" title="4.控制节点安装镜像服务Glance"></a>4.控制节点安装镜像服务Glance</h1><pre><code class="hljs">yum install -y openstack-glanceapt install -y glance</code></pre><h2 id="4-1-创建glance数据库"><a href="#4-1-创建glance数据库" class="headerlink" title="4.1 创建glance数据库"></a>4.1 创建glance数据库</h2><pre><code class="hljs">mysql -u root -pMariaDB [(none)]&gt; create database glance;MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON glance.* TO &#39;glance&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;Openstack_2024&#39;;MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON glance.* TO &#39;glance&#39;@&#39;%&#39; IDENTIFIED BY &#39;Openstack_2024&#39;;MariaDB [(none)]&gt; flush privileges;</code></pre><h2 id="4-2-创建Glance服务凭证"><a href="#4-2-创建Glance服务凭证" class="headerlink" title="4.2 创建Glance服务凭证"></a>4.2 创建Glance服务凭证</h2><h3 id="4-2-1-获取keystone管理员凭据"><a href="#4-2-1-获取keystone管理员凭据" class="headerlink" title="4.2.1 获取keystone管理员凭据"></a>4.2.1 获取keystone管理员凭据</h3><pre><code class="hljs">. admin-openrc</code></pre><h3 id="4-2-2-创建glance用户，并输入密码"><a href="#4-2-2-创建glance用户，并输入密码" class="headerlink" title="4.2.2 创建glance用户，并输入密码"></a>4.2.2 创建glance用户，并输入密码</h3><pre><code class="hljs">openstack user create --domain default --password-prompt glance</code></pre><h3 id="4-2-3-将glance用户加入到service项目并授予管理员角色"><a href="#4-2-3-将glance用户加入到service项目并授予管理员角色" class="headerlink" title="4.2.3 将glance用户加入到service项目并授予管理员角色"></a>4.2.3 将glance用户加入到service项目并授予管理员角色</h3><pre><code class="hljs">openstack role add --project service --user glance admin</code></pre><h3 id="4-2-4-创建glance服务实体"><a href="#4-2-4-创建glance服务实体" class="headerlink" title="4.2.4 创建glance服务实体"></a>4.2.4 创建glance服务实体</h3><pre><code class="hljs">openstack service create --name glance --description &quot;OpenStack Image&quot; image</code></pre><h2 id="4-3-创建Glance服务API端点"><a href="#4-3-创建Glance服务API端点" class="headerlink" title="4.3 创建Glance服务API端点"></a>4.3 创建Glance服务API端点</h2><h3 id="4-3-1-创建公有Glance服务API端点"><a href="#4-3-1-创建公有Glance服务API端点" class="headerlink" title="4.3.1 创建公有Glance服务API端点"></a>4.3.1 创建公有Glance服务API端点</h3><pre><code class="hljs">openstack endpoint create --region RegionOne image public http://controller:9292</code></pre><h3 id="4-3-2-创建私有Glance服务API端点"><a href="#4-3-2-创建私有Glance服务API端点" class="headerlink" title="4.3.2 创建私有Glance服务API端点"></a>4.3.2 创建私有Glance服务API端点</h3><pre><code class="hljs">openstack endpoint create --region RegionOne image internal http://controller:9292</code></pre><h3 id="4-3-3-创建管理Glance服务API端点"><a href="#4-3-3-创建管理Glance服务API端点" class="headerlink" title="4.3.3 创建管理Glance服务API端点"></a>4.3.3 创建管理Glance服务API端点</h3><pre><code class="hljs">openstack endpoint create --region RegionOne image admin http://controller:9292 </code></pre><h2 id="4-4-配置镜像服务"><a href="#4-4-配置镜像服务" class="headerlink" title="4.4 配置镜像服务"></a>4.4 配置镜像服务</h2><h3 id="4-4-1-备份原配置文件"><a href="#4-4-1-备份原配置文件" class="headerlink" title="4.4.1 备份原配置文件"></a>4.4.1 备份原配置文件</h3><pre><code class="hljs">sed -i.default -e &#39;/^#/d&#39; -e &#39;/^$/d&#39; /etc/glance/glance-api.conf</code></pre><h3 id="4-4-2-修改Glance-API配置文件"><a href="#4-4-2-修改Glance-API配置文件" class="headerlink" title="4.4.2 修改Glance API配置文件"></a>4.4.2 修改Glance API配置文件</h3><pre><code class="hljs">vi /etc/glance/glance-api.conf[database]connection = mysql+pymysql://glance:Openstack_2024@controller/glance[glance_store]stores = file,httpdefault_store = filefilesystem_store_datadir = /var/lib/glance/images[keystone_authtoken]www_authenticate_uri  = http://controller:5000auth_url = http://controller:5000memcached_servers = controller:11211auth_type = passwordproject_domain_name = Defaultuser_domain_name = Defaultproject_name = serviceusername = glancepassword = Openstack_2024[paste_deploy]flavor = keystone</code></pre><h2 id="4-5-同步数据，初始化glance数据库"><a href="#4-5-同步数据，初始化glance数据库" class="headerlink" title="4.5 同步数据，初始化glance数据库"></a>4.5 同步数据，初始化glance数据库</h2><pre><code class="hljs">su -s /bin/sh -c &quot;glance-manage db_sync&quot; glance</code></pre><h2 id="4-6-启动glance"><a href="#4-6-启动glance" class="headerlink" title="4.6 启动glance"></a>4.6 启动glance</h2><pre><code class="hljs">systemctl start openstack-glance-api.servicesystemctl enable openstack-glance-api.service</code></pre><h2 id="4-7-验证Glance服务"><a href="#4-7-验证Glance服务" class="headerlink" title="4.7 验证Glance服务"></a>4.7 验证Glance服务</h2><h3 id="4-7-1-下载测试镜像"><a href="#4-7-1-下载测试镜像" class="headerlink" title="4.7.1 下载测试镜像"></a>4.7.1 下载测试镜像</h3><pre><code class="hljs">wget http://download.cirros-cloud.net/0.4.0/cirros-0.4.0-x86_64-disk.img</code></pre><h3 id="4-7-2-获取keystone管理员凭据"><a href="#4-7-2-获取keystone管理员凭据" class="headerlink" title="4.7.2 获取keystone管理员凭据"></a>4.7.2 获取keystone管理员凭据</h3><pre><code class="hljs">. admin-openrc</code></pre><h3 id="4-7-3-创建测试镜像，将镜像文件上传镜像服务，磁盘格式设为qcow2，容器格式设为bare，设为公共镜像"><a href="#4-7-3-创建测试镜像，将镜像文件上传镜像服务，磁盘格式设为qcow2，容器格式设为bare，设为公共镜像" class="headerlink" title="4.7.3 创建测试镜像，将镜像文件上传镜像服务，磁盘格式设为qcow2，容器格式设为bare，设为公共镜像"></a>4.7.3 创建测试镜像，将镜像文件上传镜像服务，磁盘格式设为qcow2，容器格式设为bare，设为公共镜像</h3><pre><code class="hljs">openstack image create &quot;cirros&quot; \  --file cirros-0.4.0-x86_64-disk.img \  --disk-format qcow2 --container-format bare \  --public</code></pre><h3 id="4-7-4-查看镜像"><a href="#4-7-4-查看镜像" class="headerlink" title="4.7.4 查看镜像"></a>4.7.4 查看镜像</h3><pre><code class="hljs">openstack image list</code></pre><h1 id="5-控制节点安装资源配置服务Placement"><a href="#5-控制节点安装资源配置服务Placement" class="headerlink" title="5.控制节点安装资源配置服务Placement"></a>5.控制节点安装资源配置服务Placement</h1><pre><code class="hljs">yum install -y openstack-placement-api python3-osc-placementapt install -y placement-api</code></pre><h2 id="5-1-创建placement数据库"><a href="#5-1-创建placement数据库" class="headerlink" title="5.1 创建placement数据库"></a>5.1 创建placement数据库</h2><pre><code class="hljs">mysql -u root -pMariaDB [(none)]&gt; create database placement;MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON placement.* TO &#39;placement&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;Openstack_2024&#39;;MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON placement.* TO &#39;placement&#39;@&#39;%&#39; IDENTIFIED BY &#39;Openstack_2024&#39;;MariaDB [(none)]&gt; flush privileges;</code></pre><h2 id="5-2-创建Placement服务凭证"><a href="#5-2-创建Placement服务凭证" class="headerlink" title="5.2 创建Placement服务凭证"></a>5.2 创建Placement服务凭证</h2><h3 id="5-2-1-获取Keystone管理员凭据"><a href="#5-2-1-获取Keystone管理员凭据" class="headerlink" title="5.2.1 获取Keystone管理员凭据"></a>5.2.1 获取Keystone管理员凭据</h3><pre><code class="hljs">. admin-openrc</code></pre><h3 id="5-2-2-创建placement用户"><a href="#5-2-2-创建placement用户" class="headerlink" title="5.2.2 创建placement用户"></a>5.2.2 创建placement用户</h3><pre><code class="hljs">openstack user create --domain default --password-prompt placement</code></pre><h3 id="5-2-3-将管理员角色添加到placement用户和service项目中"><a href="#5-2-3-将管理员角色添加到placement用户和service项目中" class="headerlink" title="5.2.3 将管理员角色添加到placement用户和service项目中"></a>5.2.3 将管理员角色添加到placement用户和service项目中</h3><pre><code class="hljs">openstack role add --project service --user placement admin</code></pre><h3 id="5-2-4-创建placement服务实体"><a href="#5-2-4-创建placement服务实体" class="headerlink" title="5.2.4 创建placement服务实体"></a>5.2.4 创建placement服务实体</h3><pre><code class="hljs">openstack service create --name placement --description &quot;Placement API&quot; placement</code></pre><h2 id="5-3-创建Placement服务API端点"><a href="#5-3-创建Placement服务API端点" class="headerlink" title="5.3 创建Placement服务API端点"></a>5.3 创建Placement服务API端点</h2><h3 id="5-3-1-创建公有Placement服务API端点"><a href="#5-3-1-创建公有Placement服务API端点" class="headerlink" title="5.3.1 创建公有Placement服务API端点"></a>5.3.1 创建公有Placement服务API端点</h3><pre><code class="hljs">openstack endpoint create --region RegionOne placement public http://controller:8778</code></pre><h3 id="5-3-2-创建私有Placement服务API端点"><a href="#5-3-2-创建私有Placement服务API端点" class="headerlink" title="5.3.2 创建私有Placement服务API端点"></a>5.3.2 创建私有Placement服务API端点</h3><pre><code class="hljs">openstack endpoint create --region RegionOne placement internal http://controller:8778</code></pre><h3 id="5-3-3-创建管理Placement服务API端点"><a href="#5-3-3-创建管理Placement服务API端点" class="headerlink" title="5.3.3 创建管理Placement服务API端点"></a>5.3.3 创建管理Placement服务API端点</h3><pre><code class="hljs">openstack endpoint create --region RegionOne placement admin http://controller:8778</code></pre><h2 id="5-4-配置Placement服务"><a href="#5-4-配置Placement服务" class="headerlink" title="5.4 配置Placement服务"></a>5.4 配置Placement服务</h2><h3 id="5-4-1-备份原配置文件"><a href="#5-4-1-备份原配置文件" class="headerlink" title="5.4.1 备份原配置文件"></a>5.4.1 备份原配置文件</h3><pre><code class="hljs">sed -i.default -e &#39;/^#/d&#39; -e &#39;/^$/d&#39; /etc/placement/placement.conf</code></pre><h3 id="5-4-2-修改Placement配置文件"><a href="#5-4-2-修改Placement配置文件" class="headerlink" title="5.4.2 修改Placement配置文件"></a>5.4.2 修改Placement配置文件</h3><pre><code class="hljs">vi /etc/placement/placement.conf[api]auth_strategy = keystone[keystone_authtoken]auth_url = http://controller:5000/v3memcached_servers = controller:11211auth_type = passwordproject_domain_name = Defaultuser_domain_name = Defaultproject_name = serviceusername = placementpassword = Openstack_2024[placement_database]connection = mysql+pymysql://placement:Openstack_2024@controller/placement</code></pre><h2 id="5-5-同步数据，初始化placement数据库"><a href="#5-5-同步数据，初始化placement数据库" class="headerlink" title="5.5 同步数据，初始化placement数据库"></a>5.5 同步数据，初始化placement数据库</h2><pre><code class="hljs">su -s /bin/sh -c &quot;placement-manage db sync&quot; placement</code></pre><h2 id="5-6-重启Apache服务器，允许其他组件访问Placement-API"><a href="#5-6-重启Apache服务器，允许其他组件访问Placement-API" class="headerlink" title="5.6 重启Apache服务器，允许其他组件访问Placement API"></a>5.6 重启Apache服务器，允许其他组件访问Placement API</h2><pre><code class="hljs">systemctl restart httpd</code></pre><h2 id="5-7-验证Placement"><a href="#5-7-验证Placement" class="headerlink" title="5.7 验证Placement"></a>5.7 验证Placement</h2><h3 id="5-7-1-获取Keystone管理员凭据"><a href="#5-7-1-获取Keystone管理员凭据" class="headerlink" title="5.7.1 获取Keystone管理员凭据"></a>5.7.1 获取Keystone管理员凭据</h3><pre><code class="hljs">. admin-openrc</code></pre><h3 id="5-7-2-执行状态检查"><a href="#5-7-2-执行状态检查" class="headerlink" title="5.7.2 执行状态检查"></a>5.7.2 执行状态检查</h3><pre><code class="hljs">placement-status upgrade check</code></pre><h3 id="5-7-3-查看可用资源类别及特性"><a href="#5-7-3-查看可用资源类别及特性" class="headerlink" title="5.7.3 查看可用资源类别及特性"></a>5.7.3 查看可用资源类别及特性</h3><pre><code class="hljs">openstack --os-placement-api-version 1.2 resource class list --sort-column nameopenstack --os-placement-api-version 1.6 trait list --sort-column name</code></pre><h1 id="6-控制节点安装计算服务Nova"><a href="#6-控制节点安装计算服务Nova" class="headerlink" title="6.控制节点安装计算服务Nova"></a>6.控制节点安装计算服务Nova</h1><pre><code class="hljs">yum install -y openstack-nova-api openstack-nova-conductor openstack-nova-novncproxy openstack-nova-schedulerapt install -y nova-api nova-conductor nova-novncproxy nova-scheduler</code></pre><h2 id="6-1-创建nova数据库"><a href="#6-1-创建nova数据库" class="headerlink" title="6.1 创建nova数据库"></a>6.1 创建nova数据库</h2><pre><code class="hljs">mysql -u root -pMariaDB [(none)]&gt; create database nova;MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON nova.* TO &#39;nova&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;Openstack_2024&#39;;MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON nova.* TO &#39;nova&#39;@&#39;%&#39; IDENTIFIED BY &#39;Openstack_2024&#39;;MariaDB [(none)]&gt; create database nova_api;MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON nova_api.* TO &#39;nova&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;Openstack_2024&#39;;MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON nova_api.* TO &#39;nova&#39;@&#39;%&#39; IDENTIFIED BY &#39;Openstack_2024&#39;;MariaDB [(none)]&gt; create database nova_cell0;MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON nova_cell0.* TO &#39;nova&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;Openstack_2024&#39;;MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON nova_cell0.* TO &#39;nova&#39;@&#39;%&#39; IDENTIFIED BY &#39;Openstack_2024&#39;;MariaDB [(none)]&gt; flush privileges;</code></pre><h2 id="6-2-创建Nova服务凭证"><a href="#6-2-创建Nova服务凭证" class="headerlink" title="6.2 创建Nova服务凭证"></a>6.2 创建Nova服务凭证</h2><h3 id="6-2-1-获取Keystone管理员凭据"><a href="#6-2-1-获取Keystone管理员凭据" class="headerlink" title="6.2.1 获取Keystone管理员凭据"></a>6.2.1 获取Keystone管理员凭据</h3><pre><code class="hljs">. admin-openrc</code></pre><h3 id="6-2-2-创建nova用户"><a href="#6-2-2-创建nova用户" class="headerlink" title="6.2.2 创建nova用户"></a>6.2.2 创建nova用户</h3><pre><code class="hljs">openstack user create --domain default --password-prompt nova</code></pre><h3 id="6-2-3-将管理员角色添加都nova用户和service项目中"><a href="#6-2-3-将管理员角色添加都nova用户和service项目中" class="headerlink" title="6.2.3 将管理员角色添加都nova用户和service项目中"></a>6.2.3 将管理员角色添加都nova用户和service项目中</h3><pre><code class="hljs">openstack role add --project service --user nova admin</code></pre><h3 id="6-2-4-创建nova服务实体"><a href="#6-2-4-创建nova服务实体" class="headerlink" title="6.2.4 创建nova服务实体"></a>6.2.4 创建nova服务实体</h3><pre><code class="hljs">openstack service create --name nova --description &quot;OpenStack Compute&quot; compute</code></pre><h2 id="6-3-创建Nova服务API端点"><a href="#6-3-创建Nova服务API端点" class="headerlink" title="6.3 创建Nova服务API端点"></a>6.3 创建Nova服务API端点</h2><h3 id="6-3-1-创建公有Nova服务API端点"><a href="#6-3-1-创建公有Nova服务API端点" class="headerlink" title="6.3.1 创建公有Nova服务API端点"></a>6.3.1 创建公有Nova服务API端点</h3><pre><code class="hljs">openstack endpoint create --region RegionOne compute public http://controller:8774/v2.1</code></pre><h3 id="6-3-2-创建私有Nova服务API端点"><a href="#6-3-2-创建私有Nova服务API端点" class="headerlink" title="6.3.2 创建私有Nova服务API端点"></a>6.3.2 创建私有Nova服务API端点</h3><pre><code class="hljs">openstack endpoint create --region RegionOne compute internal http://controller:8774/v2.1</code></pre><h3 id="6-3-3-创建管理Nova服务API端点"><a href="#6-3-3-创建管理Nova服务API端点" class="headerlink" title="6.3.3 创建管理Nova服务API端点"></a>6.3.3 创建管理Nova服务API端点</h3><pre><code class="hljs">openstack endpoint create --region RegionOne compute admin http://controller:8774/v2.1</code></pre><h2 id="6-4-配置Nova服务"><a href="#6-4-配置Nova服务" class="headerlink" title="6.4 配置Nova服务"></a>6.4 配置Nova服务</h2><h3 id="6-4-1-备份原配置文件"><a href="#6-4-1-备份原配置文件" class="headerlink" title="6.4.1 备份原配置文件"></a>6.4.1 备份原配置文件</h3><pre><code class="hljs">sed -i.default -e &#39;/^#/d&#39; -e &#39;/^$/d&#39; /etc/nova/nova.conf</code></pre><h3 id="6-4-2-修改配置文件"><a href="#6-4-2-修改配置文件" class="headerlink" title="6.4.2 修改配置文件"></a>6.4.2 修改配置文件</h3><pre><code class="hljs">vi /etc/nova/nova.conf[DEFAULT]transport_url = rabbit://openstack:Openstack_2024@controller:5672/my_ip = 172.100.100.180[api]auth_strategy = keystone[api_database]connection = mysql+pymysql://nova:Openstack_2024@controller/nova_api[database]connection = mysql+pymysql://nova:Openstack_2024@controller/nova[glance]api_servers = http://controller:9292[keystone_authtoken]auth_url = http://controller:5000www_authenticate_uri = http://controller:5000/memcached_servers = controller:11211auth_type = passwordproject_domain_name = Defaultuser_domain_name = Defaultproject_name = serviceusername = novapassword = Openstack_2024[oslo_concurrency]lock_path = /var/lib/nova/tmp[placement]region_name = RegionOneproject_domain_name = Defaultproject_name = serviceauth_type = passworduser_domain_name = Defaultauth_url = http://controller:5000/v3username = placementpassword = Openstack_2024[service_user]send_service_user_token = trueauth_url = https://controller/identityauth_strategy = keystoneauth_type = passwordproject_domain_name = Defaultproject_name = serviceuser_domain_name = Defaultusername = novapassword = Openstack_2024[vnc]enabled = trueserver_listen = $my_ipserver_proxyclient_address = $my_ip</code></pre><h2 id="6-5-同步数据，初始化nova数据库"><a href="#6-5-同步数据，初始化nova数据库" class="headerlink" title="6.5 同步数据，初始化nova数据库"></a>6.5 同步数据，初始化nova数据库</h2><h3 id="6-5-1-初始化nova-api数据库"><a href="#6-5-1-初始化nova-api数据库" class="headerlink" title="6.5.1 初始化nova-api数据库"></a>6.5.1 初始化nova-api数据库</h3><pre><code class="hljs">su -s /bin/sh -c &quot;nova-manage api_db sync&quot; nova</code></pre><h3 id="6-5-2-初始化cell0数据库"><a href="#6-5-2-初始化cell0数据库" class="headerlink" title="6.5.2 初始化cell0数据库"></a>6.5.2 初始化cell0数据库</h3><pre><code class="hljs">su -s /bin/sh -c &quot;nova-manage cell_v2 map_cell0&quot; nova</code></pre><h3 id="6-5-3-初始化cell1原件"><a href="#6-5-3-初始化cell1原件" class="headerlink" title="6.5.3 初始化cell1原件"></a>6.5.3 初始化cell1原件</h3><pre><code class="hljs">su -s /bin/sh -c &quot;nova-manage cell_v2 create_cell --name=cell1 --verbose&quot; nova</code></pre><h3 id="6-5-4-初始化nova数据库"><a href="#6-5-4-初始化nova数据库" class="headerlink" title="6.5.4 初始化nova数据库"></a>6.5.4 初始化nova数据库</h3><pre><code class="hljs">su -s /bin/sh -c &quot;nova-manage db sync&quot; nova</code></pre><h3 id="6-5-5-验证novacell0、cell1的注册"><a href="#6-5-5-验证novacell0、cell1的注册" class="headerlink" title="6.5.5 验证novacell0、cell1的注册"></a>6.5.5 验证novacell0、cell1的注册</h3><pre><code class="hljs">su -s /bin/sh -c &quot;nova-manage cell_v2 list_cells&quot; nova</code></pre><h2 id="6-6-启动nova"><a href="#6-6-启动nova" class="headerlink" title="6.6 启动nova"></a>6.6 启动nova</h2><pre><code class="hljs">systemctl start openstack-nova-api.servicesystemctl enable openstack-nova-api.servicesystemctl start openstack-nova-scheduler.servicesystemctl enable openstack-nova-scheduler.servicesystemctl start openstack-nova-conductor.servicesystemctl enable openstack-nova-conductor.servicesystemctl start openstack-nova-novncproxy.servicesystemctl enable openstack-nova-novncproxy.service</code></pre><h1 id="7-计算节点安装计算服务Nova"><a href="#7-计算节点安装计算服务Nova" class="headerlink" title="7.计算节点安装计算服务Nova"></a>7.计算节点安装计算服务Nova</h1><pre><code class="hljs">yum install -y openstack-nova-computeapt install -y nova-compute</code></pre><h2 id="7-1-配置nova服务"><a href="#7-1-配置nova服务" class="headerlink" title="7.1 配置nova服务"></a>7.1 配置nova服务</h2><h3 id="7-1-1-备份原配置文件"><a href="#7-1-1-备份原配置文件" class="headerlink" title="7.1.1 备份原配置文件"></a>7.1.1 备份原配置文件</h3><pre><code class="hljs">sed -i.default -e &#39;/^#/d&#39; -e &#39;/^$/d&#39; /etc/nova/nova.conf</code></pre><h3 id="7-1-2-修改配置文件"><a href="#7-1-2-修改配置文件" class="headerlink" title="7.1.2 修改配置文件"></a>7.1.2 修改配置文件</h3><pre><code class="hljs">vi /etc/nova/nova.conf[DEFAULT]transport_url = rabbit://openstack:Openstack_2024@controllermy_ip = 172.100.100.181[api]auth_strategy = keystone[glance]api_servers = http://controller:9292[keystone_authtoken]auth_url = http://controller:5000/www_authenticate_uri = http://controller:5000/memcached_servers = controller:11211auth_type = passwordproject_domain_name = Defaultuser_domain_name = Defaultproject_name = serviceusername = novapassword = Openstack_2024[libvirt]virt_type = qemu[oslo_concurrency]lock_path = /var/lib/nova/tmp[placement]region_name = RegionOneproject_domain_name = Defaultproject_name = serviceauth_type = passworduser_domain_name = Defaultauth_url = http://controller:5000/v3username = placementpassword = Openstack_2024[scheduler]discover_hosts_in_cells_interval = 300[service_user]send_service_user_token = trueauth_url = https://controller/identityauth_strategy = keystoneauth_type = passwordproject_domain_name = Defaultproject_name = serviceuser_domain_name = Defaultusername = novapassword = Openstack_2024[vnc]enabled = trueserver_listen = 0.0.0.0server_proxyclient_address = $my_ipnovncproxy_base_url = http://controller:6080/vnc_auto.html</code></pre><h2 id="7-2-启动nova"><a href="#7-2-启动nova" class="headerlink" title="7.2 启动nova"></a>7.2 启动nova</h2><pre><code class="hljs">systemctl start libvirtd.servicesystemctl enable libvirtd.servicesystemctl start openstack-nova-compute.servicesystemctl enable openstack-nova-compute.service</code></pre><h2 id="7-3-在控制节点上添加计算节点，即添加到单元数据库"><a href="#7-3-在控制节点上添加计算节点，即添加到单元数据库" class="headerlink" title="7.3 在控制节点上添加计算节点，即添加到单元数据库"></a>7.3 在控制节点上添加计算节点，即添加到单元数据库</h2><h3 id="7-3-1-获取keystone管理员凭据"><a href="#7-3-1-获取keystone管理员凭据" class="headerlink" title="7.3.1 获取keystone管理员凭据"></a>7.3.1 获取keystone管理员凭据</h3><pre><code class="hljs">. admin-openrc</code></pre><h3 id="7-3-2-验证cell数据库的计算主机节点"><a href="#7-3-2-验证cell数据库的计算主机节点" class="headerlink" title="7.3.2 验证cell数据库的计算主机节点"></a>7.3.2 验证cell数据库的计算主机节点</h3><pre><code class="hljs">openstack compute service list --service nova-compute</code></pre><h3 id="7-3-3-计算节点同步到单元数据库"><a href="#7-3-3-计算节点同步到单元数据库" class="headerlink" title="7.3.3 计算节点同步到单元数据库"></a>7.3.3 计算节点同步到单元数据库</h3><pre><code class="hljs">su -s /bin/sh -c &quot;nova-manage cell_v2 discover_hosts --verbose&quot; nova</code></pre><h3 id="7-3-4-重启nova服务"><a href="#7-3-4-重启nova服务" class="headerlink" title="7.3.4 重启nova服务"></a>7.3.4 重启nova服务</h3><pre><code class="hljs">systemctl restart openstack-nova-api.servicesystemctl restart openstack-nova-scheduler.servicesystemctl restart openstack-nova-conductor.servicesystemctl restart openstack-nova-novncproxy.service</code></pre><h2 id="7-4-验证计算服务"><a href="#7-4-验证计算服务" class="headerlink" title="7.4 验证计算服务"></a>7.4 验证计算服务</h2><h3 id="7-4-1-获取keystone管理员凭据"><a href="#7-4-1-获取keystone管理员凭据" class="headerlink" title="7.4.1 获取keystone管理员凭据"></a>7.4.1 获取keystone管理员凭据</h3><pre><code class="hljs">. admin-openrc</code></pre><h3 id="7-4-2-查看服务组件，验证进程的成功启动和注册"><a href="#7-4-2-查看服务组件，验证进程的成功启动和注册" class="headerlink" title="7.4.2 查看服务组件，验证进程的成功启动和注册"></a>7.4.2 查看服务组件，验证进程的成功启动和注册</h3><pre><code class="hljs">openstack compute service list</code></pre><h3 id="7-4-3-查看身份服务中的API端点，验证与身份服务的连接性"><a href="#7-4-3-查看身份服务中的API端点，验证与身份服务的连接性" class="headerlink" title="7.4.3 查看身份服务中的API端点，验证与身份服务的连接性"></a>7.4.3 查看身份服务中的API端点，验证与身份服务的连接性</h3><pre><code class="hljs">openstack catalog list</code></pre><h3 id="7-4-4-查看单元格和展示位置API是否正常运行"><a href="#7-4-4-查看单元格和展示位置API是否正常运行" class="headerlink" title="7.4.4 查看单元格和展示位置API是否正常运行"></a>7.4.4 查看单元格和展示位置API是否正常运行</h3><pre><code class="hljs">nova-status upgrade check</code></pre><h1 id="8-控制节点安装网络服务Neutron"><a href="#8-控制节点安装网络服务Neutron" class="headerlink" title="8.控制节点安装网络服务Neutron"></a>8.控制节点安装网络服务Neutron</h1><pre><code class="hljs">yum install -y openstack-neutron openstack-neutron-ml2 openstack-neutron-linuxbridge ebtablesapt install -y neutron-server neutron-plugin-ml2 neutron-linuxbridge-agent neutron-dhcp-agent \neutron-metadata-agent neutron-l3-agent </code></pre><h2 id="8-1-创建neutron数据库"><a href="#8-1-创建neutron数据库" class="headerlink" title="8.1 创建neutron数据库"></a>8.1 创建neutron数据库</h2><pre><code class="hljs">mysql -u root -pMariaDB [(none)]&gt; create database neutron;MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON neutron.* TO &#39;neutron&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;Openstack_2024&#39;;MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON neutron.* TO &#39;neutron&#39;@&#39;%&#39; IDENTIFIED BY &#39;Openstack_2024&#39;;MariaDB [(none)]&gt; flush privileges;</code></pre><h2 id="8-2-创建Neutron服务凭证"><a href="#8-2-创建Neutron服务凭证" class="headerlink" title="8.2 创建Neutron服务凭证"></a>8.2 创建Neutron服务凭证</h2><h3 id="8-2-1-获取keystone管理员凭据"><a href="#8-2-1-获取keystone管理员凭据" class="headerlink" title="8.2.1 获取keystone管理员凭据"></a>8.2.1 获取keystone管理员凭据</h3><pre><code class="hljs">. admin-openrc</code></pre><h3 id="8-2-2-创建neutron用户"><a href="#8-2-2-创建neutron用户" class="headerlink" title="8.2.2 创建neutron用户"></a>8.2.2 创建neutron用户</h3><pre><code class="hljs">openstack user create --domain default --password-prompt neutron</code></pre><h3 id="8-2-3-设置neutron用户角色及所属项目"><a href="#8-2-3-设置neutron用户角色及所属项目" class="headerlink" title="8.2.3 设置neutron用户角色及所属项目"></a>8.2.3 设置neutron用户角色及所属项目</h3><pre><code class="hljs">openstack role add --project service --user neutron admin</code></pre><h3 id="8-2-4-创建neutron实体"><a href="#8-2-4-创建neutron实体" class="headerlink" title="8.2.4 创建neutron实体"></a>8.2.4 创建neutron实体</h3><pre><code class="hljs">openstack service create --name neutron --description &quot;OpenStack Networking&quot; network</code></pre><h2 id="8-3-创建Neutron服务API端点"><a href="#8-3-创建Neutron服务API端点" class="headerlink" title="8.3 创建Neutron服务API端点"></a>8.3 创建Neutron服务API端点</h2><h3 id="8-3-1-创建Neutron公有服务API端点"><a href="#8-3-1-创建Neutron公有服务API端点" class="headerlink" title="8.3.1 创建Neutron公有服务API端点"></a>8.3.1 创建Neutron公有服务API端点</h3><pre><code class="hljs">openstack endpoint create --region RegionOne network public http://controller:9696</code></pre><h3 id="8-3-2-创建Neutron私有服务API端点"><a href="#8-3-2-创建Neutron私有服务API端点" class="headerlink" title="8.3.2 创建Neutron私有服务API端点"></a>8.3.2 创建Neutron私有服务API端点</h3><pre><code class="hljs">openstack endpoint create --region RegionOne network internal http://controller:9696</code></pre><h3 id="8-3-3-创建Neutron管理员服务API端点"><a href="#8-3-3-创建Neutron管理员服务API端点" class="headerlink" title="8.3.3 创建Neutron管理员服务API端点"></a>8.3.3 创建Neutron管理员服务API端点</h3><pre><code class="hljs">openstack endpoint create --region RegionOne network admin http://controller:9696</code></pre><h2 id="8-4-配置neutron服务"><a href="#8-4-配置neutron服务" class="headerlink" title="8.4 配置neutron服务"></a>8.4 配置neutron服务</h2><h3 id="8-4-1-备份原文件"><a href="#8-4-1-备份原文件" class="headerlink" title="8.4.1 备份原文件"></a>8.4.1 备份原文件</h3><pre><code class="hljs">sed -i.default -e &#39;/^#/d&#39; -e &#39;/^$/d&#39; /etc/neutron/neutron.conf</code></pre><h3 id="8-4-2-修改配置文件"><a href="#8-4-2-修改配置文件" class="headerlink" title="8.4.2 修改配置文件"></a>8.4.2 修改配置文件</h3><pre><code class="hljs">vi /etc/neutron/neutron.conf[DEFAULT]core_plugin = ml2service_plugins = routerallow_overlapping_ips = truetransport_url = rabbit://openstack:Openstack_2024@controllerauth_strategy = keystonenotify_nova_on_port_status_changes = truenotify_nova_on_port_data_changes = true[database]connection = mysql+pymysql://neutron:Openstack_2024@controller/neutron[keystone_authtoken]www_authenticate_uri = http://controller:5000auth_url = http://controller:5000memcached_servers =controller:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = neutronpassword = Openstack_2024[nova]auth_url = http://controller:5000auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultregion_name = RegionOneproject_name = serviceusername = novapassword = Openstack_2024[oslo_concurrency]lock_path = /var/lib/neutron/tmp</code></pre><h2 id="8-5-配置网络选项"><a href="#8-5-配置网络选项" class="headerlink" title="8.5 配置网络选项"></a>8.5 配置网络选项</h2><h3 id="8-5-1-配置L2网络模块插件"><a href="#8-5-1-配置L2网络模块插件" class="headerlink" title="8.5.1 配置L2网络模块插件"></a>8.5.1 配置L2网络模块插件</h3><pre><code class="hljs">sed -i.default -e &#39;/^#/d&#39; -e &#39;/^$/d&#39; /etc/neutron/plugins/ml2/ml2_conf.inivi /etc/neutron/plugins/ml2/ml2_conf.ini[ml2]type_drivers = flat,vlan,vxlantenant_network_types = vxlanmechanism_drivers = linuxbridge,l2populationextension_drivers = port_security[ml2_type_flat]flat_networks = provider[securitygroup]enable_ipset = true</code></pre><h3 id="8-5-2-配置Linux网桥代理"><a href="#8-5-2-配置Linux网桥代理" class="headerlink" title="8.5.2 配置Linux网桥代理"></a>8.5.2 配置Linux网桥代理</h3><pre><code class="hljs">sed -i.default -e &#39;/^#/d&#39; -e &#39;/^$/d&#39; /etc/neutron/plugins/ml2/linuxbridge_agent.inivi /etc/neutron/plugins/ml2/linuxbridge_agent.ini[linux_bridge]physical_interface_mappings = provider:ens33[vxlan]enable_vxlan = truelocal_ip = 192.168.100.180l2_population = true[securitygroup]enable_security_group = truefirewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver</code></pre><h3 id="8-5-3-配置L3代理"><a href="#8-5-3-配置L3代理" class="headerlink" title="8.5.3 配置L3代理"></a>8.5.3 配置L3代理</h3><pre><code class="hljs">sed -i.default -e &#39;/^#/d&#39; -e &#39;/^$/d&#39; /etc/neutron/l3_agent.inivi /etc/neutron/l3_agent.ini[DEFAULT]interface_driver = linuxbridge</code></pre><h3 id="8-5-4-配置DHCP代理"><a href="#8-5-4-配置DHCP代理" class="headerlink" title="8.5.4 配置DHCP代理"></a>8.5.4 配置DHCP代理</h3><pre><code class="hljs">sed -i.default -e &#39;/^#/d&#39; -e &#39;/^$/d&#39; /etc/neutron/dhcp_agent.inivi /etc/neutron/dhcp_agent.ini[DEFAULT]interface_driver = linuxbridgedhcp_driver = neutron.agent.linux.dhcp.Dnsmasqenable_isolated_metadata = true</code></pre><h3 id="8-5-5-配置元数据代理"><a href="#8-5-5-配置元数据代理" class="headerlink" title="8.5.5 配置元数据代理"></a>8.5.5 配置元数据代理</h3><pre><code class="hljs">sed -i.default -e &#39;/^#/d&#39; -e &#39;/^$/d&#39; /etc/neutron/metadata_agent.inivi /etc/neutron/metadata_agent.ini[DEFAULT]nova_metadata_host = controllermetadata_proxy_shared_secret = Openstack_2024</code></pre><h3 id="8-5-6-配置计算服务neutron模块"><a href="#8-5-6-配置计算服务neutron模块" class="headerlink" title="8.5.6 配置计算服务neutron模块"></a>8.5.6 配置计算服务neutron模块</h3><pre><code class="hljs">vi /etc/nova/nova.conf[neutron]auth_url = http://controller:5000auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultregion_name = RegionOneproject_name = serviceusername = neutronpassword = Openstack_2024service_metadata_proxy = truemetadata_proxy_shared_secret = Openstack_2024</code></pre><h3 id="8-5-7-配置内核支持网桥过滤器"><a href="#8-5-7-配置内核支持网桥过滤器" class="headerlink" title="8.5.7 配置内核支持网桥过滤器"></a>8.5.7 配置内核支持网桥过滤器</h3><pre><code class="hljs">vi /etc/sysctl.confnet.bridge.bridge-nf-call-iptables = 1net.bridge.bridge-nf-call-ip6tables = 1</code></pre><h3 id="8-5-8-加载内核模块生效配置"><a href="#8-5-8-加载内核模块生效配置" class="headerlink" title="8.5.8 加载内核模块生效配置"></a>8.5.8 加载内核模块生效配置</h3><pre><code class="hljs">modprobe br_netfilter &amp;&amp; sysctl -p</code></pre><h3 id="8-5-9-创建网络服务初始化脚本的软连接"><a href="#8-5-9-创建网络服务初始化脚本的软连接" class="headerlink" title="8.5.9 创建网络服务初始化脚本的软连接"></a>8.5.9 创建网络服务初始化脚本的软连接</h3><pre><code class="hljs">ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini</code></pre><h2 id="8-6-同步数据，初始化neutron数据库"><a href="#8-6-同步数据，初始化neutron数据库" class="headerlink" title="8.6 同步数据，初始化neutron数据库"></a>8.6 同步数据，初始化neutron数据库</h2><pre><code class="hljs">su -s /bin/sh -c &quot;neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head&quot; neutron</code></pre><h2 id="8-7-重启Compute-API服务"><a href="#8-7-重启Compute-API服务" class="headerlink" title="8.7 重启Compute API服务"></a>8.7 重启Compute API服务</h2><pre><code class="hljs">systemctl restart openstack-nova-api.service</code></pre><h3 id="8-8-启动neutron"><a href="#8-8-启动neutron" class="headerlink" title="8.8 启动neutron"></a>8.8 启动neutron</h3><pre><code class="hljs">systemctl start neutron-server.servicesystemctl enable neutron-server.servicesystemctl start neutron-linuxbridge-agent.servicesystemctl enable neutron-linuxbridge-agent.servicesystemctl start neutron-dhcp-agent.servicesystemctl enable neutron-dhcp-agent.servicesystemctl start neutron-metadata-agent.servicesystemctl enable neutron-metadata-agent.servicesystemctl enable neutron-l3-agent.servicesystemctl start neutron-l3-agent.service</code></pre><h1 id="9-计算节点安装网络服务neutron"><a href="#9-计算节点安装网络服务neutron" class="headerlink" title="9.计算节点安装网络服务neutron"></a>9.计算节点安装网络服务neutron</h1><pre><code class="hljs">yum install -y openstack-neutron-linuxbridge ebtables ipsetapt install -y neutron-linuxbridge-agent</code></pre><h2 id="9-1-配置neutron服务"><a href="#9-1-配置neutron服务" class="headerlink" title="9.1 配置neutron服务"></a>9.1 配置neutron服务</h2><h3 id="9-1-1-备份原配置文件"><a href="#9-1-1-备份原配置文件" class="headerlink" title="9.1.1 备份原配置文件"></a>9.1.1 备份原配置文件</h3><pre><code class="hljs">sed -i.default -e &#39;/^#/d&#39; -e &#39;/^$/d&#39; /etc/neutron/neutron.conf</code></pre><h3 id="9-1-2-修改配置文件"><a href="#9-1-2-修改配置文件" class="headerlink" title="9.1.2 修改配置文件"></a>9.1.2 修改配置文件</h3><pre><code class="hljs">vi /etc/neutron/neutron.conf[DEFAULT]auth_strategy = keystonetransport_url = rabbit://openstack:Openstack_2024@controller[keystone_authtoken]www_authenticate_uri = http://controller:5000auth_url = http://controller:5000memcached_servers =controller:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = neutronpassword = Openstack_2024[oslo_concurrency]lock_path = /var/lib/neutron/tmp</code></pre><h2 id="9-2-配置Linux桥代理"><a href="#9-2-配置Linux桥代理" class="headerlink" title="9.2 配置Linux桥代理"></a>9.2 配置Linux桥代理</h2><h3 id="9-2-1-备份原文件"><a href="#9-2-1-备份原文件" class="headerlink" title="9.2.1 备份原文件"></a>9.2.1 备份原文件</h3><pre><code class="hljs">sed -i.default -e &#39;/^#/d&#39; -e &#39;/^$/d&#39; /etc/neutron/plugins/ml2/linuxbridge_agent.ini</code></pre><h3 id="9-2-2-修改配置文件"><a href="#9-2-2-修改配置文件" class="headerlink" title="9.2.2 修改配置文件"></a>9.2.2 修改配置文件</h3><pre><code class="hljs">vi /etc/neutron/plugins/ml2/linuxbridge_agent.ini[linux_bridge]physical_interface_mappings = provider:ens33[vxlan]enable_vxlan = truelocal_ip = 172.100.100.181l2_population = true[securitygroup]enable_security_group = truefirewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver</code></pre><h2 id="9-3-配置内核支持网桥过滤器"><a href="#9-3-配置内核支持网桥过滤器" class="headerlink" title="9.3 配置内核支持网桥过滤器"></a>9.3 配置内核支持网桥过滤器</h2><h3 id="9-3-1-创建配置文件"><a href="#9-3-1-创建配置文件" class="headerlink" title="9.3.1 创建配置文件"></a>9.3.1 创建配置文件</h3><pre><code class="hljs">vi /etc/sysctl.confnet.bridge.bridge-nf-call-iptables = 1net.bridge.bridge-nf-call-ip6tables = 1</code></pre><h3 id="9-3-2-加载内核模块"><a href="#9-3-2-加载内核模块" class="headerlink" title="9.3.2 加载内核模块"></a>9.3.2 加载内核模块</h3><pre><code class="hljs">modprobe br_netfilter &amp;&amp; sysctl -p</code></pre><h2 id="9-4-配置计算服务neutron模块"><a href="#9-4-配置计算服务neutron模块" class="headerlink" title="9.4 配置计算服务neutron模块"></a>9.4 配置计算服务neutron模块</h2><pre><code class="hljs">vi /etc/nova/nova.conf[neutron]auth_url = http://controller:5000auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultregion_name = RegionOneproject_name = serviceusername = neutronpassword = Openstack_2024</code></pre><h2 id="9-5-重启Nova-Compute服务"><a href="#9-5-重启Nova-Compute服务" class="headerlink" title="9.5 重启Nova Compute服务"></a>9.5 重启Nova Compute服务</h2><pre><code class="hljs">systemctl restart openstack-nova-compute.service</code></pre><h2 id="9-6-启动Linux网桥代理"><a href="#9-6-启动Linux网桥代理" class="headerlink" title="9.6 启动Linux网桥代理"></a>9.6 启动Linux网桥代理</h2><pre><code class="hljs">systemctl start neutron-linuxbridge-agent.servicesystemctl enable neutron-linuxbridge-agent.service</code></pre><h2 id="9-7-控制节点验证neutron"><a href="#9-7-控制节点验证neutron" class="headerlink" title="9.7 控制节点验证neutron"></a>9.7 控制节点验证neutron</h2><h3 id="9-7-1-获取keystone管理员凭据"><a href="#9-7-1-获取keystone管理员凭据" class="headerlink" title="9.7.1 获取keystone管理员凭据"></a>9.7.1 获取keystone管理员凭据</h3><pre><code class="hljs">. admin-openrc</code></pre><h3 id="9-7-2-查看neutron-server进程"><a href="#9-7-2-查看neutron-server进程" class="headerlink" title="9.7.2 查看neutron-server进程"></a>9.7.2 查看neutron-server进程</h3><pre><code class="hljs">openstack extension list --network</code></pre><h3 id="9-7-3-查看neutron发起的代理"><a href="#9-7-3-查看neutron发起的代理" class="headerlink" title="9.7.3 查看neutron发起的代理"></a>9.7.3 查看neutron发起的代理</h3><pre><code class="hljs">openstack network agent list</code></pre><h1 id="10-控制节点安装块存储服务Cinder"><a href="#10-控制节点安装块存储服务Cinder" class="headerlink" title="10.控制节点安装块存储服务Cinder"></a>10.控制节点安装块存储服务Cinder</h1><pre><code class="hljs">yum install -y openstack-cinderapt install -y cinder-api cinder-scheduler</code></pre><h2 id="10-1-创建cinder数据库"><a href="#10-1-创建cinder数据库" class="headerlink" title="10.1 创建cinder数据库"></a>10.1 创建cinder数据库</h2><pre><code class="hljs">mysql -u root -pMariaDB [(none)]&gt; create database cinder;MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON cinder.* TO &#39;cinder&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;Openstack_2024&#39;;MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON cinder.* TO &#39;cinder&#39;@&#39;%&#39; IDENTIFIED BY &#39;Openstack_2024&#39;;MariaDB [(none)]&gt; flush privileges;</code></pre><h2 id="10-2-创建Cinder服务凭证"><a href="#10-2-创建Cinder服务凭证" class="headerlink" title="10.2 创建Cinder服务凭证"></a>10.2 创建Cinder服务凭证</h2><h3 id="10-2-1-获取keystone管理员凭据"><a href="#10-2-1-获取keystone管理员凭据" class="headerlink" title="10.2.1 获取keystone管理员凭据"></a>10.2.1 获取keystone管理员凭据</h3><pre><code class="hljs">. admin-openrc</code></pre><h3 id="10-2-2-创建cinder用户"><a href="#10-2-2-创建cinder用户" class="headerlink" title="10.2.2 创建cinder用户"></a>10.2.2 创建cinder用户</h3><pre><code class="hljs">openstack user create --domain default --password-prompt cinder</code></pre><h3 id="10-2-3-设置cinder用户角色及所属项目"><a href="#10-2-3-设置cinder用户角色及所属项目" class="headerlink" title="10.2.3 设置cinder用户角色及所属项目"></a>10.2.3 设置cinder用户角色及所属项目</h3><pre><code class="hljs">openstack role add --project service --user cinder admin</code></pre><h3 id="10-2-4-创建cinder实体"><a href="#10-2-4-创建cinder实体" class="headerlink" title="10.2.4 创建cinder实体"></a>10.2.4 创建cinder实体</h3><pre><code class="hljs">openstack service create --name cinderv2 --description &quot;OpenStack Block Storage&quot; volumev2openstack service create --name cinderv3 --description &quot;OpenStack Block Storage&quot; volumev3</code></pre><h2 id="10-3-创建Cinder服务API端点"><a href="#10-3-创建Cinder服务API端点" class="headerlink" title="10.3 创建Cinder服务API端点"></a>10.3 创建Cinder服务API端点</h2><h3 id="10-3-1-创建Cinder公有服务API端点"><a href="#10-3-1-创建Cinder公有服务API端点" class="headerlink" title="10.3.1 创建Cinder公有服务API端点"></a>10.3.1 创建Cinder公有服务API端点</h3><pre><code class="hljs">openstack endpoint create --region RegionOne volumev2 public http://controller:8776/v2/%\(tenant_id\)sopenstack endpoint create --region RegionOne volumev3 public http://controller:8776/v3/%\(tenant_id\)s</code></pre><h3 id="10-3-2-创建Cinder私有服务API端点"><a href="#10-3-2-创建Cinder私有服务API端点" class="headerlink" title="10.3.2 创建Cinder私有服务API端点"></a>10.3.2 创建Cinder私有服务API端点</h3><pre><code class="hljs">openstack endpoint create --region RegionOne volumev2 internal http://controller:8776/v2/%\(tenant_id\)sopenstack endpoint create --region RegionOne volumev3 internal http://controller:8776/v3/%\(tenant_id\)s</code></pre><h3 id="10-3-3-创建Cinder管理员服务API端点"><a href="#10-3-3-创建Cinder管理员服务API端点" class="headerlink" title="10.3.3 创建Cinder管理员服务API端点"></a>10.3.3 创建Cinder管理员服务API端点</h3><pre><code class="hljs">openstack endpoint create --region RegionOne volumev2 admin http://controller:8776/v2/%\(tenant_id\)sopenstack endpoint create --region RegionOne volumev3 admin http://controller:8776/v3/%\(tenant_id\)s</code></pre><h2 id="10-4-配置cinder服务"><a href="#10-4-配置cinder服务" class="headerlink" title="10.4 配置cinder服务"></a>10.4 配置cinder服务</h2><h3 id="10-4-1-备份原文件"><a href="#10-4-1-备份原文件" class="headerlink" title="10.4.1 备份原文件"></a>10.4.1 备份原文件</h3><pre><code class="hljs">sed -i.default -e &#39;/^#/d&#39; -e &#39;/^$/d&#39; /etc/cinder/cinder.conf </code></pre><h3 id="10-4-2-修改配置文件"><a href="#10-4-2-修改配置文件" class="headerlink" title="10.4.2 修改配置文件"></a>10.4.2 修改配置文件</h3><pre><code class="hljs">vi /etc/cinder/cinder.conf[DEFAULT]auth_strategy = keystonemy_ip = 172.100.100.180transport_url = rabbit://openstack:Openstack_2024@controller[database]connection = mysql+pymysql://cinder:Openstack_2024@controller/cinder[keystone_authtoken]www_authenticate_uri = http://controller:5000auth_url = http://controller:5000memcached_servers = controller:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = cinderpassword = Openstack_2024[oslo_concurrency]lock_path = /var/lib/cinder/tmp</code></pre><h2 id="10-5-初始化cinder数据库"><a href="#10-5-初始化cinder数据库" class="headerlink" title="10.5 初始化cinder数据库"></a>10.5 初始化cinder数据库</h2><pre><code class="hljs">su -s /bin/sh -c &quot;cinder-manage db sync&quot; cinder</code></pre><h2 id="10-6-配置计算服务cinder模块"><a href="#10-6-配置计算服务cinder模块" class="headerlink" title="10.6 配置计算服务cinder模块"></a>10.6 配置计算服务cinder模块</h2><pre><code class="hljs">vi /etc/nova/nova.conf[cinder]os_region_name = RegionOne</code></pre><h2 id="10-7-重启nova-api服务"><a href="#10-7-重启nova-api服务" class="headerlink" title="10.7 重启nova-api服务"></a>10.7 重启nova-api服务</h2><pre><code class="hljs">systemctl restart openstack-nova-api.service</code></pre><h2 id="10-8-启动Cinder服务"><a href="#10-8-启动Cinder服务" class="headerlink" title="10.8 启动Cinder服务"></a>10.8 启动Cinder服务</h2><pre><code class="hljs">systemctl start openstack-cinder-api.servicesystemctl enable openstack-cinder-api.servicesystemctl start openstack-cinder-scheduler.servicesystemctl enable openstack-cinder-scheduler.service</code></pre><h1 id="11-存储节点安装块存储服务Cinder"><a href="#11-存储节点安装块存储服务Cinder" class="headerlink" title="11.存储节点安装块存储服务Cinder"></a>11.存储节点安装块存储服务Cinder</h1><pre><code class="hljs">yum install -y openstack-cinder targetcli python-keystoneapt install -y lvm2 thin-provisioning-tools </code></pre><h2 id="11-1-配置LVM"><a href="#11-1-配置LVM" class="headerlink" title="11.1 配置LVM"></a>11.1 配置LVM</h2><h3 id="11-1-1-安装LVM"><a href="#11-1-1-安装LVM" class="headerlink" title="11.1.1 安装LVM"></a>11.1.1 安装LVM</h3><pre><code class="hljs">yum install -y lvm2 device-mapper-persistent-dataapt install -y tgt cinder-volume</code></pre><h3 id="11-1-2-启动LVM元数据服务"><a href="#11-1-2-启动LVM元数据服务" class="headerlink" title="11.1.2 启动LVM元数据服务"></a>11.1.2 启动LVM元数据服务</h3><pre><code class="hljs">systemctl start lvm2-lvmetad.servicesystemctl enable lvm2-lvmetad.service</code></pre><h3 id="11-1-3-添加新盘，创建LVM"><a href="#11-1-3-添加新盘，创建LVM" class="headerlink" title="11.1.3 添加新盘，创建LVM"></a>11.1.3 添加新盘，创建LVM</h3><pre><code class="hljs">pvcreate /dev/sdb</code></pre><h3 id="11-1-4-创建块存储LVM卷组"><a href="#11-1-4-创建块存储LVM卷组" class="headerlink" title="11.1.4 创建块存储LVM卷组"></a>11.1.4 创建块存储LVM卷组</h3><pre><code class="hljs">vgcreate cinder-volumes /dev/sdb</code></pre><h3 id="11-1-5-配置块存储卷组的访问权限，使LVM卷扫描工具只扫描包含cinder-volume卷组的设备"><a href="#11-1-5-配置块存储卷组的访问权限，使LVM卷扫描工具只扫描包含cinder-volume卷组的设备" class="headerlink" title="11.1.5 配置块存储卷组的访问权限，使LVM卷扫描工具只扫描包含cinder-volume卷组的设备"></a>11.1.5 配置块存储卷组的访问权限，使LVM卷扫描工具只扫描包含cinder-volume卷组的设备</h3><pre><code class="hljs">vi /etc/lvm/lvm.confdevices &#123;filter = [ &quot;a/vdb/&quot;, &quot;r/.*/&quot;]&#125;</code></pre><h3 id="11-1-6-配置target"><a href="#11-1-6-配置target" class="headerlink" title="11.1.6 配置target"></a>11.1.6 配置target</h3><pre><code class="hljs">vi /etc/tgt/targets.confinclude /var/lib/cinder/volumes/*</code></pre><h2 id="11-2-配置Cinder服务"><a href="#11-2-配置Cinder服务" class="headerlink" title="11.2 配置Cinder服务"></a>11.2 配置Cinder服务</h2><h3 id="11-2-1-备份原文件"><a href="#11-2-1-备份原文件" class="headerlink" title="11.2.1 备份原文件"></a>11.2.1 备份原文件</h3><pre><code class="hljs">sed -i.default -e &#39;/^#/d&#39; -e &#39;/^$/d&#39; /etc/cinder/cinder.conf </code></pre><h3 id="11-2-2-修改配置文件"><a href="#11-2-2-修改配置文件" class="headerlink" title="11.2.2 修改配置文件"></a>11.2.2 修改配置文件</h3><pre><code class="hljs">vi /etc/cinder/cinder.conf[DEFAULT]transport_url = rabbit://openstack:Openstack_2024@controllerauth_strategy = keystonemy_ip = 172.100.100.180enabled_backends = lvmglance_api_servers = http://controller:9292[database]connection = mysql+pymysql://cinder:Openstack_2024@controller/cinder[oslo_concurrency]lock_path = /var/lib/cinder/tmp[lvm]volume_driver = cinder.volume.drivers.lvm.LVMVolumeDrivervolume_group = cinder-volumesiscsi_protocol = iscsiiscsi_helper = lioadmtarget_helper = tgtadm[keystone_authtoken]auth_uri = http://controller:5000www_authenticate_uri = http://controller:5000memcached_servers = controller:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = cinderpassword = Openstack_2024</code></pre><h2 id="11-3-启动块存储卷服务"><a href="#11-3-启动块存储卷服务" class="headerlink" title="11.3 启动块存储卷服务"></a>11.3 启动块存储卷服务</h2><pre><code class="hljs">systemctl start openstack-cinder-volume.servicesystemctl enable openstack-cinder-volume.servicesystemctl start target.servicesystemctl enable target.service</code></pre><h2 id="11-4-验证Cinder服务"><a href="#11-4-验证Cinder服务" class="headerlink" title="11.4 验证Cinder服务"></a>11.4 验证Cinder服务</h2><h3 id="11-4-1-获取keystone管理员凭据"><a href="#11-4-1-获取keystone管理员凭据" class="headerlink" title="11.4.1 获取keystone管理员凭据"></a>11.4.1 获取keystone管理员凭据</h3><pre><code class="hljs">. admin-openrc</code></pre><h3 id="11-4-2-查看已经启动的服务组件"><a href="#11-4-2-查看已经启动的服务组件" class="headerlink" title="11.4.2 查看已经启动的服务组件"></a>11.4.2 查看已经启动的服务组件</h3><pre><code class="hljs">openstack volume service list</code></pre><h1 id="12-控制节点安装UI服务Horizon"><a href="#12-控制节点安装UI服务Horizon" class="headerlink" title="12.控制节点安装UI服务Horizon"></a>12.控制节点安装UI服务Horizon</h1><pre><code class="hljs">yum install -y openstack-dashboardapt install -y openstack-dashboard</code></pre><h2 id="12-1-配置UI服务"><a href="#12-1-配置UI服务" class="headerlink" title="12.1 配置UI服务"></a>12.1 配置UI服务</h2><h3 id="12-1-1-备份原配置文件"><a href="#12-1-1-备份原配置文件" class="headerlink" title="12.1.1 备份原配置文件"></a>12.1.1 备份原配置文件</h3><pre><code class="hljs">cp /etc/openstack-dashboard/local_settings /etc/openstack-dashboard/local_settings.bak</code></pre><h3 id="12-1-2-修改配置文件"><a href="#12-1-2-修改配置文件" class="headerlink" title="12.1.2 修改配置文件"></a>12.1.2 修改配置文件</h3><pre><code class="hljs">vi /etc/openstack-dashboard/local_settingsOPENSTACK_HOST=&quot;192.168.1.100&#39;OPENSTACK_KEYSTONE_URL=&quot;http://%s:5000/v3 % OPENSTACK_HOST&quot;OPENSTACK_KEYSTONE_DEFAULT_ROLE=&quot;user&quot;ALLOWED_HOSTS = [&#39;*&#39;,]SESSION_ENGINE=&#39;django.contrib.sessions.backends.cache&#39;CACHE=&#123;    &#39;default&#39;:&#123;        &#39;BACKEND&#39;:&#39;django.core.cache.backends.memcached.MemcachedCache&#39;,        &#39;LOCALTION&#39;:&#39;controller:11211&#39;,    &#125;&#125;OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT=TrueOPENSTACK_KEYSTONE_DEFAULT_DOMAIN=&quot;default&quot;TIME_ZONE=&quot;Asia/Shanghai&quot;</code></pre><h2 id="12-2-配置Apache服务器"><a href="#12-2-配置Apache服务器" class="headerlink" title="12.2 配置Apache服务器"></a>12.2 配置Apache服务器</h2><pre><code class="hljs">echo &#39;WSGIApplicationGroup %&#123;GLOBAL&#125;&#39; &gt;&gt; /etc/httpd/conf.d/openstack-dashboard.conf</code></pre><h2 id="12-3-重启服务"><a href="#12-3-重启服务" class="headerlink" title="12.3 重启服务"></a>12.3 重启服务</h2><pre><code class="hljs">systemctl restart memcached.servicesystemctl restart httpd.service</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/hu_zhe_kan/article/details/101698228">https://blog.csdn.net/hu_zhe_kan/article/details/101698228</a></li><li><a href="https://blog.csdn.net/weixin_45724795/article/details/104759381">https://blog.csdn.net/weixin_45724795/article/details/104759381</a></li><li><a href="https://blog.csdn.net/qq_35550345/article/details/87855549">https://blog.csdn.net/qq_35550345/article/details/87855549</a></li><li><a href="https://blog.csdn.net/dylloveyou/article/details/80698420">https://blog.csdn.net/dylloveyou/article/details/80698420</a></li><li><a href="https://openeuler.gitee.io/openstack/install/openEuler-22.03-LTS-SP1/OpenStack-wallaby">https://openeuler.gitee.io/openstack/install/openEuler-22.03-LTS-SP1/OpenStack-wallaby</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>虚拟化</tag>
      
      <tag>私有云</tag>
      
      <tag>Openstack</tag>
      
      <tag>公有云</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes集群性能优化策略</title>
    <link href="/linux/KubernetesOptimization/"/>
    <url>/linux/KubernetesOptimization/</url>
    
    <content type="html"><![CDATA[<p>kubernetes集群默认配置已足够满足常见的中小规模的业务场景，但仍然建议将生产环境集群各组件的参数及系统内核参数进行适当的调整与优化，以应对高并发高负载的场景，提高集群运行的稳定性和故障切换能力，保障业务的连续性与稳定性</p><h1 id="1-系统参数配置"><a href="#1-系统参数配置" class="headerlink" title="1.系统参数配置"></a>1.系统参数配置</h1><pre><code class="hljs">sudo vi /etc/sysctl.conf# 设置系统级别文件句柄打开的最大数量，用于解决Too many open files和Socket/File: Can’t open so many files报错fs.file-max = 1000000# 设置单个用户ID可创建的inotify instatnces最大量，默认为128fs.inotify.max_user_instances = 524288# 设置每个inotify instance相关联的watches，默认为8192fs.inotify.max_user_watches = 524288# 设置内核允许的最大跟踪连接条目，即netfilter同时处理的任务数net.netfilter.nf_conntrack_max = 10485760# 设置内核netfilter处理tcp会话的超时时间，默认为432000，即5天net.netfilter.nf_conntrack_tcp_timeout_established = 300# 设置内核netfilter哈希表的大小，64位CPU8G内存默认为65536，以此类推net.netfilter.nf_conntrack_buckets = 655360# 设置每个网卡缓存报文的最大量，默认为1000，适用于网卡负载高于内核处理速度的场景，防止网络丢包net.core.netdev_max_backlog = 10000# 设置ARP高速缓存垃圾收集器的运行阈值，即层数超过该值则运行，默认为128，适用于内核维护的arp表过于庞大的场景# net.ipv4.neigh.default.gc_thresh1 = 1024# 设置ARP高速缓存垃圾收集器的软限制，即超过此阈值等待5秒开始运行垃圾收集器，默认为512，适用于内核维护的arp表过于庞大的场景# net.ipv4.neigh.default.gc_thresh2 = 4096# 设置ARP高速缓存垃圾收集器的硬限制，即超过此阈值立即开始运行垃圾收集器，默认为1024，适用于内核维护的arp表过于庞大的场景# net.ipv4.neigh.default.gc_thresh3 = 8192</code></pre><h1 id="2-Etcd参数配置"><a href="#2-Etcd参数配置" class="headerlink" title="2.Etcd参数配置"></a>2.Etcd参数配置</h1><h2 id="2-1-存储配额"><a href="#2-1-存储配额" class="headerlink" title="2.1 存储配额"></a>2.1 存储配额</h2><p>Etcd数据库默认磁盘空间配额大小为2G，超过将不会再写入数据，–quota-backend-bytes配置项用于设置配额，最大支持8G</p><pre><code class="hljs">--quota-backend-bytes 8589934592 </code></pre><h2 id="2-2-进程优先级"><a href="#2-2-进程优先级" class="headerlink" title="2.2 进程优先级"></a>2.2 进程优先级</h2><p>Etcd数据库写入数据时，为规避其他进程磁盘IO的影响，防止请求超时和临时Leader的丢失，建议设置etcd进程的IO调度优先级，以保集群的障稳定性</p><pre><code class="hljs">sudo ionice -c2 -n0 -p $(pgrep etcd) </code></pre><h2 id="2-3-请求数据"><a href="#2-3-请求数据" class="headerlink" title="2.3 请求数据"></a>2.3 请求数据</h2><p>Etcd数据库被设计用于元数据的小键值对的处理，数据量较大的请求可能会引发请求延迟，请求的数据量默认被限制为1.5MiB，可通过配置项–max-request-bytesetcd进行设置</p><h2 id="2-4-历史记录压缩"><a href="#2-4-历史记录压缩" class="headerlink" title="2.4 历史记录压缩"></a>2.4 历史记录压缩</h2><p>ETCD数据库中存储有多个版本数据，随着写入的主键增加，历史版本将会越来越多，且不会自动清理，可通过压缩与清理历史数据进行优化</p><pre><code class="hljs">--auto-compaction-mode--auto-compaction-retention</code></pre><h2 id="2-5-operator-etcd集群"><a href="#2-5-operator-etcd集群" class="headerlink" title="2.5 operator etcd集群"></a>2.5 operator etcd集群</h2><p>operator是CoreOS推出的旨在简化复杂有状态应用管理的框架，动态地感知应用状态的控制器，可以更加自动化智能化地管理与维护Etcd集群</p><h1 id="3-kubelet组件配置"><a href="#3-kubelet组件配置" class="headerlink" title="3.kubelet组件配置"></a>3.kubelet组件配置</h1><h2 id="3-1-设置并发度"><a href="#3-1-设置并发度" class="headerlink" title="3.1 设置并发度"></a>3.1 设置并发度</h2><p>kubelet默认情况下以串行方式拉取镜像，可将配置项–serialize-image-pulls设为false，改成并行方式，以提高镜像镜像速度，适用于docker daemon版本高于1.9，且未配置aufs存储的场景</p><pre><code class="hljs">--serialize-image-pulls=false</code></pre><h2 id="3-2-限制镜像拉取时长"><a href="#3-2-限制镜像拉取时长" class="headerlink" title="3.2 限制镜像拉取时长"></a>3.2 限制镜像拉取时长</h2><p>image-pull-progress-deadline配置项用于设置镜像拉取的超时时长，默认为1分钟，可进行调整以优化镜像拉取速度</p><pre><code class="hljs">--image-pull-progress-deadline=30</code></pre><h2 id="3-3-限制单节点运行的Pod量"><a href="#3-3-限制单节点运行的Pod量" class="headerlink" title="3.3 限制单节点运行的Pod量"></a>3.3 限制单节点运行的Pod量</h2><p>kubelet单节点允许运行的最大Pod数默认为110，可根据实际需要进行调整</p><pre><code class="hljs">--max-pods=110</code></pre><h1 id="4-ApiServer组件配置"><a href="#4-ApiServer组件配置" class="headerlink" title="4.ApiServer组件配置"></a>4.ApiServer组件配置</h1><h2 id="4-1-参数配置"><a href="#4-1-参数配置" class="headerlink" title="4.1 参数配置"></a>4.1 参数配置</h2><pre><code class="hljs"># 设置单位时间内最大非修改型请求数，即读操作数，默认为400--max-requests-inflight# 设置单位时间内最大修改型请求数，即写操作数，默认为200--max-mutating-requests-inflight# 设置集群资源的watch缓存，默认为100，资源数量较多时可相应增加--watch-cache-sizes</code></pre><ul><li>注：读&#x2F;写最大请求量的设置建议按照集群Node节点的数量进行配置，节点数大于3000，则配置为3000&#x2F;1000；节点数大于1000小于3000，则配置为1500&#x2F;500</li></ul><h2 id="4-2-内存配额"><a href="#4-2-内存配额" class="headerlink" title="4.2 内存配额"></a>4.2 内存配额</h2><pre><code class="hljs"># 设置ApiServer程序所占内存，建议按照公式进行配置--target-ram-mb=node_nums * 60</code></pre><h1 id="5-ControllerManager组件配置"><a href="#5-ControllerManager组件配置" class="headerlink" title="5.ControllerManager组件配置"></a>5.ControllerManager组件配置</h1><h2 id="5-1-参数配置"><a href="#5-1-参数配置" class="headerlink" title="5.1 参数配置"></a>5.1 参数配置</h2><pre><code class="hljs"># 设置Node节点健康检查时间间隔，默认为5s--node-monitor-period# 设置Node节点健康检查超时时长，超过即将进入ConditionUnknown状态，默认为40s--node-monitor-grace-period# 设置Node节点处于NotReady状态后开始驱逐Pod的超时时长，默认为5m--pod-eviction-timeout# 设置集群规模判断依据，默认为50，即50个节点以上的集群为大集群--large-cluster-size-threshold # 设置集群故障节点数超限率，默认为55%--unhealthy-zone-threshold# 设置Node节点Pod驱逐操作的频率，默认0.1个/s，即每10s最多驱逐某一个节点的Pod，以减缓Master节点故障导致大批量的Node节点异常驱逐--node-eviction-rate# 设置集群节点故障率超限后的Pod驱逐操作的频率，大集群超限后将会降为0.001，小集群则直接停止驱逐--secondary-node-eviction-rate</code></pre><h2 id="5-2-ApiServer通信限制"><a href="#5-2-ApiServer通信限制" class="headerlink" title="5.2 ApiServer通信限制"></a>5.2 ApiServer通信限制</h2><pre><code class="hljs"># 设置ApiServer组件会话QPS，默认为20--kube-api-qps=100# 设置ApiServer组件会话并发数，默认为30--kube-api-burst=150</code></pre><h1 id="6-Scheduler组件配置"><a href="#6-Scheduler组件配置" class="headerlink" title="6.Scheduler组件配置"></a>6.Scheduler组件配置</h1><h2 id="6-1-设置调度策略文件"><a href="#6-1-设置调度策略文件" class="headerlink" title="6.1 设置调度策略文件"></a>6.1 设置调度策略文件</h2><pre><code class="hljs"># 设置调度策略json文件，不指定则表示默认的调度策略--policy-config-file </code></pre><h2 id="6-2-ApiServer通信限制"><a href="#6-2-ApiServer通信限制" class="headerlink" title="6.2 ApiServer通信限制"></a>6.2 ApiServer通信限制</h2><pre><code class="hljs">--kube-api-qps=100--kube-api-burst=150</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/ywq935/article/details/103124541">https://blog.csdn.net/ywq935/article/details/103124541</a></li><li><a href="https://blog.csdn.net/tgzh123/article/details/134857593">https://blog.csdn.net/tgzh123/article/details/134857593</a></li><li><a href="https://blog.csdn.net/ver_mouth__/article/details/126120802">https://blog.csdn.net/ver_mouth__/article/details/126120802</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ansible自动化运维工具性能调优</title>
    <link href="/linux/AnsibleOptimization/"/>
    <url>/linux/AnsibleOptimization/</url>
    
    <content type="html"><![CDATA[<p>Ansible自动化运维工具基于SSH连接对被控主机进行控制，无需安装客户端软件，但随着着被控主机数量的增长，其执行效率将会越来越慢，因此对SSH进行调优能提高Ansible的工作效率。此外，Ansible执行Playbook时都会收集被控主机的信息，通常耗时较长，也是影响性能的重要因素，若是环境允许建议进行关闭</p><h1 id="1-OpenSSH调优"><a href="#1-OpenSSH调优" class="headerlink" title="1.OpenSSH调优"></a>1.OpenSSH调优</h1><p>默认情况下，OpenSSH服务基于安全考虑，服务器根据客户端的IP地址进行DNS反向解析以得到客户端的主机名，并将主机名经DNS查询解析为IP地址，最后验证IP地址的一致性。这个过程相当耗费时间，可以进行关闭以加速SSH连接速度</p><pre><code class="hljs">vi /etc/ssh/sshd_configUseDNS no</code></pre><h1 id="2-Ansible-SSH连接调优"><a href="#2-Ansible-SSH连接调优" class="headerlink" title="2.Ansible SSH连接调优"></a>2.Ansible SSH连接调优</h1><h2 id="2-1-关闭密钥检测"><a href="#2-1-关闭密钥检测" class="headerlink" title="2.1 关闭密钥检测"></a>2.1 关闭密钥检测</h2><p>SSH登录远程主机时默认将检查远程主机的公钥，且将之记录于~&#x2F;.ssh&#x2F;known_hosts文件，以便于下次访问时OpenSSH进行核对，若如果公钥不同，则将发出警告，将Ansible配置文件的StrictHostKeyChecking参数设为False即可关闭该功能，以提高运行效率</p><h2 id="2-2-SSH-pipelining加速"><a href="#2-2-SSH-pipelining加速" class="headerlink" title="2.2 SSH pipelining加速"></a>2.2 SSH pipelining加速</h2><p>Ansible基于兼容sudo配置的考虑，默认关闭SSH的pipelining功能，若不适用sudo可考虑开启该功能，以减少SSH在被控主机上执行任务的连接数，从而加快SSH连接的速度</p><pre><code class="hljs">pipelining = True</code></pre><h2 id="2-3-SSH长连接"><a href="#2-3-SSH长连接" class="headerlink" title="2.3 SSH长连接"></a>2.3 SSH长连接</h2><p>Openssh自5.6版本之后支持多路复用功能，即持久化的Socket，一次验证多次通信，以节省SSH连接每次验证和创建的时间，Ansible执行速度得以明显提高</p><pre><code class="hljs">vi /etc/ansible/ansible.cfg# ControlPersist，表示ssh_args = -C -o ControlMaster=auto -o ControlPersist=10d</code></pre><h1 id="3-Ansible-Facts缓存调优"><a href="#3-Ansible-Facts缓存调优" class="headerlink" title="3.Ansible Facts缓存调优"></a>3.Ansible Facts缓存调优</h1><h2 id="3-1-关闭Facts收集"><a href="#3-1-关闭Facts收集" class="headerlink" title="3.1 关闭Facts收集"></a>3.1 关闭Facts收集</h2><p>Palybook文件直接配置</p><pre><code class="hljs">gather_facts: no</code></pre><h2 id="3-2-配置Redis缓存Facts"><a href="#3-2-配置Redis缓存Facts" class="headerlink" title="3.2 配置Redis缓存Facts"></a>3.2 配置Redis缓存Facts</h2><p>Ansible默认将Facts信息写入内存，若被控主机数量庞大，该过程将会耗费大量时间，可将之写入到Redis以便随时取用</p><h3 id="3-2-1-安装Redis"><a href="#3-2-1-安装Redis" class="headerlink" title="3.2.1 安装Redis"></a>3.2.1 安装Redis</h3><h3 id="3-2-2-安装Redis模块"><a href="#3-2-2-安装Redis模块" class="headerlink" title="3.2.2 安装Redis模块"></a>3.2.2 安装Redis模块</h3><pre><code class="hljs">sudo pip install redis==3.4.1</code></pre><h3 id="3-2-3-配置Redis缓存"><a href="#3-2-3-配置Redis缓存" class="headerlink" title="3.2.3 配置Redis缓存"></a>3.2.3 配置Redis缓存</h3><pre><code class="hljs">vi /etc/ansible/ansible.cfg[defaults]gathering = smartfact_caching = redis# 设置缓存时长，单位为秒fact_caching_timeout = 86400# 设置Redis连接，格式为：IP:端口号:数据库:连接密码fact_caching_connection = localhost:6379:6</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.cnblogs.com/fengyuanfei/p/13826469.html">https://www.cnblogs.com/fengyuanfei/p/13826469.html</a></li><li><a href="https://blog.csdn.net/kakaops_qing/article/details/109551250">https://blog.csdn.net/kakaops_qing/article/details/109551250</a></li><li><a href="https://blog.csdn.net/weixin_40228200/article/details/123603649">https://blog.csdn.net/weixin_40228200/article/details/123603649</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Ansible</tag>
      
      <tag>自动化运维</tag>
      
      <tag>云计算</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Java项目源码包基于Maven工具的构建</title>
    <link href="/linux/Maven/"/>
    <url>/linux/Maven/</url>
    
    <content type="html"><![CDATA[<p>Maven，Apache旗下由Java开发的开源项目管理和构建工具，基于项目对象模型(POM)的概念，通过描述信息管理项目的构建、报告和文档，特别适用于Java项目的构建与依赖管理，也可用于其他项目，如C#、Ruby和Scala等等</p><h1 id="1-配置Java环境"><a href="#1-配置Java环境" class="headerlink" title="1.配置Java环境"></a>1.配置Java环境</h1><h1 id="2-下载Maven安装包"><a href="#2-下载Maven安装包" class="headerlink" title="2.下载Maven安装包"></a>2.下载Maven安装包</h1><pre><code class="hljs">wget https://dlcdn.apache.org/maven/maven-3/3.9.6/binaries/apache-maven-3.9.6-bin.tar.gz</code></pre><h1 id="3-安装Maven"><a href="#3-安装Maven" class="headerlink" title="3.安装Maven"></a>3.安装Maven</h1><pre><code class="hljs">    tar -xzvf apache-maven-3.9.6-bin.tar.gz    sudo mv  apache-maven-3.9.6-bin /usr/local/maven    sudo ln -s /usr/local/maven/bin/mvn /usr/local/bin/mvn</code></pre><h1 id="4-配置仓库"><a href="#4-配置仓库" class="headerlink" title="4.配置仓库"></a>4.配置仓库</h1><h2 id="4-1-配置本地仓库"><a href="#4-1-配置本地仓库" class="headerlink" title="4.1 配置本地仓库"></a>4.1 配置本地仓库</h2><pre><code class="hljs">sudo mkdir -p /usr/local/maven/repositorysudo vi /usr/local/maven/conf/settings.xml&lt;localRepository&gt;/usr/local/maven/repository&lt;/localRepository&gt;</code></pre><h2 id="4-2-配置远程仓库"><a href="#4-2-配置远程仓库" class="headerlink" title="4.2 配置远程仓库"></a>4.2 配置远程仓库</h2><pre><code class="hljs">sudo vi /usr/local/maven/conf/settings.xml&lt;mirror&gt;    &lt;id&gt;alimaven&lt;/id&gt;    &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;    &lt;name&gt;aliyun maven&lt;/name&gt;    &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;&lt;/mirror&gt;</code></pre><h1 id="5-下载开源项目Jpress的Java源码包"><a href="#5-下载开源项目Jpress的Java源码包" class="headerlink" title="5.下载开源项目Jpress的Java源码包"></a>5.下载开源项目Jpress的Java源码包</h1><h1 id="6-编译Jpress项目"><a href="#6-编译Jpress项目" class="headerlink" title="6.编译Jpress项目"></a>6.编译Jpress项目</h1><pre><code class="hljs">unzip jpress-v5.x.zipcd jpress# 源码包打包sudo mvn clean package# 将源代码打包为jar包，并下载到本地仓库# sudo mvn install</code></pre><h1 id="7-验证编译文件"><a href="#7-验证编译文件" class="headerlink" title="7.验证编译文件"></a>7.验证编译文件</h1><pre><code class="hljs">ls -l starter-tomcat/target</code></pre><h1 id="8-Tomcat部署Jpress，验证编译结果"><a href="#8-Tomcat部署Jpress，验证编译结果" class="headerlink" title="8.Tomcat部署Jpress，验证编译结果"></a>8.Tomcat部署Jpress，验证编译结果</h1><pre><code class="hljs">sudo cp starter-tomcat/target/starter-tomcat-5.0.war /usr/local/tomcat/webapps/jpress.war</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="http://doc.jpress.cn/manual/maven_config.html">http://doc.jpress.cn/manual/maven_config.html</a></li><li><a href="https://www.runoob.com/maven/maven-setup.html">https://www.runoob.com/maven/maven-setup.html</a></li><li><a href="https://blog.csdn.net/m0_63684495/article/details/129046405">https://blog.csdn.net/m0_63684495/article/details/129046405</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Java</tag>
      
      <tag>Maven</tag>
      
      <tag>Tomcat</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ansible自动化运维工具Role详解</title>
    <link href="/linux/AnsibleRole/"/>
    <url>/linux/AnsibleRole/</url>
    
    <content type="html"><![CDATA[<p>Role，即角色，用于Ansible自动化运维工具Playbook的层次化、结构化地组织与编排，并根据层次结构自动装载变量、tasks及handlers等。实际上，Role分别将变量、文件、任务、模板及处理器存放于单独的目录，再通过include指令完成调用，最终将这些元素组合为一个整体。Role特别适用于复杂的大型场景，以免出现不利于管理与维护的几百上千行的Playbook文件的出现，提高代码的复用度</p><h1 id="目录结构"><a href="#目录结构" class="headerlink" title="目录结构"></a>目录结构</h1><p>Ansible的Roles配置存放于&#x2F;etc&#x2F;ansible&#x2F;roles，其下每个子文件夹即为一个Role，每个Role目录结构通常如下：</p><h2 id="1-tasks-x2F"><a href="#1-tasks-x2F" class="headerlink" title="1.tasks&#x2F;"></a>1.tasks&#x2F;</h2><p>用于存放任务文件，其下必须存在一个main.yml文件，用于通过include指令调用其他文件</p><h2 id="2-vars-x2F"><a href="#2-vars-x2F" class="headerlink" title="2.vars&#x2F;"></a>2.vars&#x2F;</h2><p>用于存放变量文件，其下必须存在一个main.yml文件，用于通过include指令调用其他文件。若整个roles配置变量使用较少，只使用一个main.yml文件即可，否则就应将变量进行分类后存放于不同的目录，再通过main.yml进行调用</p><h2 id="3-files-x2F"><a href="#3-files-x2F" class="headerlink" title="3.files&#x2F;"></a>3.files&#x2F;</h2><p>用于存放copy或script模块的脚本文件</p><h2 id="4-templates-x2F"><a href="#4-templates-x2F" class="headerlink" title="4.templates&#x2F;"></a>4.templates&#x2F;</h2><p>用于存放此Role需要使用的jinjia2模板文件</p><h2 id="5-handlers-x2F"><a href="#5-handlers-x2F" class="headerlink" title="5.handlers&#x2F;"></a>5.handlers&#x2F;</h2><p>用于存放Role中触发条件后执行的动作，也必须存在一个main.yml文件</p><h2 id="6-meta-x2F"><a href="#6-meta-x2F" class="headerlink" title="6.meta&#x2F;"></a>6.meta&#x2F;</h2><p>用于存放此Role的特殊设定及其依赖关系，也必须存在一个main.yml文件</p><h2 id="7-default-x2F"><a href="#7-default-x2F" class="headerlink" title="7.default&#x2F;"></a>7.default&#x2F;</h2><p>用于存放此Role的设定默认变量，也必须存在一个main.yml文件</p><hr><h1 id="1-创建项目"><a href="#1-创建项目" class="headerlink" title="1.创建项目"></a>1.创建项目</h1><pre><code class="hljs">mkdir -p /home/project/ansible/kubernetes/rolescd /home/project/ansible/kubernetes/roles</code></pre><h1 id="2-系统初始化配置"><a href="#2-系统初始化配置" class="headerlink" title="2.系统初始化配置"></a>2.系统初始化配置</h1><pre><code class="hljs">ansible-galaxy init system </code></pre><h2 id="2-1-创建Playbook"><a href="#2-1-创建Playbook" class="headerlink" title="2.1 创建Playbook"></a>2.1 创建Playbook</h2><pre><code class="hljs">vi system/tasks/main.yml- name: 系统软件包更新  shell: apt update -y &amp;&amp; apt upgrade -y &amp;&amp; apt autoremove -y- name: 安装依赖包  apt: name=&quot;&#123;&#123; item &#125;&#125;&quot; state: latest  loop:    - curl    - gnupg2    - ipvsadm    - ca-certificates    - apt-transport-https- name: 导入阿里云apt源密钥  shell: curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add -- name: 禁用swap  shell: sed -ri &#39;s/.*swap.*/#&amp;/&#39; /etc/fstab &amp;&amp; swapoff -a- name: 开启路由转发  copy: src=k8s.conf dest=/etc/sysctl.d- name: 生效配置文件  shell: sysctl -p</code></pre><h2 id="2-2-创建配置文件"><a href="#2-2-创建配置文件" class="headerlink" title="2.2 创建配置文件"></a>2.2 创建配置文件</h2><pre><code class="hljs">vi files/k8s.confnet.ipv4.ip_forward=1net.bridge.bridge-nf-call-iptables=1net.bridge.bridge-nf-call-ip6tables=1</code></pre><h1 id="3-Docker配置"><a href="#3-Docker配置" class="headerlink" title="3.Docker配置"></a>3.Docker配置</h1><pre><code class="hljs">ansible-galaxy init docker</code></pre><h2 id="3-1-创建Playbook"><a href="#3-1-创建Playbook" class="headerlink" title="3.1 创建Playbook"></a>3.1 创建Playbook</h2><pre><code class="hljs">vi docker/tasks/main.yaml- name: 解压docker安装包  unarchive: src=docker-19.03.12.tgz dest=/home/sword- name: 安装docker  shell: mv docker/* /usr/bin &amp;&amp; mkdir -p /etc/docker &amp;&amp; mkdir -p /root/.docker- name: 配置docker仓库加速器  copy: src=daemon.json dest=/etc/docker  - name: 创建创建docker服务启动脚本  copy: src=docker.service dest=/lib/systemd/system- name: 启动docker  service: name=docker state=started enabled=yes</code></pre><h2 id="3-2-创建配置文件"><a href="#3-2-创建配置文件" class="headerlink" title="3.2 创建配置文件"></a>3.2 创建配置文件</h2><h3 id="3-2-1-下载安装包"><a href="#3-2-1-下载安装包" class="headerlink" title="3.2.1 下载安装包"></a>3.2.1 下载安装包</h3><pre><code class="hljs">cp /home/works/Linux/software/docker-19.03.12.tgz docker/files/</code></pre><h3 id="3-2-2-创建配置文件"><a href="#3-2-2-创建配置文件" class="headerlink" title="3.2.2 创建配置文件"></a>3.2.2 创建配置文件</h3><pre><code class="hljs">vi docker/files/daemon.json&#123;    &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],  &quot;registry-mirrors&quot;: [      &quot;https://registry.docker-cn.com&quot;,      &quot;https://docker.mirrors.ustc.edu.cn&quot;    ]  &#125;</code></pre><h3 id="3-2-3-创建启动脚本"><a href="#3-2-3-创建启动脚本" class="headerlink" title="3.2.3 创建启动脚本"></a>3.2.3 创建启动脚本</h3><pre><code class="hljs">vi docker/files/docker.service[Unit]Description=Docker Application Container Engine  Documentation=https://docs.docker.com  After=network-online.target firewalld.service  Wants=network-online.target  [Service]  Type=notify  ExecStart=/usr/bin/dockerd  ExecReload=/bin/kill -s HUP $MAINPID  LimitNOFILE=infinity  LimitNPROC=infinity  LimitCORE=infinity  TimeoutStartSec=0  Delegate=yes  KillMode=process  Restart=on-failure  StartLimitBurst=3  StartLimitInterval=60s  [Install]  WantedBy=multi-user.target </code></pre><h1 id="4-kubernetes集群组件配置"><a href="#4-kubernetes集群组件配置" class="headerlink" title="4.kubernetes集群组件配置"></a>4.kubernetes集群组件配置</h1><pre><code class="hljs">ansible-galaxy init kubeadm</code></pre><h2 id="4-1-创建Playbook"><a href="#4-1-创建Playbook" class="headerlink" title="4.1 创建Playbook"></a>4.1 创建Playbook</h2><pre><code class="hljs">vi kubeadm/tasks/main.yml- name: 配置kubernetes阿里云apt源  copy: src=kubernetes.list dest=/etc/apt/sources.list.d- name: 安装kubeadm、kubelet、kubectl  shell: apt update &amp;&amp; apt install -y kubelet=1.20.12-00 kubeadm=1.20.12-00 &amp;&amp; systemctl enable kubelet</code></pre><h2 id="4-2-创建配置文件"><a href="#4-2-创建配置文件" class="headerlink" title="4.2 创建配置文件"></a>4.2 创建配置文件</h2><pre><code class="hljs">vi kubeadm/files/kubernetes.listdeb https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial main</code></pre><h1 id="5-高可用配置"><a href="#5-高可用配置" class="headerlink" title="5.高可用配置"></a>5.高可用配置</h1><pre><code class="hljs">ansible-galaxy init ha</code></pre><h2 id="5-1-创建Playbook"><a href="#5-1-创建Playbook" class="headerlink" title="5.1 创建Playbook"></a>5.1 创建Playbook</h2><pre><code class="hljs">vi ha/tasks/main.yml- name: master节点安装haroxy、keepalived  apt:     name:      - haproxy      - keepalived    state: latest- name: master节点创建haproxy配置文件  copy: src=haproxy.cfg dest=/etc/haproxy- name: master节点启动haproxy  service: name=haproxy state=started enabled=yes- name: master节点创建keepalived配置文件  template: src=keepalived.conf.j2 dest=/etc/keepalived/keepalived.conf  notify: 重启keepalived- name: master节点创建haproxy服务状态监控脚本  copy: src=haproxy_check.sh dest=/etc/keepalived</code></pre><h2 id="5-2-创建配置文件"><a href="#5-2-创建配置文件" class="headerlink" title="5.2 创建配置文件"></a>5.2 创建配置文件</h2><pre><code class="hljs">vi ha/files/haproxy.cfggloballog    127.0.0.1 local2# chroot    /usr/local/haproxypidfile    /var/run/haproxy.piduser    swordgroup    sworddaemonnbproc    1maxconn   1024# node    haproxy-001stats socket    /var/lib/haproxy/statsdefaults  log    global  mode    http  option    httplog  option    httpclose  option    forwardfor except 127.0.0.0/8                option    dontlognull  option    redispatch  option    abortonclose  http-reuse    safe  retries    3  timeout client    10s  timeout http-request    2s  timeout http-keep-alive 10s  timeout queue    10s  timeout connect    1s  timeout check    2s  timeout server    3slisten monitor  mode    http  bind    :1443  stats    enable  stats    hide-version  stats    refresh 10       　　　　  stats uri    /status  stats realm    Haproxy Manager  stats auth    admin:admin  stats admin if    TRUE frontend    master  mode    tcp  option   tcplog  bind    0.0.0.0:8443  default_backend    api-servers   backend api-servers    mode    tcp    balance    roundrobin    stick-table type ip size 200k expire 30m    stick on src    server master01 172.100.100.201:6443 check port 6443 inter 2000 rise 2 fall 3    server master02 172.100.100.202:6443 check port 6443 inter 2000 rise 2 fall 3    server master03 172.100.100.203:6443 check port 6443 inter 2000 rise 2 fall 3</code></pre><hr><pre><code class="hljs">vi ha/files/haproxy_check.sh#!/bin/bashpid=`ps -ef|grep haproxy|grep -v grep|wc -l`port=`netstat -anp|grep :8443|grep LISTEN|wc -l`if [[ $pid -gt 1 &amp;&amp; $port -gt 0 ]]  then    exit 0  else    pkill keepalivedfi</code></pre><h2 id="5-3-创建模版文件"><a href="#5-3-创建模版文件" class="headerlink" title="5.3 创建模版文件"></a>5.3 创建模版文件</h2><pre><code class="hljs">vi ha/templates/keepalived.conf.j2global_defs &#123;  notification_email  &#123;    admin@sword.com  &#125;  notification_email_from  smtp_server 127.0.0.1  smtp_connect_timeout 30  router_id &#123;&#123; ansible_hostname &#125;&#125;&#125;vrrp_script check_haproxy &#123;  script &quot;/etc/keepalived/haproxy_check.sh&quot;  interval 2  weight -10&#125;vrrp_instance Haproxy &#123;  state &#123;&#123; keepalived_role &#125;&#125;  interface eth0  virtual_router_id 51  priority &#123;&#123; keepalived_rank &#125;&#125;  advert_int 1  authentication &#123;    auth_type PASS    auth_pass 123456  &#125;  virtual_ipaddress &#123;    172.100.100.210/24  &#125;  track_script &#123;    check_haproxy  &#125;&#125;</code></pre><h2 id="5-4-创建变量"><a href="#5-4-创建变量" class="headerlink" title="5.4 创建变量"></a>5.4 创建变量</h2><pre><code class="hljs">vi /etc/ansible/hosts [master]master01 keepalived_role=MASTER keepalived_rank=100master02 keepalived_role=BACKUP keepalived_rank=80master03 keepalived_role=BACKUP keepalived_rank=60</code></pre><h2 id="5-5-创建处理器"><a href="#5-5-创建处理器" class="headerlink" title="5.5 创建处理器"></a>5.5 创建处理器</h2><pre><code class="hljs">vi ha/handlers/main.yml- name: 重启keepalived  systemd:    name: keepalived    state: restarted</code></pre><h1 id="6-创建项目调度入口脚本"><a href="#6-创建项目调度入口脚本" class="headerlink" title="6.创建项目调度入口脚本"></a>6.创建项目调度入口脚本</h1><pre><code class="hljs">vi /home/project/ansible/kubernetes/site.yml- hosts: k8s  become: yes  roles:    - role: system    - role: docker    - role: kubeadm    - role: ha      when: ansible_hostname in groups [&#39;master&#39;]</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.cnblogs.com/FGdeHB/p/16685985.html">https://www.cnblogs.com/FGdeHB/p/16685985.html</a></li><li><a href="https://www.cnblogs.com/caodan01/p/14859879.html">https://www.cnblogs.com/caodan01/p/14859879.html</a></li><li><a href="https://blog.csdn.net/Promise_410/article/details/118520364">https://blog.csdn.net/Promise_410/article/details/118520364</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Ansible</tag>
      
      <tag>自动化运维</tag>
      
      <tag>云计算</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ansible自动化运维工具Template与Handler详解</title>
    <link href="/linux/AnsibleTemplate/"/>
    <url>/linux/AnsibleTemplate/</url>
    
    <content type="html"><![CDATA[<p>Template，即模版，Playbook生成目标文件的模版，即由jinjia2语言编写的后缀名为.j2的模板文件可调用变量替换其中的内容动态生成被控主机的配置文件，配合Handler处理器触发服务重启，从而生效配置</p><h1 id="1-创建模版文件"><a href="#1-创建模版文件" class="headerlink" title="1.创建模版文件"></a>1.创建模版文件</h1><pre><code class="hljs"># 模版文件存储目录为templates，与Playbook文件平级mkdir -p test/templatesvi test/templates/keepalived.conf.j2global_defs &#123;  notification_email  &#123;    admin@sword.com  &#125;  notification_email_from  smtp_server 127.0.0.1  smtp_connect_timeout 30  router_id &#123;&#123; ansible_hostname &#125;&#125;&#125;vrrp_script check_haproxy &#123;  script &quot;/etc/keepalived/haproxy_check.sh&quot;  interval 2  weight -10&#125;vrrp_instance Haproxy &#123;  state &#123;&#123;  &#125;&#125;  interface eth0  virtual_router_id 51  priority &#123;&#123;  &#125;&#125;  advert_int 1  authentication &#123;    auth_type PASS    auth_pass 123456  &#125;  virtual_ipaddress &#123;    172.16.100.150/24  &#125;  track_script &#123;    check_haproxy  &#125;&#125;</code></pre><h1 id="2-定义变量"><a href="#2-定义变量" class="headerlink" title="2.定义变量"></a>2.定义变量</h1><pre><code class="hljs">vi test/group_vars/master.ymlmaster01: 100master02: 80master03: 60</code></pre><h1 id="3-生成keepalived配置文件"><a href="#3-生成keepalived配置文件" class="headerlink" title="3.生成keepalived配置文件"></a>3.生成keepalived配置文件</h1><pre><code class="hljs">- name: Keepalived安装  hosts: master  remote_user: sword  become: yes  become_user: root  tasks:    - name: 安装keepalived      apt:         name:          - keepalived        state: latest    - name: 创建Keepalived配置文件      template: src=keepalived.conf.j2 dest=/etc/keepalived/keepalived.conf</code></pre><h1 id="4-配置Handler"><a href="#4-配置Handler" class="headerlink" title="4.配置Handler"></a>4.配置Handler</h1><pre><code class="hljs"> - name: Keepalived安装  hosts: master  remote_user: sword  become: yes  become_user: root  tasks:    - name: 安装keepalived      apt:         name:          - keepalived        state: latest    - name: 创建Keepalived配置文件      template: src=keepalived.conf.j2 dest=/etc/keepalived/keepalived.conf      notify: 重启keepalived  handlers:  - name: 重启keepalived    service: name=keepalived state=restarted</code></pre><h1 id="5-循环调用变量"><a href="#5-循环调用变量" class="headerlink" title="5.循环调用变量"></a>5.循环调用变量</h1><pre><code class="hljs">&#123;% for vhost in nginx_vhosts %&#125;    server&#123;        listen &#123;&#123; vhost.port &#125;&#125;;        server_name &#123;&#123; vhost.server_name &#125;&#125;;        root &#123;&#123; vhost.root &#125;&#125;;    &#125;    &#123;% endfor %&#125;</code></pre><h1 id="6-实战项目：Kubernetes集群巡检"><a href="#6-实战项目：Kubernetes集群巡检" class="headerlink" title="6.实战项目：Kubernetes集群巡检"></a>6.实战项目：Kubernetes集群巡检</h1><pre><code class="hljs">- name: Kubernetes集群巡检  hosts: k8s  tasks:    - name: 磁盘空间巡检      shell: df -h      register: disk_usage    - name: 检查处理器使用率      shell: top -bn1 | grep &quot;Cpu(s)&quot; | awk &#39;&#123;print $2 + $4&#125;&#39;      register: cpu_usage    - name: 内存占用巡检      shell: free | grep Mem | awk &#39;&#123;print $3/$2 * 100.0&#125;&#39;      register: memory_usage    - name: 运行进程巡检      shell: ps -ef | grep &quot;&#123;&#123; process_name &#125;&#125;&quot; | grep -v grep      register: process_status      ignore_errors: yes    - name: 端口监听巡检      wait_for:        host: 127.0.0.1        port: &quot;&#123;&#123; item &#125;&#125;&quot;        state: started        timeout: 5      loop: &quot;&#123;&#123; ports_to_check &#125;&#125;&quot;      register: port_status      ignore_errors: yes    - name: 巡检报告      template:        src: report_template.j2        dest: &quot;&#123;&#123; report_file &#125;&#125;&quot;      vars:        disk_usage: &quot;&#123;&#123; disk_usage.stdout &#125;&#125;&quot;        cpu_usage: &quot;&#123;&#123; cpu_usage.stdout &#125;&#125;&quot;        memory_usage: &quot;&#123;&#123; memory_usage.stdout &#125;&#125;&quot;        process_status: &quot;&#123;&#123; process_status.stdout &#125;&#125;&quot;        port_status: &quot;&#123;&#123; port_status.results &#125;&#125;&quot;    - name: 巡检报告邮箱发送      mail:        host: smtp.example.com        port: 587        username: your_username        password: your_password        to: your_email@example.com        subject: &quot;巡检报告&quot;        body: &quot;&#123;&#123; lookup('file', report_file) &#125;&#125;&quot;</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/weixin_40228200/article/details/123513678">https://blog.csdn.net/weixin_40228200/article/details/123513678</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Ansible</tag>
      
      <tag>自动化运维</tag>
      
      <tag>云计算</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ansible自动化运维工具流程控制与错误处理</title>
    <link href="/linux/AnsibleCondition/"/>
    <url>/linux/AnsibleCondition/</url>
    
    <content type="html"><![CDATA[<h1 id="1-条件判断"><a href="#1-条件判断" class="headerlink" title="1.条件判断"></a>1.条件判断</h1><p>when语句表示只执行满足条件的Task，其后是python表达式，支持变量与facts</p><pre><code class="hljs">- name: 配置高可用  apt:     name:      - haproxy      - keepalived    state: latest  when: ansible_hostname in groups [&#39;master&#39;] </code></pre><h1 id="2-循环遍历"><a href="#2-循环遍历" class="headerlink" title="2.循环遍历"></a>2.循环遍历</h1><p>Ansible关键字loop和with_items用于创建循环，格式为：Playbook的task使用关键字with_items或loop设置要循环遍历的元素列表；固定变量名item的变量引用迭代项作为变量值以供调用</p><h2 id="2-1-列表循环"><a href="#2-1-列表循环" class="headerlink" title="2.1 列表循环"></a>2.1 列表循环</h2><pre><code class="hljs">- hosts: hosts  tasks:    - name: 启动服务      systemd:        name: &quot;&#123;&#123; item &#125;&#125;&quot;        state: started      with_items:        - nginx        - php-fpm        - mariadb</code></pre><h2 id="2-2-变量循环"><a href="#2-2-变量循环" class="headerlink" title="2.2 变量循环"></a>2.2 变量循环</h2><pre><code class="hljs">- hosts: hosts  vars:    services:      - nginx      - php-fpm      - mariadb  tasks:    - name: 启动服务      systemd:        name: &quot;&#123;&#123; services &#125;&#125;&quot;        state: started    # 设置循环关键字为loop，逻辑上更为直观    - name: 启动服务      systemd:        name: &quot;&#123;&#123; item &#125;&#125;&quot;        state: started      loop: &quot;&#123;&#123; services &#125;&#125;&quot;</code></pre><h2 id="2-3-字典循环"><a href="#2-3-字典循环" class="headerlink" title="2.3 字典循环"></a>2.3 字典循环</h2><pre><code class="hljs">- hosts: web_group  tasks:    - name: copy conf and code      copy:        src: &quot;&#123;&#123; item.src &#125;&#125;&quot;        dest: &quot;&#123;&#123; item.dest &#125;&#125;&quot;        mode: &quot;&#123;&#123; item.mode &#125;&#125;&quot;      with_items:        - &#123; src: &quot;./httpd.conf&quot;, dest: &quot;/etc/httpd/conf/&quot;, mode: &quot;0644&quot; &#125;        - &#123; src: &quot;./upload_file.php&quot;, dest: &quot;/var/www/html/&quot;, mode: &quot;0600&quot; &#125;</code></pre><h2 id="2-4-重试循环"><a href="#2-4-重试循环" class="headerlink" title="2.4 重试循环"></a>2.4 重试循环</h2><pre><code class="hljs">- action: shell /usr/bin/foo  register: result  until: result.stdout.find(&quot;all systems go&quot;) != -1  # 设置重试次数，默认为3  retries: 5  # 设置每次重试的超时时长，默认为10s  delay: 10</code></pre><h1 id="3-任务打标"><a href="#3-任务打标" class="headerlink" title="3.任务打标"></a>3.任务打标</h1><p>Ansible支持给Playbook的task定义一个或多个Tag标签，执行时使用-t参数来调用标签所指定的任务，以执行其中的某些Task</p><h2 id="3-1-定义标签"><a href="#3-1-定义标签" class="headerlink" title="3.1 定义标签"></a>3.1 定义标签</h2><pre><code class="hljs">- name: 配置高可用  apt:     name:      - haproxy      - keepalived    state: latest  tags:  - haproxy  - keepalived- name: 启动haproxy  service: name=haproxy state=started enabled=yes</code></pre><h2 id="3-2-调用标签"><a href="#3-2-调用标签" class="headerlink" title="3.2 调用标签"></a>3.2 调用标签</h2><pre><code class="hljs">ansible-playbook test.yml -t haproxy</code></pre><h1 id="4-错误处理"><a href="#4-错误处理" class="headerlink" title="4.错误处理"></a>4.错误处理</h1><h2 id="4-1-忽略错误"><a href="#4-1-忽略错误" class="headerlink" title="4.1 忽略错误"></a>4.1 忽略错误</h2><p>Ansible执行Playbook时会检测任务执行的返回状态，遇到错误就会终止任务的执行，后续所有的任务都将不再执行，若是执行时遇到错误仍然要求继续执行，只需调用参数ignore_errors，设为yes，即可忽略错误继续执行</p><pre><code class="hljs">- hosts: hosts  tasks:    - name:       command: /bin/false      ignore_errors: yes</code></pre><h2 id="4-2-强制执行Handler"><a href="#4-2-强制执行Handler" class="headerlink" title="4.2 强制执行Handler"></a>4.2 强制执行Handler</h2><p>force_handlers，强制执行Handler，即只有Palybook所有Task都运行正确，执行完成时才会通知执行Handlers，若执行时遇到错误就不会执行Handlers<br>，建议生产环境设为yes</p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.jianshu.com/p/9255d4e07e8d">https://www.jianshu.com/p/9255d4e07e8d</a></li><li><a href="https://www.cnblogs.com/dgp-zjz/p/15683467.html">https://www.cnblogs.com/dgp-zjz/p/15683467.html</a></li><li><a href="https://blog.csdn.net/qq_37510195/article/details/130250414">https://blog.csdn.net/qq_37510195/article/details/130250414</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Ansible</tag>
      
      <tag>自动化运维</tag>
      
      <tag>云计算</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ansible自动化运维工具变量详解</title>
    <link href="/linux/AnsibleVariables/"/>
    <url>/linux/AnsibleVariables/</url>
    
    <content type="html"><![CDATA[<p>Variables，即变量，用于管理Playbook中Tasks、Templates的动态值或多次引用值，如要创建的用户、安装的软件包、重启的服务、删除的文件以及要下载的文档等等，都可以通过定义的变量来继承，极大地简化项目的创建与维护</p><h1 id="1-命令行定义变量"><a href="#1-命令行定义变量" class="headerlink" title="1.命令行定义变量"></a>1.命令行定义变量</h1><p>Ansible执行Playbook时可直接定义变量，并直接将变量值传入Playbook以供调用，适用于变量较少的场景</p><h2 id="1-1-创建Playbook"><a href="#1-1-创建Playbook" class="headerlink" title="1.1 创建Playbook"></a>1.1 创建Playbook</h2><pre><code class="hljs">vi test.yaml---- hosts: cluster  tasks:  - name: 安装Docker    # 调用yum模块，service为变量    yum: name=&#123;&#123; service &#125;&#125; state=installed  - name: 启动Docker    service: name=&#123;&#123; service &#125;&#125; state=started</code></pre><h2 id="1-2-执行Playbook"><a href="#1-2-执行Playbook" class="headerlink" title="1.2 执行Playbook"></a>1.2 执行Playbook</h2><pre><code class="hljs"># 执行剧本，并定义变量service，其值为nginxansible-playbook test.yml -e &quot;service = docker&quot;</code></pre><h1 id="2-主机清单定义变量"><a href="#2-主机清单定义变量" class="headerlink" title="2.主机清单定义变量"></a>2.主机清单定义变量</h1><p>Ansible主机清单文件支持变量的定义，inventory变量，用于定义主机或主机组的变量</p><h2 id="2-1-定义主机清单变量"><a href="#2-1-定义主机清单变量" class="headerlink" title="2.1 定义主机清单变量"></a>2.1 定义主机清单变量</h2><pre><code class="hljs">sudo vi /etc/ansible/hosts[master]# 定义主机master的变量data，其值为etcdmaster data = etcd[worker]worker0[1:3]# 定义主机组变量，:vars为固定格式[worker:vars]# 定义主机组worker的变量port，其值为8443port = 8443# 设置主机组cluster，master和worker这两个主机组都属于其内置组[cluster:children]masterworker</code></pre><h2 id="2-2-创建Playbook"><a href="#2-2-创建Playbook" class="headerlink" title="2.2 创建Playbook"></a>2.2 创建Playbook</h2><pre><code class="hljs">vi test.yaml---- hosts: master  tasks:  - name: 安装etcd    # 调用yum模块，data为变量    yum: name=&#123;&#123; data &#125;&#125; state=installed  - name: 启动etcd    service: name=&#123;&#123; data &#125;&#125; state=started </code></pre><h1 id="3-Playbook定义变量"><a href="#3-Playbook定义变量" class="headerlink" title="3.Playbook定义变量"></a>3.Playbook定义变量</h1><p>Ansible最常用的变量声明方式之一是通过Playbook定义变量，并可直接进行调用</p><pre><code class="hljs">vi test.yaml---- hosts:   # 定义全局变量，所有模块均可引用   vars:    packages：      - docker      - ipvsadm  tasks:  - name:     # 定义任务变量，仅限当前任务引用    vars：       port:        - 80        - 443        - 8443    # 调用全局变量packages    yum: name=&#123;&#123; packages &#125;&#125; state=installed</code></pre><h1 id="4-变量文件定义变量"><a href="#4-变量文件定义变量" class="headerlink" title="4.变量文件定义变量"></a>4.变量文件定义变量</h1><p>Ansible支持将变量存储为单独的变量文件，可供所有的Playbook调用，推荐使用</p><h2 id="4-1-变量文件"><a href="#4-1-变量文件" class="headerlink" title="4.1 变量文件"></a>4.1 变量文件</h2><pre><code class="hljs">mkdir -p /etc/ansible/vars</code></pre><h3 id="4-1-1-创建变量文件"><a href="#4-1-1-创建变量文件" class="headerlink" title="4.1.1 创建变量文件"></a>4.1.1 创建变量文件</h3><pre><code class="hljs">vi /etc/ansible/vars/vars.ymlpackages：  - ipvsadm  - docker</code></pre><h3 id="4-1-2-创建Playbook"><a href="#4-1-2-创建Playbook" class="headerlink" title="4.1.2 创建Playbook"></a>4.1.2 创建Playbook</h3><pre><code class="hljs">vi test.yaml---- hosts: master  # 定义变量文件所定义的变量   vars_files: /etc/ansible/vars/vars.yml  tasks:  - name: 安装依赖包    # 调用全局变量packages    yum: name=&#123;&#123; packages &#125;&#125; state=installed</code></pre><h2 id="4-2-主机组变量文件"><a href="#4-2-主机组变量文件" class="headerlink" title="4.2 主机组变量文件"></a>4.2 主机组变量文件</h2><pre><code class="hljs">mkdir -p /etc/ansible/grous_vars</code></pre><h3 id="4-2-1-创建所有主机组变量文件"><a href="#4-2-1-创建所有主机组变量文件" class="headerlink" title="4.2.1 创建所有主机组变量文件"></a>4.2.1 创建所有主机组变量文件</h3><pre><code class="hljs">vi /etc/ansible/grous_vars/all.yml</code></pre><h1 id="5-内置变量"><a href="#5-内置变量" class="headerlink" title="5.内置变量"></a>5.内置变量</h1><p>facts是ansible自带的预定义变量，用于描述被控端软硬件信息，通过setup模块获得</p><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><hr><ul><li><a href="https://www.cnblogs.com/wuqiuyin/p/15214880.html">https://www.cnblogs.com/wuqiuyin/p/15214880.html</a></li><li><a href="https://blog.csdn.net/weixin_40228200/article/details/123504990">https://blog.csdn.net/weixin_40228200/article/details/123504990</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Ansible</tag>
      
      <tag>自动化运维</tag>
      
      <tag>云计算</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>计算机网络原理01：网络体系结构</title>
    <link href="/linux/NetworkProtocol/"/>
    <url>/linux/NetworkProtocol/</url>
    
    <content type="html"><![CDATA[<p>计算机网络，即将地理位置不同的具有独立功能的多台计算机及其外部设备通过通信线路和通信设备连接起来，在网络操作系统、网络管理软件及网络通信协议的管理和协调下实现资源共享和信息交换的计算机通信系统。计算机网络使得信息能够快速、便捷地交流和传递，因此自诞生以来得到十分迅速的发展与非常广泛的应用，逐渐地形成了一个全球性的互联网络，即Internet，为人类在各个领域带来了极大的便利</p><h1 id="1-发展历史"><a href="#1-发展历史" class="headerlink" title="1.发展历史"></a>1.发展历史</h1><h2 id="1-1-第一代计算机网络"><a href="#1-1-第一代计算机网络" class="headerlink" title="1.1 第一代计算机网络"></a>1.1 第一代计算机网络</h2><p>1946年，计算机诞生，其研发初衷是致力于冷战期间军事模拟与弹道计算任务。由于特殊的用途、巨大的体积以及昂贵的价格，起初计算机基本上都是位于一个集中的机房，只有少量的机器在同时运行，数据交换的需求较少</p><p>1954年，美国军方的半自动地面防空系统将远距离雷达和测控仪器所探测到的信息，通过通信线路汇集到某个基地的一台IBM计算机上进行信息集中处理，再将处理完成的数据通过通信线路送回到各自的终端设备。这种以单个计算机为中心、面向终端设备的联机系统形成了以数据通信为核心的网络雏形</p><h2 id="1-2-第二代计算机网络"><a href="#1-2-第二代计算机网络" class="headerlink" title="1.2 第二代计算机网络"></a>1.2 第二代计算机网络</h2><p>1958年，美国国防部受命组建高级研究计划局（Advanced Research Project Agency，APRA），致力于军用通信系统的研发</p><p>1966年，来自美国航空航天局（NASA）的罗伯特·泰勒（Robert Taylor）成为ARPA IPTO（Information Processing Techniques Office，信息处理技术办公室）的第三任主管，提出不兼容的计算机通信没有任何意义，应着手建立一个允许所有终端之间互相通信的兼容协议。随后，新型通信网络项目内部立项，被命名为ARPANET，即阿帕网</p><p>1969年，第一个ARPANET组建完成，由加利福尼亚州大学洛杉矶分校、加州大学圣巴巴拉分校、斯坦福大学、犹他州大学四所大学的4台大型计算机进行互联。自此，人类进入网络时代</p><h2 id="1-3-第三代计算机网络"><a href="#1-3-第三代计算机网络" class="headerlink" title="1.3 第三代计算机网络"></a>1.3 第三代计算机网络</h2><p>随着计算机的迅猛应用与发展，数据传递与共享这一需求变得越来越迫切，需要互联的各种不同体系结构的网络越来越多，很多组织机构意识到计算机联网的重要性，全球涌现了大量的新型网络，如计算机科学研究网络CSNET、加拿大网络CDnet、因时网BITNET等。但由于各种设备厂商所生产的各种不同的设备并不兼容，所组成的网络进行信息交换基本无法完成，网络互联的标准化成为迫切需求</p><p>1978年，国际标准化组织(ISO)和国际电报电话咨询委员会(CCITT)联合制定了开放系统互连参考模型（Open System Interconnect，简称OSI），为开放式互连信息系统提供了一种功能结构的框架。OSI参考模型清晰地分开了服务、接口和协议的概念：服务描述每层的功能，接口定义某层提供的服务如何被高层访问，而协议是每层功能的实现方法。通过区分这些抽象概念，OSI参考模型将功能定义与实现细节区分开来，其概括性和普适性非常强。但由于其实用性较差，后续只能作为理论上的参考，商业化困难</p><p>事实上，早在1973年，为了解决ARPANET所用协议NCP扩展性与兼容性的问题，鲍伯·卡恩和温顿.瑟夫就联合提出了开放的网络架构这一思想，并最终于1978年建立了实用化的TCP&#x2F;IPv3协议（Transmission Control Protocol&#x2F;Internet Protocol，传输控制&#x2F;网际协议）</p><p>1982年，ARPANET开始采用TCP&#x2F;IP协议替代NCP协议<br>1983年，ARPANET全部替换为TCP&#x2F;IP协议<br>1985年，TCP&#x2F;IP协议栈成为UNIX操作系统密不可分的组成部分，极大地助力了其发展<br>1986年，美国国家科学基金会NSF（National Science Foundation）基于TCP&#x2F;IP协议将分布在美国各地的5个为科研教育服务的超级计算机中心互联，并支持地区网络，这就是NSFnet。NSFnet由校园网或企业网（用户互联）、地区网（本地互联）和主干网（地区互联）三部分组成，基本覆盖了全美主要的大学和研究所，是现代互联网的雏形<br>1988年，NSFnet完全替代ARPAnet，作为Internet的主干网，准许各大学、政府或私人科研机构的网络加入<br>1989年，ARPAnet解散，Internet从军用转向民用</p><p>此后，几乎所有的操作系统都开始支持TCP&#x2F;IP协议，使得其成为计算机领域实际上共同遵守的主流标准</p><h2 id="1-4-第四代计算机网络"><a href="#1-4-第四代计算机网络" class="headerlink" title="1.4 第四代计算机网络"></a>1.4 第四代计算机网络</h2><p>20世纪90年代以来，随着网络连接设备的指数级增长和信息高速公路计划的实施，Internet迅猛发展，嗅觉敏锐的商业公司逐渐从中发掘出了新的商机<br>1992年，美国IBM、MCI、MERIT三家公司联合组建高级网络服务公司（ANS），建立了新的网络，ANSnet，成为Internet的另一个主干网，Internet开始具备商业价值<br>1993年，由美国政府资助的NSFNET逐渐被多家商用的互联网主干网代替，而政府机构不再负责互联网的运营。于是，互联网服务提供商ISP（Internet Service Provider）应运而生。这些ISP拥有大量的通信线路及路由器等连网设备，任何机构和个人只要缴纳规定的费用即可通过其向互联网管理机构申请IP地址并获取租用权，最终接入到Internet<br>1994年，中国正式接入Internet<br>1995年4月30日，NSFnet正式宣布停止运作。此时，Internet骨干网已经覆盖全球91个国家，主机超过400万台</p><p>此后，将分散在世界各地的计算机和各种网络连接起来的Internet，以十分惊人的速度迅速扩展，逐渐形成了现今覆盖全世界的大型网络，正式将人类带入了以网络为核心的信息时代</p><h1 id="2-核心概念"><a href="#2-核心概念" class="headerlink" title="2.核心概念"></a>2.核心概念</h1><h2 id="2-1-网络"><a href="#2-1-网络" class="headerlink" title="2.1 网络"></a>2.1 网络</h2><p>网络，由若干结点和连接这些结点的链路所组成，结点可以是计算机、集线器、交换机或路由器等设备，最终形成本地局域网</p><h2 id="2-2-internet"><a href="#2-2-internet" class="headerlink" title="2.2 internet"></a>2.2 internet</h2><p>internet，互联网，通用名词，即由多个计算机网络通过路由器连接而成的覆盖范围更大的计算机网络，如政务网</p><h2 id="2-3-Internet"><a href="#2-3-Internet" class="headerlink" title="2.3 Internet"></a>2.3 Internet</h2><p>因特网，专用名词，当前全球最大的、开放的、由众多网络相互连接而成的特定计算机网络，基于TCP&#x2F;IP协议族作为通信规则</p><h2 id="2-4-网络协议"><a href="#2-4-网络协议" class="headerlink" title="2.4 网络协议"></a>2.4 网络协议</h2><p>起初，各大设备生产制造商所生产的各种个样的网络设备并不兼容，甚至所传输的数据类型都不相同，因而不可能进行通信。因此，建立一套用于计算机网络互相通信及网络硬件设备之间进行信息交换时必须遵守的、通用的规则、标准或约定是必然趋势，这就是网络协议。网络协议明确规定了通信设备交换数据的格式、传输方式及同步问题，使得不同的设备和应用程序能够相互理解和交互</p><h1 id="3-计算机网络体系结构"><a href="#3-计算机网络体系结构" class="headerlink" title="3.计算机网络体系结构"></a>3.计算机网络体系结构</h1><p>计算机网络体系结构，即计算机网络层次结构模型，是逻辑上将计算机网络划分为各自独立的承担特定功能与任务的、通过接口逐步传递与交互信息的多个层次模型，使得计算机网络的设计和实现模块化、简洁化与工程化。计算机网络每层都具备各自的网络协议，不同层次间相互协作，最终完成数据在整个网络的传输与共享。也即是说，计算机网络体系结构是抽象的逻辑概念，网络协议是真正运行于计算机硬件和软件的具体实现</p><h2 id="3-1-OSI七层模型"><a href="#3-1-OSI七层模型" class="headerlink" title="3.1 OSI七层模型"></a>3.1 OSI七层模型</h2><p>OSI模型将计算机网络划分为七层，自下而上依次为：物理层、数据链路层、网络层、传输层、会话层、表示层、应用层，其中第四层完成数据传输，最上面三层面向用户。OSI的七层体系结构概念清楚，理论完整，也得到了一些大公司甚至国家政府的支持，但由于复杂度过高导致并不实用，运行效率也不高，且标准的制定缺乏商业驱动力，所生产的设备无法及时进入市场。因此，OSI七层模型主要停留在理论阶段，但也为实际应用的网络标准提供了扎实的理论基础与参考模型</p><p><img src="/img/wiki/network/network001.jpg" alt="OSI网络模型"></p><h2 id="3-2-TCP-x2F-IP四层模型"><a href="#3-2-TCP-x2F-IP四层模型" class="headerlink" title="3.2 TCP&#x2F;IP四层模型"></a>3.2 TCP&#x2F;IP四层模型</h2><p>TCP&#x2F;IP四层模型由实际应用发展总结而来，是OSI七层模型的一个合并与简化，也是目前计算机网络的实际运行标准。TCP&#x2F;IP四层模型自下而上依次为：网络接口层、网际层、传输层和应用层。然而，基于实践而来的TCP&#x2F;IP四层结构并没有规定什么具体的内容，所以对计算机网络的学习与研究，通常是通过结合OSI和TCP&#x2F;IP模型的特点所形成的五层模型，即物理层、数据链路层、网络层、传输层和应用层</p><p><img src="/img/wiki/network/network002.jpg" alt="TCP/IP网络模型"></p><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://mp.weixin.qq.com/s/aVzrtPpy2tri3EFdWTJNcQ">https://mp.weixin.qq.com/s/aVzrtPpy2tri3EFdWTJNcQ</a></li><li><a href="https://www.cnblogs.com/cxuanBlog/p/13844269.html">https://www.cnblogs.com/cxuanBlog/p/13844269.html</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prometheus监控高可用集群方案</title>
    <link href="/linux/PrometheusClusterHA/"/>
    <url>/linux/PrometheusClusterHA/</url>
    
    <content type="html"><![CDATA[<p>Prometheus创作者及社区核心开发者始终秉承一个理念，即只聚焦核心的功能，扩展性的功能留给社区解决。所以，自诞生至今，Prometheus都是单实例架构，且不可扩展。这对于云计算与大数据领域而言十分罕见，而Prometheus的核心开发者这样解释：Prometheus基于Go语言的特性和优势，能够以更小的代价抓取并存储更多数据，而Elasticsearch或Cassandra等Java实现的大数据项目处理同样的数据量会消耗更多的资源。也即是说，单实例、不可扩展的Prometheus已强大到可以满足大部分用户的需求</p><p>但是，随着基础架构的不断扩展，监控数据终将会增长到单个Prometheus Server无法处理的量级。鉴于此，Prometheus提供了联邦集群的解决方案</p><h1 id="集群方案"><a href="#集群方案" class="headerlink" title="集群方案"></a>集群方案</h1><h2 id="1-基本HA"><a href="#1-基本HA" class="headerlink" title="1.基本HA"></a>1.基本HA</h2><p>部署多个Prometheus Server实例，前段部署反向代理，各实例采集相同的Exporter目标，也就是将监控数据保留同样的几份。该方案只能确保Promthues服务的可用性问题，但数据一致性问题及持久化问题无法解决，也无法进行动态的扩展。因此，只适用于监控规模不大，Promthues Server也不会频繁发生迁移，且只需保存短周期监控数据的场景</p><h2 id="2-基本HA-远程存储"><a href="#2-基本HA-远程存储" class="headerlink" title="2.基本HA+远程存储"></a>2.基本HA+远程存储</h2><p>基本HA模式的基础上通过添加Remote Storage存储支持，将监控数据保存在第三方存储服务。该方案在保证Promthues服务可用性的基础上，同时确保了数据的持久化，Promthues Server宕机或数据丢失也可以快速恢复，也可以很好的进行迁移，适用于用户监控规模不大，监控数据有持久化需求，且需要确保Promthues Server迁移性的场景</p><h2 id="3-联邦集群-远程存储"><a href="#3-联邦集群-远程存储" class="headerlink" title="3.联邦集群+远程存储"></a>3.联邦集群+远程存储</h2><p>联邦集群，即Prometheus中心实例负责数据聚合，分区实例负责监控数据采集所构成的监控系统。联邦集群将大量采集任务所形成的压力分散于不同的实例，在任务级别进行功能分区，即每个实例只负责采集一部分监控任务(Job)，如将应用监控和主机监控分离到不同的实例，监控规模持续扩大就将分区方式进一步细化，大大地提高了整个监控系统的吞吐量。联邦集群适用于如下场景：</p><ul><li>巨量采集任务的单数据中心</li></ul><p>该场景下，Promethues的性能瓶颈主要在于大量的采集任务，Prometheus联邦集群能将不同类型的采集任务划分到不同的Promethues实例，从而分散了任务采集的压力</p><ul><li>垮机房的多数据中心</li></ul><p>跨机房与数据中心的场景，由于不同内网之间的通信问题，单独部署的监控体系之间很难进行集中管理与聚合。因此，每个机房或数据中部署单独的实例负责当前数据中心的采集任务，并通过中心实例进行数据聚合，也就无需进行额外的网络配置即可构建起完整的监控体系</p><h1 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h1><ul><li>172.100.100.200 Master</li><li>172.100.100.150 Node01 KVM虚拟机监控</li><li>172.100.100.180 Node02 应用程序监控</li></ul><h1 id="1-部署Prometheus"><a href="#1-部署Prometheus" class="headerlink" title="1.部署Prometheus"></a>1.部署Prometheus</h1><h1 id="2-配置分区节点"><a href="#2-配置分区节点" class="headerlink" title="2.配置分区节点"></a>2.配置分区节点</h1><h2 id="2-1-配置分区Node01"><a href="#2-1-配置分区Node01" class="headerlink" title="2.1 配置分区Node01"></a>2.1 配置分区Node01</h2><pre><code class="hljs">sudo vi /usr/local/prometheus/prometheus.ymlglobal:  scrape_interval: 15s   evaluation_interval: 15s   scrape_timeout: 10s alerting:  alertmanagers:    - static_configs:        - targets:            - 172.100.100.200:9093- job_name: node  relabel_configs:    - source_labels: [ &#39;__address__&#39; ]      target_label: &#39;hostname&#39;  static_configs:    - targets: [&quot;172.100.100.180:9100&quot;]      labels:        clusters: &#39;Openstack&#39;    - targets: [&quot;172.100.100.181:9100&quot;]      labels:        clusters: &#39;Openstack&#39;    - targets: [&quot;172.100.100.182:9100&quot;]      labels:        clusters: &#39;Openstack&#39;    - targets: [&quot;172.16.100.100:9100&quot;]      labels:        clusters: &#39;Kubernetes&#39;    - targets: [&quot;172.16.100.101:9100&quot;]      labels:        clusters: &#39;Kubernetes&#39;    - targets: [&quot;172.16.100.102:9100&quot;]      labels:        clusters: &#39;Kubernetes&#39;</code></pre><h2 id="2-2-配置分区Node02"><a href="#2-2-配置分区Node02" class="headerlink" title="2.2 配置分区Node02"></a>2.2 配置分区Node02</h2><pre><code class="hljs">sudo vi /usr/local/prometheus/prometheus.ymlglobal:  scrape_interval: 15s   evaluation_interval: 15s   scrape_timeout: 10s alerting:  alertmanagers:    - static_configs:        - targets:            - 172.100.100.200:9093scrape_configs:  - job_name: prometheus    relabel_configs:      - source_labels: [ &#39;__address__&#39; ]        target_label: &#39;hostname&#39;    static_configs:      - targets: [&quot;172.100.100.150:9090&quot;]        labels:          clusters: &#39;Kubernetes&#39;      - targets: [&quot;172.100.100.180:9090&quot;]        labels:          clusters: &#39;Openstack&#39;  - job_name: memcached    relabel_configs:      - source_labels: [ &#39;__address__&#39; ]        target_label: &#39;hostname&#39;    static_configs:      - targets: [&quot;172.100.100.180:9150&quot;]        labels:          clusters: &#39;Openstack&#39;  - job_name: mysql    relabel_configs:      - source_labels: [ &#39;__address__&#39; ]        target_label: &#39;hostname&#39;    static_configs:      - targets: [&quot;172.100.100.180:9104&quot;]        labels:          clusters: &#39;Openstack&#39;  - job_name: rabbitmq    relabel_configs:      - source_labels: [ &#39;__address__&#39; ]        target_label: &#39;hostname&#39;    static_configs:      - targets: [&quot;172.100.100.180:9419&quot;]        labels:          clusters: &#39;Openstack&#39;</code></pre><h1 id="3-配置联邦节点"><a href="#3-配置联邦节点" class="headerlink" title="3.配置联邦节点"></a>3.配置联邦节点</h1><pre><code class="hljs">sudo vi /usr/local/prometheus/prometheus.ymlglobal:  scrape_interval: 15s   evaluation_interval: 15s   scrape_timeout: 10s scrape_configs:  - job_name: &quot;prometheus&quot;    static_configs:      - targets: [&quot;172.100.100.200:9090&quot;]        labels:          clusters: &#39;CloudServer&#39;  - job_name: &quot;node&quot;    static_configs:      - targets: [&quot;172.100.100.200:9100&quot;]        labels:          clusters: &#39;CloudServer&#39;  - job_name: &#39;prometheus150&#39;    scrape_interval: 10s    metrics_path: /federate    honor_labels: true    params:      # 设置查询条件，即只从其他Prometheus抓取符合条件的数据      &#39;match[]&#39;:      # 设置匹配任务名称的表达式，该表达式表示job名称以node开头的指标数据      - &#39;&#123;job=~&quot;node.*&quot;&#125;&#39;    static_configs:      # 设置指定的Prometheus实例      - targets: [&quot;192.168.100.150:9090&quot;]  - job_name: &#39;prometheus180&#39;    scrape_interval: 10s    metrics_path: /federate    honor_labels: true    params:      # 设置查询条件，即只从其他Prometheus抓取符合条件的数据      &#39;match[]&#39;:      # 设置匹配任务名称的表达式，该表达式表示job名称为prometheus的指标数据      # - &#39;&#123;job=&quot;prometheus&quot;&#125;&#39;       # 设置匹配指标名称的表达式，__name__表示匹配指定名称的指标数据，该表达式表示指标名称以job开头的指标数据      # - &#39;&#123;__name__=~&quot;job:.*&quot;&#125;&#39;      # 设置匹配任务名称的表达式，该表达式表示job名称以node开头的指标数据      - &#39;&#123;job=~&quot;prometheus.*&quot;&#125;&#39;       - &#39;&#123;job=~&quot;mysql.*&quot;&#125;&#39;       - &#39;&#123;job=~&quot;rabbitmq.*&quot;&#125;&#39;       - &#39;&#123;job=~&quot;memcached.*&quot;&#125;&#39;     static_configs:      # 设置指定的Prometheus实例      - targets: [&quot;172.100.100.180:9090&quot;]       </code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/hanjinjuan/article/details/121363870">https://blog.csdn.net/hanjinjuan/article/details/121363870</a></li><li><a href="https://blog.csdn.net/m0_37749659/article/details/130794785">https://blog.csdn.net/m0_37749659/article/details/130794785</a></li><li><a href="https://blog.csdn.net/agonie201218/article/details/126249715">https://blog.csdn.net/agonie201218/article/details/126249715</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>云原生</tag>
      
      <tag>Prometheus</tag>
      
      <tag>监控告警</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RockyLinux9配置静态IP</title>
    <link href="/linux/RockyLinux/"/>
    <url>/linux/RockyLinux/</url>
    
    <content type="html"><![CDATA[<p>Rocky Linux，Centos的创始人Gregory Kurtzer所引领的社区企业级操作系统，是CentOS官方宣布停止Centos的维护之后的替换项目，其命名是为了纪念CentOS早期的联合创始人Rocky McGaugh</p><h1 id="1-查看系统网卡名称"><a href="#1-查看系统网卡名称" class="headerlink" title="1.查看系统网卡名称"></a>1.查看系统网卡名称</h1><pre><code class="hljs">ip addr</code></pre><h1 id="2-修改网卡配置"><a href="#2-修改网卡配置" class="headerlink" title="2.修改网卡配置"></a>2.修改网卡配置</h1><pre><code class="hljs">sudo cp /etc/NetworkManager/system-connections/ens32.nmconnection /etc/NetworkManager/system-connections/ens32.nmconnection.baksudo vi /etc/NetworkManager/system-connections/ens32.nmconnection[connection]id=ens32uuid=887ca84f-46c5-38ad-a5c4-757b56999fbdtype=ethernetautoconnect-priority=-999interface-name=ens32timestamp=1686652135[ethernet][ipv4]method=manualaddress1=192.168.100.120/24,192.168.100.1dns=8.8.8.8[ipv6]addr-gen-mode=eui64method=auto[proxy]</code></pre><h1 id="3-重新加载网络配置"><a href="#3-重新加载网络配置" class="headerlink" title="3.重新加载网络配置"></a>3.重新加载网络配置</h1><pre><code class="hljs">nmcli c reloadnmcli c up ens32ip a</code></pre><h1 id="4-配置国内Yum源"><a href="#4-配置国内Yum源" class="headerlink" title="4.配置国内Yum源"></a>4.配置国内Yum源</h1><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.cnblogs.com/tigerisnotcat/p/17479898.html">https://www.cnblogs.com/tigerisnotcat/p/17479898.html</a></li><li><a href="https://blog.csdn.net/qq_41195336/article/details/134255595">https://blog.csdn.net/qq_41195336/article/details/134255595</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>网络</tag>
      
      <tag>Rocky9</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python面向对象</title>
    <link href="/linux/PythonObject/"/>
    <url>/linux/PythonObject/</url>
    
    <content type="html"><![CDATA[<p>Object Oriented Programming，即面向对象编程，简称为OOP，一种程序设计范式。不同于以函数及函数调用为核心的面相过程式编程，面向对象编程以对象为核心，将数据和操作数据的函数封装为类，从更高的层次进行系统建模，更贴近事物的自然运行模式。其思想概括为：把一组数据和处理数据的方法组成对象，把行为相同的对象归纳为类，通过封装隐藏对象的内部细节，通过继承实现类的特化和泛化，通过多态实现基于对象类型的动态分派。C语言即为典型的面向过程的语言，C++、Python是典型的面向对象的语言</p><h1 id="1-类与对象"><a href="#1-类与对象" class="headerlink" title="1.类与对象"></a>1.类与对象</h1><h2 id="1-1-类"><a href="#1-1-类" class="headerlink" title="1.1 类"></a>1.1 类</h2><p>class，即类，具有相同属性和方法的一组对象的集合，属性是用于存储对象状态的类变量，方法是用于定义对象行为和操作的类函数。类可看作为模版或蓝图，用于创建具有相似特征的多个对象</p><h2 id="1-2-对象"><a href="#1-2-对象" class="headerlink" title="1.2 对象"></a>1.2 对象</h2><p>object，即对象，也称实例，是基于类这个模版创建的可直接引用的个体，是类的具象化，拥有类所定义的属性和方法。面向对象编程即是以对象为核心，以类和继承为构造机制，来认识、理解、刻画客观世界，从而设计、构建相应的软件系统。Python中一切皆对象</p><hr><p>总结，类是对象的抽象表现，是对象的设计蓝图，如动物类、植物类等；对象是类的具体表现，是依据类这个模版设计出的具体产物，如绿箩，由植物类所派生，具有植物通用的属性（如具有绿色叶片）和动作（如制造氧气）</p><h2 id="1-3-创建类"><a href="#1-3-创建类" class="headerlink" title="1.3 创建类"></a>1.3 创建类</h2><p>创建类，即定义类，由class关键字实现，语法格式为：</p><pre><code class="hljs">class 类名:    方法列表</code></pre><ul><li>class，用于定义类的关键字，其后为类名，最后的“:”代表类定义的起始</li><li>类名，即为符合Python语法的标识符，以大写字母开头，并采用驼峰命名法，建议以能够简洁明了地体现出其功能的词汇来命名</li></ul><hr><pre><code class="hljs">&gt;&gt;&gt; class Student:...     classroom = &#39;101&#39;...     address = &#39;BeiJing&#39; ...     def __init__(self,name,age):...         self.name = name...         self.age = age...     def introduce(self):...         print(f&#39;我是学生&#123;self.name&#125;,今年&#123;self.age&#125;岁.&#39;)</code></pre><h2 id="1-4-创建对象"><a href="#1-4-创建对象" class="headerlink" title="1.4 创建对象"></a>1.4 创建对象</h2><p>创建对象，即根据类模板创建对象，也就是类的实例化，故对象也被称为实例，语法格式为：</p><pre><code class="hljs">对象名 = 类名(属性参数列表) </code></pre><ul><li>对象名，就是变量名，命名规则与普通变量一致，首字母小写，以免与类名混淆</li><li>属性参数列表，由类方法__init__()定义，其中self参数自动传递不需要指定。对象创建成功后即可访问类的属性与方法，使用句点.表示法，语法为：对象名.属性名、对象名.方法名</li></ul><hr><pre><code class="hljs"># Student类实例化对象John&gt;&gt;&gt; John = Student(&#39;John&#39;,15)# John对象访问Student类的属性age&gt;&gt;&gt; John.age15# John对象访问Student类的方法introduce()&gt;&gt;&gt; John.introduce()我是学生John,今年15岁.</code></pre><h1 id="2-类成员"><a href="#2-类成员" class="headerlink" title="2.类成员"></a>2.类成员</h1><p>类，由属性与方法这两个成员构成</p><h2 id="2-1-类的属性"><a href="#2-1-类的属性" class="headerlink" title="2.1 类的属性"></a>2.1 类的属性</h2><p>属性，即类的数据成员，用于存储数据，可以被类的实例访问与修改，通常由变量定义，也被称为成员变量，分为实例变量与类变量两种</p><h3 id="2-1-1-实例变量"><a href="#2-1-1-实例变量" class="headerlink" title="2.1.1 实例变量"></a>2.1.1 实例变量</h3><p>实例变量，实例对象个体本身拥有的数据，如Student类学生的名字和年龄，通过实例名加圆点的方式进行调用</p><pre><code class="hljs"># John对象调用实例变量age&gt;&gt;&gt; John.age15</code></pre><h3 id="2-1-2-类变量"><a href="#2-1-2-类变量" class="headerlink" title="2.1.2 类变量"></a>2.1.2 类变量</h3><p>类变量，属于类的变量，不属于单个的具体的对象，适用于所有实例具有相同字段的场景，所有实例共有且都可访问、修改，如Student类的classroom和address即为类变量，通过类名或实例名加圆点的方式进行调用，建议使用类名.类变量的访问方式，以免与实例变量混淆</p><pre><code class="hljs">&gt;&gt;&gt; Student.classroom&#39;101&#39;&gt;&gt;&gt; John.address&#39;BeiJing&#39;</code></pre><h2 id="2-2-类的方法"><a href="#2-2-类的方法" class="headerlink" title="2.2 类的方法"></a>2.2 类的方法</h2><p>方法，即类的函数，用于实现类的行为，可以访问、修改类属性，与普通的函数一样也是通过def关键字定义，不同的是方法的第一个参数必须为self且不需要传递实参。类方法分为实例方法、静态方法和类方法三种</p><h3 id="2-2-1-实例方法"><a href="#2-2-1-实例方法" class="headerlink" title="2.2.1 实例方法"></a>2.2.1 实例方法</h3><p>实例方法，由实例对象调用，至少有一个self参数且为第一个参数，执行实例方法时将会自动将调用该方法的实例并赋值给self。通常所说的类方法默认即为实例方法，定义时不需要使用使用特殊的关键字进行标识，如Student类所定义的方法introduce()即为实例方法</p><ul><li>注：self表示类的实例而不是类本身，也不是Python的关键字，而是约定成俗的命名方式，完全可以由其他词汇代替，但不建议</li></ul><h3 id="2-2-2-构造方法"><a href="#2-2-2-构造方法" class="headerlink" title="2.2.2 构造方法"></a>2.2.2 构造方法</h3><p>构造方法，一个特殊的实例方法，又被称为类的构造函数或初始化方法，实例化对象时将会被自动调用以初始化实例的属性。构造方法一般命名为__init__()，并传递对应的参数，其中第一个参数为self且无需传递实参</p><h4 id="2-2-2-1-默认构造方法"><a href="#2-2-2-1-默认构造方法" class="headerlink" title="2.2.2.1 默认构造方法"></a>2.2.2.1 默认构造方法</h4><p>定义类时不是必须要定义构造方法，若没有显式地定义__init__() 方法，Python将会提供默认的无参数构造方法，且不进行任何初始化操作，即表示创建一个对象实例，并将其返回</p><pre><code class="hljs">&gt;&gt;&gt; class Student:...     name = &#39;John&#39;...     age = 15... &gt;&gt;&gt; obj1 = Student()</code></pre><h4 id="2-2-2-2-无参构造方法"><a href="#2-2-2-2-无参构造方法" class="headerlink" title="2.2.2.2 无参构造方法"></a>2.2.2.2 无参构造方法</h4><p>构造方法如下情况可以没有参数：</p><ul><li>简单初始化，类属性的初始值都是固定的，不需要根据外部参数进行修改时，使用无参构造函数来设置属性的默认值，对类属性进行初始化</li><li>不依赖外部数据，若对象的创建和初始化过程不依赖于外部的变量或状态，也没有其他需要执行的初始化操作，可使用无参构造函数</li></ul><hr><pre><code class="hljs">&gt;&gt;&gt; class Circle:...     def __init__(self):...         self.radius = 2.0...     def get_area(self):...         return 3.14 * self.radius * self.radius... &gt;&gt;&gt; c1 = Circle()&gt;&gt;&gt; c1.get_area()12.56</code></pre><h4 id="2-2-2-3-有参构造方法"><a href="#2-2-2-3-有参构造方法" class="headerlink" title="2.2.2.3 有参构造方法"></a>2.2.2.3 有参构造方法</h4><p>构造方法通过接收外部参数将属性赋予特定的值对实例进行初始化，从而完成基于不同需求或条件创建具有特定属性和行为的对象</p><pre><code class="hljs">&gt;&gt;&gt; class Circle:...     def __init__(self,radius):...         self.radius = radius...     def get_area(self):...         return 3.14 * self.radius * self.radius... &gt;&gt;&gt; c1 = Circle(2.0)&gt;&gt;&gt; c1.get_area()12.56</code></pre><h3 id="2-2-3-静态方法"><a href="#2-2-3-静态方法" class="headerlink" title="2.2.3 静态方法"></a>2.2.3 静态方法</h3><p>静态方法，由类调用，属于类，与实例无关，无默认参数，使用类名.静态方法的调用方式。将实例方法的参数self去掉，再于方法定义的上方加上@staticmethod，就成为类静态方法</p><h3 id="2-2-4-类方法"><a href="#2-2-4-类方法" class="headerlink" title="2.2.4 类方法"></a>2.2.4 类方法</h3><p>类方法，由类调用，使用类名.类方法的调用方式，以@classmethod装饰，至少传入一个cls参数（代指类本身，类似self），执行类方法时Python将自动将调用该方法的类赋值给cls。通常情况下，需要先实例化一个类对象才能调用该类的方法或属性，但类方法提供了不实例化类的情况下直接创建对象的方式，实现了封装实例化的过程，并提供更方便的接口给用户，即直接通过类名调用而无需先创建类的实例</p><hr><pre><code class="hljs">&gt;&gt;&gt; class Deo:...     def __init__(self,name):...         self.name = name...     def ord(self):...         print(&quot;实例方法&quot;)...     @classmethod...     def class_func(cls):...         print(&quot;类方法&quot;)...     @staticmethod...     def static_func():...         print(&quot;静态方法&quot;)... # 创建实例对象f&gt;&gt;&gt; f = Deo(&quot;John&quot;)# 实例对象调用实例方法&gt;&gt;&gt; f.ord()实例方法# 类调用类方法&gt;&gt;&gt; Deo.class_func()类方法# 实例对象调用类方法&gt;&gt;&gt; f.class_func()类方法# 类调用静态方法&gt;&gt;&gt; Deo.static_func()静态方法</code></pre><h1 id="3-封装、继承与多态"><a href="#3-封装、继承与多态" class="headerlink" title="3.封装、继承与多态"></a>3.封装、继承与多态</h1><p>面向对象编程具有三大重要特性，即封装、继承与多态</p><h2 id="3-1-封装性"><a href="#3-1-封装性" class="headerlink" title="3.1 封装性"></a>3.1 封装性</h2><p>封装，即将数据与操作数据的功能代码置于对象内部以隐藏内部细节，只保留接口以供外界调用，而不能通过任何形式修改对象内部实现。封装性使得对象的调用更为简单而无需关心其内部逻辑，代码更易维护，同时一定程度上保障了代码的安全性。Python语言的变量默认为公有，可以在类的外部进行访问。但若是不希望所有的变量和方法能被外部访问，即限制某些成员的访问，以提高程序的健壮性、可靠性及业务的逻辑性，可在成员的名字前加上两个下划线__，使其成为私有成员（private）。私有成员只能在类的内部访问，外部无法访问，若需要外部访问时，在类的内部创建外部可以访问的get和set方法即可</p><pre><code class="hljs">&gt;&gt;&gt; class People:...     def __init__(self,name,age):...          self.__name = name...          self.__age = age...     def print_age(self):...         print(f&quot;我是&#123;self.__name&#125;,&#123;self.__age&#125;岁.&quot;)...     def get_name(self):...         return self.__name...     def set_name(self,name):...         self.__name = name...     def set_age(self,age):...         self.__age = age... &gt;&gt;&gt; person = People(&#39;John&#39;,15)&gt;&gt;&gt; person.get_name()John&gt;&gt;&gt; person.set_name(&#39;Tom&#39;)</code></pre><h2 id="3-2-继承性"><a href="#3-2-继承性" class="headerlink" title="3.2 继承性"></a>3.2 继承性</h2><p>继承，即基于已有类的基础上创建新类，新类被称为子类或派生类，被继承的类被称为父类、基类或超类。子类继承了父类中的所有属性和方法，但也还可以拥有自己的属性和方法</p><p>继承机制实现了代码的复用，即将多个类实现公共功能的共用代码置入父类，而其他类只需作为子类去继承即可，避免了代码的重复性</p><pre><code class="hljs">&gt;&gt;&gt; class People:...     def __init__(self,name,age):...         self.name = name...         self.age = age...     def intro(self):...         print(f&quot;我是&#123;self.name&#125;,&#123;self.age&#125;岁.&quot;) </code></pre><h3 id="3-2-1-单继承"><a href="#3-2-1-单继承" class="headerlink" title="3.2.1 单继承"></a>3.2.1 单继承</h3><pre><code class="hljs">&gt;&gt;&gt; class Student(People):...     pass......&gt;&gt;&gt; s = Student(&#39;John&#39;,15)&gt;&gt;&gt; s.intro()我是John,15岁.</code></pre><ul><li>继承的语法是定义类的时候，类名后面的圆括号()中指定父类，若未指定父类，则表明继承object类，即python最顶级的类</li><li>定义类Student，指定其父类为People，尽管Student类未定义任何方法，但仍可继承父类的intro()方法，并实例化对象s</li></ul><h3 id="3-2-2-多重继承"><a href="#3-2-2-多重继承" class="headerlink" title="3.2.2 多重继承"></a>3.2.2 多重继承</h3><p>多重继承，即子类继承多个父类，按照父类的顺序从左向右依次继承，具体机制为：子类调用某个方法或变量时，首先内部查找，若未找到则开始根据继承机制查找父类；根据父类定义顺序，以深度优先的方式逐一查找父类，且一旦查找到则直接调用，之后不再继续查找</p><pre><code class="hljs">&gt;&gt;&gt; class Senior(Student,People):...     def __init__(self,name,age,grade):...         People.__init__(self,name,age)...         self.grade = grade...     def intros(self):...         print(f&quot;我是&#123;self.name&#125;,&#123;self.age&#125;岁,&#123;self.grade&#125;年级在读.&quot;)... &gt;&gt;&gt; s = Senior(&#39;John&#39;,15,1)&gt;&gt;&gt; s.intros()我是John,15岁,1年级在读.&gt;&gt;&gt; s.intro()我是John,15岁.</code></pre><ul><li>Senior类继承了两个父类，即Student与People</li><li>People.<strong>init</strong>(self,name,age)，即调用父类的初始化方法</li><li>对象s调用intros()方法，内部查找即可；调用intro()方法，首先内部查找，之后按照父类的定义顺序查找Student类，再查找People类</li></ul><h3 id="3-2-3-方法重写"><a href="#3-2-3-方法重写" class="headerlink" title="3.2.3 方法重写"></a>3.2.3 方法重写</h3><p>方法重写，即子类对继承父类的方法进行重些以实现不同的功能，调用子类的该方法时将会执行重写后的方法</p><pre><code class="hljs">&gt;&gt;&gt; class Exam(Senior,Student,People):...     def __init__(self,name,age,grade,score):...         Senior.__init__(self,name,age,grade)...         self.score = score# 重写父类Senior的intros()方法...     def intros(self):...         print(f&quot;我是&#123;self.name&#125;,&#123;self.age&#125;岁,&#123;self.grade&#125;年级在读,总分为&#123;self.score&#125;.&quot;)... &gt;&gt;&gt; s = Exam(&#39;John&#39;,15,1,365)&gt;&gt;&gt; s.intros()我是John,15岁,1年级在读,总分为365.</code></pre><h3 id="3-2-4-方法扩展"><a href="#3-2-4-方法扩展" class="headerlink" title="3.2.4 方法扩展"></a>3.2.4 方法扩展</h3><p>Python语言内置函数super()用于调用父类的方法，通常在子类的方法中使用，以便在对父类方法进行扩展或重写时能够保持父类的行为，特别适用于子类调用父类的初始化方法</p><pre><code class="hljs">&gt;&gt;&gt; class Exam(Senior,Student,People):...     def __init__(self,name,age,grade,score):...         super().__init__(name,age,grade)...         self.score = score...     def intros(self):...         print(f&quot;我是&#123;self.name&#125;,&#123;self.age&#125;岁,&#123;self.grade&#125;年级在读,总分为&#123;self.score&#125;.&quot;)... &gt;&gt;&gt; s = Exam(&#39;John&#39;,15,1,365)&gt;&gt;&gt; s.intros()我是John,15岁,1年级在读,总分为365.</code></pre><h2 id="3-3-多态性"><a href="#3-3-多态性" class="headerlink" title="3.3 多态性"></a>3.3 多态性</h2><p>多态，即一类事物的多种形态，具体是指多个子类继承父类并重写方法后，同一方法所表现出的不同行为，也即是：一个接口，多种实现。多态性增强了程序的灵活性与扩展性，无论如何对象变化，使用同一种形式调用即可，如下所示：函数show()可通过同一方式去调用类Senior和Coll继承的并重写的父类方法intro()</p><pre><code class="hljs">&gt;&gt;&gt; class Stu:...     def intro(self):...         print(&#39;我是学生.&#39;)... &gt;&gt;&gt; class Senior(Stu):...     def intro(self):...         print(&#39;我是中学生.&#39;)... &gt;&gt;&gt; class Coll(Stu):...     def intro(self):...         print(&#39;我是大学生.&#39;)... &gt;&gt;&gt; def show(s):...     s.intro()... &gt;&gt;&gt; a = Senior()&gt;&gt;&gt; b = Coll()&gt;&gt;&gt; show(a)我是中学生.&gt;&gt;&gt; show(b)我是大学生.</code></pre>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prometheus监控标签重写</title>
    <link href="/linux/PrometheusRelabel/"/>
    <url>/linux/PrometheusRelabel/</url>
    
    <content type="html"><![CDATA[<p>Relabel，即标签重写，是Prometheus监控在数据抓取之前对其标签动态改写的功能，由配置项relabel_configs完成对标签的添加、删除、重命名、合并及提取替换等操作，以满足不同场景下的需求，更好地分析和查询数据</p><h1 id="1-内置标签"><a href="#1-内置标签" class="headerlink" title="1.内置标签"></a>1.内置标签</h1><p>targets以”__”为前置的标签一般是系统内置标签，不会写入到指标数据，但可通过标签重新操作将之写入指标数据，如instance标签就是通过对target实例的内置标签__address__所做的重写操作。常见内置标签如下：</p><ul><li>__address__，当前Target实例的访问地址<host>:<port></li><li>__scheme__，采集目标服务访问地址的HTTP Scheme，HTTP或者HTTPS</li><li>__metrics_path__，采集目标服务访问地址的访问路径</li><li>_<em>param</em><name>，采集任务目标服务的中包含的请求参数</li></ul><h1 id="2-自定义标签"><a href="#2-自定义标签" class="headerlink" title="2.自定义标签"></a>2.自定义标签</h1><h2 id="2-1-修改Prometheus配置文件"><a href="#2-1-修改Prometheus配置文件" class="headerlink" title="2.1 修改Prometheus配置文件"></a>2.1 修改Prometheus配置文件</h2><pre><code class="hljs">sudo vi /usr/local/prometheus/prometheus.yml- job_name: &quot;node&quot;    static_configs:      - targets: [&quot;worker01:9100&quot;]        labels:          env: formal          __hostname__: worker01</code></pre><h2 id="2-2-重载Prometheus"><a href="#2-2-重载Prometheus" class="headerlink" title="2.2 重载Prometheus"></a>2.2 重载Prometheus</h2><pre><code class="hljs">curl -XPOST http://127.0.0.1:9090/-/reload</code></pre><h2 id="2-3-验证自定义标签"><a href="#2-3-验证自定义标签" class="headerlink" title="2.3 验证自定义标签"></a>2.3 验证自定义标签</h2><h1 id="3-标签重写"><a href="#3-标签重写" class="headerlink" title="3.标签重写"></a>3.标签重写</h1><h2 id="3-1-配置解析"><a href="#3-1-配置解析" class="headerlink" title="3.1 配置解析"></a>3.1 配置解析</h2><pre><code class="hljs">relabel_configs:  # 设置源标签，支持正则表达式  - source_labels: [ &#39;labelname&#39; ]    # 设置多个源标签的分隔符，默认为分号;    separator: _    # 设置重写操作的目标标签    target_label: [ &#39;labelname&#39; ]    # 设置匹配源标签所用的正则表达式，默认为(.*)，即全部匹配    regex: &quot;正则表达式&quot;    # 设置正则表达式匹配用于替换的值，默认为$1，以代替正则匹配到的值    replacement: $1-$2    # 设置基于正则表达式匹配要执行的动作，如keep、drop等，默认为replace    action: relabel_action    # 设置哈希模式下源标签值哈希值的模数，作为系数计算源标签的哈希值    modulus: int</code></pre><p>action类型</p><ul><li>replace，缺省类型，正则匹配源标签的值用于替换目标标签</li><li>keep，如果正则没有匹配到源标签的值，则删除该targets，不进行采集</li><li>drop，与keep相反，正则匹配到源标签则删除该targets</li><li>labelmap，正则匹配所有标签名，将匹配的标签值部分做为新标签名，原标签值做为新标签的值</li><li>labeldrop，正则匹配所有标签名，匹配则移除标签</li><li>labelkeep，正则匹配所有标签名，不匹配的标签会被移除</li><li>hashmod，将一个或多个源标签的值经过哈希运算后作为目标标签的值，用于Prometheus集群的负载均衡</li></ul><h2 id="3-2-指标新增重写"><a href="#3-2-指标新增重写" class="headerlink" title="3.2 指标新增重写"></a>3.2 指标新增重写</h2><pre><code class="hljs">relabel_configs:  - source_labels: [ &#39;__address__&#39; ]    # 将内置标签__address__重写为addr，并继承其标签的值    target_label:  &#39;addr&#39;</code></pre><h2 id="3-3-指标覆盖重写"><a href="#3-3-指标覆盖重写" class="headerlink" title="3.3 指标覆盖重写"></a>3.3 指标覆盖重写</h2><pre><code class="hljs">relabel_configs:  - source_labels: [ &#39;__address__&#39; ]    target_label:  &#39;addr&#39;    # 将内置标签__address__重写为addr，并将标签值设为自定义的localhost    replacement: &#39;localhost&#39;</code></pre><h2 id="3-4-指标拼接重写"><a href="#3-4-指标拼接重写" class="headerlink" title="3.4 指标拼接重写"></a>3.4 指标拼接重写</h2><pre><code class="hljs">relabel_configs:  - source_labels: [&#39;__address__&#39;,&#39;job&#39;]    separator: _    # 将两个内置标签以“_”为分隔符作连接，标签值继承源标签，重写结果为：192.168.100.100:9100_node        target_label: addr</code></pre><h2 id="3-5-正则重写"><a href="#3-5-正则重写" class="headerlink" title="3.5 正则重写"></a>3.5 正则重写</h2><pre><code class="hljs">relabel_configs:  - source_labels: [ &#39;__address__&#39; ]    target_label:  &#39;addr&#39;    # 将获取到的源标签的值做正则匹配，匹配结果为：192.168.100.100 9100    regex: &quot;(.*):(.*)&quot;    # 将内置标签__address__经过正则匹配的值进行重写，$1/$2表示位置变量，重写结果为：192.168.100.100_9100    replacement: $1_$2</code></pre><h2 id="3-6-指标丢弃重写"><a href="#3-6-指标丢弃重写" class="headerlink" title="3.6 指标丢弃重写"></a>3.6 指标丢弃重写</h2><h3 id="3-6-1-drop"><a href="#3-6-1-drop" class="headerlink" title="3.6.1 drop"></a>3.6.1 drop</h3><pre><code class="hljs">relabel_configs:  - source_labels: [&#39;addr&#39;]    regex: &quot;192.168.100.100_9100&quot;    # 将正则表达式匹配到源标签值的target采集到的数据丢弃    action: drop</code></pre><h3 id="3-6-2-keep"><a href="#3-6-2-keep" class="headerlink" title="3.6.2 keep"></a>3.6.2 keep</h3><pre><code class="hljs">relabel_configs:  - source_labels: [&#39;addr&#39;]    regex: &quot;192.168.100.100_9100&quot;    # 将正则表达式匹配到源标签值的target采集到的数据保留，未匹配到的则全部丢弃，与drop相反    action: keep</code></pre><h2 id="3-7-标签删除重写"><a href="#3-7-标签删除重写" class="headerlink" title="3.7 标签删除重写"></a>3.7 标签删除重写</h2><h3 id="3-7-1-labeldrop"><a href="#3-7-1-labeldrop" class="headerlink" title="3.7.1 labeldrop"></a>3.7.1 labeldrop</h3><pre><code class="hljs">relabel_configs:  - regex: &#39;(job)&#39;    #  将所有正则匹配到的标签删除    action: labeldrop</code></pre><h3 id="3-7-2-labelkeep"><a href="#3-7-2-labelkeep" class="headerlink" title="3.7.2 labelkeep"></a>3.7.2 labelkeep</h3><pre><code class="hljs">relabel_configs:  - regex: &#39;(job)&#39;    # 将所有未被正则匹配到的标签删除，与labeldrop相反    action: labelkeep    </code></pre><h2 id="3-8-标签重命名重写"><a href="#3-8-标签重命名重写" class="headerlink" title="3.8 标签重命名重写"></a>3.8 标签重命名重写</h2><pre><code class="hljs"> relabel_configs:   - regex: monitor_(.*)     replacement: &#39;$&#123;1&#125;&#39;     # 将所有以monitor_开头的标签名重写替换为去掉monitor_前缀的新标签名字，类似于覆盖重写，如monitor_mysql=&quot;01&quot;--&gt; mysql=&quot;01&quot;     action: labelmap</code></pre><h2 id="3-9-哈希重写"><a href="#3-9-哈希重写" class="headerlink" title="3.9 哈希重写"></a>3.9 哈希重写</h2><p>哈希重写是将一个或多个源标签的值进行hash运算，所得到的值作为目标标签的值，这样只要hash值一致则表示源标签一致，再对Prometheus根据该hash值标签设置不同的取舍action，如某个Prometheus只采集hash值为1的指标。这样完成了同个指标的分片拆分操作，从而实现多个Prometheus横向扩展副本的负载均衡，缓解了指标采集压力</p><pre><code class="hljs">- job_name: &quot;mysqld&quot;  file_sd_configs:  - files:    - /usr/local/prometheus/config/*.yaml    refresh_interval: 2m  relabel_configs:    - source_labels: [ &#39;instance&#39;,&#39;__address__&#39;]      action: hashmod      modulus: 2      target_label: hash_id</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://andyoung.blog.csdn.net/article/details/126263009">https://andyoung.blog.csdn.net/article/details/126263009</a></li><li><a href="https://blog.csdn.net/qq_21127151/article/details/130098062">https://blog.csdn.net/qq_21127151/article/details/130098062</a></li><li><a href="https://blog.csdn.net/qq_42883074/article/details/115894190">https://blog.csdn.net/qq_42883074/article/details/115894190</a></li><li><a href="https://blog.csdn.net/weixin_40046357/article/details/120540581">https://blog.csdn.net/weixin_40046357/article/details/120540581</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Prometheus</tag>
      
      <tag>监控告警</tag>
      
      <tag>Alertmanager</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes集群基于Kube-Prometheus配置动态服务发现</title>
    <link href="/linux/Kube-Prometheus-Discover/"/>
    <url>/linux/Kube-Prometheus-Discover/</url>
    
    <content type="html"><![CDATA[<p>Prometheus基于Kubernetes集群服务发现的原理是通过Kubernetes的REST API检索抓取监控目标，并始终与集群状态保持同步，支持Node、Service、Pod、Endpoints、Ingress这几种模式，也称为角色Role，以适用不同的场景</p><h1 id="角色类型"><a href="#角色类型" class="headerlink" title="角色类型"></a>角色类型</h1><h2 id="Node"><a href="#Node" class="headerlink" title="Node"></a>Node</h2><p>Node角色用于发现集群节点服务器的地址端口，默认为Kubelet的HTTP端口，Target地址默认为Kubernetes节点对象的第一个现有地址，地址类型顺序为NodeInternalIP、NodeExternalIP、NodeLegacyHostIP和NodeHostName</p><h2 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h2><p>Service角色用于发现集群Service的IP和Port，将其作为Target，适用于黑盒监控(blackbox)场景</p><h2 id="Pod"><a href="#Pod" class="headerlink" title="Pod"></a>Pod</h2><p>Pod角色用于发现集群所有Pod并将其IP作为Target，若存在有多个端口或容器，则将生成多个Target；若容器没有指定的端口，则会为每个容器创建一个无端口Target，以便通过Relabel手动添加端口</p><h2 id="Endpoints"><a href="#Endpoints" class="headerlink" title="Endpoints"></a>Endpoints</h2><p>Endpoints角色用于发现集群Endpoints并将其作为Targets，若Tndpoint属于Service则会附加Service角色的所有标签，后端节点是Pod则会附加Pod角色的所有标签</p><h2 id="Ingress"><a href="#Ingress" class="headerlink" title="Ingress"></a>Ingress</h2><p>Ingress角色用于发现集群Ingress每个路径的Target，其地址将设为Ingress中指定的host，通常用于黑盒监控</p><h1 id="1-创建额外监控配置文件"><a href="#1-创建额外监控配置文件" class="headerlink" title="1.创建额外监控配置文件"></a>1.创建额外监控配置文件</h1><pre><code class="hljs">vi prometheus-additional.yaml- job_name: &#39;kubernetes-endpoints&#39;  kubernetes_sd_configs:  - role: endpoints  relabel_configs:  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]    action: keep    regex: true  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]    action: replace    target_label: __scheme__    regex: (https?)  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]    action: replace    target_label: __metrics_path__    regex: (.+)  - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]    action: replace    target_label: __address__    regex: ([^:]+)(?::\d+)?;(\d+)    replacement: $1:$2  - action: labelmap    regex: __meta_kubernetes_service_label_(.+)  - source_labels: [__meta_kubernetes_namespace]    action: replace    target_label: kubernetes_namespace  - source_labels: [__meta_kubernetes_service_name]    action: replace    target_label: kubernetes_name  - source_labels: [__meta_kubernetes_pod_name]    action: replace    target_label: kubernetes_pod_name</code></pre><h1 id="2-创建额外监控配置项的Secret"><a href="#2-创建额外监控配置项的Secret" class="headerlink" title="2.创建额外监控配置项的Secret"></a>2.创建额外监控配置项的Secret</h1><pre><code class="hljs">kubectl -n monitoring create secret generic additional-configs --from-file=prometheus-additional.yaml</code></pre><h1 id="3-Prometheus配置加载额外监控项"><a href="#3-Prometheus配置加载额外监控项" class="headerlink" title="3.Prometheus配置加载额外监控项"></a>3.Prometheus配置加载额外监控项</h1><pre><code class="hljs">  version: 2.29.1  additionalScrapeConfigs:    name: additional-configs    key: prometheus-additional.yaml</code></pre><h1 id="4-创建RBAC"><a href="#4-创建RBAC" class="headerlink" title="4.创建RBAC"></a>4.创建RBAC</h1><pre><code class="hljs">vi prometheus-clusterRole.yamlapiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata:  labels:    app.kubernetes.io/component: prometheus    app.kubernetes.io/name: prometheus    app.kubernetes.io/part-of: kube-prometheus    app.kubernetes.io/version: 2.29.1  name: prometheus-k8srules:- apiGroups:  - &quot;&quot;  resources:  - nodes  - services  - endpoints  - pods  - nodes/proxy  verbs:  - get  - list  - watch- apiGroups:  - &quot;&quot;  resources:  - nodes/metrics  verbs:  - get- nonResourceURLs:  - /metrics  - /actuator/prometheus  verbs:  - get</code></pre><h1 id="5-验证监控项"><a href="#5-验证监控项" class="headerlink" title="5.验证监控项"></a>5.验证监控项</h1><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><hr><ul><li><a href="https://blog.z0ukun.com/?p=2605">https://blog.z0ukun.com/?p=2605</a></li><li><a href="https://www.cnblogs.com/deny/p/14328900.html">https://www.cnblogs.com/deny/p/14328900.html</a></li><li><a href="https://blog.csdn.net/ss810540895/article/details/130870986">https://blog.csdn.net/ss810540895/article/details/130870986</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
      <tag>Prometheus</tag>
      
      <tag>监控告警</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python文件IO</title>
    <link href="/linux/PythonIO/"/>
    <url>/linux/PythonIO/</url>
    
    <content type="html"><![CDATA[<p>file，即文件，用于将存储于内存的数据持久化到磁盘，以防程序结束或关闭后丢失数据，方便程序再次将数据读取到内存进行处理。现代操作系统不允许普通程序直接操作磁盘，而是由文件系统完成文件的读写。Python内置函数open()用于文件的读写操作，流程类似于将大象塞进冰箱，分三步走，即打开文件、操作文件、关闭文件：</p><ul><li>1.open()函数打开文件获取文件对象，并指定访问模式，可将之赋值给变量，即文件句柄（包含了文件的文件名、字符集、文件大小、硬盘上的起始位置）</li><li>2.通过文件对象执行读、写、追加操作</li><li>3.关闭并释放文件对象</li></ul><h1 id="1-文件打开与关闭"><a href="#1-文件打开与关闭" class="headerlink" title="1.文件打开与关闭"></a>1.文件打开与关闭</h1><h2 id="1-1-文件打开"><a href="#1-1-文件打开" class="headerlink" title="1.1 文件打开"></a>1.1 文件打开</h2><p>Python内置函数open()用于文件的打开，语法格式如下：</p><pre><code class="hljs">f = open(filename, mode=&#39;r&#39;, buffering=None, encoding=None，errors=None)</code></pre><ul><li>filename，设置文件名称，通常是一个文件路径，绝对路径相对路径均可</li><li>mode，设置文件打开模式，可选参数，默认为r，表示文本文件的只读模式，若文件不存在则报错；w表示只写模式，若文件不存在则新建后再写入，文件存在则先清空再写入；a表示追加模式，若文件不存在则新建再写入，文件存在则在文件末尾追加写入；x表示新建模式，若文件存在则报错，文件不存在则新建再写入，比w模式更安全；b表示二进制模式，以bytes类型操作数据，如rb、wb、ab；+表示可读可写模式，r+表示打开文件用于读写，可配合seek()和tell()方法实现更多操作，w+表示读写之前清空文件内容（不建议使用），a+表示只能在文件末尾使用（不建议使用）</li><li>buffering，设置文件IO缓冲区的大小，即先将数据存入内存缓冲区再一次性操作磁盘IO，以免频繁地操作磁盘IO导致程序效率的降低。若不设置则表示不开启缓存，默认为-1，表示全缓冲，即与系统及磁盘块大小有关，设为大于1的整数表示字节数为buffering的全缓冲，也就是多少字节后执行一次写操作；1表示行缓冲，及遇到换行符执行一次写操作；0表示无缓冲模式</li><li>encoding，设置文件编码，默认为UTF-8，用于打开文本文件，若与文件保存时的编码方式不一致，则可能因无法解码而导致文件打开失败</li><li>errors，设置文本文件发生编码错误时的处理方式，用于处理编码不规范的文件，如读取GBK编码文件，建议设为ignore，即忽略编码错误继续执行</li><li>f，变量，文件对象，其值即为open()函数的返回值所赋予，文件的读写操作都要由其来执行</li></ul><hr><pre><code class="hljs">&gt;&gt;&gt; f = open(&#39;test.txt&#39;,&#39;w&#39;)# 输出文件名称print(f.name)test.txt# 输出文件的访问模式&gt;&gt;&gt; print(f.mode)w# 判断文件是否已被关闭&gt;&gt;&gt; print(f.closed)False</code></pre><h2 id="1-2-文件关闭"><a href="#1-2-文件关闭" class="headerlink" title="1.2 文件关闭"></a>1.2 文件关闭</h2><p>Python内置函数close()用于文件对象的关闭，close()函数的调用将刷新缓冲区还未完全被写入到文件的信息，保障了数据的完整性，同时也会释放文件的读写权限与系统资源，以便于其他程序操作该文件，所以文件操作完毕后关闭文件非常有必要</p><pre><code class="hljs">&gt;&gt;&gt; f = open(&#39;test.txt&#39;,&#39;w&#39;)&gt;&gt;&gt; print(f.closed)False&gt;&gt;&gt; f.close()&gt;&gt;&gt; print(f.closed)True</code></pre><h3 id="1-2-1-文件异常关闭"><a href="#1-2-1-文件异常关闭" class="headerlink" title="1.2.1 文件异常关闭"></a>1.2.1 文件异常关闭</h3><p>文件操作往往会抛出异常，为了保障对文件的操作无论是正常结束还异常结束都能够关闭文件，建议对close()方法的调用放在异常处理的finally代码块中，以防文件操作异常导致的数据丢失</p><h3 id="1-2-2-文件自动关闭"><a href="#1-2-2-文件自动关闭" class="headerlink" title="1.2.2 文件自动关闭"></a>1.2.2 文件自动关闭</h3><p>Python内置的with as代码块用于close()函数的自动调用，以自动释放系统资源，替代finally代码块，优化了代码结构，提高程序的可读性</p><pre><code class="hljs">&gt;&gt;&gt; with open(&#39;test&#39;,&#39;w&#39;) as f:...     pass... &gt;&gt;&gt; print(f.closed)True&gt;&gt;&gt; with open(&#39;test&#39;,&#39;w&#39;) as f1,open(&#39;test.txt&#39;,&#39;r&#39;) as f2:...     pass... &gt;&gt;&gt; print(f1.closed)True&gt;&gt;&gt; print(f2.closed)True</code></pre><h1 id="2-文件读操作"><a href="#2-文件读操作" class="headerlink" title="2.文件读操作"></a>2.文件读操作</h1><p>Python内置函数open()创建的文件对象所属的read()、readline()、readlines()方法用于对文件进行读操作</p><h2 id="2-1-read"><a href="#2-1-read" class="headerlink" title="2.1 read()"></a>2.1 read()</h2><p>read()方法将从文件的当前位置一次性读取一定大小的数据（忽略文件末尾的换行符）, 并返回字符串或字节对象。该方法有一个数字类型的可选参数size，表示指定读取的数据量，适用于文件大小不确定的场景，反复调用read(size)比较保险；若将之省略或设为负值，则表示读取文件的所有数据，适用于小文件的场景</p><pre><code class="hljs">&gt;&gt;&gt; with open(&#39;test.txt&#39;,&#39;r&#39;) as f:...     str = f.read()...     print(str)... Hello!This is a test.</code></pre><hr><pre><code class="hljs">&gt;&gt;&gt; f = open(&#39;test.txt&#39;,&#39;r&#39;)&gt;&gt;&gt; str = f.read(10)&gt;&gt;&gt; print(str)Hello!Thi&gt;&gt;&gt; f.close()</code></pre><h2 id="2-2-readline"><a href="#2-2-readline" class="headerlink" title="2.2 readline()"></a>2.2 readline()</h2><p>readline()方法将从文件当前位置读取一行内容，并将文件指针移动到下一行的开始，为下一次读取做准备，返回值也是字符串或字节对象。该方法也可设置读取的数据大小，表示只读取当前位置当前行的size个字符，适用于读一行就处理一行的场景，且不能回头只能前进</p><pre><code class="hljs">&gt;&gt;&gt; f = open(&#39;test.txt&#39;,&#39;r&#39;)&gt;&gt;&gt; str = f.readline()&gt;&gt;&gt; print(str)Hello!&gt;&gt;&gt; print(str)Hello!&gt;&gt;&gt; strs = f.readline()&gt;&gt;&gt; print(strs)This is a test.&gt;&gt;&gt; str = f.readline()&gt;&gt;&gt; print(str)&gt;&gt;&gt;&gt;&gt;&gt; f.close()</code></pre><hr><pre><code class="hljs">&gt;&gt;&gt; f = open(&#39;test.txt&#39;,&#39;r&#39;)&gt;&gt;&gt; str = f.read(3)&gt;&gt;&gt; print(str)Hel&gt;&gt;&gt; str = f.read(3)&gt;&gt;&gt; print(str)lo!&gt;&gt;&gt; str = f.read(3)&gt;&gt;&gt; print(str)Th&gt;&gt;&gt; str = f.read()&gt;&gt;&gt; print(str)is is a test.&gt;&gt;&gt; &gt;&gt;&gt; f.close()</code></pre><h2 id="2-3-readlines"><a href="#2-3-readlines" class="headerlink" title="2.3 readlines()"></a>2.3 readlines()</h2><p>readlines()方法将文件的所有行一行一行全部读入一个列表，按顺序一个一个作为列表的元素，并返回这个列表。该方法将会读取文件的所有数据，并将指针移动到文件结尾处，也可指定size读取的直到指定字符所在的行，适用于配置文件</p><pre><code class="hljs">&gt;&gt;&gt; f = open(&#39;test.txt&#39;,&#39;r&#39;)&gt;&gt;&gt; list = f.readlines()&gt;&gt;&gt; print(list)[&#39;Hello!\n&#39;, &#39;This is a test.\n&#39;]&gt;&gt;&gt; f.close()</code></pre><hr><pre><code class="hljs">&gt;&gt;&gt; f = open(&#39;test.txt&#39;,&#39;r&#39;)&gt;&gt;&gt; f.readlines(3)[&#39;Hello!\n&#39;]&gt;&gt;&gt; f.close()&gt;&gt;&gt; f = open(&#39;test.txt&#39;,&#39;r&#39;)&gt;&gt;&gt; f.readlines(3)[&#39;Hello!\n&#39;]&gt;&gt;&gt; f.readlines(3)[&#39;This is a test.\n&#39;]&gt;&gt;&gt; f.readlines(3)[]</code></pre><h2 id="2-4-文件遍历"><a href="#2-4-文件遍历" class="headerlink" title="2.4 文件遍历"></a>2.4 文件遍历</h2><p>文件对象作为迭代器可快速地循环遍历文件的所有数据，每次循环一行数据而不需要一次性全部读取，且会在行末添加换行符’\n’，适用于通用场景及大文件场景，但与readline()方法一样只能前进不能回退</p><pre><code class="hljs">&gt;&gt;&gt; with open(&#39;hosts.ini&#39;,&#39;r&#39;) as f:...     for line in f:...         print(line,end=&#39;&#39;)... node01node02node03node04&gt;&gt;&gt; </code></pre><h1 id="3-文件写操作"><a href="#3-文件写操作" class="headerlink" title="3.文件写操作"></a>3.文件写操作</h1><p>Python内置函数open()创建的文件对象所属的write()与writelines()方法用于对文件进行写操作</p><h2 id="3-1-write"><a href="#3-1-write" class="headerlink" title="3.1 write()"></a>3.1 write()</h2><p>write()方法将字符串或字节对象写入文件，并返回写入的字符数，且不会在字符串的结尾添加换行符(‘\n’)。该方法在内存中操作，不会立刻写回硬盘，执行close()方法才会将所有写入操作落盘。若需立刻保存到硬盘，也即是在未关闭文件的情况下写入文件，使用flush()方法即可</p><pre><code class="hljs">&gt;&gt;&gt; with open(&#39;test.txt&#39;,&#39;w&#39;) as f:...     f.write(&#39;Hello!\n This is a test.\n&#39;)... 24</code></pre><h2 id="3-2-writelines"><a href="#3-2-writelines" class="headerlink" title="3.2 writelines()"></a>3.2 writelines()</h2><p>writelines()方法将一个字符串列表写入文件，且不添加行分隔符，因此通常需要为每一行末尾添加行分隔符</p><pre><code class="hljs">&gt;&gt;&gt; hosts = [&#39;node01\n&#39;,&#39;node02\n&#39;,&#39;node03\n&#39;]&gt;&gt;&gt; with open(&#39;hosts.ini&#39;,&#39;w&#39;) as f:...     f.writelines(hosts)... &gt;&gt;&gt; f = open(&#39;hosts.ini&#39;,&#39;r+&#39;)&gt;&gt;&gt; f.read()&#39;node01\nnode02\nnode03\n&#39;&gt;&gt;&gt; f.writelines(&quot;node04\n&quot;)&gt;&gt;&gt; f.close()&gt;&gt;&gt; with open(&#39;hosts.ini&#39;,&#39;r&#39;) as f:...     f.read()... &#39;node01\nnode02\nnode03\nnode04\n&#39;</code></pre><h1 id="4-文件指针"><a href="#4-文件指针" class="headerlink" title="4.文件指针"></a>4.文件指针</h1><p>文件读写指针用于标记文件的当前位置，第一次打开文件时，文件指针通常会指向文件的开始位置，调用read()方法后则会将文件指针移动到读取内容的末尾，默认情况下是文件末尾，重新打开文件时文件指针则会还原到文件开始位置</p><h2 id="4-1-指针定位"><a href="#4-1-指针定位" class="headerlink" title="4.1 指针定位"></a>4.1 指针定位</h2><p>tell()方法用于返回文件指针的当前位置，其值为文件开头开始算起的字节数</p><pre><code class="hljs">&gt;&gt;&gt; f = open(&#39;hosts.ini&#39;,&#39;rb&#39;)&gt;&gt;&gt; f.tell()0&gt;&gt;&gt; f.read(3)&#39;nod&#39;&gt;&gt;&gt; f.tell()3&gt;&gt;&gt; f.readline(3)&#39;e01&#39;&gt;&gt;&gt; f.tell()6</code></pre><h2 id="4-2-指针更改"><a href="#4-2-指针更改" class="headerlink" title="4.2 指针更改"></a>4.2 指针更改</h2><p>seek()方法用于指定文件指针的当前位置，语法格式如下：</p><pre><code class="hljs">f.seek(offset,from_what)</code></pre><ul><li>from_what，即起始位置，默认为0，表示如果是0表示文件开头，1表示文件指针的当前位置，2文件末尾</li><li>offset，即偏移量，seek(x)或seek(x,0)表示从文件首行首字符开始移动x个字符；seek(x,1)表示从文件当前位置往后移动x个字符；seek(-x,2)表示从文件结尾往前移动x个字符</li></ul><hr><pre><code class="hljs">&gt;&gt;&gt; f.read()b&#39;\nnode02\nnode03\nnode04\n&#39;&gt;&gt;&gt; f.tell()28&gt;&gt;&gt; f.seek(-8,2)20&gt;&gt;&gt; f.read()b&#39;\nnode04\n&#39;&gt;&gt;&gt; f.seek(0)0&gt;&gt;&gt; f.seek(3)3&gt;&gt;&gt; f.read(3)b&#39;e01&#39;&gt;&gt;&gt; f.tell()6&gt;&gt;&gt; f.seek(4,1)10&gt;&gt;&gt; f.read(3)b&#39;e02&#39;&gt;&gt;&gt; f.close()</code></pre>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prometheus监控配置Docker容器监控</title>
    <link href="/linux/Prometheus-Docker/"/>
    <url>/linux/Prometheus-Docker/</url>
    
    <content type="html"><![CDATA[<p>cAdvisor，用于收集、聚合、处理和导出容器运行状态的信息，并为每个容器保存资源隔离参数、历史资源使用情况、完整历史资源使用直方图和网络统计信息，从而对容器进行实时监控和性能数据采集，如CPU、内存、网络、文件系统等资源的使用情况</p><h1 id="1-安装cAdvisor"><a href="#1-安装cAdvisor" class="headerlink" title="1.安装cAdvisor"></a>1.安装cAdvisor</h1><h2 id="1-1-下载cAdvisor"><a href="#1-1-下载cAdvisor" class="headerlink" title="1.1 下载cAdvisor"></a>1.1 下载cAdvisor</h2><pre><code class="hljs">wget https://github.com/google/cadvisor/releases/download/v0.49.1/cadvisor-v0.49.1-linux-amd64sudo mv cadvisor-v0.49.1-linux-amd64 /usr/local/bin/cadvisor &amp;&amp; sudo chmod +x /usr/local/bin/cadvisor</code></pre><h2 id="1-2-创建启动脚本"><a href="#1-2-创建启动脚本" class="headerlink" title="1.2 创建启动脚本"></a>1.2 创建启动脚本</h2><pre><code class="hljs">sudo vi /lib/systemd/system/cadvisor.service[Unit]Description=cadvisor_exporterDocumentation=https://prometheus.ioAfter=network.target[Service]Type=simpleUser=rootExecStart = /usr/local/bin/cadvisor -port 8080Restart=on-failure[Install]WantedBy=multi-user.target</code></pre><h2 id="1-3-启动cAdvisor"><a href="#1-3-启动cAdvisor" class="headerlink" title="1.3 启动cAdvisor"></a>1.3 启动cAdvisor</h2><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start cadvisor.servicesudo systemctl enable cadvisor.service</code></pre><h1 id="2-配置Prometheus"><a href="#2-配置Prometheus" class="headerlink" title="2.配置Prometheus"></a>2.配置Prometheus</h1><pre><code class="hljs">sudo vi /usr/local/prometheus/prometheus.ymlscrape_configs:  - job_name: &quot;docker&quot;    file_sd_configs:    - files:      - /usr/local/prometheus/config/docker.yml    relabel_configs:      - source_labels: [ &#39;__address__&#39; ]        regex: &quot;(.*):(.*)&quot;        replacement: $1        target_label: &#39;hostname&#39;</code></pre><h1 id="3-创建自动发现规则"><a href="#3-创建自动发现规则" class="headerlink" title="3.创建自动发现规则"></a>3.创建自动发现规则</h1><pre><code class="hljs">sudo vi /usr/local/prometheus/config/docker.yml- targets:    - engine.sword.org:8080  labels:    clusters: KVM</code></pre><h1 id="4-创建告警规则"><a href="#4-创建告警规则" class="headerlink" title="4.创建告警规则"></a>4.创建告警规则</h1><pre><code class="hljs">sudo vi /usr/local/prometheus/rules/docker.ymlgroups:- name: docker  rules:  - alert: ContainerKilled    expr: time() - container_last_seen&#123;name!=&quot;&quot;&#125; &gt; 60    for: 1m    labels:      severity: Critical    annotations:      summary: &quot;&#123;&#123; $labels.name &#125;&#125;容器被Kill，请尽快处理!&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125;实例&#123;&#123; $labels.name &#125;&#125;容器被Kill&quot;  - alert: ContainerCpuUsage    expr: (sum by(instance, name) (rate(container_cpu_usage_seconds_total&#123;name!=&quot;&quot;&#125;[3m])) * 100) &gt; 80    for: 2m    labels:      severity: Warning      clusters: 工控机    annotations:      summary: &quot;&#123;&#123; $labels.name &#125;&#125;容器CPU使用率过高，请尽快处理!&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125;实例&#123;&#123; $labels.name &#125;&#125;容器CPU使用率超过80%, 当前值为&#123;&#123; $value &#125;&#125;&quot;  - alert: ContainerHighThrottleRate    expr: rate(container_cpu_cfs_throttled_seconds_total[3m]) &gt; 1    for: 2m    labels:      severity: Warning    annotations:      summary: &quot;&#123;&#123; $labels.name &#125;&#125;容器CPU超限，请尽快处理!&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125;实例&#123;&#123; $labels.name &#125;&#125;容器持续2分钟CPU超限, 当前值为&#123;&#123; $value &#125;&#125;&quot;  - alert: ContainerMemoryUsage    expr: (sum by(instance, name) (container_memory_working_set_bytes&#123;name!=&quot;&quot;&#125;) / sum by(instance, name) (container_spec_memory_limit_bytes&#123;name!=&quot;&quot;&#125; &gt; 0) * 100)  &gt; 80    for: 2m    labels:      severity: Warning    annotations:      summary: &quot;&#123;&#123; $labels.name &#125;&#125;容器内存使用率过高，请尽快处理!&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125;实例&#123;&#123; $labels.name &#125;&#125;容器内存使用率超过80%, 当前值为&#123;&#123; $value &#125;&#125;&quot;  - alert: ContainerVolumeUsage    expr: (1 - (sum(container_fs_inodes_free) BY (instance) / sum(container_fs_inodes_total) BY (instance))) * 100 &gt; 80    for: 5m    labels:      severity: Warning    annotations:      summary: &quot;&#123;&#123; $labels.name &#125;&#125;容器磁盘使用率过高，请尽快处理!&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125;实例&#123;&#123; $labels.name &#125;&#125;容器磁盘使用率超过80%, 当前值为&#123;&#123; $value &#125;&#125;&quot;  - alert: ContainerLowCpuUtilization    expr: (sum(rate(container_cpu_usage_seconds_total&#123;name!=&quot;&quot;&#125;[3m])) BY (instance, name) * 100) &lt; 20    for: 7d    labels:      severity: Info    annotations:      summary: &quot;&#123;&#123; $labels.name &#125;&#125;容器CPU使用率过低，建议缩减CPU配额&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125;实例&#123;&#123; $labels.name &#125;&#125;容器持续7天CPU使用率低于20%, 当前值为&#123;&#123; $value &#125;&#125;&quot;  - alert: ContainerLowMemoryUsage    expr: (sum(container_memory_working_set_bytes&#123;name!=&quot;&quot;&#125;) BY (instance, name) / sum(container_spec_memory_limit_bytes &gt; 0) BY (instance, name) * 100) &lt; 20    for: 7d    labels:      severity: Info    annotations:      summary: &quot;&#123;&#123; $labels.name &#125;&#125;容器内存使用率过低，建议缩减内存配额&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125;实例&#123;&#123; $labels.name &#125;&#125;容器持续7天内存使用率低于20%, 当前值为&#123;&#123; $value &#125;&#125;&quot;</code></pre><h1 id="5-重载Prometheus"><a href="#5-重载Prometheus" class="headerlink" title="5.重载Prometheus"></a>5.重载Prometheus</h1><pre><code class="hljs">curl -XPOST http://127.0.0.1:9090/-/reload</code></pre><h1 id="6-Grafana导入监控模版"><a href="#6-Grafana导入监控模版" class="headerlink" title="6.Grafana导入监控模版"></a>6.Grafana导入监控模版</h1><p>Dashboards —&gt; Manage —&gt; Import —&gt; 模版ID：14282</p><p><img src="/img/wiki/prometheus/docker01.jpg" alt="docker01"></p><p><img src="/img/wiki/prometheus/docker02.jpg" alt="docker02"></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://zhuanlan.zhihu.com/p/618043088">https://zhuanlan.zhihu.com/p/618043088</a></li><li><a href="https://blog.csdn.net/m0_37749659/article/details/130716421">https://blog.csdn.net/m0_37749659/article/details/130716421</a></li><li><a href="https://docs.huihoo.com/apache/mesos/chrisrc.me/dcos-admin-monitoring-docker.html">https://docs.huihoo.com/apache/mesos/chrisrc.me/dcos-admin-monitoring-docker.html</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Docker</tag>
      
      <tag>容器</tag>
      
      <tag>云原生</tag>
      
      <tag>Prometheus</tag>
      
      <tag>监控告警</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes集群基于Kube-Prometheus配置自定义监控</title>
    <link href="/linux/Kube-Prometheus-ServiceMonitor/"/>
    <url>/linux/Kube-Prometheus-ServiceMonitor/</url>
    
    <content type="html"><![CDATA[<p>Kube-Prometheus由自定义资源ServiceMonitor实现对资源的监控，该资源描述了Prometheus Server的Target列表，具体是通过Selector依据Labels选取到对应Service的endpoints，监控数据由Prometheus Server通过Service进行拉取，从而实现跨命名空间的动态服务发现。此外，ServiceMonito监听Kubernetes集群的资源变动，如服务的创建、删除或标签的变更，以及规则的更新，然后自动更新Prometheus的配置文件，以及配合PrometheusRule自动发现和生成相应的监控配置。通过这种简单的声明式配置实现了Prometheus监控系统的自动管理和扩展，从而使得监控系统的维护更加简单和可靠</p><h1 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h1><ul><li>1.创建ServiceMonitor对象，用于Prometheus添加监控项</li><li>2.创建ServiceMonitor对象所关联的metrics数据接口的Service对象</li><li>3.验证并确保Service对象能正确获取到metrics数据，主要是关于MySQL用户和集群SA的权限</li></ul><h1 id="1-部署MySQL数据库"><a href="#1-部署MySQL数据库" class="headerlink" title="1.部署MySQL数据库"></a>1.部署MySQL数据库</h1><h1 id="2-部署mysql-exporter"><a href="#2-部署mysql-exporter" class="headerlink" title="2.部署mysql-exporter"></a>2.部署mysql-exporter</h1><h2 id="2-1-创建MySQL数据库用户并授权"><a href="#2-1-创建MySQL数据库用户并授权" class="headerlink" title="2.1 创建MySQL数据库用户并授权"></a>2.1 创建MySQL数据库用户并授权</h2><pre><code class="hljs">MariaDB [(none)]&gt; create user &#39;exporter&#39;@&#39;%&#39; identified with mysql_native_password by &#39;exporter@2020&#39;;MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON *.* TO &#39;exporter&#39;@&#39;%&#39; with grant option;MariaDB [(none)]&gt; flush privileges;</code></pre><h2 id="2-2-创建MySQL数据库配置文件"><a href="#2-2-创建MySQL数据库配置文件" class="headerlink" title="2.2 创建MySQL数据库配置文件"></a>2.2 创建MySQL数据库配置文件</h2><pre><code class="hljs">vi my.cnf[client]host=192.168.100.180user=exporterpassword=exporter@2020</code></pre><h2 id="2-3-部署MySQL数据库配置项"><a href="#2-3-部署MySQL数据库配置项" class="headerlink" title="2.3 部署MySQL数据库配置项"></a>2.3 部署MySQL数据库配置项</h2><pre><code class="hljs">kubectl create configmap mysql-exporter-conf --from-file=my.cnf</code></pre><h2 id="2-4-创建mysql-exporter资源文件"><a href="#2-4-创建mysql-exporter资源文件" class="headerlink" title="2.4 创建mysql-exporter资源文件"></a>2.4 创建mysql-exporter资源文件</h2><pre><code class="hljs">vi mysql-exporter.yamlapiVersion: apps/v1kind: Deploymentmetadata:  name: mysql-exporter  namespace: monitoringspec:  replicas: 1  selector:    matchLabels:      k8s-app: mysql-exporter  template:    metadata:      labels:        k8s-app: mysql-exporter    spec:      containers:        - name: mysql-exporter          image: registry.cn-hangzhou.aliyuncs.com/swords/mysqld-exporter          env:            - name: DATA_SOURCE_NAME              value: &quot;exporter:123456@(192.168.100.180:3306)/&quot;          imagePullPolicy: IfNotPresent          ports:            - containerPort: 9104          volumeMounts:            - name: mysql-exporter-conf              mountPath: /home/.my.cnf              subPath: my.cnf      volumes:        - name: mysql-exporter-conf          configMap:            name: mysql-exporter-conf---apiVersion: v1kind: Servicemetadata:  labels:    k8s-app: mysql-exporter  name: mysql-exporter  namespace: monitoringspec:  type: ClusterIP  ports:    - name: api      protocol: TCP      port: 9104  selector:    k8s-app: mysql-exporter</code></pre><h2 id="2-5-部署mysql-exporter-yaml"><a href="#2-5-部署mysql-exporter-yaml" class="headerlink" title="2.5 部署mysql-exporter.yaml"></a>2.5 部署mysql-exporter.yaml</h2><pre><code class="hljs">kubectl apply -f mysql-exporter.yaml</code></pre><h1 id="3-部署ServiceMonitor"><a href="#3-部署ServiceMonitor" class="headerlink" title="3.部署ServiceMonitor"></a>3.部署ServiceMonitor</h1><h2 id="3-1-创建ServiceMonitor资源文件"><a href="#3-1-创建ServiceMonitor资源文件" class="headerlink" title="3.1 创建ServiceMonitor资源文件"></a>3.1 创建ServiceMonitor资源文件</h2><pre><code class="hljs">vi servicemonitor.yamlapiVersion: monitoring.coreos.com/v1kind: ServiceMonitormetadata:  name: mysql-exporter  namespace: monitoring  labels:    k8s-app: mysql-exporter    namespace: monitoringspec:  jobLabel: k8s-app  endpoints:    - port: api      interval: 30s      scheme: http  selector:    matchLabels:      k8s-app: mysql-exporter  namespaceSelector:    matchNames:      - monitoring</code></pre><h2 id="3-2-部署ServiceMonitor"><a href="#3-2-部署ServiceMonitor" class="headerlink" title="3.2 部署ServiceMonitor"></a>3.2 部署ServiceMonitor</h2><pre><code class="hljs">kubectl apply -f servicemonitor.yaml</code></pre><h1 id="4-部署PrometheusRule监控规则"><a href="#4-部署PrometheusRule监控规则" class="headerlink" title="4.部署PrometheusRule监控规则"></a>4.部署PrometheusRule监控规则</h1><h2 id="4-1-创建PrometheusRule监控规则文件"><a href="#4-1-创建PrometheusRule监控规则文件" class="headerlink" title="4.1 创建PrometheusRule监控规则文件"></a>4.1 创建PrometheusRule监控规则文件</h2><pre><code class="hljs">vi mysql-exporter-PrometheusRule.yamlapiVersion: monitoring.coreos.com/v1kind: PrometheusRulemetadata:  labels:    prometheus: k8s    role: alert-rules  name: mysql-exporter-rules  namespace: monitoringspec:  groups:    - name: mysql-exporter      rules:        - alert: MysqlDown          annotations:            description: |-              MySQL instance is down on &#123;&#123; $labels.instance &#125;&#125;                VALUE = &#123;&#123; $value &#125;&#125;                LABELS = &#123;&#123; $labels &#125;&#125;            summary: &#39;MySQL down (instance &#123;&#123; $labels.instance &#125;&#125;)&#39;          expr: mysql_up == 0          for: 0m          labels:            severity: critical        - alert: MysqlSlaveIoThreadNotRunning          annotations:            description: |-              MySQL Slave IO thread not running on &#123;&#123; $labels.instance &#125;&#125;                VALUE = &#123;&#123; $value &#125;&#125;                LABELS = &#123;&#123; $labels &#125;&#125;            summary: &gt;-              MySQL Slave IO thread not running (instance &#123;&#123; $labels.instance                  &#125;&#125;)          expr: &gt;-            mysql_slave_status_master_server_id &gt; 0 and ON (instance)            mysql_slave_status_slave_io_running == 0          for: 0m          labels:            severity: critical        - alert: MysqlSlaveSqlThreadNotRunning          annotations:            description: |-              MySQL Slave SQL thread not running on &#123;&#123; $labels.instance &#125;&#125;                VALUE = &#123;&#123; $value &#125;&#125;                LABELS = &#123;&#123; $labels &#125;&#125;            summary: &gt;-              MySQL Slave SQL thread not running (instance &#123;&#123; $labels.instance                  &#125;&#125;)          expr: &gt;-            mysql_slave_status_master_server_id &gt; 0 and ON (instance)            mysql_slave_status_slave_sql_running == 0          for: 0m          labels:            severity: critical</code></pre><h2 id="4-2-部署PrometheusRule监控规则"><a href="#4-2-部署PrometheusRule监控规则" class="headerlink" title="4.2 部署PrometheusRule监控规则"></a>4.2 部署PrometheusRule监控规则</h2><pre><code class="hljs">kubectl apply -f mysql-exporter-PrometheusRule.yaml</code></pre><h1 id="5-导入grafana模版"><a href="#5-导入grafana模版" class="headerlink" title="5.导入grafana模版"></a>5.导入grafana模版</h1><p>Dashboards —&gt; Manage —&gt; Import —&gt; 模版ID：7362</p><h1 id="6-停止MySQL服务，测试监控告警"><a href="#6-停止MySQL服务，测试监控告警" class="headerlink" title="6.停止MySQL服务，测试监控告警"></a>6.停止MySQL服务，测试监控告警</h1><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.51cto.com/liubin0505star/5767918">https://blog.51cto.com/liubin0505star/5767918</a></li><li><a href="https://www.cnblogs.com/cndarren/p/16975566.html">https://www.cnblogs.com/cndarren/p/16975566.html</a></li><li><a href="https://blog.csdn.net/knight_zhou/article/details/126241171">https://blog.csdn.net/knight_zhou/article/details/126241171</a></li><li><a href="https://blog.csdn.net/qq_43164571/article/details/127299185">https://blog.csdn.net/qq_43164571/article/details/127299185</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
      <tag>Prometheus</tag>
      
      <tag>监控告警</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prometheus监控告警信息的管理与推送</title>
    <link href="/linux/PrometheusAlertNotification/"/>
    <url>/linux/PrometheusAlertNotification/</url>
    
    <content type="html"><![CDATA[<p>Prometheus监控系统的告警通知由Alertmanager组件负责管理，如告警信息的分组、合并、抑制与静默登，之后通过路由推送给配置好的接收者，如电子邮箱、Slack、Webhook等，将告警信息通知到相关负责人进行处理</p><h1 id="1-配置解析"><a href="#1-配置解析" class="headerlink" title="1.配置解析"></a>1.配置解析</h1><pre><code class="hljs"># 设置全局参数，即作为默认值供子设置继承的公共设置，子参数中也可覆盖其设置global:  # 设置处理超时时间，即为告警的解决的时间，直接影响到警报恢复的通知时间，默认为5分钟，建议依据实际生产场景进行设置  resolve_timeout: 1m  # 设置邮箱smtp服务器  smtp_smarthost: &#39;smtp.qq.com:465&#39;  # 设置发件邮箱  smtp_from: &#39;xxxxxxxxx@qq.com&#39;  # 设置发件账号  smtp_auth_username: &#39;xxxxxxxxx@qq.com&#39;  # 设置发件人邮箱授权码，注意不是登录密码  smtp_auth_password: &#39;xxxxxxxxx&#39;   # 设置关闭邮箱的tls验证  smtp_require_tls: false# 设置告警通知的模版templates:- &#39;/etc/alertmanager/template/*.tmpl&#39;# 设置告警根路由，即分发策略route:  # 设置告警分组，即将具有相同标签的告警通知合并为告警组，作为单个通知发送  group_by: [&#39;alertname&#39;]  # 设置组内告警发送的等待时间，即组内收到第一个告警后的发送等待时间，目标是等待组内新增的告警以便同时合并发送，默认为30s  group_wait: 10s  # 设置组内不同批次告警发送的时间间隔，默认为5m  group_interval: 10s  # 设置告警未解决时重复发送的时间间隔，且此期间组内无新增告警，默认4h，建议根据告警严重程度进行设置  repeat_interval: 1h   # 设置默认告警接收者，即未被子路由的receivers.name选项匹配到的告警接收者  receiver: &#39;email&#39;  # 设置告警信息子路由  routes:  # 设置告警接收器，即指定发送人以及发送渠道，支持多种类型，如邮箱、钉钉、企业微信等  - receiver: &#39;email&#39;    # 设置匹配到该路由后是否继续匹配其余同级路由，默认为false，即匹配到后不再继续匹配    continue: true  - receiver: &#39;webhook&#39;    continue: true# 设置告警接收者receivers: - name: &#39;webhook&#39;   webhook_configs:   - url: http://localhost:8060/dingtalk/ops_dingding/send     # 设置当前收件人需要接收告警恢复通知     send_resolved: true - name: &#39;email&#39;   email_configs:   - to: &#39;xxxxxxxxxxxx@163.com&#39;   - to: &#39;xxxxxxxxxxxx@qq.com&#39;     send_resolved: true# 设置告警抑制规则，以减少垃圾告警的产生inhibit_rules:  # 设置抑制规则源告警的匹配标签、名称或注释，可为标签列表或正则表达式，可选参数  - source_match:      severity: &#39;critical&#39;    # 设置抑制规则目标告警的匹配标签、名称或注释，可为标签列表或正则表达式，可选参数    target_match:      severity: &#39;warning&#39;    # 设置源告警与目标告警相同的标签值，可选参数，意为同instance、alertname的warning告警将被critical告警抑制    equal: [&#39;alertname&#39;, &#39;instance&#39;]</code></pre><h1 id="2-配置邮件告警"><a href="#2-配置邮件告警" class="headerlink" title="2.配置邮件告警"></a>2.配置邮件告警</h1><p>邮件是目前企业最常用的告警通知方式，Alertmanager内置了对SMTP协议的支持，只需定义SMTP相关的配置，并在receiver中定义接收方邮件地址即可</p><h2 id="2-1-发送邮箱开启SMTP服务，获取登录授权码"><a href="#2-1-发送邮箱开启SMTP服务，获取登录授权码" class="headerlink" title="2.1 发送邮箱开启SMTP服务，获取登录授权码"></a>2.1 发送邮箱开启SMTP服务，获取登录授权码</h2><p><img src="/img/wiki/prometheus/email01.jpg" alt="email01"></p><ul><li>注：不是邮箱登录密码，而是发送邮箱开启SMTP服务后登录第三方邮件客户端的授权码</li></ul><h2 id="2-2-配置AlertManager"><a href="#2-2-配置AlertManager" class="headerlink" title="2.2 配置AlertManager"></a>2.2 配置AlertManager</h2><pre><code class="hljs">sudo vi /usr/local/prometheus/alertmanager.ymlglobal:  resolve_timeout: 1m  # 设置邮箱smtp服务器  smtp_smarthost: &#39;smtp.139.com:465&#39;  # 设置发件邮箱  smtp_from: &#39;sxs0618@139.com&#39;  # 设置发件账号  smtp_auth_username: &#39;sxs0618@q139.com&#39;  # 设置发件人邮箱授权码，注意不是登录密码  smtp_auth_password: &#39;xxxxxxxxx&#39;   # 设置关闭邮箱的tls验证  smtp_require_tls: false# 设置告警通知的模版templates:- &#39;/usr/local/prometheus/template/*.tmpl&#39;route:  # 设置告警分组，即将具有相同标签的告警通知合并为告警组，作为单个通知发送  group_by: [&#39;alertname&#39;]  # 设置组内告警发送的等待时间，即组内收到第一个告警后的发送等待时间，目标是等待组内新增的告警以便同时合并发送，默认为30s  group_wait: 10s  # 设置组内不同批次告警发送的时间间隔，默认为5m  group_interval: 10s  # 设置告警未解决时重复发送的时间间隔，且此期间组内无新增告警，默认4h，建议根据告警严重程度进行设置  repeat_interval: 1h   # 设置默认告警接收者，即未被子路由的receivers.name选项匹配到的告警接收者  receiver: &#39;email&#39;receivers: - name: &#39;email&#39;   email_configs:   - to: &#39;523343553@qq.com&#39;     html: &#39;&#123;&#123; template  "email.html". &#125;&#125;&#39;     headers: &#123; Subject: &quot;监控告警&#123;&#123;- if gt (len .Alerts.Resolved) 0 -&#125;&#125;恢复&#123;&#123; end &#125;&#125;&quot; &#125;     send_resolved: trueinhibit_rules:  # 设置抑制规则源告警的匹配标签、名称或注释，可为标签列表或正则表达式，可选参数  - source_match:      severity: &#39;Critical&#39;    # 设置抑制规则目标告警的匹配标签、名称或注释，可为标签列表或正则表达式，可选参数    target_match:      severity: &#39;Warning&#39;    # 设置源告警与目标告警相同的标签值，可选参数，意为同instance、alertname的warning告警将被critical告警抑制    equal: [&#39;alertname&#39;, &#39;instance&#39;]</code></pre><h1 id="2-3-创建告警模版"><a href="#2-3-创建告警模版" class="headerlink" title="2.3 创建告警模版"></a>2.3 创建告警模版</h1><pre><code class="hljs">sudo mkdir -p /usr/local/prometheus/templatesudo vi /usr/local/prometheus/template/email.tmpl&#123;&#123; define "email.html" &#125;&#125; &#123;&#123;- if gt (len .Alerts.Firing) 0 -&#125;&#125;&#123;&#123; range .Alerts &#125;&#125;=========start==========&lt;br&gt;告警集群: tc-mc &lt;br&gt;告警级别: &#123;&#123; .Labels.severity &#125;&#125; &lt;br&gt;告警类型: &#123;&#123; .Labels.alertname &#125;&#125; &lt;br&gt;故障主机: &#123;&#123; .Labels.instance &#125;&#125; &lt;br&gt;告警主题: &#123;&#123; .Annotations.summary &#125;&#125; &lt;br&gt;告警详情: &#123;&#123; .Annotations.description &#125;&#125; &lt;br&gt;告警时间：&#123;&#123; (.StartsAt.Add 28800e9).Format "2006-01-02 15:04:05" &#125;&#125;&lt;br&gt;=========end==========&lt;br&gt;&#123;&#123; end &#125;&#125;&#123;&#123; end &#125;&#125;&#123;&#123;- if gt (len .Alerts.Resolved) 0 -&#125;&#125;&#123;&#123; range .Alerts &#125;&#125;=========start==========&lt;br&gt;告警集群: tc-mc &lt;br&gt;告警级别: &#123;&#123; .Labels.severity &#125;&#125; &lt;br&gt;告警类型: &#123;&#123; .Labels.alertname &#125;&#125; &lt;br&gt;故障主机: &#123;&#123; .Labels.instance &#125;&#125; &lt;br&gt;告警主题: &#123;&#123; .Annotations.summary &#125;&#125; &lt;br&gt;告警详情: &#123;&#123; .Annotations.description &#125;&#125; &lt;br&gt;告警时间：&#123;&#123; (.StartsAt.Add 28800e9).Format "2006-01-02 15:04:05" &#125;&#125;&lt;br&gt; 恢复时间：&#123;&#123; (.EndsAt.Add 28800e9).Format "2006-01-02 15:04:05" &#125;&#125;&lt;br&gt; =========end==========&lt;br&gt;&#123;&#123;- end &#125;&#125;&#123;&#123;- end &#125;&#125;&#123;&#123;- end &#125;&#125;</code></pre><h1 id="2-4-重启AlertManager，验证告警信息"><a href="#2-4-重启AlertManager，验证告警信息" class="headerlink" title="2.4 重启AlertManager，验证告警信息"></a>2.4 重启AlertManager，验证告警信息</h1><p><img src="/img/wiki/prometheus/email02.jpg" alt="email02"></p><h1 id="3-配置钉钉告警"><a href="#3-配置钉钉告警" class="headerlink" title="3.配置钉钉告警"></a>3.配置钉钉告警</h1><h2 id="3-1-钉钉群创建机器人"><a href="#3-1-钉钉群创建机器人" class="headerlink" title="3.1 钉钉群创建机器人"></a>3.1 钉钉群创建机器人</h2><h2 id="3-2-部署钉钉告警插件"><a href="#3-2-部署钉钉告警插件" class="headerlink" title="3.2 部署钉钉告警插件"></a>3.2 部署钉钉告警插件</h2><h3 id="3-2-1-安装Webhook-dingtalk"><a href="#3-2-1-安装Webhook-dingtalk" class="headerlink" title="3.2.1 安装Webhook-dingtalk"></a>3.2.1 安装Webhook-dingtalk</h3><pre><code class="hljs">sudo mkdir -p /usr/local/prometheus/dingtalkwget https://github.com/timonwong/prometheus-webhook-dingtalk/releases/download/v2.1.0/prometheus-webhook-dingtalk-2.1.0.linux-amd64.tar.gztar -xzvf prometheus-webhook-dingtalk-2.1.0.linux-amd64.tar.gz sudo mv prometheus-webhook-dingtalk-2.1.0.linux-amd64 /usr/local/prometheus/dingtalk/webhook-dingtalksudo mv config.example.yml /usr/local/prometheus/dingtalk/config.yml</code></pre><h3 id="3-2-2-创建配置文件"><a href="#3-2-2-创建配置文件" class="headerlink" title="3.2.2 创建配置文件"></a>3.2.2 创建配置文件</h3><pre><code class="hljs">sudo vi /usr/local/prometheus/dingtalk/config.ymltemplates:  - /usr/local/prometheus/template/dingtalk.tmpltargets:  webhook:    url: https://oapi.dingtalk.com/robot/send?access_token=xxxxxxxxx    secret: xxxxxxxxx    message:      text: &#39;&#123;&#123; template "dingtalk.to.message" . &#125;&#125;&#39;    mention:      all: true</code></pre><h3 id="3-2-3-创建告警模版"><a href="#3-2-3-创建告警模版" class="headerlink" title="3.2.3 创建告警模版"></a>3.2.3 创建告警模版</h3><pre><code class="hljs">sudo mkdir -p /usr/local/prometheus/templatevi /usr/local/prometheus/template/dingtalk.tmpl&#123;&#123; define "dingtalk.to.message" &#125;&#125;&#123;&#123;- if gt (len .Alerts.Firing) 0 -&#125;&#125;&#123;&#123;- range $index, $alert := .Alerts -&#125;&#125;=========  **监控告警** ========= &lt;/br&gt;**告警集群:**    &#123;&#123; $alert.Labels.clusters &#125;&#125; &lt;/br&gt; **告警类型:**    &#123;&#123; $alert.Labels.alertname &#125;&#125; &lt;/br&gt;**告警级别:**    &#123;&#123; $alert.Labels.severity &#125;&#125;  **告警状态:**    &#123;&#123; .Status &#125;&#125;   **故障主机:**    &#123;&#123; $alert.Labels.hostname &#125;&#125; &#123;&#123; $alert.Labels.device &#125;&#125;   **告警主题:**    &#123;&#123; .Annotations.summary &#125;&#125;   **告警详情:**    &#123;&#123; $alert.Annotations.message &#125;&#125;&#123;&#123; $alert.Annotations.description&#125;&#125;   **主机标签:**    &#123;&#123; range .Labels.SortedPairs  &#125;&#125;  &lt;/br&gt; [&#123;&#123; .Name &#125;&#125;: &#123;&#123; .Value | markdown | html &#125;&#125; ] &#123;&#123;- end &#125;&#125; &lt;/br&gt;**故障时间:**    &#123;&#123; ($alert.StartsAt.Add 28800e9).Format "2006-01-02 15:04:05" &#125;&#125;  ========= = **end** =  =========  &lt;/br&gt;&#123;&#123;- end &#125;&#125;&#123;&#123;- end &#125;&#125;&#123;&#123;- if gt (len .Alerts.Resolved) 0 -&#125;&#125;&#123;&#123;- range $index, $alert := .Alerts -&#125;&#125;========= **故障恢复** ========= &lt;/br&gt; **告警集群:**    &#123;&#123; $alert.Labels.clusters &#125;&#125; &lt;/br&gt;**告警主题:**    &#123;&#123; $alert.Annotations.summary &#125;&#125;  **告警主机:**    &#123;&#123; .Labels.hostname &#125;&#125;   **告警类型:**    &#123;&#123; .Labels.alertname &#125;&#125;  **告警级别:**    &#123;&#123; $alert.Labels.severity &#125;&#125;    **告警状态:**    &#123;&#123; .Status &#125;&#125;  **告警详情:**    &#123;&#123; $alert.Annotations.message &#125;&#125;&#123;&#123; $alert.Annotations.description&#125;&#125;  **故障时间:**    &#123;&#123; ($alert.StartsAt.Add 28800e9).Format "2006-01-02 15:04:05" &#125;&#125;  **恢复时间:**    &#123;&#123; ($alert.EndsAt.Add 28800e9).Format "2006-01-02 15:04:05" &#125;&#125;  &#123;&#123;- end &#125;&#125;&#123;&#123;- end &#125;&#125;&#123;&#123;- end &#125;&#125;</code></pre><h3 id="3-2-4-创建启动脚本"><a href="#3-2-4-创建启动脚本" class="headerlink" title="3.2.4 创建启动脚本"></a>3.2.4 创建启动脚本</h3><pre><code class="hljs">sudo vi /lib/systemd/system/dingtalk-webhook.service[Unit]Description=Prometheus Webhook DingtalkDocumentation=https://github.com/timonwong/prometheus-webhook-dingtalkAfter=network.target[Service]ExecStart=/usr/local/prometheus/dingtalk/prometheus-webhook-dingtalk --config.file=/usr/local/prometheus/dingtalk/config.yml[Install]WantedBy=multi-user.target</code></pre><h3 id="3-2-5-启动Webhook-dingtalk"><a href="#3-2-5-启动Webhook-dingtalk" class="headerlink" title="3.2.5 启动Webhook-dingtalk"></a>3.2.5 启动Webhook-dingtalk</h3><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start dingtalk-webhook.service sudo systemctl enable dingtalk-webhook.service </code></pre><h2 id="3-3-配置Alertmanager"><a href="#3-3-配置Alertmanager" class="headerlink" title="3.3 配置Alertmanager"></a>3.3 配置Alertmanager</h2><pre><code class="hljs"># 设置全局参数，即作为默认值供子设置继承的公共设置，子参数中也可覆盖其设置global:  # 设置处理超时时间，即为告警的解决的时间，直接影响到警报恢复的通知时间，默认为5分钟，建议依据实际生产场景进行设置  resolve_timeout: 1m  # 设置邮箱smtp服务器  smtp_smarthost: &#39;smtp.139.com:465&#39;  # 设置发件邮箱  smtp_from: &#39;sxs0618@139.com&#39;  # 设置发件账号  smtp_auth_username: &#39;sxs0618@qq.com&#39;  # 设置发件人邮箱授权码，注意不是登录密码  smtp_auth_password: &#39;xxxxxxxxx&#39;   # 设置关闭邮箱的tls验证  smtp_require_tls: false# 设置告警通知的模版templates:- &#39;/etc/alertmanager/template/*.tmpl&#39;# 设置告警根路由，即分发策略route:  # 设置告警分组，即将具有相同标签的告警通知合并为告警组，作为单个通知发送  group_by: [&#39;alertname&#39;]  # 设置组内告警发送的等待时间，即组内收到第一个告警后的发送等待时间，目标是等待组内新增的告警以便同时合并发送，默认为30s  group_wait: 10s  # 设置组内不同批次告警发送的时间间隔，默认为5m  group_interval: 10s  # 设置告警未解决时重复发送的时间间隔，且此期间组内无新增告警，默认4h，建议根据告警严重程度进行设置  repeat_interval: 1h   # 设置默认告警接收者，即未被子路由的receivers.name选项匹配到的告警接收者  receiver: &#39;email&#39;  # 设置告警信息子路由  routes:  # 设置告警接收器，即指定发送人以及发送渠道，支持多种类型，如邮箱、钉钉、企业微信等  - receiver: &#39;email&#39;    # 设置匹配到该路由后是否继续匹配其余同级路由，默认为false，即匹配到后不再继续匹配    continue: true  - receiver: &#39;dingtalk&#39;    continue: true# 设置告警接收者receivers: - name: &#39;dingtalk&#39;   webhook_configs:   - url: http://localhost:8060/dingtalk/ops_dingding/send     # 设置当前收件人需要接收告警恢复通知     send_resolved: true - name: &#39;email&#39;   email_configs:   - to: &#39;xxxxxxxxxxxx@163.com&#39;   - to: &#39;xxxxxxxxxxxx@qq.com&#39;     send_resolved: true# 设置告警抑制规则，以减少垃圾告警的产生inhibit_rules:  # 设置抑制规则源告警的匹配标签、名称或注释，可为标签列表或正则表达式，可选参数  - source_match:      severity: &#39;Critical&#39;    # 设置抑制规则目标告警的匹配标签、名称或注释，可为标签列表或正则表达式，可选参数    target_match:      severity: &#39;Warning&#39;    # 设置源告警与目标告警相同的标签值，可选参数，意为同instance、alertname的warning告警将被critical告警抑制    equal: [&#39;alertname&#39;, &#39;instance&#39;]</code></pre><h2 id="3-4-重启Alertmanager，验证告警信息"><a href="#3-4-重启Alertmanager，验证告警信息" class="headerlink" title="3.4 重启Alertmanager，验证告警信息"></a>3.4 重启Alertmanager，验证告警信息</h2><p><img src="/img/wiki/prometheus/dingtalk.jpg" alt="dingtalk"></p><h1 id="4-配置企业微信告警"><a href="#4-配置企业微信告警" class="headerlink" title="4.配置企业微信告警"></a>4.配置企业微信告警</h1><h2 id="4-1-企业微信群创建机器人"><a href="#4-1-企业微信群创建机器人" class="headerlink" title="4.1 企业微信群创建机器人"></a>4.1 企业微信群创建机器人</h2><h2 id="4-2-部署Redis"><a href="#4-2-部署Redis" class="headerlink" title="4.2 部署Redis"></a>4.2 部署Redis</h2><h2 id="4-3-部署告警插件"><a href="#4-3-部署告警插件" class="headerlink" title="4.3 部署告警插件"></a>4.3 部署告警插件</h2><h3 id="4-3-1-下载安装包"><a href="#4-3-1-下载安装包" class="headerlink" title="4.3.1 下载安装包"></a>4.3.1 下载安装包</h3><pre><code class="hljs">wget https://github.com/Rainbowhhy/alertmanager-webhook/releases/download/v1.0/alertmanager-webhook-v1.0-linux-amd64.tar.gztar -xzvf alertmanager-webhook-v1.0-linux-amd64.tar.gzcd alertmanager-webhook-v1.0-linux-amd64 &amp;&amp; sudo cp alertmanager-webhook* /usr/local/prometheus</code></pre><h3 id="4-3-2-创建配置文件"><a href="#4-3-2-创建配置文件" class="headerlink" title="4.3.2 创建配置文件"></a>4.3.2 创建配置文件</h3><pre><code class="hljs">sudo vi /usr/local/prometheus/alertmanager-webhook.yaml# 企业微信机器人key# 使用企业微信时必须配置，不使用则留空qywechatKey: xxxxxxxx-xxx-xxxx-xxxx-xxxxxxxx# 飞书机器人key# 使用飞书时必须配置，不使用则留空feishuKey:# 钉钉机器人key# 使用钉钉时必须配置，不使用则留dingdingKey:# Redis配置redisServer: 127.0.0.1  # 必须配置redisPort:  # 可选项，为空默认为6379redisPassword:  # redis未设置密码则留空，如果设置了密码登陆则必须配置# 日志配置logFileDir: /var/log  # 可选项，为空则为程序运行目录logFilePath: alertmanager-webhook.log # 必须配置# 服务监听配置port: 9095 # 可选项，为空则默认为9095host: 0.0.0.0 # 可选项，为空默认监听 127.0.0.1</code></pre><h3 id="4-3-3-创建告警模板"><a href="#4-3-3-创建告警模板" class="headerlink" title="4.3.3 创建告警模板"></a>4.3.3 创建告警模板</h3><pre><code class="hljs">sudo vi /usr/local/prometheus/template/alert.tmpl&#123;&#123;- if eq .Status `firing` -&#125;&#125;&#123;&#123;- /*  自定义触发告警时的内容格式  */ -&#125;&#125;********************告警程序: Prometheus告警类型: &#123;&#123;.Labels.alertname&#125;&#125;告警级别: &#123;&#123;.Labels.severity&#125;&#125;告警主机: &#123;&#123;.Labels.instance&#125;&#125;告警主题: &#123;&#123;.Annotations.summary&#125;&#125;告警详情: &#123;&#123;.Annotations.description&#125;&#125;触发时间: &#123;&#123;.StartTime&#125;&#125;********************&#123;&#123;- else if eq .Status `resolved` -&#125;&#125;&#123;&#123;- /*  自定义告警恢复时的内容格式  */ -&#125;&#125;********************告警类型: &#123;&#123;.Labels.alertname&#125;&#125;告警级别: &#123;&#123;.Labels.severity&#125;&#125;告警主机: &#123;&#123;.Labels.instance&#125;&#125;告警主题: &#123;&#123;.Annotations.summary&#125;&#125;告警详情: &#123;&#123;.Annotations.description&#125;&#125;开始时间: &#123;&#123;.StartTime&#125;&#125;恢复时间: &#123;&#123;.EndTime&#125;&#125;********************&#123;&#123;- end -&#125;&#125;</code></pre><h3 id="4-3-4-创建启动脚本"><a href="#4-3-4-创建启动脚本" class="headerlink" title="4.3.4 创建启动脚本"></a>4.3.4 创建启动脚本</h3><pre><code class="hljs">sudo vi /lib/systemd/system/alertmanager-webhook.service[Unit]Description=Alertmanager Server WebhookAfter=network.target[Service]Type=simpleUser=rootGroup=rootWorkingDirectory=/opt/prometheusExecStart=/opt/prometheus/alertmanager-webhook -c /opt/prometheus/alertmanager-webhook.yamlExecReload=/bin/kill -s HUP $MAINPIDExecStop=/bin/kill -s QUIT $MAINPIDRestart=on-failure[Install]WantedBy=multi-user.target</code></pre><h3 id="4-3-5-启动alertmanager-webhook"><a href="#4-3-5-启动alertmanager-webhook" class="headerlink" title="4.3.5 启动alertmanager-webhook"></a>4.3.5 启动alertmanager-webhook</h3><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start alertmanager-webhook.servicesudo systemctl enable alertmanager-webhook.service</code></pre><h2 id="4-4-配置Alertmanager"><a href="#4-4-配置Alertmanager" class="headerlink" title="4.4 配置Alertmanager"></a>4.4 配置Alertmanager</h2><pre><code class="hljs">sudo vi /opt/prometheus/alertmanager.yml  # 设置告警信息子路由  routes:  # 设置告警接收器，即指定发送人以及发送渠道，支持多种类型，如邮箱、钉钉、企业微信等  - receiver: &#39;email&#39;    # 设置匹配到该路由后是否继续匹配其余同级路由，默认为false，即匹配到后不再继续匹配    continue: true  - receiver: &#39;dingtalk&#39;    continue: true  - receiver: &#39;wechat&#39;    continue: true# 设置告警接收者receivers: - name: &#39;dingtalk&#39;   webhook_configs:   - url: http://localhost:8060/dingtalk/ops_dingding/send     # 设置当前收件人需要接收告警恢复通知     send_resolved: true - name: &#39;email&#39;   email_configs:   - to: &#39;xxxxxxxxxxxx@163.com&#39;   - to: &#39;xxxxxxxxxxxx@qq.com&#39;     send_resolved: true - name: &#39;wechat&#39;   webhook_configs:   - url: &#39;http://127.0.0.1:9095/qywechat&#39;     send_resolved: true</code></pre><h2 id="4-5-重启Alertmanager，验证告警信息"><a href="#4-5-重启Alertmanager，验证告警信息" class="headerlink" title="4.5 重启Alertmanager，验证告警信息"></a>4.5 重启Alertmanager，验证告警信息</h2><p><img src="/img/wiki/prometheus/wechat.jpg" alt="wechat"></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://zhuanlan.zhihu.com/p/672779794">https://zhuanlan.zhihu.com/p/672779794</a></li><li><a href="https://github.com/Rainbowhhy/alertmanager-webhook">https://github.com/Rainbowhhy/alertmanager-webhook</a></li><li><a href="https://andyoung.blog.csdn.net/article/details/126243110">https://andyoung.blog.csdn.net/article/details/126243110</a></li><li><a href="https://blog.csdn.net/qq_43164571/article/details/113104877">https://blog.csdn.net/qq_43164571/article/details/113104877</a></li><li><a href="https://blog.csdn.net/weixin_45310323/article/details/134103279">https://blog.csdn.net/weixin_45310323/article/details/134103279</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Prometheus</tag>
      
      <tag>监控告警</tag>
      
      <tag>Alertmanager</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prometheus监控服务自动发现机制详解</title>
    <link href="/linux/PrometheusDiscover/"/>
    <url>/linux/PrometheusDiscover/</url>
    
    <content type="html"><![CDATA[<p>服务发现，即Prometheus自动检测、分类和识别新增及变更的监控目标的机制，分为基于DNS、文件、Consul和Kubernetes集群这几种类型</p><h1 id="监控指标抓取流程"><a href="#监控指标抓取流程" class="headerlink" title="监控指标抓取流程"></a>监控指标抓取流程</h1><p>Prometheus监控标签，附加到指标名称或指标值的标识监控数据元数据的键值对，以便于监控数据的过滤、聚合和查询，用于标识应用程序实例与环境，如生产、测试、开发环境或分散的数据中心等</p><h2 id="1-指标发现"><a href="#1-指标发现" class="headerlink" title="1.指标发现"></a>1.指标发现</h2><p>Prometheus在每个scrape_interval期间都会检查执行的任务（Job），这些Job将根据指定的发现配置生成Target列表，完成服务发现过程</p><h2 id="2-配置标签"><a href="#2-配置标签" class="headerlink" title="2.配置标签"></a>2.配置标签</h2><p>服务发现将会返回一个Target列表，并为之配置内置标签和自定义标签，以”__”前缀为前缀均为内置标签，如以”_<em>meta</em>“为前缀的元数据标签，只供Prometheus使用而不写入时序数据库，也无法在promql查到；“__scheme__”表示target支持使用协议（http或https，默认为http），“__address__”表示target的地址，“__metrics_path__”表示指标的URI路径（默认为&#x2F;metrics），若URI路径中存在请求参数，则前缀将设置为“__param_；被重复利用以生成其他标签，如instance标签的默认值就来自于__address__标签。自然，也包括用户为方便多维度查询的自定义标签</p><h2 id="3-relabel-configs"><a href="#3-relabel-configs" class="headerlink" title="3.relabel_configs"></a>3.relabel_configs</h2><p>抓取指标之前通过relabel_configs对Target实例的标签进行重写，较为常用，如过滤不需要被抓取的target、删除不需要或者敏感标签、根据已有的标签生成新标签等，重写标签之后以__开头的标签将被从标签集中删除，可确定需要抓取的目标及其标签</p><h2 id="4-指标抓取"><a href="#4-指标抓取" class="headerlink" title="4.指标抓取"></a>4.指标抓取</h2><p>Prometheus按照指标名称及标签抓取监控数据，完成样本数据采集</p><h2 id="5-metric-relabel-configs"><a href="#5-metric-relabel-configs" class="headerlink" title="5.metric_relabel_configs"></a>5.metric_relabel_configs</h2><p>为了更好的标识监控指标，Prometheus允许对原始数据进行编辑，类似于relabel_configs也是对标签进行重写，如添加、修改、删除标签及其格式。不同于relabel_configs操作target，metric_relabel_configs的操作对象Metric，也即时间序列</p><h2 id="6-指标保存"><a href="#6-指标保存" class="headerlink" title="6.指标保存"></a>6.指标保存</h2><p>Prometheus最后将指标、值及其标签写入时序数据库，以供后续的聚合、查询与分析</p><h1 id="1-基于文件自动发现"><a href="#1-基于文件自动发现" class="headerlink" title="1.基于文件自动发现"></a>1.基于文件自动发现</h1><p>该类型是实现最为简单的服务发现方式，不依赖于其他任何平台或第三方服务，略由于静态配置，主要是通过Prometheus Server周期性地读取与重载文件所定义的target的信息，包括target列表及可选的标签，文件格式支持json或yaml</p><h2 id="1-1-创建配置文件"><a href="#1-1-创建配置文件" class="headerlink" title="1.1 创建配置文件"></a>1.1 创建配置文件</h2><pre><code class="hljs">sudo mkdir -p /usr/local/prometheus/configssudo vi /usr/local/prometheus/configs/node.yaml- targets:  - node01:9100  - node02:9100  - node03:9100  labels:    app: node-exporter</code></pre><h2 id="1-2-Prometheus加载配置"><a href="#1-2-Prometheus加载配置" class="headerlink" title="1.2 Prometheus加载配置"></a>1.2 Prometheus加载配置</h2><pre><code class="hljs">sudo vi /usr/local/prometheus/prometheus.yml  - job_name: &quot;node&quot;    file_sd_configs:    - files:      - /usr/local/prometheus/configs/*.yaml      refresh_interval: 2m</code></pre><h2 id="1-3-重载Prometheus"><a href="#1-3-重载Prometheus" class="headerlink" title="1.3 重载Prometheus"></a>1.3 重载Prometheus</h2><pre><code class="hljs">curl -XPOST http://127.0.0.1:9090/-/reload</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/q2524607033/article/details/134574252">https://blog.csdn.net/q2524607033/article/details/134574252</a></li><li><a href="https://blog.csdn.net/qq_30614345/article/details/131776198">https://blog.csdn.net/qq_30614345/article/details/131776198</a></li><li><a href="https://blog.csdn.net/m0_59430185/article/details/121853136">https://blog.csdn.net/m0_59430185/article/details/121853136</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Prometheus</tag>
      
      <tag>监控告警</tag>
      
      <tag>Alertmanager</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python异常处理</title>
    <link href="/linux/PythonError/"/>
    <url>/linux/PythonError/</url>
    
    <content type="html"><![CDATA[<p>异常，即程序运行过程所出现的异常中断和退出错误，如语法错误、调用错误、文件不存在、磁盘空间不足、网络拥堵、权限不足等等。大多数的异常事件都不会被程序处理，而是以错误信息的形式展现出来，最终中断程序的正常执行，甚至对文件造成损害。因此，为了增强程序的健壮性，防止程序的突然中止引发严重后果，编写程序时需要考虑可能发生的异常及其处理方案</p><p>Python内置了多种类型的异常，无需导入即可直接使用，由try&#x2F;except语句进行捕获，工作机制为：先执行可能引发异常的try子句，若无异常则忽略掉except子句后执行else语句（若存在else语句）。若try子句的执行过程发生了异常，则忽略掉try子句的剩余部分后，执行与异常类型相符的except子句，完成异常处理；若try子句异常类型不在except所定义的捕获列表，则将异常逐层递交到上层的try，直到程序最上层，最后抛出异常信息</p><h1 id="1-捕获异常"><a href="#1-捕获异常" class="headerlink" title="1.捕获异常"></a>1.捕获异常</h1><pre><code class="hljs">&gt;&gt;&gt; try:...     print(1)...     print(&quot;未发生异常，语句正常执行&quot;)...     print(1/0)...     print(&quot;上一条语句发生异常，在其之后的语句将不被执行&quot;)... except:...     print(&quot;程序错误，请检查!&quot;)... 1未发生异常，语句正常执行程序错误，请检查!</code></pre><ul><li>注：该语句except部分没有指定具体的异常类型，try语句发生的所有异常都将被捕获，不能识别出具体的异常信息，不推荐</li></ul><h1 id="2-捕获通用异常"><a href="#2-捕获通用异常" class="headerlink" title="2.捕获通用异常"></a>2.捕获通用异常</h1><p>Python内置的通用异常Exception可以捕获任意异常，适用于无法预判到所有可能发生的异常及其类型的场景，以免异常引发的程序阻塞。但捕获所有发生的异常将会降低程序的效率，所以精确捕获异常还是非常必要的措施</p><pre><code class="hljs">&gt;&gt;&gt; try:...     n = input(&#39;请输入数字:&#39;)...     print(int(n))... except Exception as e:...     print(e)... 请输入数字:testinvalid literal for int() with base 10: &#39;test&#39;</code></pre><h1 id="3-捕获特定类型异常"><a href="#3-捕获特定类型异常" class="headerlink" title="3.捕获特定类型异常"></a>3.捕获特定类型异常</h1><p>为了识别出异常的具体类型，以便于根据不同类型的异常做不同的异常处理，可在except关键字后面说明具体的异常类型，以捕获特定异常类型的异常对象，as关键字用于给异常对象命名</p><pre><code class="hljs">&gt;&gt;&gt; m = 10&gt;&gt;&gt; n = input(&#39;请输入数字:&#39;)请输入数字:0&gt;&gt;&gt; try:...     print(m/int(n))... except ZeroDivisionError as e:...     print(e)...     n = input(&#39;发生了零除错误，请重新输入变量n的值:&#39;)...     print(m/int(n))... division by zero发生了零除错误，请重新输入变量n的值:110.0</code></pre><h2 id="3-1-捕获多种异常类型"><a href="#3-1-捕获多种异常类型" class="headerlink" title="3.1 捕获多种异常类型"></a>3.1 捕获多种异常类型</h2><h1 id="4-内置异常类"><a href="#4-内置异常类" class="headerlink" title="4.内置异常类"></a>4.内置异常类</h1><p>Python内置异常类是从由BaseException类派生，常见异常类如下</p><table><thead><tr><th align="center">异常</th><th align="center">描述</th></tr></thead><tbody><tr><td align="center">IOError</td><td align="center">输入&#x2F;输出异常</td></tr><tr><td align="center">ImportError</td><td align="center">路径或名称错误引发的模块或包导入异常</td></tr><tr><td align="center">IndentationError</td><td align="center">缩进错误</td></tr><tr><td align="center">IndexError</td><td align="center">索引下标错误</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes集群节点维护策略</title>
    <link href="/linux/KubernetesDrains/"/>
    <url>/linux/KubernetesDrains/</url>
    
    <content type="html"><![CDATA[<p>Kubernetes集群节点的维护操作将导致业务的连续性不能得到保障，如操作系统升级、硬件维修与更换等，甚至需要下线停机，因此需要规划Pod平滑迁移的策略或方案</p><h1 id="1-创建PodDisruptionBudget"><a href="#1-创建PodDisruptionBudget" class="headerlink" title="1.创建PodDisruptionBudget"></a>1.创建PodDisruptionBudget</h1><p>PodDisruptionBudget，即Pod干扰预算或主动驱逐保护，是Kubernetes集群控制可以中断的Pod数量资源对象，用于确保系统维护、升级或其他操作时应用Pod不会被意外中断或终止，从而保障系统的可用性和可靠性</p><h2 id="1-1-创建PDB资源文件"><a href="#1-1-创建PDB资源文件" class="headerlink" title="1.1 创建PDB资源文件"></a>1.1 创建PDB资源文件</h2><pre><code class="hljs">vi pdb-nginx-test.yamlapiVersion: policy/v1kind: PodDisruptionBudgetmetadata:  name: nginx-testspec:  # 设置可用Pod的最小值，或者设置不可用最大值maxUnavailable，二者不可共存  minAvailable: 1  # 设置标签选择器，即所要匹配的Pod组  selector:    matchLabels:      app: nginx-test</code></pre><h2 id="1-2-部署PDB"><a href="#1-2-部署PDB" class="headerlink" title="1.2 部署PDB"></a>1.2 部署PDB</h2><pre><code class="hljs">kubectl apply -f pdb-nginx-test.yaml</code></pre><h1 id="2-设置节点不可调度"><a href="#2-设置节点不可调度" class="headerlink" title="2.设置节点不可调度"></a>2.设置节点不可调度</h1><pre><code class="hljs">kubectl cordon worker01</code></pre><h1 id="3-驱逐节点运行的Pod"><a href="#3-驱逐节点运行的Pod" class="headerlink" title="3.驱逐节点运行的Pod"></a>3.驱逐节点运行的Pod</h1><pre><code class="hljs">kubectl drain worker01 --delete-local-data --ignore-daemonsets --force</code></pre><h1 id="4-维护结束，设置节点可调度"><a href="#4-维护结束，设置节点可调度" class="headerlink" title="4.维护结束，设置节点可调度"></a>4.维护结束，设置节点可调度</h1><pre><code class="hljs">kubectl uncordon worker01</code></pre><h1 id="5-节点删除"><a href="#5-节点删除" class="headerlink" title="5.节点删除"></a>5.节点删除</h1><pre><code class="hljs">kubectl delete node worker01</code></pre><h1 id="6-节点重新加入集群"><a href="#6-节点重新加入集群" class="headerlink" title="6.节点重新加入集群"></a>6.节点重新加入集群</h1><h2 id="6-1-主节点获取加入节点的命令"><a href="#6-1-主节点获取加入节点的命令" class="headerlink" title="6.1 主节点获取加入节点的命令"></a>6.1 主节点获取加入节点的命令</h2><pre><code class="hljs">kubeadm token create --print-join-command</code></pre><h2 id="6-2-Worker节点重新加入集群"><a href="#6-2-Worker节点重新加入集群" class="headerlink" title="6.2 Worker节点重新加入集群"></a>6.2 Worker节点重新加入集群</h2><pre><code class="hljs">kubeadm join 192.168.100.189:8443 --token dcwrhm.6wi8mn63s10gxrcf --discovery-token-ca-cert-hash sha256:af6e4d737cbd7e294036d7391a5931fba589942e777811bb6f74b77ccbda3cfc</code></pre><h2 id="6-3-验证节点状态"><a href="#6-3-验证节点状态" class="headerlink" title="6.3 验证节点状态"></a>6.3 验证节点状态</h2><pre><code class="hljs">kubectl get nodes -o wide </code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.ngui.cc/zz/2318348.html">https://www.ngui.cc/zz/2318348.html</a></li><li><a href="https://bertram.blog.csdn.net/article/details/129385498">https://bertram.blog.csdn.net/article/details/129385498</a></li><li><a href="https://blog.csdn.net/weixin_43616190/article/details/126433485">https://blog.csdn.net/weixin_43616190/article/details/126433485</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python格式化输出</title>
    <link href="/linux/PythonFormat/"/>
    <url>/linux/PythonFormat/</url>
    
    <content type="html"><![CDATA[<p>格式化输出，即字符的转化与连接操作，通常配合输出函数print()使用，所以又被称为字符串格式化输出。格式化使得程序的输出更加的美观、易读，工作机制是输出的文本预先为变量留出一个位置，被称为占位符，输出时直接用变量的值代替这个占位符进行输出，即可按照将占位符标明的格式进行输出。Python程序的格式化方式分为三种，即%方式、format()方法和f-string方式</p><h1 id="1-格式化"><a href="#1-格式化" class="headerlink" title="1.%格式化"></a>1.%格式化</h1><p>%格式化是Python最早使用的传统格式化方式，即以百分号%作为占位符，输出时按照顺序将变量的值填充到占位符百分号%前面所对应的位置</p><h2 id="1-1-字符串格式化"><a href="#1-1-字符串格式化" class="headerlink" title="1.1 字符串格式化"></a>1.1 字符串格式化</h2><p>%s表示字符串类型的变量，%%表示百分号%</p><pre><code class="hljs">str = &#39;Python&#39;&gt;&gt;&gt; print(&quot;This is %s!&quot; %str)This is Python!&gt;&gt;&gt; name = &#39;Lilei&#39;&gt;&gt;&gt; age = 15&gt;&gt;&gt; print(&quot;My name is %s,I am %d years old&quot; %(name,age))My name is Lilei,I am 15 years old</code></pre><h2 id="1-2-整数格式化"><a href="#1-2-整数格式化" class="headerlink" title="1.2 整数格式化"></a>1.2 整数格式化</h2><p>%d表示十进制整数，%b表示二进制整数，%o表示八进制整数，%x表示十六进制的整数</p><pre><code class="hljs">&gt;&gt;&gt; x = 16&gt;&gt;&gt; x8 = 0o20&gt;&gt;&gt; x16 = 0x10print(&quot;%d的八进制数为%o,十六进制数为%x&quot; %(x,x8,x16))16的八进制数为20,十六进制数为10</code></pre><h2 id="1-3-浮点数格式化"><a href="#1-3-浮点数格式化" class="headerlink" title="1.3 浮点数格式化"></a>1.3 浮点数格式化</h2><p>%f表示浮点类型的变量，小数点保留位数用”.数字f”表示，如.3f表示保留三位小数</p><pre><code class="hljs">&gt;&gt;&gt; r = 3&gt;&gt;&gt; s = 3.14 * r * r&gt;&gt;&gt; print(&quot;半径为%d的圆面积为%.3f&quot; %(r,s))半径为3的圆面积为28.260</code></pre><h1 id="2-format-方法"><a href="#2-format-方法" class="headerlink" title="2.format()方法"></a>2.format()方法</h1><p>format()方法格式化自由度更高，不再区分填充值的类型,使用花括号{}作为占位符，即用传入的参数依次替换字符串内的占位符，如{变量1}、{变量2}。此外，format()方法还支持指定顺序填充及关键字填充等功能，用法浅显易懂</p><pre><code class="hljs">&gt;&gt;&gt; name = &#39;Lilei&#39;&gt;&gt;&gt; no = 1000&gt;&gt;&gt; print(&quot;My name is &#123;&#125;,no is &#123;&#125;&quot;.format(name,no))My name is Lilei,no is 1000# 指定顺序填充，即是按照format()方法所传入参数的索引进行填充，序号从0开始&gt;&gt;&gt; print(&quot;&#123;1&#125; is th no of &#123;0&#125;&quot; .format(name,no))1000 is th no of Lilei# 指定关键字进行填充&gt;&gt;&gt; print(&quot;&#123;name&#125; no is &#123;no&#125;&quot;.format(name=&#39;Lilei&#39;,no=1000))Lilei no is 1000</code></pre><h1 id="3-f-string格式化"><a href="#3-f-string格式化" class="headerlink" title="3.f-string格式化"></a>3.f-string格式化</h1><p>f-string格式化方法自Python3.6开始引入，以f开头，可在其中使用花括号{}替换变量，非常简单易懂，建议使用</p><pre><code class="hljs">&gt;&gt;&gt; print(f&#39;My name is &#123;name&#125;,no is &#123;no&#125;&#39;)My name is Lilei,no is 1000</code></pre>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Centos7升级内核</title>
    <link href="/linux/CentOSkernelUpdate/"/>
    <url>/linux/CentOSkernelUpdate/</url>
    
    <content type="html"><![CDATA[<p>Linux内核，即kernel，是由林纳斯·托瓦兹编写的开源操作系统核心程序，负责管理硬件、文件系统和网络等资源，目前由Linux开源社区维护、更新与修复。Linux内核版本是指其版本号，是其独特的标识，包括主版本号、次版本号和修订号，通常主版本号表示基本的架构和功能的重大改变，次版本号表示较小的变化（偶数为稳定版，奇数为开发版），修订号表示修复次数，如4.18.10。Linux内核版本大致分为两类，即ml和lt，也就是mainline stable（稳定主线版）和long term support（长期支持版），两者可以共存，但每种类型内核只能存在一个版本。为了获得更好的使用体验和漏洞的修复，及时的更新内核版本非常有必要</p><h1 id="1-查看当前版本号"><a href="#1-查看当前版本号" class="headerlink" title="1.查看当前版本号"></a>1.查看当前版本号</h1><pre><code class="hljs">uname -r</code></pre><h1 id="2-导入仓库源"><a href="#2-导入仓库源" class="headerlink" title="2.导入仓库源"></a>2.导入仓库源</h1><pre><code class="hljs">sudo rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.orgsudo rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm</code></pre><h1 id="3-查看可安装的软件包"><a href="#3-查看可安装的软件包" class="headerlink" title="3.查看可安装的软件包"></a>3.查看可安装的软件包</h1><pre><code class="hljs">sudo yum --enablerepo=&quot;elrepo-kernel&quot; list --showduplicates | sort -r | grep kernel-ml.x86_64</code></pre><h1 id="4-安装内核版本"><a href="#4-安装内核版本" class="headerlink" title="4.安装内核版本"></a>4.安装内核版本</h1><pre><code class="hljs"># 安装ML版本yum --enablerepo=elrepo-kernel install  kernel-ml-devel kernel-ml -y   # 安装LT版本yum --enablerepo=elrepo-kernel install kernel-lt-devel kernel-lt -y</code></pre><h1 id="5-配置内核启动顺序"><a href="#5-配置内核启动顺序" class="headerlink" title="5.配置内核启动顺序"></a>5.配置内核启动顺序</h1><h2 id="5-1-查看当前内核启动顺序"><a href="#5-1-查看当前内核启动顺序" class="headerlink" title="5.1 查看当前内核启动顺序"></a>5.1 查看当前内核启动顺序</h2><pre><code class="hljs">sudo awk -F\&#39; &#39;$1==&quot;menuentry &quot; &#123;print $2&#125;&#39; /etc/grub2.cfg</code></pre><h2 id="5-2-配置内核启动顺序"><a href="#5-2-配置内核启动顺序" class="headerlink" title="5.2 配置内核启动顺序"></a>5.2 配置内核启动顺序</h2><pre><code class="hljs"># 设置启动项的版本的序号，从0开始计数grub2-set-default 0</code></pre><h1 id="6-重启服务器，验证内核版本"><a href="#6-重启服务器，验证内核版本" class="headerlink" title="6.重启服务器，验证内核版本"></a>6.重启服务器，验证内核版本</h1><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/weixin_39094034/article/details/127523196">https://blog.csdn.net/weixin_39094034/article/details/127523196</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>CentOS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes集群网络策略详解</title>
    <link href="/linux/KubernetesNetworkPolicy/"/>
    <url>/linux/KubernetesNetworkPolicy/</url>
    
    <content type="html"><![CDATA[<p>NetworkPolicy，即网络策略，是Kubernetes集群控制Pod通信规则的命名空间级别的资源对象，通过标签筛选Pod组并定义规则管控组内流量，从而为集群提供更精细的流量控制和租户隔离机制，最终削减应用网络攻击的影响范围。网络策略实现依靠CNI网络插件完成，如Calico、Weave和Antrea等，flannel则不支持</p><h1 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h1><ul><li>1.集群管理员通过API或Web页面的方式创建NetworkPolicy资源对象</li><li>2.CNI插件的NetworkPolicyController监听到新增的NetworkPolicy资源，读取到具体信息后写入etcd数据库</li><li>3.节点运行的CNI Agent（calico-felix）组件从etcd数据库中获取NetworkPolicy资源，调用iptables做相应配置</li></ul><h1 id="1-配置默认网络策略"><a href="#1-配置默认网络策略" class="headerlink" title="1.配置默认网络策略"></a>1.配置默认网络策略</h1><p>Kubernetes集群默认不对Pod流量做任何限制，既能够与集群上其他任何Pod通信，也能够与集群外部的网络端点通信，也即不具备网络隔离功能。NetworkPolicy通过附加到命名空间所属Pod的方式管控Pod的入站（Ingress）和出站（Egress）流量，被附加的Pod只放行网络策略明确允许的流量，未被标签选择器选中的Pod流量则不受影响，且网络策略是叠加的关系，不会产生冲突，其顺序也不影响策略的结果</p><pre><code class="hljs">kubectl create namespace testkubectl -n test run nginx001 --image nginx --image-pull-policy=IfNotPresent --labels app=nginxkubectl -n test run nginx002 --image nginx --image-pull-policy=IfNotPresent --port 80 --expose --labels app=nginx# 查看命名空间下所有网络策略，默认为空kubectl -n test get networkpolicy</code></pre><h2 id="1-1-配置默认入站流量策略"><a href="#1-1-配置默认入站流量策略" class="headerlink" title="1.1 配置默认入站流量策略"></a>1.1 配置默认入站流量策略</h2><h3 id="1-1-1-默认拒绝所有入口流量"><a href="#1-1-1-默认拒绝所有入口流量" class="headerlink" title="1.1.1 默认拒绝所有入口流量"></a>1.1.1 默认拒绝所有入口流量</h3><p>命名空间所有容器都不允许任何进入</p><pre><code class="hljs">apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata:   name: default-deny-ingress spec:   podSelector: &#123;&#125;   policyTypes:     - Ingress</code></pre><h3 id="1-1-2-默认允许所有入口流量"><a href="#1-1-2-默认允许所有入口流量" class="headerlink" title="1.1.2 默认允许所有入口流量"></a>1.1.2 默认允许所有入口流量</h3><p>命名空间所有容器都允许任何流量进入</p><pre><code class="hljs">apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata:   name: allow-all-ingress spec:   podSelector: &#123;&#125;   ingress:     - &#123;&#125;   policyTypes:     - Ingress</code></pre><h2 id="1-2-配置默认出站流量策略"><a href="#1-2-配置默认出站流量策略" class="headerlink" title="1.2 配置默认出站流量策略"></a>1.2 配置默认出站流量策略</h2><h3 id="1-2-1-默认拒绝所有出口流量"><a href="#1-2-1-默认拒绝所有出口流量" class="headerlink" title="1.2.1 默认拒绝所有出口流量"></a>1.2.1 默认拒绝所有出口流量</h3><pre><code class="hljs">apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata:   name: default-deny-egress spec:   podSelector: &#123;&#125;   policyTypes:     - Egress</code></pre><h3 id="1-2-2-默认允许所有出口流量"><a href="#1-2-2-默认允许所有出口流量" class="headerlink" title="1.2.2 默认允许所有出口流量"></a>1.2.2 默认允许所有出口流量</h3><pre><code class="hljs">apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata:   name: allow-all-egress spec:   podSelector: &#123;&#125;   egress:     - &#123;&#125;   policyTypes:     - Egress</code></pre><h2 id="1-3-默认拒绝所有入口和所有出口流量"><a href="#1-3-默认拒绝所有入口和所有出口流量" class="headerlink" title="1.3 默认拒绝所有入口和所有出口流量"></a>1.3 默认拒绝所有入口和所有出口流量</h2><pre><code class="hljs">apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata:   name: default-deny-all spec:   podSelector: &#123;&#125;   policyTypes:     - Ingress     - Egress</code></pre><h1 id="2-配置Pod标签选择器方式网络策略"><a href="#2-配置Pod标签选择器方式网络策略" class="headerlink" title="2.配置Pod标签选择器方式网络策略"></a>2.配置Pod标签选择器方式网络策略</h1><h2 id="2-1-入站网络策略"><a href="#2-1-入站网络策略" class="headerlink" title="2.1 入站网络策略"></a>2.1 入站网络策略</h2><pre><code class="hljs">apiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata:  name: my-network-policy  namaspace: testspec:  podSelector:    matchLabels:      # 设置网络策略所匹配Pod的标签，若标签为空则匹配命名空间所属全部Pod      app: nginx  policyTypes:  - Ingress  ingress:  # 设置允许访问的Pod  - from:    - podSelector:        matchLabels:          role: podclient    ports:    - protocol: TCP      port: 80</code></pre><h2 id="2-2-出站网络策略"><a href="#2-2-出站网络策略" class="headerlink" title="2.2 出站网络策略"></a>2.2 出站网络策略</h2><pre><code class="hljs">apiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata:  name: my-network-policy  namaspace: testspec:  podSelector:    matchLabels:      # 设置网络策略所匹配Pod的标签，若标签为空则匹配命名空间所属全部Pod      app: nginx  policyTypes:  - Egress  egress:  # 设置允许访问的Pod  - to:    - podSelector:        matchLabels:          role: podclient    ports:    - protocol: TCP      port: 80</code></pre><h1 id="3-配置命名空间选择器方式网络策略"><a href="#3-配置命名空间选择器方式网络策略" class="headerlink" title="3.配置命名空间选择器方式网络策略"></a>3.配置命名空间选择器方式网络策略</h1><h2 id="3-1-配置入站网络策略"><a href="#3-1-配置入站网络策略" class="headerlink" title="3.1 配置入站网络策略"></a>3.1 配置入站网络策略</h2><pre><code class="hljs">apiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata:  name: my-network-policy  namaspace: testspec:  podSelector:    matchLabels:      # 设置网络策略所匹配Pod的标签，若标签为空则匹配命名空间所属全部Pod      app: nginx  policyTypes:  - Ingress  ingress:  # 设置允许访问的命名空间，即命名空间所属的Pod均可访问  - from:    - namespaceSelector:        matchLabels:          name: default    - podSelector:        matchLabels:          app: nginx     ports:    - protocol: TCP      port: 80</code></pre><h2 id="3-2-配置出站网络策略"><a href="#3-2-配置出站网络策略" class="headerlink" title="3.2 配置出站网络策略"></a>3.2 配置出站网络策略</h2><pre><code class="hljs">apiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata:  name: my-network-policy  namaspace: testspec:  podSelector:    matchLabels:      # 设置网络策略所匹配Pod的标签，若标签为空则匹配命名空间所属全部Pod      app: nginx  policyTypes:  - Egress  egress:  # 设置允许访问的命名空间，即命名空间所属的Pod均可访问  - to:    - namespaceSelector:        matchLabels:          name: default    ports:    - protocol: TCP      port: 80</code></pre><h1 id="4-配置IP地址范围方式网络策略"><a href="#4-配置IP地址范围方式网络策略" class="headerlink" title="4.配置IP地址范围方式网络策略"></a>4.配置IP地址范围方式网络策略</h1><h2 id="4-1-配置入站策略"><a href="#4-1-配置入站策略" class="headerlink" title="4.1 配置入站策略"></a>4.1 配置入站策略</h2><pre><code class="hljs">apiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata:  name: my-network-policy  namaspace: testspec:  podSelector:    matchLabels:      # 设置网络策略所匹配Pod的标签，若标签为空则匹配命名空间所属全部Pod      app: nginx  policyTypes:  - Ingress  ingress:  # 设置允许访问Pod网段  - from:    - ipBlock:        cidr: 172.30.100.0/24    ports:    - protocol: TCP      port: 80</code></pre><h1 id="5-配置端口范围方式网络策略"><a href="#5-配置端口范围方式网络策略" class="headerlink" title="5.配置端口范围方式网络策略"></a>5.配置端口范围方式网络策略</h1><h2 id="5-1-配置出站网络策略"><a href="#5-1-配置出站网络策略" class="headerlink" title="5.1 配置出站网络策略"></a>5.1 配置出站网络策略</h2><pre><code class="hljs">apiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata:  name: multi-port-egress  namespace: testspec:  podSelector:    matchLabels:      app: nginx  policyTypes:  - Egress  egress:  - to:    - ipBlock:        cidr: 172.30.0.0/24    ports:    - protocol: TCP      port: 32000      endPort: 32768</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.cnblogs.com/renshengdezheli/p/17479289.html">https://www.cnblogs.com/renshengdezheli/p/17479289.html</a></li><li><a href="https://blog.csdn.net/junbaozi/article/details/127893277">https://blog.csdn.net/junbaozi/article/details/127893277</a></li><li><a href="https://blog.csdn.net/lingshengxiyou/article/details/129998725">https://blog.csdn.net/lingshengxiyou/article/details/129998725</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>网络</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prometheus监控告警规则实例</title>
    <link href="/linux/PrometheusRules/"/>
    <url>/linux/PrometheusRules/</url>
    
    <content type="html"><![CDATA[<h1 id="1-Prometheus监控规则"><a href="#1-Prometheus监控规则" class="headerlink" title="1.Prometheus监控规则"></a>1.Prometheus监控规则</h1><pre><code class="hljs">vi /usr/local/prometheus/rules/prometheus.yamlgroups:- name: prometheus  rules:  - alert: PrometheusAllTargetsMissing    expr: count by (job) (up) == 0    for: 2m    labels:      severity: Critical      cluster: Prometheus    annotations:      summary: &#39;Prometheus系统所有监控指标丢失,请及时处理!&#39;      description: &quot;Prometheus系统所有监控指标丢失&quot;  - alert: PrometheusConfigurationReloadFailure    expr: prometheus_config_last_reload_successful != 1    for: 0m    labels:      severity: Warning      cluster: Prometheus    annotations:      summary: &#39;Prometheus系统配置重载失败,请及时处理!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125;Prometheus系统配置重载失败&quot;  - alert: PrometheusTooManyRestarts    expr: changes(process_start_time_seconds&#123;job=~&quot;prometheus|pushgateway|alertmanager&quot;&#125;[15m]) &gt; 2    for: 0m    labels:      severity: Warning      cluster: Prometheus    annotations:      summary: &#39;Prometheus系统多次重启,请及时处理!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125;Prometheus系统15分钟内重启两次&quot;  - alert: PrometheusAlertmanagerConfigurationReloadFailure    expr: alertmanager_config_last_reload_successful != 1    for: 0m    labels:      severity: Warning      cluster: Prometheus    annotations:      summary: &#39;Prometheus系统AlertManager告警组件配置重载失败,请及时处理!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125;Prometheus系统AlertManager告警组件配置重载失败&quot;  - alert: PrometheusNotificationsBacklog    expr: min_over_time(prometheus_notifications_queue_length[10m]) &gt; 0    for: 1m    labels:      severity: Warning      cluster: Prometheus    annotations:      summary: &#39;Prometheus系统告警消息堆积,请及时处理!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125;Prometheus系统告警消息堆积超过10分钟&quot;  - alert: PrometheusAlertmanagerNotificationFailing    expr: rate(alertmanager_notifications_failed_total[1m]) &gt; 0    for: 1m    labels:      severity: Critical      cluster: Prometheus    annotations:      summary: &#39;Prometheus系统AlertManager告警组件告警信息发送失败,请及时处理!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125;Prometheus系统AlertManager告警组件告警信息发送失败&quot;  - alert: PrometheusTsdbCheckpointCreationFailures    expr: increase(prometheus_tsdb_checkpoint_creations_failed_total[1m]) &gt; 0    for: 0m    labels:      severity: Critical      cluster: Prometheus    annotations:      summary: &#39;Prometheus系统TSDB数据库checkpoint创建失败,请及时处理!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125;Prometheus系统TSDB数据库checkpoint创建失败,当前状态为&#123;&#123; $value &#125;&#125;&quot;  - alert: PrometheusTsdbCheckpointDeletionFailures    expr: increase(prometheus_tsdb_checkpoint_deletions_failed_total[1m]) &gt; 0    for: 1m    labels:      severity: Critical      cluster: Prometheus    annotations:      summary: &#39;Prometheus系统TSDB数据库checkpoint删除失败,请及时处理!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125;Prometheus系统TSDB数据库checkpoint删除失败,当前状态为&#123;&#123; $value &#125;&#125;&quot;  - alert: PrometheusTsdbCompactionsFailed    expr: increase(prometheus_tsdb_compactions_failed_total[1m]) &gt; 0    for: 1m    labels:      severity: Critical      cluster: Prometheus    annotations:      summary: &#39;Prometheus系统TSDB数据库数据压缩失败,请及时处理!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125;Prometheus系统TSDB数据库数据压缩失败,当前频次为&#123;&#123; $value &#125;&#125;&quot;  - alert: PrometheusTsdbHeadTruncationsFailed    expr: increase(prometheus_tsdb_head_truncations_failed_total[1m]) &gt; 0    for: 1m    labels:      severity: Critical      cluster: Prometheus    annotations:      summary: &#39;Prometheus系统TSDB数据库内存数据压缩失败,请及时处理!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125;Prometheus系统TSDB数据库内存数据压缩失败,当前频次为&#123;&#123; $value &#125;&#125;&quot;  - alert: PrometheusTsdbReloadFailures    expr: increase(prometheus_tsdb_reloads_failures_total[1m]) &gt; 0    for: 1m    labels:      severity: Critical      cluster: Prometheus    annotations:      summary: &#39;Prometheus系统TSDB数据库配置重载失败,请及时处理!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125;Prometheus系统TSDB数据库配置重载失败,当前状态为&#123;&#123; $value &#125;&#125;&quot;</code></pre><h1 id="2-Alertmanager监控规则"><a href="#2-Alertmanager监控规则" class="headerlink" title="2.Alertmanager监控规则"></a>2.Alertmanager监控规则</h1><pre><code class="hljs">vi /usr/local/prometheus/rules/alertmanager.yamlgroups:- name: alertmanager.rules  rules:  - alert: AlertmanagerFailedReload    annotations:      description: Configuration has failed to load for &#123;&#123; $labels.namespace &#125;&#125;/&#123;&#123; $labels.pod&#125;&#125;.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/alertmanagerfailedreload      summary: Reloading an Alertmanager configuration has failed.    expr: max_over_time(alertmanager_config_last_reload_successful&#123;job=&quot;alertmanager-main&quot;,namespace=&quot;monitoring&quot;&#125;[5m]) == 0    for: 10m    labels:      severity: Critical  - alert: AlertmanagerMembersInconsistent    annotations:      description: Alertmanager &#123;&#123; $labels.namespace &#125;&#125;/&#123;&#123; $labels.pod&#125;&#125; has only found &#123;&#123; $value &#125;&#125; members of the &#123;&#123;$labels.job&#125;&#125; cluster.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/alertmanagermembersinconsistent      summary: A member of an Alertmanager cluster has not found all other cluster members.    expr: max_over_time(alertmanager_cluster_members&#123;job=&quot;alertmanager-main&quot;,namespace=&quot;monitoring&quot;&#125;[5m]) &lt; on (namespace,service) group_left count by (namespace,service) (max_over_time(alertmanager_cluster_members&#123;job=&quot;alertmanager-main&quot;,namespace=&quot;monitoring&quot;&#125;[5m]))    for: 10m    labels:      severity: Critical  - alert: AlertmanagerFailedToSendAlerts    annotations:      description: Alertmanager &#123;&#123; $labels.namespace &#125;&#125;/&#123;&#123; $labels.pod&#125;&#125; failed to send &#123;&#123; $value | humanizePercentage &#125;&#125; of notifications to &#123;&#123; $labels.integration &#125;&#125;.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/alertmanagerfailedtosendalerts      summary: An Alertmanager instance failed to send notifications.    expr: (rate(alertmanager_notifications_failed_total&#123;job=&quot;alertmanager-main&quot;,namespace=&quot;monitoring&quot;&#125;[5m]) /          rate(alertmanager_notifications_total&#123;job=&quot;alertmanager-main&quot;,namespace=&quot;monitoring&quot;&#125;[5m])) &gt; 0.01    for: 5m    labels:      severity: Warning  - alert: AlertmanagerClusterFailedToSendAlerts    annotations:      description: The minimum notification failure rate to &#123;&#123; $labels.integration &#125;&#125; sent from any instance in the &#123;&#123;$labels.job&#125;&#125; cluster is &#123;&#123; $value | humanizePercentage &#125;&#125;.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/alertmanagerclusterfailedtosendalerts      summary: All Alertmanager instances in a cluster failed to send notifications to a critical integration.    expr: min by (namespace,service, integration) (rate(alertmanager_notifications_failed_total&#123;job=&quot;alertmanager-main&quot;,namespace=&quot;monitoring&quot;, integration=~`.*`&#125;[5m]) / rate(alertmanager_notifications_total&#123;job=&quot;alertmanager-main&quot;,namespace=&quot;monitoring&quot;, integration=~`.*`&#125;[5m])) &gt; 0.01    for: 5m    labels:      severity: Critical  - alert: AlertmanagerClusterFailedToSendAlerts    annotations:      description: The minimum notification failure rate to &#123;&#123; $labels.integration &#125;&#125; sent from any instance in the &#123;&#123;$labels.job&#125;&#125; cluster is &#123;&#123; $value | humanizePercentage &#125;&#125;.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/alertmanagerclusterfailedtosendalerts      summary: All Alertmanager instances in a cluster failed to send notifications to a non-critical integration.    expr: |      min by (namespace,service, integration) (        rate(alertmanager_notifications_failed_total&#123;job=&quot;alertmanager-main&quot;,namespace=&quot;monitoring&quot;, integration!~`.*`&#125;[5m])      /        rate(alertmanager_notifications_total&#123;job=&quot;alertmanager-main&quot;,namespace=&quot;monitoring&quot;, integration!~`.*`&#125;[5m])      )      &gt; 0.01    for: 5m    labels:      severity: Warning  - alert: AlertmanagerConfigInconsistent    annotations:      description: Alertmanager instances within the &#123;&#123;$labels.job&#125;&#125; cluster have different configurations.      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/alertmanagerconfiginconsistent      summary: Alertmanager instances within the same cluster have different configurations.    expr: count by (namespace,service) (count_values by (namespace,service) (&quot;config_hash&quot;, alertmanager_config_hash&#123;job=&quot;alertmanager-main&quot;,namespace=&quot;monitoring&quot;&#125;)) != 1    for: 20m    labels:      severity: Critical  - alert: AlertmanagerClusterDown    annotations:      description: &#39;&#123;&#123; $value | humanizePercentage &#125;&#125; of Alertmanager instances within the &#123;&#123;$labels.job&#125;&#125; cluster have been up for less than half of the last 5m.&#39;      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/alertmanagerclusterdown      summary: Half or more of the Alertmanager instances within the same cluster are down.    expr: |      ( count by (namespace,service) ( avg_over_time(up&#123;job=&quot;alertmanager-main&quot;,namespace=&quot;monitoring&quot;&#125;[5m]) &lt; 0.5 )      /      count by (namespace,service) ( up&#123;job=&quot;alertmanager-main&quot;,namespace=&quot;monitoring&quot;&#125; )      )      &gt;= 0.5    for: 5m    labels:      severity: Critical  - alert: AlertmanagerClusterCrashlooping    annotations:      description: &#39;&#123;&#123; $value | humanizePercentage &#125;&#125; of Alertmanager instances within the &#123;&#123;$labels.job&#125;&#125; cluster have restarted at least 5 times in the last 10m.&#39;      runbook_url: https://github.com/prometheus-operator/kube-prometheus/wiki/alertmanagerclustercrashlooping      summary: Half or more of the Alertmanager instances within the same cluster are crashlooping.    expr: |      (        count by (namespace,service) (          changes(process_start_time_seconds&#123;job=&quot;alertmanager-main&quot;,namespace=&quot;monitoring&quot;&#125;[10m]) &gt; 4        )      /        count by (namespace,service) ( up&#123;job=&quot;alertmanager-main&quot;,namespace=&quot;monitoring&quot;&#125; )      )      &gt;= 0.5    for: 5m    labels:      severity: Critical</code></pre><h1 id="3-Linux主机监控规则"><a href="#3-Linux主机监控规则" class="headerlink" title="3.Linux主机监控规则"></a>3.Linux主机监控规则</h1><pre><code class="hljs">vi /usr/local/prometheus/rules/nodes.yamlgroups:- name: Linux  rules:  - alert: InstanceDown    expr: up == 0    for: 3m    labels:      severity: Critical      cluster: CloudServer    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例宕机，请及时处理！&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例宕机超过3分钟，当前状态为&#123;&#123; $value &#125;&#125;&quot;  - alert: HostCpuLoadAvage    expr: sum(node_load5) by (instance) &gt; 10    for: 5m    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例CPU负载过高，请及时处理！&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例CPU负载过高，当前负载为&#123;&#123; $value &#125;&#125;&quot;    labels:      severity: Warning      cluster: CloudServer  - alert: HostCpuUsage    expr: (1-((sum(increase(node_cpu_seconds_total&#123;mode=&quot;idle&quot;&#125;[5m])) by (instance))/ (sum(increase(node_cpu_seconds_total[5m])) by (instance))))*100 &gt; 80    for: 5m    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例CPU使用率过高，请及时处理！&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例CPU使用率超过80%，当前使用率为&#123;&#123; $value &#125;&#125;&quot;    labels:      severity: Warning      cluster: CloudServer  - alert: HostMemoryUsage    expr: (1-((node_memory_Buffers_bytes + node_memory_Cached_bytes + node_memory_MemFree_bytes)/node_memory_MemTotal_bytes))*100 &gt; 80    for: 5m    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例内存使用率过高，请及时处理！&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例内存使用率超过80%，当前使用率为&#123;&#123; $value &#125;&#125;%&quot;    labels:      severity: Warning      cluster: CloudServer  - alert: HostIOWait    expr: ((sum(increase(node_cpu_seconds_total&#123;mode=&quot;iowait&quot;&#125;[5m])) by (instance))/(sum(increase(node_cpu_seconds_total[5m])) by (instance)))*100 &gt; 10    for: 5m    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例磁盘IO过高，请及时处理！&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例磁盘IO过高，当前负载值为&#123;&#123; $value &#125;&#125;&quot;    labels:      severity: Warning      cluster: CloudServer  - alert: HostFileSystemUsage    expr: (1-(node_filesystem_free_bytes&#123;fstype=~&quot;ext4|xfs&quot;,mountpoint!~&quot;.*tmp|.*boot&quot; &#125;/node_filesystem_size_bytes&#123;fstype=~&quot;ext4|xfs&quot;,mountpoint!~&quot;.*tmp|.*boot&quot; &#125;))*100 &gt; 80    for: 5m    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例磁盘分区 &#123;&#123; $labels.mountpoint &#125;&#125;容量不足，请及时处理！&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例磁盘分区 &#123;&#123; $labels.mountpoint &#125;&#125;使用率超过80%, 当前使用率为&#123;&#123; $value &#125;&#125;%&quot;    labels:      severity: Warning      cluster: CloudServer  - alert: HostSwapIsFillingUp    expr: (1 - (node_memory_SwapFree_bytes / node_memory_SwapTotal_bytes)) * 100 &gt; 80    for: 5m    labels:      severity: Warning      cluster: CloudServer    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例Swap分区容量不足，请及时处理！&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例Swap分区使用超过80%，当前使用率为&#123;&#123; $value &#125;&#125;%&quot;  - alert: HostNetworkConnection-ESTABLISHED    expr:  sum(node_netstat_Tcp_CurrEstab) by (instance) &gt; 1000    for: 5m    labels:      severity: Warning      cluster: CloudServer    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例TCP ESTABLISHED连接数过高，请及时处理！&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例TCP ESTABLISHED连接数超过1000，当前连接数为&#123;&#123; $value &#125;&#125;&quot;  - alert: HostNetworkConnection-TIME_WAIT    expr:  sum(node_sockstat_TCP_tw) by (instance) &gt; 1000    for: 5m    labels:      severity: Warning      cluster: CloudServer    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例TCP TIME_WAIT连接数过高，请及时处理！&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例TCP TIME_WAIT连接数超过1000，当前连接数为&#123;&#123; $value &#125;&#125;&quot;  - alert: HostUnusualNetworkThroughputIn    expr:  sum by (instance, device) (rate(node_network_receive_bytes_total&#123;device=~&quot;ens.*&quot;&#125;[5m])) * 8 &gt; 100    for: 5m    labels:      severity: Warning      cluster: CloudServer    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例网卡 &#123;&#123; $labels.device &#125;&#125;入流量过高，请及时处理！&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例网卡 &#123;&#123; $labels.device &#125;&#125;持续5分钟入流量带宽超过100 b/s, 当前带宽为&#123;&#123; $value &#125;&#125;&quot;  - alert: HostUnusualNetworkThroughputOut    expr: sum by (instance, device) (rate(node_network_transmit_bytes_total&#123;device=~&quot;ens.*&quot;&#125;[2m])) / 1024 / 1024 &gt; 100    for: 5m    labels:      severity: Warning      cluster: CloudServer    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例网卡 &#123;&#123; $labels.device &#125;&#125;出流量过高，请及时处理！&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例网卡 &#123;&#123; $labels.device &#125;&#125;持续5分钟出流量带宽超过100 MB/s, 当前带宽为&#123;&#123; $value &#125;&#125;&quot;  - alert: HostUnusualDiskReadRate    expr: sum by (instance, device) (rate(node_disk_read_bytes_total&#123;device=~&quot;sd.*|vd.*&quot;&#125;[2m])) / 1024 / 1024 &gt; 50    for: 5m    labels:      severity: Warning      cluster: CloudServer    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例磁盘分区 &#123;&#123; $labels.device &#125;&#125;读取速率过高，请及时处理！&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例磁盘分区 &#123;&#123; $labels.device &#125;&#125;持续5分钟读取速率超过50 MB/s, 当前速率为&#123;&#123; $value &#125;&#125;&quot;       - alert: HostUnusualDiskWriteRate    expr: sum by (instance, device) (rate(node_disk_written_bytes_total&#123;device=~&quot;sd.*|vd.*&quot;&#125;[2m])) / 1024 / 1024 &gt; 50    for: 2m    labels:      severity: Warning      cluster: CloudServer    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例磁盘分区 &#123;&#123; $labels.device &#125;&#125;写入速率过高，请及时处理！&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例磁盘分区 &#123;&#123; $labels.device &#125;&#125;写入速率超过50 MB/s, 当前速率为&#123;&#123; $value &#125;&#125;&quot;      - alert: HostOutOfInodes    expr: (1 - node_filesystem_files_free&#123;fstype=~&quot;ext4|xfs&quot;,mountpoint!~&quot;.*tmp|.*boot&quot; &#125; / node_filesystem_files&#123;fstype=~&quot;ext4|xfs&quot;,mountpoint!~&quot;.*tmp|.*boot&quot; &#125;) * 100 &gt; 90    for: 1m    labels:      severity: Warning      cluster: CloudServer    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例 &#123;&#123; $labels.device &#125;&#125;磁盘分区Inode不足，请及时处理！&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例 &#123;&#123; $labels.mountpoint &#125;&#125;磁盘分区Inode使用率大于90%，当前使用率为&#123;&#123; $value &#125;&#125;&quot;      - alert: HostUnusualDiskReadLatency    expr: (rate(node_disk_read_time_seconds_total&#123;device=~&quot;sd.*|vd.*&quot;&#125;[5m]) / rate(node_disk_reads_completed_total&#123;device=~&quot;sd.*|vd.*&quot;&#125;[5m]) &gt; 0.1 ) and rate(node_disk_reads_completed_total&#123;device=~&quot;sd.*|vd.*&quot;&#125;[5m]) &gt; 10     for: 5m    labels:      severity: Warning      cluster: CloudServer    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例磁盘分区 &#123;&#123; $labels.device &#125;&#125;读延迟过高，请及时处理！&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例磁盘分区 &#123;&#123; $labels.device &#125;&#125;读延迟超过100ms, 当前延迟为&#123;&#123; $value &#125;&#125;&quot;  - alert: HostUnusualDiskWriteLatency    expr: (rate(node_disk_write_time_seconds_total&#123;device=~&quot;sd.*|vd.*&quot;&#125;[5m]) / rate(node_disk_writes_completed_total&#123;device=~&quot;sd.*|vd.*&quot;&#125;[5m])) &gt; 0.1 and rate(node_disk_writes_completed_total&#123;device=~&quot;sd.*|vd.*&quot;&#125;[5m]) &gt; 10 and rate(node_disk_io_time_seconds_total&#123;device=~&quot;sd.*|vd.*&quot;&#125;[5m]) &gt; 50    for: 5m    labels:      severity: Warning      cluster: CloudServer    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例磁盘分区 &#123;&#123; $labels.device &#125;&#125;写延迟过高，请及时处理！&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; 实例磁盘分区 &#123;&#123; $labels.device &#125;&#125;写延迟超过100ms, 当前延迟为&#123;&#123; $value &#125;&#125;&quot;</code></pre><h1 id="4-MySQL监控规则"><a href="#4-MySQL监控规则" class="headerlink" title="4.MySQL监控规则"></a>4.MySQL监控规则</h1><pre><code class="hljs">vi /usr/local/prometheus/rules/mysql.yamlgroups:- name: MySQL  rules:  - alert: MysqlDown    expr: mysql_up == 0    for: 1m    labels:      severity: Critical      cluster: MysqlServer    annotations:      summary: &#39;&#123;&#123; $labels.instance &#125;&#125; Mysql数据库实例宕机，请及时处理!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Mysql数据库实例宕机超过1分钟，当前状态为&#123;&#123; $value &#125;&#125;&quot;  - alert: MysqlRestarted    expr: mysql_global_status_uptime &lt; 60    for: 0m    labels:      severity: Info      cluster: MysqlServer    annotations:      summary: &#39;&#123;&#123; $labels.instance &#125;&#125; Mysql数据库实例已重启!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Mysql数据库实例1分钟前已重启，当前状态为&#123;&#123; $value &#125;&#125;&quot;  - alert: MysqlConnectionError    expr: rate(mysql_global_status_connection_errors_total[5m]) &gt; 0    for: 5m    labels:      severity: Warning      cluster: MysqlServer    annotations:      summary: &#39;&#123;&#123; $labels.instance &#125;&#125; Mysql数据库实例存在错误连接!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Mysql数据库实例5分钟内存在错误连接，错误连接数为&#123;&#123; $value &#125;&#125;&quot;  - alert: MysqlTooManyConnections    expr: max_over_time(mysql_global_status_threads_connected[1m]) / mysql_global_variables_max_connections * 100 &gt; 80    for: 2m    labels:      severity: Warning      cluster: MysqlServer    annotations:      summary: &#39;&#123;&#123; $labels.instance &#125;&#125; Mysql数据库实例当前连接数过高，请及时处理!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Mysql数据库实例当前连接数超过最大连接数的80%，当前连接数为&#123;&#123; $value &#125;&#125;&quot;  - alert: MysqlHighThreadsRunning    expr: max_over_time(mysql_global_status_threads_running[1m]) / mysql_global_variables_max_connections * 100 &gt; 60    for: 2m    labels:      severity: Warning      cluster: MysqlServer    annotations:      summary: &#39;&#123;&#123; $labels.instance &#125;&#125; Mysql数据库实例当前并发连接数过高，请及时处理!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Mysql数据库实例并发连接数超过最大连接数的60%，当前并发连接数为&#123;&#123; $value &#125;&#125;&quot;  - alert: MysqlSlaveIoThreadNotRunning    expr: ( mysql_slave_status_slave_io_running and ON (instance) mysql_slave_status_master_server_id &gt; 0 ) == 0    for: 0m    labels:      severity: Critical      cluster: MysqlServer    annotations:      summary: &#39;&#123;&#123; $labels.instance &#125;&#125; Mysql数据库实例从库IO线程未启动，请及时处理!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Mysql数据库实例从库IO线程未启动，当前状态为&#123;&#123; $value &#125;&#125;&quot;  - alert: MysqlSlaveSqlThreadNotRunning    expr: ( mysql_slave_status_slave_sql_running and ON (instance) mysql_slave_status_master_server_id &gt; 0) == 0    for: 0m    labels:      severity: Critical      cluster: MysqlServer    annotations:      summary: &#39;&#123;&#123; $labels.instance &#125;&#125; Mysql数据库实例从库SQL线程未启动，请及时处理!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Mysql数据库实例从库SQL线程未启动，当前状态为&#123;&#123; $value &#125;&#125;&quot;  - alert: MysqlSlaveReplicationLag    expr: ( (mysql_slave_status_seconds_behind_master - mysql_slave_status_sql_delay) and ON (instance) mysql_slave_status_master_server_id &gt; 0 ) &gt; 30    for: 1m    labels:      severity: Critical      cluster: MysqlServer    annotations:      summary: &#39;&#123;&#123; $labels.instance &#125;&#125; Mysql数据库实例从库数据延迟，请及时处理!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Mysql数据库实例从库数据延迟，当前延迟时长为&#123;&#123; $value &#125;&#125;&quot;  - alert: MysqlSlowQueries    expr: rate(mysql_global_status_slow_queries[5m]) &gt; 0    for: 2m    labels:      severity: Warning      cluster: MysqlServer    annotations:      summary: &#39;&#123;&#123; $labels.instance &#125;&#125; Mysql数据库实例存在慢查询，请及时处理!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Mysql数据库实例存在慢查询，当前状态为&#123;&#123; $value &#125;&#125;&quot;  - alert: MysqlInnodbLogWaits    expr: rate(mysql_global_status_innodb_log_waits[15m]) &gt; 10    for: 0m    labels:      severity: Warning      cluster: MysqlServer    annotations:      summary: &#39;&#123;&#123; $labels.instance &#125;&#125; Mysql数据库实例存在事务日志写入等待，请及时处理!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Mysql数据库实例存在事务日志写入等待，当前等待写入的日志数为&#123;&#123; $value &#125;&#125;&quot;</code></pre><h1 id="5-Redis监控规则"><a href="#5-Redis监控规则" class="headerlink" title="5.Redis监控规则"></a>5.Redis监控规则</h1><pre><code class="hljs">vi /usr/local/prometheus/rules/redis.yamlgroups:- name: Redis  rules:  - alert: RedisDown    expr: redis_up == 0    for: 1m    labels:      severity: Critical      cluster: RedisServer    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; Redis实例宕机，请及时处理！&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Redis实例宕机超过1分钟,当前状态为&#123;&#123; $value &#125;&#125;&quot;  - alert: RedisMissingMaster    expr: count(redis_instance_info&#123;role=&quot;master&quot;&#125;) &lt; 1    for: 2m    labels:      severity: Critical      cluster: RedisServer    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; Redis实例主节点缺失！&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Redis实例主节点缺失！&quot;  - alert: RedisTooManyMasters    expr: count(redis_instance_info&#123;role=&quot;master&quot;&#125;) &gt; 1    for: 2m    labels:      severity: Critical      cluster: RedisServer    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; Redis实例主节点过多！&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Redis实例主节点过多&quot;  - alert: RedisReplicationBroken    expr: delta(redis_connected_slaves[1m]) &lt; 0    for: 0m    labels:      severity: Critical      cluster: RedisServer    annotations:      summary: &#39;&#123;&#123; $labels.instance &#125;&#125; Redis实例从节点失联，请及时处理!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Redis实例复制中断，从节点丢失&quot;  - alert: RedisClusterFlapping    expr: changes(redis_connected_slaves[1m]) &gt; 1    for: 2m    labels:      severity: Critical      cluster: RedisServer    annotations:      summary: &#39;&#123;&#123; $labels.instance &#125;&#125; Redis集群复制重建，请及时处理！&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Redis实例从节点与主节点失联并重新建立连接，发生集群扰动&quot;  - alert: RedisMissingBackup    expr: time() - redis_rdb_last_save_timestamp_seconds &gt; 60 * 60 * 24    for: 0m    labels:      severity: Critical      cluster: RedisServer    annotations:      summary: &#39;&#123;&#123; $labels.instance &#125;&#125; Redis实例备份缺失，请及时处理!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Redis实例超过24小时备份缺失&quot;  - alert: RedisOutOfConfiguredMaxmemory    expr: redis_memory_used_bytes / redis_memory_max_bytes * 100 &gt; 80    for: 2m    labels:      severity: Warning      cluster: RedisServer    annotations:      summary: &#39;&#123;&#123; $labels.instance &#125;&#125; Redis实例内存超限，请及时处理!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Redis实例内存占用超过最大限制量的80%, 当前内存占用为&#123;&#123; $value &#125;&#125;&quot;  - alert: RedisTooManyConnections    expr: redis_connected_clients / redis_config_maxclients * 100 &gt; 80    for: 2m    labels:      severity: Warning      cluster: RedisServer    annotations:      summary: &#39;&#123;&#123; $labels.instance &#125;&#125; Redis实例连接数过多，请及时处理!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Redis实例连接数超过最大连接数的80%, 当前连接数为&#123;&#123; $value &#125;&#125;&quot;  - alert: RedisNotEnoughConnections    expr: redis_connected_clients &lt; 5    for: 2m    labels:      severity: Warning      cluster: RedisServer    annotations:      summary: &#39;&#123;&#123; $labels.instance &#125;&#125; Redis实例连接数过少!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Redis实例连接数过少,当前连接数为&#123;&#123; $value &#125;&#125;&quot;  - alert: RedisRejectedConnections    expr: increase(redis_rejected_connections_total[1m]) &gt; 0    for: 0m    labels:      severity: Critical      cluster: RedisServer    annotations:      summary: &#39;&#123;&#123; $labels.instance &#125;&#125; Redis实例1分钟内存在被拒绝的连接!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Redis实例被拒绝的连接数为&#123;&#123; $value &#125;&#125;&quot;</code></pre><h1 id="6-Elasticsearch监控规则"><a href="#6-Elasticsearch监控规则" class="headerlink" title="6.Elasticsearch监控规则"></a>6.Elasticsearch监控规则</h1><pre><code class="hljs">vi /usr/local/prometheus/rules/elasticsearch.yamlgroups:- name: Elasticsearch  rules:  - alert: ElasticsearchDown    expr: elasticsearch_up == 0    for: 1m    labels:      severity: Critical      cluster: ElasticsearchServer    annotations:      summary: &#39;&#123;&#123; $labels.instance &#125;&#125; Elasticsearch实例宕机，请及时处理!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Elasticsearch实例宕机超过1分钟，当前状态为&#123;&#123; $value &#125;&#125;&quot;  - alert: ElasticsearchHeapUsageTooHigh    expr: (elasticsearch_jvm_memory_used_bytes&#123;area=&quot;heap&quot;&#125; / elasticsearch_jvm_memory_max_bytes&#123;area=&quot;heap&quot;&#125;) * 100 &gt; 90    for: 2m    labels:      severity: Critical      cluster: ElasticsearchServer    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; Elasticsearch实例jvm内存超限，请及时处理!&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Elasticsearch实例jvm内存超过最大限制的90%，当前内存占用为&#123;&#123; $value &#125;&#125;&quot;  - alert: ElasticsearchDiskOutOfSpace    expr: elasticsearch_filesystem_data_available_bytes / elasticsearch_filesystem_data_size_bytes * 100 &lt; 10    for: 0m    labels:      severity: Critical      cluster: ElasticsearchServer    annotations:      summary: &#39;&#123;&#123; $labels.instance &#125;&#125; Elasticsearch实例磁盘空间不足，请及时处理!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Elasticsearch实例磁盘空间占用超过90%, 当前值为&#123;&#123; $value &#125;&#125;&quot;  - alert: ElasticsearchClusterRed    expr: elasticsearch_cluster_health_status&#123;color=&quot;red&quot;&#125; == 1    for: 0m    labels:      severity: Critical      cluster: ElasticsearchServer    annotations:      summary: &#39;&#123;&#123; $labels.instance &#125;&#125; Elasticsearch集群状态为Red，请及时处理!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Elasticsearch实例全部副本分片均不可用，集群状态为Red&quot;  - alert: ElasticsearchClusterYellow    expr: elasticsearch_cluster_health_status&#123;color=&quot;yellow&quot;&#125; == 1    for: 0m    labels:      severity: Warning      cluster: ElasticsearchServer    annotations:      summary: &#39;&#123;&#123; $labels.instance &#125;&#125; Elasticsearch集群状态为Yellow，请及时处理!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Elasticsearch实例存在不可用的副本分片，集群状态为Yellow&quot;  - alert: ElasticsearchHealthyNodes    expr: elasticsearch_cluster_health_number_of_nodes &lt; 3    for: 0m    labels:      severity: Critical      cluster: ElasticsearchServer    annotations:      summary: &#39;&#123;&#123; $labels.instance &#125;&#125; Elasticsearch集群健康节点数不足，请及时处理!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Elasticsearch集群健康节点数不足3，当前状态为&#123;&#123; $value &#125;&#125;&quot;  - alert: ElasticsearchHealthyDataNodes    expr: elasticsearch_cluster_health_number_of_data_nodes &lt; 3    for: 0m    labels:      severity: Critical      cluster: ElasticsearchServer    annotations:      summary: &#39;&#123;&#123; $labels.instance &#125;&#125; Elasticsearch集群健康数据节点数不足，请及时处理!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Elasticsearch集群健康数据节点数不足3,当前状态为&#123;&#123; $value &#125;&#125;&quot;  - alert: ElasticsearchRelocatingShards    expr: elasticsearch_cluster_health_relocating_shards &gt; 0    for: 0m    labels:      severity: Info      cluster: ElasticsearchServer    annotations:      summary: &#39;&#123;&#123; $labels.instance &#125;&#125; Elasticsearch实例正在重新定位分片&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Elasticsearch实例正在重新定位分片&quot;  - alert: ElasticsearchRelocatingShardsTooLong    expr: elasticsearch_cluster_health_relocating_shards &gt; 0    for: 15m    labels:      severity: Warning      cluster: ElasticsearchServer    annotations:      summary: &#39;&#123;&#123; $labels.instance &#125;&#125; Elasticsearch实例重新定位分片时间过长,请及时处理!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Elasticsearch实例重新定位分片时间超过15分钟&quot;  - alert: ElasticsearchInitializingShards    expr: elasticsearch_cluster_health_initializing_shards &gt; 0    for: 0m    labels:      severity: Info      cluster: ElasticsearchServer    annotations:      summary: &#39;&#123;&#123; $labels.instance &#125;&#125; Elasticsearch实例正在初始化分片&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Elasticsearch实例正在初始化分片&quot;  - alert: ElasticsearchInitializingShardsTooLong    expr: elasticsearch_cluster_health_initializing_shards &gt; 0    for: 15m    labels:      severity: Warning      cluster: ElasticsearchServer    annotations:      summary: &#39;&#123;&#123; $labels.instance &#125;&#125; Elasticsearch实例初始化分片时间过长,请及时处理!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Elasticsearch实例初始化分片时间超过15分钟&quot;  - alert: ElasticsearchUnassignedShards    expr: elasticsearch_cluster_health_unassigned_shards &gt; 0    for: 0m    labels:      severity: Critical      cluster: ElasticsearchServer    annotations:      summary: &#39;&#123;&#123; $labels.instance &#125;&#125; Elasticsearch实例存在未分配的分片,请及时处理!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Elasticsearch实例存在未分配的分片&quot;  - alert: ElasticsearchPendingTasks    expr: elasticsearch_cluster_health_number_of_pending_tasks &gt; 0    for: 15m    labels:      severity: Warning      cluster: ElasticsearchServer    annotations:      summary: &#39;&#123;&#123; $labels.instance &#125;&#125; Elasticsearch实例存在待处理的任务，请及时处理!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Elasticsearch实例存在待处理的任务，集群运行存在延迟, 当前状态为&#123;&#123; $value &#125;&#125;&quot;  - alert: ElasticsearchNoNewDocuments    expr: increase(elasticsearch_indices_docs&#123;es_data_node=&quot;true&quot;&#125;[10m]) &lt; 1    for: 0m    labels:      severity: Info      cluster: ElasticsearchServer    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; Elasticsearch实例持续10分钟没有文档写入&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Elasticsearch实例持续10分钟没有文档写入&quot;</code></pre><h1 id="7-Zookeeper告警规则"><a href="#7-Zookeeper告警规则" class="headerlink" title="7.Zookeeper告警规则"></a>7.Zookeeper告警规则</h1><pre><code class="hljs">vi /usr/local/prometheus/rules/Zookeeper.yamlgroups:- name: Zookeeper  rules:  - alert: ZookeeperDown    expr: zk_up == 0    for: 1m    labels:      severity: Critical      cluster: ZookeeperServer    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; Zookeeper实例宕机,请及时处理!&quot;      description: &#39;&#123;&#123; $labels.instance &#125;&#125; Zookeeper实例宕机超过1分钟,当前状态为&#123;&#123; $value &#125;&#125;&#39;  - alert: ZookeeperMissLeader    expr: absent(zk_server_state&#123;state=&quot;leader&quot;&#125;)  != 1    for: 1m    labels:      severity: Critical      cluster: ZookeeperServer    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; Zookeeper集群Leader丢失,请及时处理!&quot;      description: &#39;&#123;&#123; $labels.instance &#125;&#125; Zookeeper集群Leader丢失,当前状态为&#123;&#123; $value &#125;&#125;&#39;  - alert: RequestsHeapTooHigh    expr: avg(zk_outstanding_requests) by (instance) &gt; 10        for: 1m    labels:            severity: Critical      cluster: ZookeeperServer    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; Zookeeper集群堆积请求量过高,请及时处理!&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Zookeeper集群堆积请求量大于10,当前状态为&#123;&#123; $value &#125;&#125;&quot;  - alert: PendingSyncHeapTooHigh    expr: avg(zk_pending_syncs) by (instance) &gt; 10    for: 1m    labels:      severity: Critical      cluster: ZookeeperServer    annotations:      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; Zookeeper集群Leader节点Sync阻塞量过高,请及时处理! &quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Zookeeper集群Leader节点Sync阻塞量大于10,当前状态为&#123;&#123; $value &#125;&#125;&quot;  - alert: AvgResponseLatencyTooHigh    expr: avg(zk_avg_latency) by (instance) &gt; 10    for: 1m    labels:      severity: Critical      cluster: ZookeeperServer    annotations:      summary: &quot;Zookeeper集群平均响应延迟过高,请及时处理!&quot;      description: &#39;&#123;&#123; $labels.instance &#125;&#125; Zookeeper集群平均响应延迟大于10,当前状态为&#123;&#123; $value &#125;&#125;&#39;  - alert: OpenFileTooMany    expr: zk_open_file_descriptor_count &gt; zk_max_file_descriptor_count * 0.85    for: 1m    labels:      severity: Critical      cluster: ZookeeperServer    annotations:      summary: &quot;Zookeeper集群文件打开量超限,请及时处理! &quot;      description: &#39;&#123;&#123; $labels.instance &#125;&#125; Zookeeper集群打开文件描述符数量超过最大限量的85%,当前状态为&#123;&#123; $value &#125;&#125;&#39;</code></pre><h1 id="8-Kafka监控规则"><a href="#8-Kafka监控规则" class="headerlink" title="8.Kafka监控规则"></a>8.Kafka监控规则</h1><pre><code class="hljs">vi /usr/local/prometheus/rules/kafka.yamlgroups:- name: Kafka  rules:  - alert: KafkaDown    expr: kafka_up == 0    for: 1m    labels:      severity: Critical      cluster: KafkaServer    annotations:      summary: &#39;&#123;&#123; $labels.instance &#125;&#125; Kafka实例宕机，请及时处理!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Kafka实例宕机超过1分钟,当前状态为&#123;&#123; $value &#125;&#125;&quot;  - alert: KafkaTopicsReplicas    expr: sum(kafka_topic_partition_in_sync_replica) by (topic) &lt; 3    for: 0m    labels:      severity: Critical      cluster: KafkaServer    annotations:      summary: &#39;&#123;&#123; $labels.instance &#125;&#125; Kafka实例 &#123;&#123; $labels.topic &#125;&#125;Topic分区副本数不足,请及时处理!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Kafka实例 &#123;&#123; $labels.topic &#125;&#125;Topic分区副本数少于3,当前值为&#123;&#123; $value &#125;&#125;&quot;  - alert: KafkaConsumersGroupLag    expr: sum(kafka_consumergroup_lag) by (consumergroup) &gt; 50    for: 1m    labels:      severity: Critical      cluster: KafkaServer    annotations:      summary: &#39;&#123;&#123; $labels.instance &#125;&#125; Kafka实例消费组消息堆积,请及时处理!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125;Kafka实例消费组消费堆积量超过50, 当前Lag值为&#123;&#123; $value &#125;&#125;&quot;  - alert: KafkaConsumersTopicLag    expr: sum(kafka_consumergroup_lag) by (topic) &gt; 50    for: 1m    labels:      severity: Critical      cluster: KafkaServer    annotations:      summary: &#39;&#123;&#123; $labels.instance &#125;&#125; Kafka实例Topic消息堆积,请及时处理!&#39;      description: &quot;&#123;&#123; $labels.instance &#125;&#125; Kafka实例Topic消息堆积量已超过50, 当前Lag值为&#123;&#123; $value &#125;&#125;&quot;</code></pre><h1 id="9-Docker监控规则"><a href="#9-Docker监控规则" class="headerlink" title="9.Docker监控规则"></a>9.Docker监控规则</h1><pre><code class="hljs">vi /usr/local/prometheus/rules/docker.yamlgroups:- name: Docker  rules:  - alert: ContainerKilled    expr: time() - container_last_seen&#123;name!=&quot;&quot;&#125; &gt; 60    for: 1m    labels:      severity: Critical      cluster: CloudServer    annotations:      summary: &quot;&#123;&#123; $labels.name &#125;&#125;容器被Kill，请及时处理!&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125;实例&#123;&#123; $labels.name &#125;&#125;容器被Kill&quot;  - alert: ContainerCpuUsage    expr: (sum by(instance, name) (rate(container_cpu_usage_seconds_total&#123;name!=&quot;&quot;&#125;[3m])) * 100) &gt; 80    for: 2m    labels:      severity: Warning      cluster: CloudServer    annotations:      summary: &quot;&#123;&#123; $labels.name &#125;&#125;容器CPU使用率过高，请及时处理!&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125;实例&#123;&#123; $labels.name &#125;&#125;容器CPU使用率超过80%, 当前值为&#123;&#123; $value &#125;&#125;&quot;  - alert: ContainerHighThrottleRate    expr: rate(container_cpu_cfs_throttled_seconds_total[3m]) &gt; 1    for: 2m    labels:      severity: Warning      cluster: CloudServer    annotations:      summary: &quot;&#123;&#123; $labels.name &#125;&#125;容器CPU超限，请及时处理!&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125;实例&#123;&#123; $labels.name &#125;&#125;容器持续2分钟CPU超限, 当前值为&#123;&#123; $value &#125;&#125;&quot;  - alert: ContainerMemoryUsage    expr: (sum by(instance, name) (container_memory_working_set_bytes&#123;name!=&quot;&quot;&#125;) / sum by(instance, name) (container_spec_memory_limit_bytes&#123;name!=&quot;&quot;&#125; &gt; 0) * 100)  &gt; 80    for: 2m    labels:      severity: Warning      cluster: CloudServer    annotations:      summary: &quot;&#123;&#123; $labels.name &#125;&#125;容器内存使用率过高，请及时处理!&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125;实例&#123;&#123; $labels.name &#125;&#125;容器内存使用率超过80%, 当前值为&#123;&#123; $value &#125;&#125;&quot;  - alert: ContainerVolumeUsage    expr: (1 - (sum(container_fs_inodes_free) BY (instance) / sum(container_fs_inodes_total) BY (instance))) * 100 &gt; 80    for: 5m    labels:      severity: Warning      cluster: CloudServer    annotations:      summary: &quot;&#123;&#123; $labels.name &#125;&#125;容器磁盘使用率过高，请及时处理!&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125;实例&#123;&#123; $labels.name &#125;&#125;容器磁盘使用率超过80%, 当前值为&#123;&#123; $value &#125;&#125;&quot;  - alert: ContainerLowCpuUtilization    expr: (sum(rate(container_cpu_usage_seconds_total&#123;name!=&quot;&quot;&#125;[3m])) BY (instance, name) * 100) &lt; 20    for: 7d    labels:      severity: Info      cluster: CloudServer    annotations:      summary: &quot;&#123;&#123; $labels.name &#125;&#125;容器CPU使用率过低，建议缩减CPU配额&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125;实例&#123;&#123; $labels.name &#125;&#125;容器持续7天CPU使用率低于20%, 当前值为&#123;&#123; $value &#125;&#125;&quot;  - alert: ContainerLowMemoryUsage    expr: (sum(container_memory_working_set_bytes&#123;name!=&quot;&quot;&#125;) BY (instance, name) / sum(container_spec_memory_limit_bytes &gt; 0) BY (instance, name) * 100) &lt; 20    for: 7d    labels:      severity: Info      cluster: CloudServer    annotations:      summary: &quot;&#123;&#123; $labels.name &#125;&#125;容器内存使用率过低，建议缩减内存配额&quot;      description: &quot;&#123;&#123; $labels.instance &#125;&#125;实例&#123;&#123; $labels.name &#125;&#125;容器持续7天内存使用率低于20%, 当前值为&#123;&#123; $value &#125;&#125;&quot;</code></pre><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.51cto.com/u_64214/6080565">https://blog.51cto.com/u_64214/6080565</a></li><li><a href="https://codeleading.com/article/55986015391">https://codeleading.com/article/55986015391</a></li><li><a href="https://codeleading.com/article/61863346404">https://codeleading.com/article/61863346404</a></li><li><a href="https://samber.github.io/awesome-prometheus-alerts/rules">https://samber.github.io/awesome-prometheus-alerts/rules</a></li><li><a href="https://blog.csdn.net/yeqinghanwu/article/details/126367493">https://blog.csdn.net/yeqinghanwu/article/details/126367493</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Prometheus</tag>
      
      <tag>监控告警</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes集群Pod服务质量等级与驱逐机制</title>
    <link href="/linux/KubernetesQos/"/>
    <url>/linux/KubernetesQos/</url>
    
    <content type="html"><![CDATA[<p>Qos，Quality of Service，即服务质量等级，是kubernetes集群通过对容器资源限制的方式以保障容器生命周期内有足够运行资源的机制，标记了集群对每个Pod服务质量的预期，决定了节点资源紧张时Pod被驱逐的级别</p><h1 id="1-资源限制"><a href="#1-资源限制" class="headerlink" title="1.资源限制"></a>1.资源限制</h1><p>kubernetes集群以公平、合理的方式整体统筹集群资源的分配，但由于CPU、内存等资源的独占性，即资源已经分配给了某容器，同样的资源不会在分配给其他容器，就不可避免的存在着资源利用率相对较低的容器所造成的资源浪费或资源竞争。因此，通常会对Pod的资源使用量进行限制，以保障系统能够稳定的运行</p><p>kubernetes集群通过设置requests和limits这两个属性对资源进行分配与限制，作用的指标即为CPU与内存，若不设置则表示容器可占用全部的节点资源。建议资源限制定义在容器而非Pod上，因为不同容器的资源需求可能不一致</p><h2 id="1-1-Requests"><a href="#1-1-Requests" class="headerlink" title="1.1 Requests"></a>1.1 Requests</h2><p>Pod启动时申请分配的资源大小，即容器运行可能用不到这些额度的资源，但用到时必须确保有这么多的资源使用，主要体现在Pod调度时，申请范围为0到节点的最大配置，即0 &lt;&#x3D; Requests &lt;&#x3D;Node Allocatable</p><h2 id="1-2-Limits"><a href="#1-2-Limits" class="headerlink" title="1.2 Limits"></a>1.2 Limits</h2><p>Pod运行时最大可用的资源大小，即硬限制，主要体现在Pod运行时，申请范围为requests到无限，即Requests &lt;&#x3D; Limits &lt;&#x3D; Infinity</p><h1 id="2-Qos等级"><a href="#2-Qos等级" class="headerlink" title="2.Qos等级"></a>2.Qos等级</h1><p>Qos基于资源限制对Pod服务质量的预期进行管理，提供了节点OOM控制的级别，即对于内存这种不可压缩型资源，若发生超载触发节点OOM机制就将销毁或驱逐Pod，其优先级就取决于QOS。QoS级别分为三类，即Guaranteed、Burstable和BestEffort</p><pre><code class="hljs"># 查看Pod的Qos等级kubectl get pod nginx -o yaml | grep qos</code></pre><h2 id="2-1-Guaranteed"><a href="#2-1-Guaranteed" class="headerlink" title="2.1 Guaranteed"></a>2.1 Guaranteed</h2><p>Pod所有容器CPU和Memory同时设置相同的limits即为该级别，优先级最高，最不易被销毁或驱逐，除非内存需求超限或OMM时没有其他更低优先级的存在</p><pre><code class="hljs">apiVersion: apps/v1kind: Deploymentmetadata:  name: test-nginxspec:  selector:    matchLabels:      app: test  replicas: 3  template:    metadata:      labels:        app: test    spec:      containers:        - name: nginx          image: registry.cn-hangzhou.aliyuncs.com/swords/nginx          imagePullPolicy: IfNotPresent          ports:            - containerPort: 80              name: nginx              protocol: TCP          resources:            limits:              cpu: 200m              memory: 200Mi            requests:              cpu: 200m              memory: 200Mi          startupProbe:            httpGet:              path: /              port: 80            initialDelaySeconds: 10            periodSeconds: 5      imagePullSecrets:        - name: regcred</code></pre><ul><li>注：若容器只指明limit而未设定request，则request等于limit</li></ul><h2 id="2-2-Burstable"><a href="#2-2-Burstable" class="headerlink" title="2.2 Burstable"></a>2.2 Burstable</h2><p>Pod任一容器的requests和limits设置不同即为该级别，优先级中等，同级别类容器资源占用多的先被销毁或驱逐</p><pre><code class="hljs">apiVersion: apps/v1kind: Deploymentmetadata:  name: test-nginxspec:  selector:    matchLabels:      app: test  replicas: 3  template:    metadata:      labels:        app: test    spec:      containers:        - name: nginx          image: registry.cn-hangzhou.aliyuncs.com/swords/nginx          imagePullPolicy: IfNotPresent          ports:            - containerPort: 80              name: nginx              protocol: TCP          resources:            limits:              cpu: 200m              memory: 200Mi            requests:              cpu: 100m              memory: 100Mi          startupProbe:            httpGet:              path: /              port: 80            initialDelaySeconds: 10            periodSeconds: 5      imagePullSecrets:        - name: regcred</code></pre><h2 id="2-3-BestEffort"><a href="#2-3-BestEffort" class="headerlink" title="2.3 BestEffort"></a>2.3 BestEffort</h2><p>Pod所有容器的requests与limits均未设置即为该级别，优先级最低，最先被销毁或驱逐</p><pre><code class="hljs">apiVersion: apps/v1kind: Deploymentmetadata:  name: test-nginxspec:  selector:    matchLabels:      app: test  replicas: 3  template:    metadata:      labels:        app: test    spec:      containers:        - name: nginx          image: registry.cn-hangzhou.aliyuncs.com/swords/nginx          imagePullPolicy: IfNotPresent          ports:            - containerPort: 80              name: nginx              protocol: TCP          startupProbe:            httpGet:              path: /              port: 80            initialDelaySeconds: 10            periodSeconds: 5      imagePullSecrets:        - name: regcred</code></pre><p>Qos虽然能规避Pod耗尽节点全部资源，导致sshd、docker、kubelet等关键进程OOM，最终引发集群雪崩的重大故障，但也可能会导致资源占用远远大于Pod正常运行所需量，加大业务成本。建议详细分析历史资源的请求、使用情况和利用率，并优化Pod的资源占用，再设置合理的Qos策略</p><h1 id="3-驱逐机制"><a href="#3-驱逐机制" class="headerlink" title="3.驱逐机制"></a>3.驱逐机制</h1><p>Eviction，即驱逐，即kubernetes集群节点出现异常时为保障工作负载的可用性而由kubelet将该节点上运行的Pod销毁再调度的机制。kubernetes集群Pod驱逐分为三类，即手工驱逐、污点驱逐和抢占与节点压力驱逐</p><h2 id="3-1-手工驱逐"><a href="#3-1-手工驱逐" class="headerlink" title="3.1 手工驱逐"></a>3.1 手工驱逐</h2><p>手动驱逐，使用drain命令直接删除节点上所有Pod，建议先禁止节点调度，然后再执行排空节点的命令</p><h2 id="3-2-污点驱逐"><a href="#3-2-污点驱逐" class="headerlink" title="3.2 污点驱逐"></a>3.2 污点驱逐</h2><p>污点驱逐，将节点打上不可调度的污点而将Pod驱逐出去</p><h2 id="3-3-抢占驱逐"><a href="#3-3-抢占驱逐" class="headerlink" title="3.3 抢占驱逐"></a>3.3 抢占驱逐</h2><p>抢占驱逐，Pod调度时所有节点的可用资源都不足以承载而将其挂起，即触发抢占逻辑，然后由scheduler组件基于PriorityClass所定义的优先级进行轮询，若某个节点存在优先级低于所要调度Pod的优先级，则将其驱逐以回收部分资源，从而完成调度</p><h2 id="3-4-节点压力驱逐"><a href="#3-4-节点压力驱逐" class="headerlink" title="3.4 节点压力驱逐"></a>3.4 节点压力驱逐</h2><p>节点压力驱逐，即基于Qos策略的驱逐机制，由kubelet组件执行</p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="http://docs.kubernetes.org.cn/769.html">http://docs.kubernetes.org.cn/769.html</a></li><li><a href="https://blog.51cto.com/liruilong/5929798">https://blog.51cto.com/liruilong/5929798</a></li><li><a href="https://blog.51cto.com/u_15715098/5733127">https://blog.51cto.com/u_15715098/5733127</a></li><li><a href="https://developer.aliyun.com/article/1237138">https://developer.aliyun.com/article/1237138</a></li><li><a href="https://www.cnblogs.com/xunweidezui/p/16531596.html">https://www.cnblogs.com/xunweidezui/p/16531596.html</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prometheus监控配置Alertmanager告警组件</title>
    <link href="/linux/PrometheusAlertmanager/"/>
    <url>/linux/PrometheusAlertmanager/</url>
    
    <content type="html"><![CDATA[<p>Prometheus基于降低耦合度的设计理念，并没有集成告警这一核心功能，而是将基于告警规则所计算出告警信息推送给Alertmanager组件并进行管理，如告警信息的分组、合并、抑制与静默等，最终分发到不同的告警介质</p><h1 id="1-告警管理"><a href="#1-告警管理" class="headerlink" title="1.告警管理"></a>1.告警管理</h1><h2 id="1-1-告警分组"><a href="#1-1-告警分组" class="headerlink" title="1.1 告警分组"></a>1.1 告警分组</h2><p>分组，即是将具有相同性质的告警合并后作为单个通知进行发送，如两台主机的CPU&#x2F;内存&#x2F;磁盘使用率同时告警，则这些告警就可合并为一个告警通知，从而避免大量同类错误产生的告警风暴所导致关键告警信息的淹没。分组规则由group_by配置项按照告警标签指定，匹配到的告警合并为一个组</p><h2 id="1-2-告警抑制"><a href="#1-2-告警抑制" class="headerlink" title="1.2 告警抑制"></a>1.2 告警抑制</h2><p>抑制，即是禁止触发相互依赖的级联告警，从而可以集中于真正的故障所在，如主机宕机的告警即可抑制该主机上所有运行的服务，Docker、MySQL等等</p><h2 id="1-3-告警静默"><a href="#1-3-告警静默" class="headerlink" title="1.3 告警静默"></a>1.3 告警静默</h2><p>静默，即某些预期内的告警不再进行发送，若从Prometheus推送过来的告警事件被静默规则匹配到，Alertmanager则将之设为静默状态，不再进行发送。告警静默通常用于系统维护升级，或上游服务器故障所导致的下游服务器告警，这些某个时间段内不希望触发告警通知的场景。直到维护结束，手动解除静默，恢复对应服务的告警通知功能。告警静默功能由Alertmanager UI通过定义标签的匹配规则(字符串或者正则表达式)启用</p><h2 id="1-4-告警路由"><a href="#1-4-告警路由" class="headerlink" title="1.4 告警路由"></a>1.4 告警路由</h2><p>路由，即基于告警匹配规则将告警事件推送给不同的接受者，以便于进行进一步的处理。告警路由分为两部分，即顶级根路由和子路由，其本质上就是一个基于标签匹配规则的树状结构，所有告警信息从顶级路由开始，根据标签匹配规则进入到不同的子路由，并且根据子路由设置的接收器发送告警，匹配不到子路由的告警则由默认接收者接收。子路由的告警匹配方式有两种，基于字符串验证，通过设置match规则判断告警是否存在标签label.name，且其值等于label.value；基于正则表达式，通过设置match_re验证告警标签的值是否满足正则表达式</p><h1 id="2-告警流程"><a href="#2-告警流程" class="headerlink" title="2.告警流程"></a>2.告警流程</h1><h2 id="2-1-告警规则解析"><a href="#2-1-告警规则解析" class="headerlink" title="2.1 告警规则解析"></a>2.1 告警规则解析</h2><p>Prometheus加载告警规则文件，持续采集指标数据，并定期计算监控指标是否满足告警规则，计算周期由配置参数evaluation_interval指定，默认为1分钟</p><h2 id="2-2-告警触发"><a href="#2-2-告警触发" class="headerlink" title="2.2 告警触发"></a>2.2 告警触发</h2><p>监控指标触发告警规则，指标告警状态转为Pending，若持续时间超过for所指定的时间，则转换为Firing，并将告警信息发送到Alertmanager</p><h2 id="2-3-告警分组与路由"><a href="#2-3-告警分组与路由" class="headerlink" title="2.3 告警分组与路由"></a>2.3 告警分组与路由</h2><p>Alertmanager收到告警后，等待所定义的分组时间后，通过配置的告警介质发送到告警通知；若在此期间该分组又持续收到了告警，则会等待一个分组告警间隔时间，再次为该分组发送告警</p><h2 id="2-4-告警恢复或抑制"><a href="#2-4-告警恢复或抑制" class="headerlink" title="2.4 告警恢复或抑制"></a>2.4 告警恢复或抑制</h2><p>若该告警一直存在，Alertmanager则会按照重发时间间隔重复发送告警通知，直到告警恢复或被抑制</p><h1 id="1-部署Alertmanager"><a href="#1-部署Alertmanager" class="headerlink" title="1.部署Alertmanager"></a>1.部署Alertmanager</h1><h2 id="1-1-下载安装包"><a href="#1-1-下载安装包" class="headerlink" title="1.1 下载安装包"></a>1.1 下载安装包</h2><pre><code class="hljs">wget https://github.com/prometheus/alertmanager/releases/download/v0.25.0/alertmanager-0.25.0.linux-amd64.tar.gz</code></pre><h2 id="1-2-安装Alertmanager"><a href="#1-2-安装Alertmanager" class="headerlink" title="1.2 安装Alertmanager"></a>1.2 安装Alertmanager</h2><pre><code class="hljs">tar -xzvf alertmanager-0.25.0.linux-amd64.tar.gzsudo mv alertmanager-0.25.0.linux-amd64/alertmanager* /usr/local/prometheussudo mkdir -p /usr/local/prometheus/data/alertmanager</code></pre><h2 id="1-3-创建启动脚本"><a href="#1-3-创建启动脚本" class="headerlink" title="1.3 创建启动脚本"></a>1.3 创建启动脚本</h2><pre><code class="hljs">sudo vi /lib/systemd/system/alertmanager.service[Unit]Description=Alertmanager ServerDocumentation=https://github.com/prometheus/alertmanagerAfter=network.target[Service]Type=simpleUser=rootGroup=rootWorkingDirectory=/usr/local/prometheusExecStart=/usr/local/prometheus/alertmanager --config.file=/usr/local/prometheus/alertmanager.yml --storage.path=/usr/local/prometheus/data/alertmanagerExecReload=/bin/kill -s HUP $MAINPIDExecStop=/bin/kill -s QUIT $MAINPIDRestart=on-failure[Install]WantedBy=multi-user.target</code></pre><h2 id="1-4-配置文件语法检查"><a href="#1-4-配置文件语法检查" class="headerlink" title="1.4 配置文件语法检查"></a>1.4 配置文件语法检查</h2><pre><code class="hljs">/usr/local/prometheus/promtool check config /usr/local/prometheus/alertmanager.yml </code></pre><h2 id="1-5-启动Alertmanager"><a href="#1-5-启动Alertmanager" class="headerlink" title="1.5 启动Alertmanager"></a>1.5 启动Alertmanager</h2><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start alertmanager.servicesudo systemctl enable alertmanager.service</code></pre><h1 id="2-配置Prometheus"><a href="#2-配置Prometheus" class="headerlink" title="2.配置Prometheus"></a>2.配置Prometheus</h1><pre><code class="hljs">sudo vi /usr/local/prometheus/prometheus.ymlglobal:  scrape_interval: 15s   evaluation_interval: 15s   scrape_timeout: 10salerting:  alertmanagers:    - static_configs:        - targets:            - localhost:9093scrape_configs:  - job_name: &quot;prometheus&quot;    static_configs:      - targets: [&quot;localhost:9090&quot;]    - job_name: &quot;node&quot;    static_configs:      - targets: [&quot;192.168.100.120:9090&quot;]      - targets: [&quot;192.168.100.121:9090&quot;]  </code></pre><h1 id="3-配置告警规则"><a href="#3-配置告警规则" class="headerlink" title="3.配置告警规则"></a>3.配置告警规则</h1><p>Prometheus告警规则基于PromQL语句定义了监控指标的告警阈值及计算方式，并定期按照规则进行计算，若计算结果超过了所定义的阈值则会触发告警，再将产生的告警信息发送给Alertmanager进行处理</p><h2 id="3-1-规则组成"><a href="#3-1-规则组成" class="headerlink" title="3.1 规则组成"></a>3.1 规则组成</h2><p>告警规则由五部分组成，即规则名称、规则条件、等待时间、标签和附加信息，如下所示：</p><pre><code class="hljs"># 设置告警规则分组，告警组由若干告警规则组成，便于规则管理groups:- name: nodes  rules:  # 设置告警规则名称  - alert: HostCpuUsage    # 设置基于PromQL语句的计算规则    expr: (1-((sum(increase(node_cpu_seconds_total&#123;mode=&quot;idle&quot;&#125;[5m])) by (instance))/ (sum(increase(node_cpu_seconds_total[5m])) by (instance))))*100 &gt; 80    # 设置告警评估时间，即触发条件持续一段时间后才发送告警    for: 1m    # 设置告警规则标签    labels:      severity: Warning      clusters: CloudServer    annotations:      # 设置告警内容摘要，可用表达式获取变量的值      summary: &quot;&#123;&#123; $labels.instance &#125;&#125;实例CPU使用率过高，请尽快处理！&quot;      # 设置告警内容详情，可用表达式获取变量的值      description: &quot;&#123;&#123; $labels.instance &#125;&#125;实例CPU使用率超过80%，当前值为&#123;&#123; $value &#125;&#125;&quot;</code></pre><h3 id="3-1-1-alert"><a href="#3-1-1-alert" class="headerlink" title="3.1.1 alert"></a>3.1.1 alert</h3><p>告警规则名称，规则组内规则名称必须唯一</p><h3 id="3-1-2-expr"><a href="#3-1-2-expr" class="headerlink" title="3.1.2 expr"></a>3.1.2 expr</h3><p>基于PromQL表达式配置的规则条件，用于计算时间序列指标是否满足规则</p><h3 id="3-1-3-for"><a href="#3-1-3-for" class="headerlink" title="3.1.3 for"></a>3.1.3 for</h3><p>评估等待时间，可选参数，告警规则初始状态为inactive，当监控指标触发规则后，在for定义的时间区间内该规则会处于Pending状态，超过该时间后规则状态转换为Firing，并发送告警信息到Alertmanager</p><h3 id="3-1-4-labels"><a href="#3-1-4-labels" class="headerlink" title="3.1.4 labels"></a>3.1.4 labels</h3><p>自定义标签，允许用户指定要添加到告警信息上的一组附加标签，如告警等级等，告警等级一般分为三种，即warning、critical和emergency，严重等级依次递增</p><h3 id="3-1-5-annotations"><a href="#3-1-5-annotations" class="headerlink" title="3.1.5 annotations"></a>3.1.5 annotations</h3><p>用于指定一组附加信息，如描述告警的信息文字等，summary用于描述主要信息，description用于描述详细的告警内容，支持两种类型的模版变量，$labels.<labelname>类型变量支持告警实例指定标签的值，$value则是获取当前PromQL计算的变量(expr里表达式的值)</p><h2 id="3-2-告警状态"><a href="#3-2-告警状态" class="headerlink" title="3.2 告警状态"></a>3.2 告警状态</h2><p>Prometheus告警状态分为三种，即Inactive、Pending、Firing</p><ul><li>Inactive，非活动状态，表示正在监控，但还未有任何告警触发</li><li>Pending，表示告警已被触发，等待分组、抑制或静默验证，验证通过则将转到Firing状态</li><li>Firing，告警信息已发送到AlertManager，之后按照配置发送给接收者，告警解除后则将状态转为Inactive</li></ul><h2 id="3-3-创建告警规则文件"><a href="#3-3-创建告警规则文件" class="headerlink" title="3.3 创建告警规则文件"></a>3.3 创建告警规则文件</h2><pre><code class="hljs">sudo mkdir -p /usr/local/prometheus/rulessudo vi /usr/local/prometheus/rules/nodes.ymlgroups:- name: nodes  rules:  - alert: InstanceDown    expr: up == 0    for: 1m    labels:      severity: Critical    annotations:      summary: &quot;&#123;&#123;$labels.instance&#125;&#125;实例宕机，请尽快处理!&quot;      description: &quot;&#123;&#123;$labels.instance&#125;&#125;实例宕机超过1分钟，当前状态为&#123;&#123;$value&#125;&#125;&quot;</code></pre><h1 id="4-告警规则文件语法检查"><a href="#4-告警规则文件语法检查" class="headerlink" title="4.告警规则文件语法检查"></a>4.告警规则文件语法检查</h1><pre><code class="hljs">/usr/local/prometheus/promtool check rules /usr/local/prometheus/rules/nodes.yml</code></pre><h1 id="5-重载Prometheus，加载告警规则"><a href="#5-重载Prometheus，加载告警规则" class="headerlink" title="5.重载Prometheus，加载告警规则"></a>5.重载Prometheus，加载告警规则</h1><pre><code class="hljs">curl -XPOST http://127.0.0.1:9090/-/reload </code></pre><h1 id="6-验证Alertmanager告警"><a href="#6-验证Alertmanager告警" class="headerlink" title="6.验证Alertmanager告警"></a>6.验证Alertmanager告警</h1><p><img src="/img/wiki/prometheus/alertmanager.jpg" alt="alertmanager"></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/qq_30614345/article/details/131616940">https://blog.csdn.net/qq_30614345/article/details/131616940</a></li><li><a href="https://blog.csdn.net/weixin_46902396/article/details/125570792">https://blog.csdn.net/weixin_46902396/article/details/125570792</a></li><li><a href="https://blog.csdn.net/weixin_56752399/article/details/121596299">https://blog.csdn.net/weixin_56752399/article/details/121596299</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Prometheus</tag>
      
      <tag>监控告警</tag>
      
      <tag>Alertmanager</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prometheus监控指标详解</title>
    <link href="/linux/PrometheusMetric/"/>
    <url>/linux/PrometheusMetric/</url>
    
    <content type="html"><![CDATA[<p>Metric，即指标，是Prometheus系统所采集到的监控数据，以时间序列的方式保存于内存数据库并定时写入到硬盘，即以时间戳和值的序列顺序存储的连续的数据集合。可以将之形象的理解为向量vector，或是以时间为Y轴的数字矩阵，每条时间序列以指标名和一组标签集命名</p><pre><code class="hljs">^│   . . . . . . . . . . . . . . . . . . . .  node_cpu&#123;cpu=&quot;cpu0&quot;,mode=&quot;idle&quot;&#125;│   . . . . . . . . . . . . . . . . . . . .  node_cpu&#123;cpu=&quot;cpu0&quot;,mode=&quot;system&quot;&#125;│   . . . . . . . . . . . . . . . . . . . .  node_load1&#123;&#125;│   . . . . . . . . . . . . . . . . . . . .  node_load3&#123;&#125;|------------------ 时间 -------------------&gt;</code></pre><h1 id="1-指标组成"><a href="#1-指标组成" class="headerlink" title="1.指标组成"></a>1.指标组成</h1><p>Metric每一个点称为一个样本sample，由三部分组成，即指标名及其标签集、时间戳、样本值</p><h2 id="1-1-指标名及其标签集"><a href="#1-1-指标名及其标签集" class="headerlink" title="1.1 指标名及其标签集"></a>1.1 指标名及其标签集</h2><ul><li><p>指标名，metric name，表示指标的监控项，描述了指标的性质，格式包括ASCII字符、数字、下划线和冒号，命名应该具有语义化，以直观的表示度量指标，如http_request_total，即请求量</p></li><li><p>指标标签集，labelsets，表示指标的特征维度，用于对一个或一组样本数据进行不同维度的查询、过滤、聚合操作，是一组key&#x2F;value键值对，如http_request_total指标，请求状态码标签集为code &#x3D; 200&#x2F;400&#x2F;500，请求方式标签集为method &#x3D; get&#x2F;post。标签名由ASCII字符、数字及下划线组成，以__为前缀则表示为系统保留关键字，只在系统内部使用，标签值可以是任何Unicode字符，也支持中文，可来自被监控资源，也可由Prometheus在抓取期间和之后添加</p></li></ul><h2 id="1-2-时间戳"><a href="#1-2-时间戳" class="headerlink" title="1.2 时间戳"></a>1.2 时间戳</h2><p>时间戳，即timestamp，描述了当前时间序列的时间，单位为毫秒</p><h2 id="1-3-样本值"><a href="#1-3-样本值" class="headerlink" title="1.3 样本值"></a>1.3 样本值</h2><p>样本值，value，float64浮点型数据，表示当前监控指标的具体数值，如http_request_total的值就是请求数</p><h1 id="2-指标格式"><a href="#2-指标格式" class="headerlink" title="2.指标格式"></a>2.指标格式</h1><p>Prometheus监控指标的时间序列格式为：<metric name>{<label name>&#x3D;<label value>, …}，如node-exporter部分数据指标样本：</p><pre><code class="hljs"># HELP node_network_receive_packets_total Network device statistic receive_packets.# TYPE node_network_receive_packets_total counternode_network_receive_packets_total&#123;device=&quot;docker0&quot;&#125; 1.787294e+06node_network_receive_packets_total&#123;device=&quot;eth0&quot;&#125; 2.22892757e+08node_network_receive_packets_total&#123;device=&quot;lo&quot;&#125; 9.48425344e+08node_network_receive_packets_total&#123;device=&quot;veth5cbe2df&quot;&#125; 1.006495e+06</code></pre><ul><li>注：第一行，#开头，指标的说明介绍；第二行，#开头，表示指标的类型，必须项且格式固定，即TYPE+指标名称+类型；第三行开始为指标名及其标签集和样本值，即node_network_receive_packets_total为指标名，{}为标签集，标明了指标样本的特征和维度，最后的数值则为样本值</li></ul><h1 id="3-指标类型"><a href="#3-指标类型" class="headerlink" title="3.指标类型"></a>3.指标类型</h1><p>Prometheus指标类型分为四种，即Counter、Gauge、Histogram和Summary</p><h1 id="3-1-Counter"><a href="#3-1-Counter" class="headerlink" title="3.1 Counter"></a>3.1 Counter</h1><p>counter，即计数器，其值只增不减，用于统计单调递增的数据，如Http请求量、请求错误数、接口调用次数等，命名时建议以_total作为后缀。此外，结合increase、rate等函数可用于统计指标的变化速率</p><h1 id="3-2-Gauge"><a href="#3-2-Gauge" class="headerlink" title="3.2 Gauge"></a>3.2 Gauge</h1><p>gauge，即仪表盘，一般数值，可增可减，用于统计系统当前的状态，且无需经过内置函数即可直观反映数据的动态变化情况，如当前内存占用量、CPU利用、Gc次数等动态数据</p><h1 id="3-3-Histogram"><a href="#3-3-Histogram" class="headerlink" title="3.3 Histogram"></a>3.3 Histogram</h1><p>Histogram，即直方图或柱状图，累计值，用于分析指标在不同区间范围的分布情况，如对象存储不同存储桶请求耗时的分布。此外，Histogram还可对指标进行分组，提供count和sum功能，通过histogram_quantile函数可用于统计百分位数</p><h1 id="3-4-Summary"><a href="#3-4-Summary" class="headerlink" title="3.4 Summary"></a>3.4 Summary</h1><p>Summary，即摘要，类似于Histogram，也是用于统计分析，但其值是直接由客户端计算好的百分位数，而非像Histogram那样通过内置函数 histogram_quantile由Prometheus进行计算，如prometheus_tsdb_wal_fsync_duration_seconds 的指标，表示Prometheus wal_fsync处理时间的分布</p><h1 id="4-指标查询"><a href="#4-指标查询" class="headerlink" title="4.指标查询"></a>4.指标查询</h1><p>PromQL，即Prometheus Query Language，Prometheus内置的数据查询语言，提供对时间序列的查询、聚合运算及逻辑运算功能，广泛应用于Prometheus的日常运维，如数据查询、可视化、告警处理等</p><h2 id="4-1-基础查询"><a href="#4-1-基础查询" class="headerlink" title="4.1 基础查询"></a>4.1 基础查询</h2><p>基础查询，即是直接通过指标名及其标签进行数据查询，查询结果就是当前指标最新的时间序列，也称为瞬时向量，表达式格式为：<metric name>{label&#x3D;value}，如Prometheus的监控指标请求次数的表达式为：</p><pre><code class="hljs">prometheus_http_requests_total&#123;handler=&quot;/metrics&quot;&#125;</code></pre><h3 id="4-1-1-标签匹配查询"><a href="#4-1-1-标签匹配查询" class="headerlink" title="4.1.1 标签匹配查询"></a>4.1.1 标签匹配查询</h3><p>PromQL标签匹配查询分为两种方式，即&#x3D;和!&#x3D;，前者通过label&#x3D;value查询满足标签表达式的时间序列，后者通过label!&#x3D;value则会排除满足条件的时间序列</p><pre><code class="hljs"># 查询实例为localhost:9100且访问状态码不是200的时间序列promhttp_metric_handler_requests_total&#123;instance=&quot;localhost:9100&quot;,code!=&quot;200&quot;&#125;</code></pre><h3 id="4-1-2-标签正则匹配查询"><a href="#4-1-2-标签正则匹配查询" class="headerlink" title="4.1.2 标签正则匹配查询"></a>4.1.2 标签正则匹配查询</h3><pre><code class="hljs"># 查询访问状态码为503或500的时间序列promhttp_metric_handler_requests_total&#123;code=~&quot;.*503|500&quot;&#125;</code></pre><h2 id="4-2-时间范围查询"><a href="#4-2-时间范围查询" class="headerlink" title="4.2 时间范围查询"></a>4.2 时间范围查询</h2><p>时间范围查询，即是对基础查询的一个时间限定，查询结果集所返回的时间序列是某个时间范围内的一组数据，被称为范围向量或区间向量，通过[]指定时间范围</p><pre><code class="hljs"># 以当前时间为基准，查询5分钟内访问状态码为503或500的时间序列promhttp_metric_handler_requests_total&#123;code=~&quot;.*503|500&quot;&#125;[5m]# 时间位移操作，即以1小时前的时间点为基准，查询5分钟内的时间序列http_requests_total&#123;&#125;[5m] offset 1h</code></pre><h2 id="4-3-聚合查询"><a href="#4-3-聚合查询" class="headerlink" title="4.3 聚合查询"></a>4.3 聚合查询</h2><p>PromQL内置的聚合操作符对瞬时向量的样本进行聚合，从而形成新的时间序列，以供复杂的分析与汇总，类似于MySQL的聚合查询、分组统计等</p><pre><code class="hljs"># sum，求和运算，查询所有接口请求量的总和sum(http_requests_total&#123;&#125;)</code></pre><h1 id="5-指标计算"><a href="#5-指标计算" class="headerlink" title="5.指标计算"></a>5.指标计算</h1><p>PromQL还支持对指标进行各种运算，如算术运算与逻辑运算，以便完成更加复杂的查询</p><pre><code class="hljs"># 数学运算，内存占用百分比100 - (node_memory_Buffers_bytes + node_memory_Cached_bytes + node_memory_MemFree_bytes) / node_memory_MemTotal_bytes * 100# 逻辑运算，匹配出大于100小于1000区间的时间序列样本promhttp_metric_handler_requests_total&#123;code=&quot;200&quot;&#125; &lt; 1000 or promhttp_metric_handler_requests_total&#123;code=&quot;200&quot;&#125;  &gt;100</code></pre><h1 id="6-指标可视化"><a href="#6-指标可视化" class="headerlink" title="6.指标可视化"></a>6.指标可视化</h1><p>Prometheus提供了简洁的可视化Web UI以供操作，以便于监控系统的维护与查询，默认端口为9090，访问方式即为<a href="http://ip:9090/">http://ip:9090</a></p><p><img src="/img/wiki/prometheus/ui.jpg" alt="ui"></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://mp.weixin.qq.com/s/wbteQZWXA-aDAusvBw0nOQ">https://mp.weixin.qq.com/s/wbteQZWXA-aDAusvBw0nOQ</a></li><li><a href="https://andyoung.blog.csdn.net/article/details/122056239">https://andyoung.blog.csdn.net/article/details/122056239</a></li><li><a href="https://blog.csdn.net/qq_48059971/article/details/125517243">https://blog.csdn.net/qq_48059971/article/details/125517243</a></li><li><a href="https://blog.csdn.net/weixin_43883625/article/details/129757109">https://blog.csdn.net/weixin_43883625/article/details/129757109</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>云原生</tag>
      
      <tag>Prometheus</tag>
      
      <tag>监控告警</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prometheus监控Linux系统</title>
    <link href="/linux/Prometheus-Linux/"/>
    <url>/linux/Prometheus-Linux/</url>
    
    <content type="html"><![CDATA[<p>Node Exporter，基于Go语言构建的暴露Linux系统资源监控指标的Prometheus Exporter，如CPU、内存、磁盘及硬件等信息。Node Exporter通过内置的收集器采集监控指标的数据，但基于采集速度和资源占用的考虑，默认只启用了基础指标的部分，一些额外功能的采集器都被禁用掉，如主机服务采集器systemd、自定义监控项的采集器textfile及监控系统事件的采集器vmstat等，通过–collector.<name>启动参数即可进行启用，从而实现一些特定的监控需求</p><h1 id="1-下载安装包"><a href="#1-下载安装包" class="headerlink" title="1.下载安装包"></a>1.下载安装包</h1><pre><code class="hljs">wget https://github.com/prometheus/node_exporter/releases/download/v1.6.1/node_exporter-1.6.1.linux-amd64.tar.gz</code></pre><h1 id="2-安装node-exporter"><a href="#2-安装node-exporter" class="headerlink" title="2.安装node-exporter"></a>2.安装node-exporter</h1><pre><code class="hljs">tar -xzvf node_exporter-1.6.1.linux-amd64.tar.gzsudo mv node_exporter-1.6.1.linux-amd64 /usr/local/bin/node_exporter</code></pre><h1 id="3-创建启动脚本"><a href="#3-创建启动脚本" class="headerlink" title="3.创建启动脚本"></a>3.创建启动脚本</h1><pre><code class="hljs">sudo vi /lib/systemd/system/node_exporter.service[Unit]Description=node_exporterDocumentation=https://prometheus.ioAfter=network.target[Service]Type=simpleUser=rootExecStart=/usr/local/bin/node_exporterRestart=on-failure[Install]WantedBy=multi-user.target</code></pre><h2 id="3-1-启用系统服务收集器"><a href="#3-1-启用系统服务收集器" class="headerlink" title="3.1 启用系统服务收集器"></a>3.1 启用系统服务收集器</h2><pre><code class="hljs"># 监控指标为node_systemd_unit_state，监控项支持正则匹配/usr/local/bin/node_exporter --collector.systemd --collector.systemd.unit-whitelist=(docker|sshd|keepalived).service</code></pre><h2 id="3-2-启用系统事件收集器"><a href="#3-2-启用系统事件收集器" class="headerlink" title="3.2 启用系统事件收集器"></a>3.2 启用系统事件收集器</h2><pre><code class="hljs"># 监控指标为node_vmstat_oom_kill，监控项支持正则表达式/usr/local/bin/node_exporter --collector.vmstat.fields=&quot;^(oom_kill|pgpg|pswp|pg.*fault).*&quot;</code></pre><h2 id="3-3-自定义监控项"><a href="#3-3-自定义监控项" class="headerlink" title="3.3 自定义监控项"></a>3.3 自定义监控项</h2><p>textfile采集器用于暴露没有相关Exporter的自定义的指标，具体机制是通过定时任务执行自定义脚本并将获取的监控指标及其值以K&#x2F;V的形式存储于指定目录下的.prom文件，最后由Prometheus扫描该文件并进行指标抓取</p><h3 id="3-3-1-创建监控脚本"><a href="#3-3-1-创建监控脚本" class="headerlink" title="3.3.1 创建监控脚本"></a>3.3.1 创建监控脚本</h3><pre><code class="hljs">sudo vi /usr/local/prometheus/script/collected.sh# 监控系统登录用户数，监控项为node_login_usersecho &quot;node_login_users $(who |wc -l)&quot; &gt; /tmp/system.collected/login_users.prom# 监控系统进程数，监控项为node_processesecho &quot;node_processes $(ps -ef |wc -l)&quot; &gt; /tmp/system.collected/processes.prom</code></pre><h3 id="3-3-2-创建定时任务"><a href="#3-3-2-创建定时任务" class="headerlink" title="3.3.2 创建定时任务"></a>3.3.2 创建定时任务</h3><pre><code class="hljs">crontab -l*/5 * * * * sh /usr/local/prometheus/script/collected.sh</code></pre><h3 id="3-3-3-配置textfile采集器"><a href="#3-3-3-配置textfile采集器" class="headerlink" title="3.3.3 配置textfile采集器"></a>3.3.3 配置textfile采集器</h3><pre><code class="hljs">/usr/local/bin/node_exporter --collector.textfile.directory=/tmp/system.collected</code></pre><h1 id="4-启动node-exporter"><a href="#4-启动node-exporter" class="headerlink" title="4.启动node-exporter"></a>4.启动node-exporter</h1><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start node_exporter.servicesudo systemctl enable node_exporter.service</code></pre><h1 id="5-配置Prometheus"><a href="#5-配置Prometheus" class="headerlink" title="5.配置Prometheus"></a>5.配置Prometheus</h1><h2 id="5-1-配置监控实例"><a href="#5-1-配置监控实例" class="headerlink" title="5.1 配置监控实例"></a>5.1 配置监控实例</h2><pre><code class="hljs">sudo vi /usr/local/prometheus/prometheus.ymlglobal:  scrape_interval: 15s   evaluation_interval: 15s   scrape_timeout: 10s scrape_configs:  - job_name: prometheus    static_configs:      - targets:        - localhost:9090  - job_name: node    static_configs:      - targets:         - 192.168.100.120:9090</code></pre><h2 id="5-2-重载Prometheus"><a href="#5-2-重载Prometheus" class="headerlink" title="5.2 重载Prometheus"></a>5.2 重载Prometheus</h2><pre><code class="hljs">curl -XPOST http://127.0.0.1:9090/-/reload</code></pre><h1 id="6-Grafana导入监控模版"><a href="#6-Grafana导入监控模版" class="headerlink" title="6.Grafana导入监控模版"></a>6.Grafana导入监控模版</h1><p>Dashboards —&gt; New —&gt; Import —&gt; 模版ID：11074</p><p><img src="/img/wiki/prometheus/linux.jpg" alt="linux"></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.cnblogs.com/gered/p/16567438.html">https://www.cnblogs.com/gered/p/16567438.html</a></li><li><a href="https://www.cnblogs.com/heian99/p/17026955.html">https://www.cnblogs.com/heian99/p/17026955.html</a></li><li><a href="https://cloud.tencent.com/developer/article/2350339">https://cloud.tencent.com/developer/article/2350339</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>云原生</tag>
      
      <tag>Prometheus</tag>
      
      <tag>监控告警</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes集群服务发布方式详解</title>
    <link href="/linux/KubernetesServiceRelease/"/>
    <url>/linux/KubernetesServiceRelease/</url>
    
    <content type="html"><![CDATA[<p>应用程序升级面临的最大挑战是新旧业务切换，也即是将软件从测试最后阶段发布到生产环境时保证系统不间断提供服务。为了减小或避免应用更新时对客户使用的影响，以及因发布导致的流量丢失或服务不可用问题，针对不同的业务场景和技术需求，最为常见的发布方式分为三种，即蓝绿发布、灰度发布和滚动发布</p><h1 id="1-蓝绿发布"><a href="#1-蓝绿发布" class="headerlink" title="1.蓝绿发布"></a>1.蓝绿发布</h1><p>蓝绿发布，即将应用服务集群分为逻辑上的蓝绿两组，先将绿组集群从负载均衡中移除进行升级，蓝组则继续对用户提供服务，直到顺利升级完毕再从新接入负载均衡。此后，蓝组重复绿组的流程，移除负载均衡、服务升级、升级完毕接入负载均衡。最后，整个项目集群升级完毕</p><p>蓝绿发布策略简单、易操作，升级与回滚速度快，全量迭代也更易测试各种场景，且无需顾虑瞬时流量的高压问题，但为了防范单组无法承载业务突发的情况，升级期间需要两倍的正常业务运行时所需资源，所需成本较高，特别是集群比较大的场景，如上千个节点的集群几乎不可实现</p><h2 id="1-1-部署蓝组应用"><a href="#1-1-部署蓝组应用" class="headerlink" title="1.1 部署蓝组应用"></a>1.1 部署蓝组应用</h2><pre><code class="hljs">apiVersion: apps/v1kind: Deploymentmetadata:  name: hexo  namespace: devopsspec:  selector:    matchLabels:      app: hexo-server      version: &quot;v1.0&quot;  replicas: 6  template:    metadata:      labels:        app: hexo-server        version: &quot;v1.0&quot;    spec:      containers:        - name: hexo          image: registry.sword.org/nginx:v1.0          imagePullPolicy: IfNotPresent          ports:            - containerPort: 80              name: hexo-nginx          resources:            limits:              cpu: 100m              memory: 100M            requests:              cpu: 100m              memory: 64M          volumeMounts:            - name: nginx-conf              mountPath: /etc/nginx/nginx.conf              subPath: nginx.conf      volumes:        - name: nginx-conf          configMap:            name: nginx.conf      imagePullSecrets:        - name: regcred</code></pre><h2 id="1-2-部署绿组应用"><a href="#1-2-部署绿组应用" class="headerlink" title="1.2 部署绿组应用"></a>1.2 部署绿组应用</h2><pre><code class="hljs">apiVersion: apps/v1kind: Deploymentmetadata:  name: hexo-v2.0  namespace: devopsspec:  selector:    matchLabels:      app: hexo-server      version: &quot;v2.0&quot;  replicas: 6  template:    metadata:      labels:        app: hexo-server        version: &quot;v2.0&quot;    spec:      containers:        - name: hexo          image: registry.sword.org/nginx:v2.0          imagePullPolicy: IfNotPresent          ports:            - containerPort: 80              name: hexo-nginx          resources:            limits:              cpu: 100m              memory: 100M            requests:              cpu: 100m              memory: 64M          volumeMounts:            - name: nginx-conf              mountPath: /etc/nginx/nginx.conf              subPath: nginx.conf      volumes:        - name: nginx-conf          configMap:            name: nginx.conf      imagePullSecrets:        - name: regcred</code></pre><h2 id="1-3-部署service"><a href="#1-3-部署service" class="headerlink" title="1.3 部署service"></a>1.3 部署service</h2><pre><code class="hljs">apiVersion: v1kind: Servicemetadata:  name: hexo-service  namespace: devopsspec:  type: NodePort  sessionAffinity: ClientIP  selector:    app: hexo-server    version: &quot;v1.0&quot;  ports:    - port: 80      targetPort: 80      nodePort: 32080</code></pre><h2 id="1-4-切换流量入口，完成蓝绿发布"><a href="#1-4-切换流量入口，完成蓝绿发布" class="headerlink" title="1.4 切换流量入口，完成蓝绿发布"></a>1.4 切换流量入口，完成蓝绿发布</h2><pre><code class="hljs">apiVersion: v1kind: Servicemetadata:  name: hexo-service  namespace: devopsspec:  type: NodePort  sessionAffinity: ClientIP  selector:    app: hexo-server    version: &quot;v2.0&quot;  ports:    - port: 80      targetPort: 80      nodePort: 32080</code></pre><h1 id="2-滚动发布"><a href="#2-滚动发布" class="headerlink" title="2.滚动发布"></a>2.滚动发布</h1><p>滚动发布，即每次只升级一个或多个服务，升级完成后加入生产环境，不断执行这个过程，直到集群中的全部旧版本升级新版本</p><p>滚动发布需要配置自动更新策略和流量控制能力，以缩减部署时长与复杂度，且发布期间的新旧版本共存将会加大故障排查的难度，若是新版本出现的问题则切换过来的流量将全部受到影响，老版本的问题则会增加故障排查的迷惑性，系统将处于不稳定状态</p><hr><p>Kubernetes集群Deployment控制器通过rollingUpdate属性集成了滚动更新策略：</p><ul><li>maxSurge，最大可超期望节点数，百分比或绝对数值，默认为25%，建议配置为1</li><li>maxUnavailable，最大不可用节点数，百分比或者绝对数值，默认为25%，建议配置为0</li></ul><hr><h2 id="2-1-设置滚动发布策略"><a href="#2-1-设置滚动发布策略" class="headerlink" title="2.1 设置滚动发布策略"></a>2.1 设置滚动发布策略</h2><pre><code class="hljs">kubectl -n devops patch deployments.apps hexo -p &#39;&quot;spec&quot;:&quot;strategy&quot;:&quot;rollingUpdate&quot;:&quot;maxSurge&quot;:1,&quot;maxUnavailable&quot;:0&#39;</code></pre><h2 id="2-2-执行滚动发布"><a href="#2-2-执行滚动发布" class="headerlink" title="2.2 执行滚动发布"></a>2.2 执行滚动发布</h2><pre><code class="hljs">kubectl -n devops set image deployment hexo *=registry.sword.org/hexo:v0.48</code></pre><h2 id="2-3-应用回滚，滚动发布失败"><a href="#2-3-应用回滚，滚动发布失败" class="headerlink" title="2.3 应用回滚，滚动发布失败"></a>2.3 应用回滚，滚动发布失败</h2><pre><code class="hljs"># 查看历史版本kubectl -n devops rollout history deployment hexo# 查看版本2的详细内容kubectl -n devops rollout history deployment hexo --revision 3# 回滚到版本2kubectl -n devops rollout undo deployment hexo --to-revision=3</code></pre><h1 id="3-灰度发布"><a href="#3-灰度发布" class="headerlink" title="3.灰度发布"></a>3.灰度发布</h1><p>灰度发布，即金丝雀发布，来源于矿工下矿前以金丝雀是否能存活来判断矿洞是否有毒气的探测方式，类似于游戏体验服，即部分用户进行升级测试，如新版本业务正常则逐步推广，直到所有用户完成迁移</p><p>灰度发布保障了系统整体稳定性，以小步快跑的快速完成迭代，初始阶段就能发现、调整问题，新功能的性能、稳定性和健康状况将经过逐步部署、验证、评估，若出现问题由于体验用户不多，影响范围相对可控，但只适用于兼容迭代的方式，大版本不兼容的场景不适用，且对业务有自动化要求</p><h2 id="3-1-内置命令方式"><a href="#3-1-内置命令方式" class="headerlink" title="3.1 内置命令方式"></a>3.1 内置命令方式</h2><h3 id="3-1-1-执行金丝雀发布"><a href="#3-1-1-执行金丝雀发布" class="headerlink" title="3.1.1 执行金丝雀发布"></a>3.1.1 执行金丝雀发布</h3><pre><code class="hljs">kubectl -n devops set image deployment hexo *=registry.sword.org/hexo:v0.50 &amp;&amp; kubectl -n devops rollout pause deployment hexo</code></pre><ul><li>注：将命名空间devops的hexo deployment的镜像更新到hexo:v0.50版本，创建一个新pod就立即暂停更新，之后经过一段时间的验证再决定取消暂停继续更新还是回滚</li></ul><h3 id="3-1-2-继续更新，完成金丝雀发布"><a href="#3-1-2-继续更新，完成金丝雀发布" class="headerlink" title="3.1.2 继续更新，完成金丝雀发布"></a>3.1.2 继续更新，完成金丝雀发布</h3><pre><code class="hljs">kubectl -n devops rollout resume deployment hexo</code></pre><h3 id="3-1-3-应用回滚，金丝雀发布失败"><a href="#3-1-3-应用回滚，金丝雀发布失败" class="headerlink" title="3.1.3 应用回滚，金丝雀发布失败"></a>3.1.3 应用回滚，金丝雀发布失败</h3><pre><code class="hljs"># 查看历史版本kubectl -n devops rollout history deployment hexokubectl -n devops rollout history deployment hexo --revision 3kubectl -n devops rollout undo deployment hexo --to-revision=3</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://developer.aliyun.com/article/895549">https://developer.aliyun.com/article/895549</a></li><li><a href="https://www.freesion.com/article/41971508841">https://www.freesion.com/article/41971508841</a></li><li><a href="https://blog.csdn.net/ll945608651/article/details/131507220">https://blog.csdn.net/ll945608651/article/details/131507220</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
      <tag>蓝绿发布</tag>
      
      <tag>滚动发布</tag>
      
      <tag>灰度发布</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ceph集群文件存储的配置与管理</title>
    <link href="/linux/CephFS/"/>
    <url>/linux/CephFS/</url>
    
    <content type="html"><![CDATA[<p>CephFS，即Ceph文件系统，标准的POSIX文件系统，至少一个元数据服务器(Metadata Server，MDS)用于元数据的管理，以实现和数据的分离。MDS提供了一个包含智能缓存层的共享一致的文件系统，基于动态子树分区算法设计，具备高效的元数据组织和索引方式的功能，极大地降低了读写频率</p><p>Ceph MDS以守护进程方式运行，不直接向客户端提供任何数据，所有的数据都由OSD管理。MDS服务可启用多个，主MDS节点活跃时，其余节点都将处于standby状态，待主MDS节点发生故障再手动指定一个standby节点跟踪活动节点，同时将会在内存维护一份和活跃节点一样的数据，以达到预加载缓存的目的</p><h1 id="1-部署mds"><a href="#1-部署mds" class="headerlink" title="1.部署mds"></a>1.部署mds</h1><h2 id="1-1-安装ceph-mds"><a href="#1-1-安装ceph-mds" class="headerlink" title="1.1 安装ceph-mds"></a>1.1 安装ceph-mds</h2><pre><code class="hljs">sudo apt install -y ceph-mds</code></pre><h2 id="1-2-创建节点目录"><a href="#1-2-创建节点目录" class="headerlink" title="1.2 创建节点目录"></a>1.2 创建节点目录</h2><pre><code class="hljs">sudo -u ceph mkdir -p /var/lib/ceph/mds/ceph-node01</code></pre><h2 id="1-3-创建mds密钥环"><a href="#1-3-创建mds密钥环" class="headerlink" title="1.3 创建mds密钥环"></a>1.3 创建mds密钥环</h2><pre><code class="hljs">ceph auth get-or-create mds.node01 osd &quot;allow rwx&quot; mds &quot;allow&quot; mon &quot;allow profile mds&quot;</code></pre><h2 id="1-4-导出mds密钥环"><a href="#1-4-导出mds密钥环" class="headerlink" title="1.4 导出mds密钥环"></a>1.4 导出mds密钥环</h2><pre><code class="hljs">sudo ceph auth get mds.node01 -o /var/lib/ceph/mds/ceph-node01/keyring</code></pre><h2 id="1-5-集群配置文件添加mds"><a href="#1-5-集群配置文件添加mds" class="headerlink" title="1.5 集群配置文件添加mds"></a>1.5 集群配置文件添加mds</h2><pre><code class="hljs">sudo vi /etc/ceph/ceph.conf[mds.node01]host = node01</code></pre><h2 id="1-6-启动服务"><a href="#1-6-启动服务" class="headerlink" title="1.6 启动服务"></a>1.6 启动服务</h2><pre><code class="hljs">sudo systemctl start ceph-mds@node01sudo systemctl enable ceph-mds@node01</code></pre><h1 id="2-配置存储池"><a href="#2-配置存储池" class="headerlink" title="2.配置存储池"></a>2.配置存储池</h1><pre><code class="hljs"># 创建数据存储池ceph osd pool create cephfs_data 128# 创建元数据存储池ceph osd pool create cephfs_metadata 128</code></pre><h1 id="7-创建CephFS文件系统"><a href="#7-创建CephFS文件系统" class="headerlink" title="7. 创建CephFS文件系统"></a>7. 创建CephFS文件系统</h1><pre><code class="hljs">ceph fs new cephfs cephfs_metadata cephfs_data</code></pre><h1 id="8-验证CephFS"><a href="#8-验证CephFS" class="headerlink" title="8.验证CephFS"></a>8.验证CephFS</h1><pre><code class="hljs">ceph fs lsceph mds statceph -s</code></pre><h1 id="9-客户端挂载使用"><a href="#9-客户端挂载使用" class="headerlink" title="9.客户端挂载使用"></a>9.客户端挂载使用</h1><h2 id="9-1-Ceph集群创建CephFS用户"><a href="#9-1-Ceph集群创建CephFS用户" class="headerlink" title="9.1 Ceph集群创建CephFS用户"></a>9.1 Ceph集群创建CephFS用户</h2><pre><code class="hljs">ceph auth get-or-create client.cephfs mon &#39;allow r&#39; mds &#39;allow rw&#39; osd &#39;allow rw pool=cephfs_data, allow rw pool=cephfs_metadata&#39; -o /etc/ceph/ceph.client.cephfs.keyring</code></pre><h2 id="9-2-挂载CephFS"><a href="#9-2-挂载CephFS" class="headerlink" title="9.2 挂载CephFS"></a>9.2 挂载CephFS</h2><pre><code class="hljs">mount -t ceph 192.168.100.201:6789,192.168.100.202:6789,192.168.100.203:6789:/ /home/works/ -o name=cephfs,secret=XXXXXXXXXXXXXXXXXXXX</code></pre><h2 id="9-3-验证挂载"><a href="#9-3-验证挂载" class="headerlink" title="9.3 验证挂载"></a>9.3 验证挂载</h2><pre><code class="hljs">df -h</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://cloud.tencent.com/developer/article/2015907">https://cloud.tencent.com/developer/article/2015907</a></li><li><a href="https://blog.csdn.net/A1100886/article/details/131764704">https://blog.csdn.net/A1100886/article/details/131764704</a></li><li><a href="https://blog.csdn.net/weixin_42795092/article/details/151800440">https://blog.csdn.net/weixin_42795092/article/details/151800440</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Ceph</tag>
      
      <tag>存储</tag>
      
      <tag>分布式存储</tag>
      
      <tag>云存储</tag>
      
      <tag>文件存储</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes集群配置Ceph块存储</title>
    <link href="/linux/KubernetesCeph/"/>
    <url>/linux/KubernetesCeph/</url>
    
    <content type="html"><![CDATA[<h1 id="1-Ceph集群创建存储池及账户，并授予权限"><a href="#1-Ceph集群创建存储池及账户，并授予权限" class="headerlink" title="1.Ceph集群创建存储池及账户，并授予权限"></a>1.Ceph集群创建存储池及账户，并授予权限</h1><pre><code class="hljs">ceph osd pool create kubernetes 128 128ceph osd pool application enable kubernetes rbdrbd pool init kubernetessudo ceph auth get-or-create client.kubernetes mon &#39;allow r&#39; osd &#39;allow class-read object_prefix rbd_children, allow rwx pool=kubernetes&#39; -o /etc/ceph/ceph.client.kubernetes.keyring</code></pre><h1 id="2-Kubernetes集群worker节点配置ceph"><a href="#2-Kubernetes集群worker节点配置ceph" class="headerlink" title="2.Kubernetes集群worker节点配置ceph"></a>2.Kubernetes集群worker节点配置ceph</h1><h2 id="2-1-安装ceph客户端"><a href="#2-1-安装ceph客户端" class="headerlink" title="2.1 安装ceph客户端"></a>2.1 安装ceph客户端</h2><pre><code class="hljs">sudo apt install -y ceph-common</code></pre><h2 id="2-2-Ceph集群密钥环发送worker节点"><a href="#2-2-Ceph集群密钥环发送worker节点" class="headerlink" title="2.2 Ceph集群密钥环发送worker节点"></a>2.2 Ceph集群密钥环发送worker节点</h2><pre><code class="hljs">scp -r /etc/ceph worker01:/etcscp -r /etc/ceph worker02:/etcscp -r /etc/ceph worker03:/etc</code></pre><h1 id="3-Kubernetes集群创建rbd-provisioner"><a href="#3-Kubernetes集群创建rbd-provisioner" class="headerlink" title="3.Kubernetes集群创建rbd provisioner"></a>3.Kubernetes集群创建rbd provisioner</h1><h2 id="3-1-创建资源配置文件"><a href="#3-1-创建资源配置文件" class="headerlink" title="3.1 创建资源配置文件"></a>3.1 创建资源配置文件</h2><pre><code class="hljs">vi ceph-rbd-provisioner.yamlapiVersion: v1kind: ServiceAccountmetadata:  name: rbd-provisioner  namespace: kube-system---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata:  name: rbd-provisionerrules:  - apiGroups: [&quot;&quot;]    resources: [&quot;persistentvolumes&quot;]    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;]  - apiGroups: [&quot;&quot;]    resources: [&quot;persistentvolumeclaims&quot;]    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;]  - apiGroups: [&quot;storage.k8s.io&quot;]    resources: [&quot;storageclasses&quot;]    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]  - apiGroups: [&quot;&quot;]    resources: [&quot;events&quot;]    verbs: [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]  - apiGroups: [&quot;&quot;]    resources: [&quot;endpoints&quot;]    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]  - apiGroups: [&quot;&quot;]    resources: [&quot;services&quot;]    resourceNames: [&quot;kube-dns&quot;]    verbs: [&quot;list&quot;, &quot;get&quot;]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata:  name: rbd-provisionersubjects:  - kind: ServiceAccount    name: rbd-provisioner    namespace: kube-systemroleRef:  kind: ClusterRole  ame: rbd-provisioner  apiGroup: rbac.authorization.k8s.io---apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata:  name: rbd-provisioner  namespace: kube-systemrules:- apiGroups: [&quot;&quot;]  resources: [&quot;secrets&quot;]  verbs: [&quot;get&quot;]---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata:  name: rbd-provisioner  namespace: kube-systemroleRef:  apiGroup: rbac.authorization.k8s.io  kind: Role  name: rbd-provisionersubjects:- kind: ServiceAccount  name: rbd-provisioner  namespace: kube-system---apiVersion: apps/v1kind: Deploymentmetadata:  name: rbd-provisioner  namespace: kube-systemspec:  selector:    matchLabels:      app: rbd-provisioner  replicas: 1  strategy:    type: Recreate  template:    metadata:      labels:        app: rbd-provisioner    spec:      containers:      - name: rbd-provisioner        image: quay.io/external_storage/rbd-provisioner        env:        - name: PROVISIONER_NAME          value: ceph-rbd        resources:          limits:            cpu: 50m            memory: 100Mi          requests:            cpu: 20m            memory: 50Mi        volumeMounts:          - name: ceph-config            mountPath: /etc/ceph          - name: localtime            mountPath: /etc/localtime      volumes:        - name: ceph-config          hostPath:            path: /etc/ceph        - name: localtime          hostPath:            path: /etc/localtime      serviceAccount: rbd-provisioner</code></pre><h2 id="3-2-创建rbd-provisioner"><a href="#3-2-创建rbd-provisioner" class="headerlink" title="3.2 创建rbd provisioner"></a>3.2 创建rbd provisioner</h2><pre><code class="hljs">kubectl apply -f ceph-rbd-provisioner.yaml</code></pre><h1 id="4-Ceph集群查看账户密钥环"><a href="#4-Ceph集群查看账户密钥环" class="headerlink" title="4.Ceph集群查看账户密钥环"></a>4.Ceph集群查看账户密钥环</h1><pre><code class="hljs"># 管理员账户密钥环ceph auth get-key client.admin# kubernetes账户密钥环ceph auth get-key client.kubernetes</code></pre><h1 id="5-Kubernetes集群Ceph-Storageclass"><a href="#5-Kubernetes集群Ceph-Storageclass" class="headerlink" title="5.Kubernetes集群Ceph Storageclass"></a>5.Kubernetes集群Ceph Storageclass</h1><h2 id="5-1-创建管理员账户secret"><a href="#5-1-创建管理员账户secret" class="headerlink" title="5.1 创建管理员账户secret"></a>5.1 创建管理员账户secret</h2><pre><code class="hljs">kubectl create secret generic ceph-secret --type=&quot;kubernetes.io/rbd&quot; \--from-literal=key=AQDaWq5kcpcgGxAAJrrD4pgmOUvCH1TE9a6taQ== \--namespace=kube-system</code></pre><h2 id="5-2-创建kubernetes账户secret"><a href="#5-2-创建kubernetes账户secret" class="headerlink" title="5.2 创建kubernetes账户secret"></a>5.2 创建kubernetes账户secret</h2><pre><code class="hljs">kubectl create secret generic ceph-user-secret --type=&quot;kubernetes.io/rbd&quot; \--from-literal=key=AQDaWq5kcpcgGxAAJrrD4pgmOUvCH1TE9a6taQ==</code></pre><h2 id="5-3-创建资源配置文件"><a href="#5-3-创建资源配置文件" class="headerlink" title="5.3 创建资源配置文件"></a>5.3 创建资源配置文件</h2><pre><code class="hljs">vi sc-ceph-rbd.yamlkind: StorageClassapiVersion: storage.k8s.io/v1metadata:  name: sc-ceph-rbdprovisioner: ceph-rbdparameters:  monitors: 192.168.100.108:6789,192.168.100.118:6789,192.168.100.128:6789  adminId: admin  adminSecretName: ceph-secret  adminSecretNamespace: kube-system  pool: kubernetes  userId: kubernetes  userSecretName: ceph-user-secret  fsType: ext4  imageFormat: &quot;2&quot;  imageFeatures: &quot;layering&quot;</code></pre><h2 id="5-4-创建StorageClass"><a href="#5-4-创建StorageClass" class="headerlink" title="5.4 创建StorageClass"></a>5.4 创建StorageClass</h2><pre><code class="hljs">kubectl apply -f sc-ceph-rbd.yaml</code></pre><h1 id="6-创建POD，测试Ceph-StorageClass"><a href="#6-创建POD，测试Ceph-StorageClass" class="headerlink" title="6.创建POD，测试Ceph StorageClass"></a>6.创建POD，测试Ceph StorageClass</h1><h2 id="6-1-创建PVC"><a href="#6-1-创建PVC" class="headerlink" title="6.1 创建PVC"></a>6.1 创建PVC</h2><h3 id="6-1-1-创建资源配置文件"><a href="#6-1-1-创建资源配置文件" class="headerlink" title="6.1.1 创建资源配置文件"></a>6.1.1 创建资源配置文件</h3><pre><code class="hljs">vi nginx-ceph-pvc.yamlkind: PersistentVolumeClaimapiVersion: v1metadata:  name: nginx-ceph-rbdspec:  accessModes:         - ReadWriteOnce  storageClassName: sc-ceph-rbd  resources:    requests:      storage: 1Gi</code></pre><h3 id="6-1-2-创建PVC"><a href="#6-1-2-创建PVC" class="headerlink" title="6.1.2 创建PVC"></a>6.1.2 创建PVC</h3><pre><code class="hljs">kubectl apply -f nginx-ceph-pvc.yaml</code></pre><h3 id="6-1-3-查看PV-x2F-PVC"><a href="#6-1-3-查看PV-x2F-PVC" class="headerlink" title="6.1.3 查看PV&#x2F;PVC"></a>6.1.3 查看PV&#x2F;PVC</h3><pre><code class="hljs">kubectl get pvkubectl get pvc</code></pre><h2 id="6-2-创建测试POD"><a href="#6-2-创建测试POD" class="headerlink" title="6.2 创建测试POD"></a>6.2 创建测试POD</h2><h3 id="6-2-1-创建资源配置文件"><a href="#6-2-1-创建资源配置文件" class="headerlink" title="6.2.1 创建资源配置文件"></a>6.2.1 创建资源配置文件</h3><pre><code class="hljs">vi nginx.yamlapiVersion: v1kind: Podmetadata:  name: nginx  labels:    name: nginxspec:  containers:  - name: nginx    image: nginx    ports:    - name: web      containerPort: 80    volumeMounts:    - name: nginx-html      mountPath: /usr/share/nginx/html  volumes:  - name: nginx-html    persistentVolumeClaim:      claimName: nginx-ceph-rbd</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.51cto.com/u_13710166/5290959">https://blog.51cto.com/u_13710166/5290959</a></li><li><a href="https://www.cnblogs.com/chuyiwang/p/17612831.html">https://www.cnblogs.com/chuyiwang/p/17612831.html</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Ceph</tag>
      
      <tag>存储</tag>
      
      <tag>分布式存储</tag>
      
      <tag>云存储</tag>
      
      <tag>块存储</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ceph集群块存储的配置与管理</title>
    <link href="/linux/CephRBD/"/>
    <url>/linux/CephRBD/</url>
    
    <content type="html"><![CDATA[<p>Block，即块，一个固定大小的字节序列，如一个512字节的数据块，每个块都有自己的地址。块存储接口是最为常见的数据存储方式，基于旋转介质，如硬盘、CD、软盘，甚至传统的磁道磁带，应用极为广泛。RBD即是Ceph提供的块设备，是Ceph集群当前能提供的最稳定、应用最广泛的存储接口，数据条带化的存储到集群内的多个OSD，配置精简，大小可调，且具备RADOS的多种能力，如快照、复制和数据一致性，Linux系统可通过rbd命令将RBD块存储映射为本地的块设备文件，就像使用硬盘一样</p><h1 id="1-创建存储池"><a href="#1-创建存储池" class="headerlink" title="1.创建存储池"></a>1.创建存储池</h1><h2 id="1-1-查看存储池"><a href="#1-1-查看存储池" class="headerlink" title="1.1 查看存储池"></a>1.1 查看存储池</h2><pre><code class="hljs">ceph osd lspools</code></pre><h2 id="1-2-创建存储池"><a href="#1-2-创建存储池" class="headerlink" title="1.2 创建存储池"></a>1.2 创建存储池</h2><pre><code class="hljs">ceph osd pool create rbd 128 128</code></pre><ul><li>注：存储池创建的语法格式为ceph osd pool create <pool_name> pg_mun pgp_mun，若不指定pg_num、pgp_num数，则将集群配置文件作为默认值</li></ul><h2 id="1-3-设置存储池类型"><a href="#1-3-设置存储池类型" class="headerlink" title="1.3 设置存储池类型"></a>1.3 设置存储池类型</h2><pre><code class="hljs"># rbd表示块设备存储类型，cephfs表示文件存储类型，rgw表示对象存储类型ceph osd pool application enable rbd rbd</code></pre><h2 id="1-4-初始化存储池"><a href="#1-4-初始化存储池" class="headerlink" title="1.4 初始化存储池"></a>1.4 初始化存储池</h2><pre><code class="hljs">rbd pool init rbd</code></pre><h1 id="2-创建磁盘镜像"><a href="#2-创建磁盘镜像" class="headerlink" title="2.创建磁盘镜像"></a>2.创建磁盘镜像</h1><pre><code class="hljs">rbd create -p rbd --image kvm-centos7 --size 50G</code></pre><ul><li>注：块存储磁盘镜像的创建语法格式为：rbd create –size {megabytes} {pool-name}&#x2F;{image-name}或rbd create -p pool-name –image image-name –size 100G，若不指定存储池，则默认为rbd</li></ul><h1 id="3-禁用镜像附加特性"><a href="#3-禁用镜像附加特性" class="headerlink" title="3.禁用镜像附加特性"></a>3.禁用镜像附加特性</h1><pre><code class="hljs">rbd feature disable --pool rbd --image kvm-centos7 exclusive-lock, object-map, fast-diff, deep-flatten</code></pre><h1 id="4-验证存储池镜像"><a href="#4-验证存储池镜像" class="headerlink" title="4.验证存储池镜像"></a>4.验证存储池镜像</h1><pre><code class="hljs"># 查看存储池的镜像rbd ls --pool rbd# 查看镜像信息rbd info rbd/kvm-centos7</code></pre><h1 id="5-创建ceph客户端用户并授权"><a href="#5-创建ceph客户端用户并授权" class="headerlink" title="5.创建ceph客户端用户并授权"></a>5.创建ceph客户端用户并授权</h1><h2 id="5-1-创建ceph客户端用户"><a href="#5-1-创建ceph客户端用户" class="headerlink" title="5.1 创建ceph客户端用户"></a>5.1 创建ceph客户端用户</h2><pre><code class="hljs">ceph auth get-or-create client.rbd</code></pre><ul><li>注：ceph集群默认启用Cephx身份验证系统验证用户和守护进程，但只负责认证授权，没有数据传输加密功能。Ceph管理员将创建用户时生成的用户名和密钥环保存于monitor，客户端或应用连接集群任一monitor节点即可完成身份验证过程，有效的用户名语法为TYPE.ID，如client.admin、client.cinder</li></ul><h2 id="5-2-ceph用户授权"><a href="#5-2-ceph用户授权" class="headerlink" title="5.2 ceph用户授权"></a>5.2 ceph用户授权</h2><pre><code class="hljs">ceph auth caps client.rbd mon &#39;allow r&#39; osd &#39;allow rwx pool=rbd&#39;</code></pre><ul><li>注：ceph集群用户授权语法格式为{daemon-type} ‘allow {capability}’，daemon-type表示集群组件，如mon、osd、mds等；capability即为具体的权限，如r、w、x，与Linux文件权限相一致</li></ul><h2 id="5-3-导出客户端用户密钥环"><a href="#5-3-导出客户端用户密钥环" class="headerlink" title="5.3 导出客户端用户密钥环"></a>5.3 导出客户端用户密钥环</h2><pre><code class="hljs">sudo ceph auth get client.rbd -o /etc/ceph/ceph.client.rbd.keyring</code></pre><h2 id="5-4-验证客户端用户及其权限"><a href="#5-4-验证客户端用户及其权限" class="headerlink" title="5.4 验证客户端用户及其权限"></a>5.4 验证客户端用户及其权限</h2><pre><code class="hljs">ceph auth list</code></pre><h1 id="6-ceph客户端配置"><a href="#6-ceph客户端配置" class="headerlink" title="6.ceph客户端配置"></a>6.ceph客户端配置</h1><h2 id="6-1-安装ceph客户端"><a href="#6-1-安装ceph客户端" class="headerlink" title="6.1 安装ceph客户端"></a>6.1 安装ceph客户端</h2><pre><code class="hljs">sudo apt install -y ceph-common</code></pre><h2 id="6-2-同步客户端用户密钥环"><a href="#6-2-同步客户端用户密钥环" class="headerlink" title="6.2 同步客户端用户密钥环"></a>6.2 同步客户端用户密钥环</h2><pre><code class="hljs">scp worker01:/etc/ceph/ceph.client.rbd.keyring /etc/ceph</code></pre><h2 id="6-3-创建ceph配置文件"><a href="#6-3-创建ceph配置文件" class="headerlink" title="6.3 创建ceph配置文件"></a>6.3 创建ceph配置文件</h2><pre><code class="hljs">scp worker01:/etc/ceph/ceph.conf /etc/cephsudo vi /etc/ceph/ceph.conf[global]fsid = c649ba53-2f1a-431c-8fd4-eb4b423527d5mon initial members = node01,node02,node03mon host = 192.168.100.150,192.168.100.160,192.168.100.180public network = 192.168.100.0/24cluster network = 172.100.100.0/24auth cluster required = cephxauth service required = cephxauth client required = cephx[client.rbd]keyring=/etc/ceph/ceph.client.rbd.keyring</code></pre><h2 id="6-4-验证客户端连接集群服务器"><a href="#6-4-验证客户端连接集群服务器" class="headerlink" title="6.4 验证客户端连接集群服务器"></a>6.4 验证客户端连接集群服务器</h2><pre><code class="hljs">ceph -s --user rbdceph osd lspools --user rbd</code></pre><h1 id="7-ceph客户端挂载块存储镜像"><a href="#7-ceph客户端挂载块存储镜像" class="headerlink" title="7.ceph客户端挂载块存储镜像"></a>7.ceph客户端挂载块存储镜像</h1><h2 id="7-1-查看镜像"><a href="#7-1-查看镜像" class="headerlink" title="7.1 查看镜像"></a>7.1 查看镜像</h2><pre><code class="hljs">rbd ls -l rbd --id rbd</code></pre><h2 id="7-2-将镜像映射到本地"><a href="#7-2-将镜像映射到本地" class="headerlink" title="7.2 将镜像映射到本地"></a>7.2 将镜像映射到本地</h2><pre><code class="hljs">sudo rbd map rbd/kvm-centos7 --id rbd</code></pre><h2 id="7-3-验证镜像映射"><a href="#7-3-验证镜像映射" class="headerlink" title="7.3 验证镜像映射"></a>7.3 验证镜像映射</h2><pre><code class="hljs">rbd showmapped</code></pre><h2 id="7-4-格式化存储池镜像映射的分区"><a href="#7-4-格式化存储池镜像映射的分区" class="headerlink" title="7.4 格式化存储池镜像映射的分区"></a>7.4 格式化存储池镜像映射的分区</h2><pre><code class="hljs">sudo mkfs.xfs /dev/rbd0</code></pre><h2 id="7-5-挂载分区"><a href="#7-5-挂载分区" class="headerlink" title="7.5 挂载分区"></a>7.5 挂载分区</h2><pre><code class="hljs">sudo mount /dev/rbd0 /home/works</code></pre><h2 id="7-6-验证磁盘挂载"><a href="#7-6-验证磁盘挂载" class="headerlink" title="7.6 验证磁盘挂载"></a>7.6 验证磁盘挂载</h2><pre><code class="hljs">df -h</code></pre><h1 id="8-镜像管理"><a href="#8-镜像管理" class="headerlink" title="8.镜像管理"></a>8.镜像管理</h1><h2 id="8-1-镜像快照"><a href="#8-1-镜像快照" class="headerlink" title="8.1 镜像快照"></a>8.1 镜像快照</h2><h3 id="8-1-1-创建镜像快照"><a href="#8-1-1-创建镜像快照" class="headerlink" title="8.1.1 创建镜像快照"></a>8.1.1 创建镜像快照</h3><pre><code class="hljs">rbd snap create kvm/ops.img@centos7</code></pre><h3 id="8-1-2-快照克隆镜像"><a href="#8-1-2-快照克隆镜像" class="headerlink" title="8.1.2 快照克隆镜像"></a>8.1.2 快照克隆镜像</h3><pre><code class="hljs">rbd snap protect kvm/ops.img@centos7rbd clone kvm/ops.img@centos7 kvm/master.img</code></pre><h3 id="8-1-2-删除镜像快照"><a href="#8-1-2-删除镜像快照" class="headerlink" title="8.1.2 删除镜像快照"></a>8.1.2 删除镜像快照</h3><pre><code class="hljs">rbd snap unprotect kvm/ops.img@centos7rbd snap purge kvm/ops.img</code></pre><h2 id="8-2-镜像扩容"><a href="#8-2-镜像扩容" class="headerlink" title="8.2 镜像扩容"></a>8.2 镜像扩容</h2><h2 id="8-3-镜像删除"><a href="#8-3-镜像删除" class="headerlink" title="8.3 镜像删除"></a>8.3 镜像删除</h2><pre><code class="hljs">rbd rm kvm/ops.img</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.jianshu.com/p/ed9f022c2aa6">https://www.jianshu.com/p/ed9f022c2aa6</a></li><li><a href="https://www.r9it.com/20200215/ceph-rbd.html">https://www.r9it.com/20200215/ceph-rbd.html</a></li><li><a href="https://blog.csdn.net/NancyLCL/article/details/126917721">https://blog.csdn.net/NancyLCL/article/details/126917721</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Ceph</tag>
      
      <tag>存储</tag>
      
      <tag>分布式存储</tag>
      
      <tag>云存储</tag>
      
      <tag>块存储</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Zabbix监控自动发现详解</title>
    <link href="/linux/ZabbixDiscover/"/>
    <url>/linux/ZabbixDiscover/</url>
    
    <content type="html"><![CDATA[<hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/qq_45392321/article/details/123140818">https://blog.csdn.net/qq_45392321/article/details/123140818</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>监控告警</tag>
      
      <tag>Zabbix</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ceph分布式存储集群部署</title>
    <link href="/linux/Ceph/"/>
    <url>/linux/Ceph/</url>
    
    <content type="html"><![CDATA[<p>Ceph，由C++开发的高性能、高可靠性、可扩展的开源分布式存储系统，提供块存储、文件存储和对象存储服务，以满足不同场景的应用需求，是架设于廉价通用的商用硬件之上的统一企业级存储方案，其规模可轻易地扩展至PB级乃至于EB级，其核心数据分布算法CRUSH摒弃了传统的集中式存储元数据寻址方式，规避了数据库查找这种中心化架构存在的单点故障、性能瓶颈以及不易扩展的缺陷</p><h1 id="发展历程"><a href="#发展历程" class="headerlink" title="发展历程"></a>发展历程</h1><ul><li>2004年，Ceph项目起源于加州大学Sage Weil博士的研究课题，设计初衷是致力于避免单节点故障的分布式文件系统</li><li>2006年，Sage在OSDI学术会议上发表了关于Ceph的论文，并提供了项目的下载链接，随后贡献给了开源社区</li><li>2008年，Ceph发布第一个版本，版本号为0.1</li><li>2010年，Ceph被正式集成于Linux内核</li><li>2011年，Sage Weil创建Inktank公司，主导Ceph的开发和社区维护</li><li>2014年，Inktank公司被ReaHat收购，Ceph从此进入商用领域</li></ul><p>此后，乘着Openstack与云原生的东风，Ceph作为底层存储的基石得以迅速发展，目前已得到众多云计算厂商的广泛支持与应用，如RedHat、OpenStack都将其作为虚拟机镜像及云盘的后端存储</p><h1 id="逻辑架构"><a href="#逻辑架构" class="headerlink" title="逻辑架构"></a>逻辑架构</h1><p>Ceph存储系统自下而上分为三层，即基础存储系统层RADOS、基础库层Librados和应用接口层</p><h2 id="1-RADOS"><a href="#1-RADOS" class="headerlink" title="1.RADOS"></a>1.RADOS</h2><p>RADOS，Reliable,Autonomic,Distributed,Object Store，即可靠、自动化、分布式的对象存储，实质上就是一套完整的对象存储系统，是真实数据存放的最底层</p><h2 id="2-Librados"><a href="#2-Librados" class="headerlink" title="2.Librados"></a>2.Librados</h2><p>底层RADOS存储集群的抽象和封装，提供访问接口，也即是屏蔽底层逻辑，便于用户基于RADOS进行应用开发及深度定制，支持C&#x2F;C++&#x2F;JAVA&#x2F;python&#x2F;ruby&#x2F;php&#x2F;go等编程语言客户端</p><h2 id="3-应用接口"><a href="#3-应用接口" class="headerlink" title="3.应用接口"></a>3.应用接口</h2><p>负责基于Librados库提供抽象层次更高的、更便于应用或客户端使用的、即插即用的应用接口，由三部分构成，即RADOS GWRADOS Gateway 、RBD Reliable Block Device和Ceph FS Ceph File System</p><ul><li><p>RADOS GW，Amazon S3和Swift兼容的RESTful API的网关接口，以供相应的对象存储应用开发使用，提供对象存储服务，功能不如Librados强大，不能深入到最底层的RADOS</p></li><li><p>RBD，提供标准的块设备接口，具备自动精简配置、数据快照、数据克隆及动态调整空间的功能，数据分散存储于OSD节点，常用于在虚拟化的场景下为虚拟机创建数据卷，目前已被Red Hat集成到KVM&#x2F;QEMU</p></li><li><p>Ceph FS，POSIX兼容的分布式文件系统，用户可直接通过客户端挂载使用，内核态程序，通过内核net模块与Rados交互，无需调用用户空间的Librados库</p></li></ul><h1 id="功能组件"><a href="#功能组件" class="headerlink" title="功能组件"></a>功能组件</h1><h2 id="1-Monitors"><a href="#1-Monitors" class="headerlink" title="1.Monitors"></a>1.Monitors</h2><p>Ceph Monitors，ceph-mon，即Ceph监视器进程，核心组件，用于维护集群状态的映射、客户端的连接验证及日志的维护，包括监视器映射、管理器映射、OSD映射、MDS映射和CRUSH映射，这些映射是Ceph守护进程相互协调所需的关键集群状态，描述了对象、块存储的物理位置，以及⼀个将设备聚合到物理位置的桶列表，保障了集群数据的一致性，至少三个监视器才可保障集群的高可用性</p><h2 id="2-Managers"><a href="#2-Managers" class="headerlink" title="2.Managers"></a>2.Managers</h2><p>Ceph Managers，ceph-mgr，即Ceph管理器进程，核心组件，用于跟踪、收集集群状态和运行指标，如存储利用率、当前性能指标和系统负载，至少两个管理器才可保障集群的高可用性</p><h2 id="3-OSD"><a href="#3-OSD" class="headerlink" title="3.OSD"></a>3.OSD</h2><p>Ceph OSD，Object Storage Daemon，ceph-osd，即对象存储守护进程，核心组件，以对象的方式管理物理硬盘的数据，如数据的存储、复制、恢复、再平衡等，且各个进程之间通过心跳检测的方式向监视器和管理器提供监视信息，至少三个OSD才可保障集群的高可用性。OSD的构建建议采用SSD磁盘，且以xfs文件系统格式化分区</p><h2 id="4-MDS"><a href="#4-MDS" class="headerlink" title="4.MDS"></a>4.MDS</h2><p>Ceph MDS，Metadata server，ceph-mds，即元数据服务器，负责为存储于OSD节点的Ceph文件存储服务的元数据提供计算、缓存与同步服务，类似于元数据的代理缓存服务器</p><ul><li>注：Ceph块存储和Ceph对象存储服务不需要MDS</li></ul><h1 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h1><p>Ceph集群将多台存储服务器的磁盘合并为一个整体，从而形成一个大容量的虚拟磁盘设备。文件写入这个分区中时，Ceph将文件切分为多个固定大小（默认为4M）的Objects，并以其为最小单元将之随机均匀地存储到集群各节点的物理磁盘。Object的分布CRUSH经过运算所得，存取操作由每个物理磁盘的对应OSD来完成</p><p>Ceph集群为了保障数据的安全，设计了两种数据备份机制，即多副本和纠删码（1）多副本方法，让每个object保存到多个不同节点的OSDs中。ceph默认使用了3副本方法，则表示object保存到了3个不同的OSDs，从而实现数据安全，但是这样会让有效的存储空间降到三分之一。即使使用2副本的方法，也只有50%的空间利用率。但是Ceph的多副本的方法能获得最好的性能。（2）纠删码方法，则是将数据分割成k份，再根据这k份数据计算出m份同样大小的纠删码数据，将数据保存到k+m个不同的OSDs上。若某一份数据丢失，则可以根据仍然存在的&gt;&#x3D;k份数据计算出丢失的数据，从而获得完整的数据。只要不超过m份数据丢失，则数据依然可以根据通过解码方法计算出完整的数据。对于纠删码的简单理解：1+2+3+4&#x3D;10，根据前面4份数据，算出了第5份数据的值；当某一份数据丢失时，1+x+3+4&#x3D;10，则可以根据纠删码的数据算出丢失的数据x&#x3D;2，从而获得完整的数据。纠删码方式存储数据的好处是能获得更高的空间利用率。例如，k&#x3D;4，m&#x3D;1的空间利用率为80%，k&#x3D;8，m&#x3D;2的空间利用率也为80%。但是前者是解一元方程，后者是解二元方程，其计算量更大，但是允许故障的OSDs更多。因此，纠删码方法存储数据会消耗较大的计算量，建议给服务器配置更多的CPU线程。特别是当某个OSDs出故障情况下，对整个OSD的数据重建会消耗极大的计算资源和网络负荷。因为该OSD中数据可能和其他所有的OSDs都有联系，对该OSD数据进行重建，需要读取其他OSDs的数据并进行计算恢复。而多副本的方法，则只需要复制相应的数据即可。此外，将一个object存储到多个OSDs，描述其存放位置的术语为放置组（PG &#x2F; Placement Group），即一个object数据是保存到一个PG中的。而PG如何映射到不同节点上的多个OSDs中，保证数据可靠性，则是 使用CRUSH算法进行。</p><h1 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h1><p>文件上传，先将文件切片成N个object（如果开启了cephFS，可以使用MDS缓存）<br>2:切片后的文件object会存入到Ceph中<br>3:文件存储前，会经过CRUSH算法，计算当前文件存储归结于哪个PG<br>4:PG是逻辑概念上对文件存储范围划分的索引<br>5:根据PG索引将文件存储到指定服务器的OSD中</p><hr><h1 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h1><ul><li>172.100.100.201&#x2F;192.168.100.201 ceph01</li><li>172.100.100.202&#x2F;192.168.100.202 ceph02</li><li>172.100.100.203&#x2F;192.168.100.203 ceph03</li></ul><hr><h1 id="1-系统环境配置"><a href="#1-系统环境配置" class="headerlink" title="1.系统环境配置"></a>1.系统环境配置</h1><h2 id="1-1-配置hosts"><a href="#1-1-配置hosts" class="headerlink" title="1.1 配置hosts"></a>1.1 配置hosts</h2><pre><code class="hljs">sudo vi /etc/hosts172.100.100.201 ceph01192.168.100.201 ceph01172.100.100.202 ceph02192.168.100.202 ceph02172.100.100.203 ceph03192.168.100.203 ceph03</code></pre><h2 id="1-2-关闭防火墙"><a href="#1-2-关闭防火墙" class="headerlink" title="1.2 关闭防火墙"></a>1.2 关闭防火墙</h2><h2 id="1-3-禁用selinux"><a href="#1-3-禁用selinux" class="headerlink" title="1.3 禁用selinux"></a>1.3 禁用selinux</h2><h2 id="1-4-配置系统内核参数"><a href="#1-4-配置系统内核参数" class="headerlink" title="1.4 配置系统内核参数"></a>1.4 配置系统内核参数</h2><pre><code class="hljs">sudo vi /etc/sysctl.conffs.file-max = 10000000000fs.nr_open = 1000000000</code></pre><h2 id="1-5-配置用户进程数限制"><a href="#1-5-配置用户进程数限制" class="headerlink" title="1.5 配置用户进程数限制"></a>1.5 配置用户进程数限制</h2><pre><code class="hljs">sudo vi /etc/security/limits.conf* soft nproc 102400* hard nproc 104800* soft nofile 102400* hard nofile 104800</code></pre><h2 id="1-6-配置集群免密登录"><a href="#1-6-配置集群免密登录" class="headerlink" title="1.6 配置集群免密登录"></a>1.6 配置集群免密登录</h2><h1 id="2-配置网络"><a href="#2-配置网络" class="headerlink" title="2.配置网络"></a>2.配置网络</h1><h2 id="2-1-新增集群通信网卡"><a href="#2-1-新增集群通信网卡" class="headerlink" title="2.1 新增集群通信网卡"></a>2.1 新增集群通信网卡</h2><pre><code class="hljs"># 新增网卡sudo virsh attach-interface ceph01 --type network --source default --model virtio# 加载到虚拟机配置文件sudo virsh attach-interface ceph01 --type network --source default --model virtio  --configsudo virsh attach-interface ceph02 --type network --source default --model virtiosudo virsh attach-interface ceph02 --type network --source default --model virtio  --configsudo virsh attach-interface ceph03 --type network --source default --model virtiosudo virsh attach-interface ceph03 --type network --source default --model virtio  --config</code></pre><h2 id="2-2-配置集群通信网卡IP"><a href="#2-2-配置集群通信网卡IP" class="headerlink" title="2.2 配置集群通信网卡IP"></a>2.2 配置集群通信网卡IP</h2><pre><code class="hljs">sudo vi /etc/netplan/01-netcfg.yamlnetwork:  version: 2  renderer: networkd  ethernets:    ens2:      dhcp4: no      addresses:        - 192.168.100.201/24      gateway4: 192.168.100.1      nameservers:          addresses: [192.168.100.1,8.8.8.8]    ens6:      dhcp4: no      addresses:        - 172.100.100.201/24      gateway4: 172.100.100.1      nameservers:          addresses: [172.100.100.1,8.8.8.8]</code></pre><h2 id="2-3-应用网卡配置"><a href="#2-3-应用网卡配置" class="headerlink" title="2.3 应用网卡配置"></a>2.3 应用网卡配置</h2><pre><code class="hljs">sudo netplan apply</code></pre><h1 id="3-配置集群磁盘"><a href="#3-配置集群磁盘" class="headerlink" title="3.配置集群磁盘"></a>3.配置集群磁盘</h1><h2 id="3-1-新增磁盘"><a href="#3-1-新增磁盘" class="headerlink" title="3.1 新增磁盘"></a>3.1 新增磁盘</h2><pre><code class="hljs">sudo qemu-img create -f qcow2 /home/kvm/ceph/ceph01-001.qcow2 30Gsudo virsh attach-disk ceph01 /home/kvm/ceph/ceph01-001.qcow2 vdb --live --cache=none --subdriver=qcow2 --configsudo qemu-img create -f qcow2 /home/kvm/ceph/ceph02-001.qcow2 30Gsudo virsh attach-disk ceph02 /home/kvm/ceph/ceph02-001.qcow2 vdb --live --cache=none --subdriver=qcow2 --configsudo qemu-img create -f qcow2 /home/kvm/ceph/ceph03-001.qcow2 30Gsudo virsh attach-disk ceph03 /home/kvm/ceph/ceph03-001.qcow2 vdb --live --cache=none --subdriver=qcow2 --config</code></pre><h2 id="3-2-验证磁盘"><a href="#3-2-验证磁盘" class="headerlink" title="3.2 验证磁盘"></a>3.2 验证磁盘</h2><pre><code class="hljs">sudo lsblk</code></pre><h1 id="4-安装ceph"><a href="#4-安装ceph" class="headerlink" title="4.安装ceph"></a>4.安装ceph</h1><pre><code class="hljs">sudo apt install -y apt-transport-https ca-certificates curl software-properties-common wget lvm2wget -q -O- &#39;https://mirrors.tuna.tsinghua.edu.cn/ceph/keys/release.asc&#39; | sudo apt-key add -echo deb https://mirrors.tuna.tsinghua.edu.cn/ceph/debian-pacific/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.listsudo apt install -y ceph</code></pre><h1 id="5-部署mon"><a href="#5-部署mon" class="headerlink" title="5.部署mon"></a>5.部署mon</h1><h2 id="5-1-创建集群配置文件"><a href="#5-1-创建集群配置文件" class="headerlink" title="5.1 创建集群配置文件"></a>5.1 创建集群配置文件</h2><pre><code class="hljs">sudo vi /etc/ceph/ceph.conf[global]# 设置集群ID，可用uuidgen命令生成fsid = c649ba53-2f1a-431c-8fd4-eb4b423527d5mon initial members = ceph01mon host = 192.168.100.201# 设置集群管理网络的网段public network = 192.168.100.0/24# 设置集群通信网络的网段cluster network = 172.100.100.0/24auth cluster required = cephxauth service required = cephxauth client required = cephxosd journal size = 1024# 设置osd副本数，默认为3osd pool default size = 3# 设置osd最小副本数                    osd pool default min size = 2                 osd pool default pg num = 16               osd pool default pgp num = 16osd crush chooseleaf type = 1</code></pre><h2 id="5-2-创建集群mon密钥环"><a href="#5-2-创建集群mon密钥环" class="headerlink" title="5.2 创建集群mon密钥环"></a>5.2 创建集群mon密钥环</h2><pre><code class="hljs">sudo ceph-authtool --create-keyring /tmp/ceph.mon.keyring --gen-key -n mon. --cap mon &#39;allow *&#39;</code></pre><h2 id="5-3-创建集群管理密钥环"><a href="#5-3-创建集群管理密钥环" class="headerlink" title="5.3 创建集群管理密钥环"></a>5.3 创建集群管理密钥环</h2><pre><code class="hljs">sudo ceph-authtool --create-keyring /etc/ceph/ceph.client.admin.keyring --gen-key -n client.admin --cap mon &#39;allow *&#39; --cap osd &#39;allow *&#39; --cap mds &#39;allow *&#39; --cap mgr &#39;allow *&#39;sudo chmod +r /etc/ceph/ceph.client.admin.keyring</code></pre><h2 id="5-4-创建osd引导密钥环"><a href="#5-4-创建osd引导密钥环" class="headerlink" title="5.4 创建osd引导密钥环"></a>5.4 创建osd引导密钥环</h2><pre><code class="hljs">sudo ceph-authtool --create-keyring /var/lib/ceph/bootstrap-osd/ceph.keyring --gen-key -n client.bootstrap-osd --cap mon &#39;profile bootstrap-osd&#39; --cap mgr &#39;allow r&#39;</code></pre><h2 id="5-5-mon密钥环导入到管理密钥环、osd引导密钥环"><a href="#5-5-mon密钥环导入到管理密钥环、osd引导密钥环" class="headerlink" title="5.5 mon密钥环导入到管理密钥环、osd引导密钥环"></a>5.5 mon密钥环导入到管理密钥环、osd引导密钥环</h2><pre><code class="hljs">sudo ceph-authtool /tmp/ceph.mon.keyring --import-keyring /etc/ceph/ceph.client.admin.keyringsudo ceph-authtool /tmp/ceph.mon.keyring --import-keyring /var/lib/ceph/bootstrap-osd/ceph.keyringsudo chown ceph:ceph /tmp/ceph.mon.keyring</code></pre><h2 id="5-6-创建ceph监视图"><a href="#5-6-创建ceph监视图" class="headerlink" title="5.6 创建ceph监视图"></a>5.6 创建ceph监视图</h2><pre><code class="hljs">monmaptool --create --add ceph01 192.168.100.201 --fsid c649ba53-2f1a-431c-8fd4-eb4b423527d5 /tmp/monmap</code></pre><h2 id="5-7-初始化mon"><a href="#5-7-初始化mon" class="headerlink" title="5.7 初始化mon"></a>5.7 初始化mon</h2><pre><code class="hljs"># 创建mon目录，默认命名格式为ceph-hostnamesudo -u ceph mkdir /var/lib/ceph/mon/ceph-ceph01sudo -u ceph ceph-mon --mkfs -i ceph01 --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring# 防止重新安装，创建空的done文件sudo -u ceph touch /var/lib/ceph/mon/ceph-ceph01/done</code></pre><h2 id="5-8-启动监视器服务，查看状态"><a href="#5-8-启动监视器服务，查看状态" class="headerlink" title="5.8 启动监视器服务，查看状态"></a>5.8 启动监视器服务，查看状态</h2><pre><code class="hljs">sudo systemctl start ceph-mon@ceph01sudo systemctl enable ceph-mon@ceph01</code></pre><h2 id="5-9-其余节点部署监视器"><a href="#5-9-其余节点部署监视器" class="headerlink" title="5.9 其余节点部署监视器"></a>5.9 其余节点部署监视器</h2><h3 id="5-9-1-将配置文件发往其余节点"><a href="#5-9-1-将配置文件发往其余节点" class="headerlink" title="5.9.1 将配置文件发往其余节点"></a>5.9.1 将配置文件发往其余节点</h3><pre><code class="hljs">scp -r /etc/ceph ceph02:/etcscp -r /etc/ceph ceph03:/etc</code></pre><h3 id="5-9-2-ceph02部署mon"><a href="#5-9-2-ceph02部署mon" class="headerlink" title="5.9.2 ceph02部署mon"></a>5.9.2 ceph02部署mon</h3><pre><code class="hljs">sudo -u ceph mkdir /var/lib/ceph/mon/ceph-ceph02# 获取监视器密钥环ceph auth get mon. -o /tmp/ceph.mon.keyring# 获取监视器运行图ceph mon getmap -o /tmp/ceph.mon.mapsudo chown ceph.ceph /tmp/ceph.mon.keyringsudo -u ceph ceph-mon --mkfs -i ceph02 --monmap /tmp/ceph.mon.map --keyring /tmp/ceph.mon.keyringsudo -u ceph touch /var/lib/ceph/mon/ceph-ceph02/donesudo systemctl start ceph-mon@ceph02sudo systemctl enable ceph-mon@ceph02</code></pre><h3 id="5-9-3-ceph03部署mon"><a href="#5-9-3-ceph03部署mon" class="headerlink" title="5.9.3 ceph03部署mon"></a>5.9.3 ceph03部署mon</h3><pre><code class="hljs">sudo -u ceph mkdir /var/lib/ceph/mon/ceph-ceph03ceph auth get mon. -o /tmp/ceph.mon.keyringceph mon getmap -o /tmp/ceph.mon.mapsudo chown ceph.ceph /tmp/ceph.mon.keyringsudo -u ceph ceph-mon --mkfs -i ceph03 --monmap /tmp/ceph.mon.map --keyring /tmp/ceph.mon.keyringsudo -u ceph touch /var/lib/ceph/mon/ceph-ceph03/donesudo systemctl start ceph-mon@ceph03sudo systemctl enable ceph-mon@ceph03</code></pre><h3 id="5-9-4-验证服务状态"><a href="#5-9-4-验证服务状态" class="headerlink" title="5.9.4 验证服务状态"></a>5.9.4 验证服务状态</h3><pre><code class="hljs">ceph mon stat</code></pre><h1 id="6-部署osd"><a href="#6-部署osd" class="headerlink" title="6.部署osd"></a>6.部署osd</h1><h2 id="6-1-导出集群osd引导密钥环"><a href="#6-1-导出集群osd引导密钥环" class="headerlink" title="6.1 导出集群osd引导密钥环"></a>6.1 导出集群osd引导密钥环</h2><pre><code class="hljs">sudo ceph auth get client.bootstrap-osd -o /var/lib/ceph/bootstrap-osd/ceph.keyring</code></pre><h2 id="6-2-初始化osd"><a href="#6-2-初始化osd" class="headerlink" title="6.2 初始化osd"></a>6.2 初始化osd</h2><pre><code class="hljs">sudo ceph-volume lvm create --data /dev/vdbsudo ceph-volume lvm create --data /dev/vdcsudo ceph-volume lvm create --data /dev/vdd</code></pre><h2 id="6-3-启动osd"><a href="#6-3-启动osd" class="headerlink" title="6.3 启动osd"></a>6.3 启动osd</h2><pre><code class="hljs">sudo systemctl start ceph-osd@0sudo systemctl enable ceph-osd@0sudo systemctl start ceph-osd@1sudo systemctl enable ceph-osd@1sudo systemctl start ceph-osd@2sudo systemctl enable ceph-osd@2</code></pre><ul><li>注：osd编号从0开始自动增加，可逐个节点逐个盘的创建，守护进程也是用该ID作为标识</li></ul><h2 id="6-4-集群其余节点部署osd"><a href="#6-4-集群其余节点部署osd" class="headerlink" title="6.4 集群其余节点部署osd"></a>6.4 集群其余节点部署osd</h2><h3 id="6-4-1-ceph02部署osd"><a href="#6-4-1-ceph02部署osd" class="headerlink" title="6.4.1 ceph02部署osd"></a>6.4.1 ceph02部署osd</h3><pre><code class="hljs">sudo ceph auth get client.bootstrap-osd -o /var/lib/ceph/bootstrap-osd/ceph.keyringsudo ceph-volume lvm create --data /dev/vdbsudo ceph-volume lvm create --data /dev/vdcsudo ceph-volume lvm create --data /dev/vddsudo systemctl start ceph-osd@3sudo systemctl enable ceph-osd@3sudo systemctl start ceph-osd@4sudo systemctl enable ceph-osd@4sudo systemctl start ceph-osd@5sudo systemctl enable ceph-osd@5</code></pre><h3 id="6-4-2-ceph03部署osd"><a href="#6-4-2-ceph03部署osd" class="headerlink" title="6.4.2 ceph03部署osd"></a>6.4.2 ceph03部署osd</h3><pre><code class="hljs">sudo ceph auth get client.bootstrap-osd -o /var/lib/ceph/bootstrap-osd/ceph.keyringsudo ceph-volume lvm create --data /dev/vdbsudo ceph-volume lvm create --data /dev/vdcsudo ceph-volume lvm create --data /dev/vddsudo systemctl start ceph-osd@6sudo systemctl enable ceph-osd@6sudo systemctl start ceph-osd@7sudo systemctl enable ceph-osd@7sudo systemctl start ceph-osd@8sudo systemctl enable ceph-osd@8</code></pre><h2 id="6-5-验证服务状态"><a href="#6-5-验证服务状态" class="headerlink" title="6.5 验证服务状态"></a>6.5 验证服务状态</h2><pre><code class="hljs">ceph osd stat# 查看osd目录树及osd所属服务器与状态ceph osd tree</code></pre><h1 id="7-部署mgr"><a href="#7-部署mgr" class="headerlink" title="7.部署mgr"></a>7.部署mgr</h1><h2 id="7-1-创建mgr密钥环"><a href="#7-1-创建mgr密钥环" class="headerlink" title="7.1 创建mgr密钥环"></a>7.1 创建mgr密钥环</h2><pre><code class="hljs">ceph auth get-or-create mgr.ceph01 mon &#39;allow profile mgr&#39; osd &#39;allow *&#39; mds &#39;allow *&#39;</code></pre><h2 id="7-2-创建mgr节点目录"><a href="#7-2-创建mgr节点目录" class="headerlink" title="7.2 创建mgr节点目录"></a>7.2 创建mgr节点目录</h2><pre><code class="hljs"># 节点目录默认名称为集群名称+节点名称sudo -u ceph mkdir /var/lib/ceph/mgr/ceph-ceph01</code></pre><h2 id="7-3-获取mgr密钥环，导出到节点目录下，默认为keyring"><a href="#7-3-获取mgr密钥环，导出到节点目录下，默认为keyring" class="headerlink" title="7.3 获取mgr密钥环，导出到节点目录下，默认为keyring"></a>7.3 获取mgr密钥环，导出到节点目录下，默认为keyring</h2><pre><code class="hljs">sudo -u ceph ceph auth get mgr.ceph01 -o /var/lib/ceph/mgr/ceph-ceph01/keyring</code></pre><h2 id="7-4-启动mgr"><a href="#7-4-启动mgr" class="headerlink" title="7.4 启动mgr"></a>7.4 启动mgr</h2><pre><code class="hljs">sudo systemctl start ceph-mgr@ceph01sudo systemctl enable ceph-mgr@ceph01</code></pre><h2 id="7-5-部署热备mgr"><a href="#7-5-部署热备mgr" class="headerlink" title="7.5 部署热备mgr"></a>7.5 部署热备mgr</h2><pre><code class="hljs">ceph auth get-or-create mgr.ceph02 mon &#39;allow profile mgr&#39; osd &#39;allow *&#39; mds &#39;allow *&#39;sudo -u ceph mkdir /var/lib/ceph/mgr/ceph-ceph02sudo -u ceph ceph auth get mgr.ceph02 -o /var/lib/ceph/mgr/ceph-ceph02/keyringsudo systemctl start ceph-mgr@ceph02sudo systemctl enable ceph-mgr@ceph02</code></pre><h1 id="8-部署dashboard"><a href="#8-部署dashboard" class="headerlink" title="8.部署dashboard"></a>8.部署dashboard</h1><h2 id="8-1-启用dashboard"><a href="#8-1-启用dashboard" class="headerlink" title="8.1 启用dashboard"></a>8.1 启用dashboard</h2><pre><code class="hljs">ceph mgr module enable dashboard</code></pre><h2 id="8-2-启用自签名证书"><a href="#8-2-启用自签名证书" class="headerlink" title="8.2 启用自签名证书"></a>8.2 启用自签名证书</h2><pre><code class="hljs">ceph dashboard create-self-signed-cert</code></pre><h2 id="8-3-创建自签名SSL证书"><a href="#8-3-创建自签名SSL证书" class="headerlink" title="8.3 创建自签名SSL证书"></a>8.3 创建自签名SSL证书</h2><pre><code class="hljs">openssl req -new -nodes -x509 \-subj &quot;/O=OPS/CN=ceph-mgr-dashboard&quot; -days 3650 \-keyout dashboard.key -out dashboard.crt -extensions v3_ca</code></pre><h2 id="8-4-导入证书文件和私钥文件"><a href="#8-4-导入证书文件和私钥文件" class="headerlink" title="8.4 导入证书文件和私钥文件"></a>8.4 导入证书文件和私钥文件</h2><pre><code class="hljs">ceph dashboard set-ssl-certificate -i dashboard.crt</code></pre><h2 id="8-5-设置访问地址及端口"><a href="#8-5-设置访问地址及端口" class="headerlink" title="8.5 设置访问地址及端口"></a>8.5 设置访问地址及端口</h2><pre><code class="hljs">ceph config set mgr mgr/dashboard/server_addr 192.168.100.201ceph config set mgr mgr/dashboard/server_port 8080ceph config set mgr mgr/dashboard/ssl_server_port 8443</code></pre><h2 id="8-6-设置用户及密码"><a href="#8-6-设置用户及密码" class="headerlink" title="8.6 设置用户及密码"></a>8.6 设置用户及密码</h2><pre><code class="hljs">echo &quot;admin@ceph&quot; &gt; dashboard_passwd.txtceph dashboard ac-user-create admin administrator -i dashboard_passwd.txt</code></pre><h2 id="8-7-禁用SSL认证"><a href="#8-7-禁用SSL认证" class="headerlink" title="8.7 禁用SSL认证"></a>8.7 禁用SSL认证</h2><pre><code class="hljs"># 也可禁用SSL认证，使用8080端口进行http访问# ceph config set mgr mgr/dashboard/ssl false# 重启mgr# sudo systemctl restart ceph-mgr@ceph01</code></pre><h2 id="8-8-登录web页面，验证dashboard"><a href="#8-8-登录web页面，验证dashboard" class="headerlink" title="8.8 登录web页面，验证dashboard"></a>8.8 登录web页面，验证dashboard</h2><p><img src="/img/wiki/ceph/ceph.jpg" alt="ceph"></p><h1 id="9-验证集群状态"><a href="#9-验证集群状态" class="headerlink" title="9.验证集群状态"></a>9.验证集群状态</h1><pre><code class="hljs"># 集群禁止非安全模式ceph config set mon auth_allow_insecure_global_id_reclaim false# 开启mon组件msgr2消息传递协议ceph mon enable-msgr2# 查看ceph集群整体状态ceph -s# 查看ceph集群健康状态ceph health detail# 查看ceph集群的存储空间ceph df# 查看ceph的实时运行状态ceph -w</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.cnblogs.com/punchlinux/p/17053624.html">https://www.cnblogs.com/punchlinux/p/17053624.html</a></li><li><a href="https://blog.csdn.net/jkjgj/article/details/128785139">https://blog.csdn.net/jkjgj/article/details/128785139</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Ceph</tag>
      
      <tag>存储</tag>
      
      <tag>分布式存储</tag>
      
      <tag>云存储</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Jenkins工作流Pipeline语法</title>
    <link href="/linux/JenkinsPipeline/"/>
    <url>/linux/JenkinsPipeline/</url>
    
    <content type="html"><![CDATA[<p>Pipeline，即管道，是Jenkins用于集成持续交付与实施的一套插件，各种管道的综合运用完整定义了自动化持续交付流程，即将基于版本控制管理的软件代码经过持续的构建、测试、修复、部署直到最终交付到线上正式环境这一整套流程一站式完整的整合，这样就将原本独立运行于单个或者多个节点的任务连接起来，实现单个任务难以完成的复杂流程编排与可视化。可将这些管道组合而成的交付过程形象的理解为流水线模型，也由此展现了Jenkins自动化引擎的功能，其特性如下：</p><ul><li>代码，流水线在代码中实现，通常会检查源代码控制，如代码编辑、审查和迭代的能力</li><li>可持续，Jenkins重启或者中断后都不会影响流水线的任务</li><li>可停顿，流水线可有选择的停止或等待人工输入或批准，然后才能继续运行流水线</li><li>多功能，流水线支持复杂的现实世界的CD需求，包括fork&#x2F;join、循环以及并行执行工作的能力</li><li>可扩展，流水线插件支持扩展到它的DSL的惯例和与其他插件集成的多个选项</li></ul><hr><p>Jenkins流水线存储于文本文件Jenkinsfile，分为声明式流水线与脚本化流水线两种，官方推荐声明式流水线，其语法类似于gradle，都是基于groovy的闭包语法</p><p>Jenkins官方建议将Jenkinsfile提交到项目的源代码托管仓库，体现了Jenkins流水线即代码的特性，即将交付流水线作为应用程序的一部分，像其他代码一样进行版本化和审查，以便于项目成员的查看、编辑、复查或迭代、跟踪审计，还能自动地为所有分支创建流水线构建过程并拉取</p><hr><h1 id="1-基础结构"><a href="#1-基础结构" class="headerlink" title="1.基础结构"></a>1.基础结构</h1><pre><code class="hljs">pipeline &#123;  agent any    stages &#123;      stage(&#39;Build&#39;) &#123;        steps &#123;          echo &#39;Building..&#39;        &#125;      &#125;      stage(&#39;Test&#39;) &#123;        steps &#123;          echo &#39;Testing..&#39;        &#125;      &#125;      stage(&#39;Deploy&#39;) &#123;        steps &#123;          echo &#39;Deploying....&#39;        &#125;      &#125;    &#125;&#125;</code></pre><h2 id="语法规范"><a href="#语法规范" class="headerlink" title="语法规范"></a>语法规范</h2><ul><li>1.声明式流水线的所有内容都被包含在一个Pipeline块中，标明其为声明式的pipeline脚本</li><li>2.语句无需分隔符但每条语句必须在一行内，且循环判断语句需被script{}包裹</li><li>3.顶层pipeline块由章节Sections、指令Directives、步骤Steps以及赋值语句assignment statements组成</li><li>4.属性引用语句将会被当做是无参数的方法调用，如input会被当做input()</li></ul><h1 id="2-Sections"><a href="#2-Sections" class="headerlink" title="2.Sections"></a>2.Sections</h1><p>Sections，即章节，是包含一个或多个Agent、Stages、post、Directives和Steps的代码区域块</p><h2 id="2-1-agent"><a href="#2-1-agent" class="headerlink" title="2.1 agent"></a>2.1 agent</h2><p>agent，即代理，用于指定任务或某个阶段的执行节点，也即是任务运行的slave或者master节点，位于pipeline块的顶层表示整个流水线的运行节点，位于stage则表示只在某个特定的阶段指定运行节点，可选参数为any、none、label、node、docker、dockerfile、kubernetes</p><h3 id="2-1-1-any"><a href="#2-1-1-any" class="headerlink" title="2.1.1 any"></a>2.1.1 any</h3><p>表示任务可运行于任意的可用节点</p><pre><code class="hljs">pipeline &#123;    agent any    stages &#123;        stage(&#39;Build&#39;) &#123;            steps(&quot;Build&quot;)&#123;                 echo &#39;Project Build&#39;             &#125;        &#125;        stage(&#39;Deploy&#39;) &#123;            steps &#123;                 echo &#39;Project Deploy&#39;             &#125;        &#125;    &#125;&#125;</code></pre><h3 id="2-1-2-none"><a href="#2-1-2-none" class="headerlink" title="2.1.2 none"></a>2.1.2 none</h3><p>表示不声明全局agent，也即需要为每一个stage声明各自单独的agent</p><h3 id="2-1-3-label"><a href="#2-1-3-label" class="headerlink" title="2.1.3 label"></a>2.1.3 label</h3><p>表示任务运行于指定标签的节点，可配合逻辑运算符</p><pre><code class="hljs">agent &#123;  label &#39;test-project&#39;&#125;agent &#123;  label &#39;test&#39; &amp;&amp; &#39;devops&#39;&#125;agent &#123;  label &#39;project&#39; || &#39;devops&#39;&#125;</code></pre><h3 id="2-1-4-node"><a href="#2-1-4-node" class="headerlink" title="2.1.4 node"></a>2.1.4 node</h3><p>表示任务运行于指定的节点，功能类似于label，但可附加选项参数，如指定工作目录customWorkspace</p><pre><code class="hljs">agent &#123;  node &#123;    label &#39;test-node&#39;    customWorkspace &#39;/home/jenkins&#39;  &#125;&#125;</code></pre><h3 id="2-1-5-docker"><a href="#2-1-5-docker" class="headerlink" title="2.1.5 docker"></a>2.1.5 docker</h3><p>表示任务运行于预配置节点或指定label节点创建的docker容器，可接收docker run、docker registry等参数，也可指定工作目录customWorkspace参数</p><pre><code class="hljs">agent &#123;  docker &#39;nginx:v1.20.0&#39;&#125;agent &#123;  docker &#123;    image &#39;nginx:v1.20.0&#39;    label &#39;nginx&#39;    args  &#39;-v /tmp:/tmp&#39;    registryUrl &#39;https://myregistry.com/&#39;    registryCredentialsId &#39;myPredefinedCredentialsInJenkins&#39;  &#125;&#125;</code></pre><h3 id="2-1-6-dockerfile"><a href="#2-1-6-dockerfile" class="headerlink" title="2.1.6 dockerfile"></a>2.1.6 dockerfile</h3><p>表示任务运行于一个由Dockerfile创建docker容器，默认会从构建的根目录搜索Dockerfile文件，也可指定工作目录customWorkspace参数</p><pre><code class="hljs">agent &#123;   dockerfile true &#125;agent &#123;  dockerfile &#123;    filename &#39;my_dockerfile&#39;    dir &#39;build_dir&#39;    label &#39;my-defined-label&#39;    additionalBuildArgs  &#39;--build-arg version=1.0.2&#39;    args &#39;-v /tmp:/tmp&#39;    registryUrl &#39;https://myregistry.com/&#39;    registryCredentialsId &#39;myPredefinedCredentialsInJenkins&#39;  &#125;&#125;</code></pre><h3 id="2-1-7-kubernetes"><a href="#2-1-7-kubernetes" class="headerlink" title="2.1.7 kubernetes"></a>2.1.7 kubernetes</h3><p>表示任务运行于kubernetes集群的一个pod，pod模版被定义在kubernetes{}模块</p><pre><code class="hljs">agent &#123;  kubernetes &#123;    label jenkins-slave    yaml &quot;&quot;&quot;            kind: Pod            metadata:              name: devops            spec:              containers:              - name: nginx                image: nginx        &quot;&quot;&quot;  &#125;&#125;</code></pre><h2 id="2-2-stages"><a href="#2-2-stages" class="headerlink" title="2.2 stages"></a>2.2 stages</h2><p>stages，即阶段集合，项目交付所有的阶段，如拉取源代码、构建打包、测试、部署等各个阶段</p><h2 id="2-3-stage"><a href="#2-3-stage" class="headerlink" title="2.3 stage"></a>2.3 stage</h2><p>stage，即阶段，被stages包裹，一个stages可以有多个stage</p><h2 id="2-4-steps"><a href="#2-4-steps" class="headerlink" title="2.4 steps"></a>2.4 steps</h2><p>steps，即步骤，阶段的最小执行单元，被stage包裹</p><h2 id="2-5-post"><a href="#2-5-post" class="headerlink" title="2.5 post"></a>2.5 post</h2><p>post，即发布，任务构建执行完成后根据构建结果所执行的对应操作，如发送任务构建通知邮件等，定义于整个流水线或某个stage，执行场景如下：</p><h3 id="2-5-1-always"><a href="#2-5-1-always" class="headerlink" title="2.5.1 always"></a>2.5.1 always</h3><p>不考虑pipeline或stage的执行结果，总是会执行</p><h3 id="2-5-2-changed"><a href="#2-5-2-changed" class="headerlink" title="2.5.2 changed"></a>2.5.2 changed</h3><p>只有pipeline或stage的执行结果状态与前一次执行相比发生改变时执行</p><h3 id="2-5-3-fixed"><a href="#2-5-3-fixed" class="headerlink" title="2.5.3 fixed"></a>2.5.3 fixed</h3><p>当前pipeline或stage执行成功且前一次执行结果是failure或unstable时执行</p><h3 id="2-5-4-regression"><a href="#2-5-4-regression" class="headerlink" title="2.5.4 regression"></a>2.5.4 regression</h3><p>当前pipeline或stage执行结果是failure、unstable或aborted且前一次执行成功时执行</p><h3 id="2-5-5-aborted"><a href="#2-5-5-aborted" class="headerlink" title="2.5.5 aborted"></a>2.5.5 aborted</h3><p>当前pipeline或stage执行结果是aborted，即人工停止pipeline时执行</p><h3 id="2-5-6-failure"><a href="#2-5-6-failure" class="headerlink" title="2.5.6 failure"></a>2.5.6 failure</h3><p>当前pipeline或stage执行结果失败时执行</p><h3 id="2-5-7-success"><a href="#2-5-7-success" class="headerlink" title="2.5.7 success"></a>2.5.7 success</h3><p>当前pipeline或stage执行结果成功时执行</p><h3 id="2-5-8-unstable"><a href="#2-5-8-unstable" class="headerlink" title="2.5.8 unstable"></a>2.5.8 unstable</h3><p>当前pipeline或stage执行结果unstable时执行</p><h3 id="2-5-9-unsuccessful"><a href="#2-5-9-unsuccessful" class="headerlink" title="2.5.9 unsuccessful"></a>2.5.9 unsuccessful</h3><p>当前pipeline或stage执行结果不是成功时执行</p><h3 id="2-5-10-cleanup"><a href="#2-5-10-cleanup" class="headerlink" title="2.5.10 cleanup"></a>2.5.10 cleanup</h3><p>其余所有post场景脚本都处理完之后执行，无论当前pipeline或stage执行结果是什么</p><pre><code class="hljs">pipeline &#123;  agent any  stages &#123;    stage(&#39;Example1&#39;) &#123;      steps &#123;        echo &#39;Hello World1&#39;      &#125;    &#125;    stage(&#39;Example2&#39;) &#123;      steps &#123;        echo &#39;Hello World2&#39;      &#125;    &#125;  &#125;  post &#123;    always &#123;      echo &#39;I will always say Hello again!&#39;    &#125;  &#125;&#125;pipeline &#123;  agent any  stages &#123;    stage(&#39;Example1&#39;) &#123;      steps &#123;        sh &#39;ip a&#39;      &#125;      post &#123;        failure &#123;          echo &#39;I will always say Hello again!&#39;        &#125;      &#125;    &#125;  &#125;&#125;</code></pre><h1 id="3-Directives"><a href="#3-Directives" class="headerlink" title="3.Directives"></a>3.Directives</h1><p>Directive，即指令，用于stage执行时的条件判断或数据的预处理，包含了environment、options、parameters、triggers、stage、tools、input、when等配置</p><h2 id="3-1-environment"><a href="#3-1-environment" class="headerlink" title="3.1 environment"></a>3.1 environment</h2><p>environment指令用于指定一个键&#x2F;值对序列作为环境变量，其作用域取决于定义的位置，位于顶层pipeline时作为全局变量，位于stage时作为该stage的局部环境变量。此外，该指令的credentials()方法还可定义访问凭证，支持如下credential类型：</p><ul><li>Secret Text类型，该环境变量的值将会被设置为Secret Text的内容</li><li>Secret File类型，该环境变量的值将会被设置为临时创建的文件路径</li><li>Username and password类型，该环境变量的值将会被设置为username:password，并且还会自动创建两个环境变量MYVARNAME_USR和MYVARNAME_PSW</li><li>SSH with Private Key类型，该环境变量的值将会被设置为临时创建的ssh key文件路径，并且还会自动创建两个环境变量<br>MYVARNAME_USR和MYVARNAME_PSW</li></ul><h3 id="3-1-1-环境变量"><a href="#3-1-1-环境变量" class="headerlink" title="3.1.1 环境变量"></a>3.1.1 环境变量</h3><pre><code class="hljs">pipeline &#123;  agent any  environment &#123;    NAME= &#39;test&#39;  &#125;  stages &#123;    stage(&#39;env1&#39;) &#123;      environment &#123;        REGISTRY = &#39;https://registry.sword.org&#39;      &#125;      steps &#123;        sh &quot;env&quot;      &#125;    &#125;    stage(&#39;env2&#39;) &#123;      steps &#123;        sh &quot;env&quot;      &#125;    &#125;  &#125;&#125;</code></pre><h3 id="3-3-2-访问凭证"><a href="#3-3-2-访问凭证" class="headerlink" title="3.3.2 访问凭证"></a>3.3.2 访问凭证</h3><pre><code class="hljs">pipeline &#123;  agent any  environment &#123;    REGCRED = credentials(&#39;registry-auth&#39;)    KUBECONFIG = credentials(&#39;kubernetes-auth&#39;)  &#125;  stages &#123;    stage(&#39;env&#39;) &#123;      steps &#123;        sh &quot;env&quot;      &#125;    &#125;  &#125;&#125;</code></pre><h2 id="3-2-options"><a href="#3-2-options" class="headerlink" title="3.2 options"></a>3.2 options</h2><p>options指令用于配置流水线特定的选项参数，以适用于特定的需求，一般定义于顶层Pipeline，某些指令也可用于stage</p><h3 id="3-2-1-buildDiscarder"><a href="#3-2-1-buildDiscarder" class="headerlink" title="3.2.1 buildDiscarder"></a>3.2.1 buildDiscarder</h3><p>该指令用于指定构架历史与构建日志的保存数量</p><pre><code class="hljs">options &#123;   buildDiscarder(logRotator(numToKeepStr: &#39;1&#39;)) &#125;</code></pre><h3 id="3-2-2-disableConcurrentBuilds"><a href="#3-2-2-disableConcurrentBuilds" class="headerlink" title="3.2.2 disableConcurrentBuilds"></a>3.2.2 disableConcurrentBuilds</h3><p>该指令用于禁止流水线并行执行，防止并行流水线同时访问共享资源导致流水线失败</p><pre><code class="hljs">options &#123;   disableConcurrentBuilds()&#125;</code></pre><h3 id="3-2-3-disableResume"><a href="#3-2-3-disableResume" class="headerlink" title="3.2.3 disableResume"></a>3.2.3 disableResume</h3><p>该指令用于禁止控制器重启后流水线自动开启</p><pre><code class="hljs">options &#123;   disableResume() &#125;</code></pre><h3 id="3-2-4-newContainerPerStage"><a href="#3-2-4-newContainerPerStage" class="headerlink" title="3.2.4 newContainerPerStage"></a>3.2.4 newContainerPerStage</h3><p>该指令用于设定docker或dockerfile类型的agent每个阶段将在同一个节点的新容器中运行，而不是所有的阶段都在同一个容器中运行</p><pre><code class="hljs">options &#123;  newContainerPerStage()&#125;</code></pre><h3 id="3-2-5-quietPeriod"><a href="#3-2-5-quietPeriod" class="headerlink" title="3.2.5 quietPeriod"></a>3.2.5 quietPeriod</h3><p>该指令用于设置流水线静默期，也即是触发流水线后任务启动的延迟时间，单位为秒</p><pre><code class="hljs">options &#123;   quietPeriod(30) &#125;</code></pre><h3 id="3-2-6-retry"><a href="#3-2-6-retry" class="headerlink" title="3.2.6 retry"></a>3.2.6 retry</h3><p>该指令用于指定流水线失败重试次数，可用于stage</p><pre><code class="hljs">options &#123;   retry(3) &#125;</code></pre><h3 id="3-2-7-timeout"><a href="#3-2-7-timeout" class="headerlink" title="3.2.7 timeout"></a>3.2.7 timeout</h3><p>该指令用于设置流水线构建的超时时间，超时构建自动终止，不加unit参数默认为1分钟，可用于stage</p><pre><code class="hljs">options &#123;  timeout(time: 1, unit: &#39;HOURS&#39;) &#125;</code></pre><h3 id="3-2-8-timestamps"><a href="#3-2-8-timestamps" class="headerlink" title="3.2.8 timestamps"></a>3.2.8 timestamps</h3><p>该指令用于设置控制台构建日志输出的时间戳，可用于stage</p><pre><code class="hljs">options &#123;   timestamps() &#125;</code></pre><h3 id="3-2-9-checkoutToSubdirectory"><a href="#3-2-9-checkoutToSubdirectory" class="headerlink" title="3.2.9 checkoutToSubdirectory"></a>3.2.9 checkoutToSubdirectory</h3><p>该指令用于将源码拉取到工作空间指定的子目录</p><pre><code class="hljs">options &#123;   checkoutToSubdirectory(&#39;source&#39;) &#125;</code></pre><h2 id="3-3-parameters"><a href="#3-3-parameters" class="headerlink" title="3.3 parameters"></a>3.3 parameters</h2><p>parameters指令用于设置构建触发时用户所需的参数，steps指令通过params对象获取这些参数值，只可作用于顶层pipeline，且只能出现一次，支持的参数类型如下：</p><ul><li>string，字符串类型</li><li>text，文本类型，一般用于定义多行文本内容的变量</li><li>booleanParam，布尔类型</li><li>choice，选择类型，一般用于给定几个可选的值，然后选择其中一个进行赋值</li><li>password，密码类型，一般用于定义敏感型变量，在Jenkins控制台的输出为*</li><li>imageTag，镜像tag，需安装Image Tag Parameter插件</li><li>gitParameter，获取git仓库分支，需安装Git Parameter插件</li></ul><h3 id="3-3-1-string"><a href="#3-3-1-string" class="headerlink" title="3.3.1 string"></a>3.3.1 string</h3><pre><code class="hljs">pipeline &#123;  agent any  parameters &#123;    string(      name: &#39;DEPLOY_ENV&#39;,      defaultValue: &#39;staging&#39;,      description: &#39;1&#39;    )  &#125;&#125;</code></pre><h3 id="3-3-2-text"><a href="#3-3-2-text" class="headerlink" title="3.3.2 text"></a>3.3.2 text</h3><pre><code class="hljs">pipeline &#123;  agent any  parameters &#123;    text(      name: &#39;DEPLOY_TEXT&#39;,       defaultValue: &#39;One\nTwo\nThree\n&#39;,       description: &#39;2&#39;    )  &#125;&#125;</code></pre><h3 id="3-3-3-booleanParam"><a href="#3-3-3-booleanParam" class="headerlink" title="3.3.3 booleanParam"></a>3.3.3 booleanParam</h3><pre><code class="hljs">pipeline &#123;  agent any  parameters &#123;    booleanParam(      name: &#39;DEBUG_BUILD&#39;,      defaultValue: true,      description: &#39;3&#39;    )  &#125;&#125;</code></pre><h3 id="3-3-4-choice"><a href="#3-3-4-choice" class="headerlink" title="3.3.4 choice"></a>3.3.4 choice</h3><pre><code class="hljs">pipeline &#123;  agent any  parameters &#123;    choice(      name: &#39;CHOICES&#39;,      choices: [&#39;one&#39;, &#39;two&#39;, &#39;three&#39;],      description: &#39;4&#39;    )  &#125;&#125;</code></pre><h3 id="3-3-5-password"><a href="#3-3-5-password" class="headerlink" title="3.3.5 password"></a>3.3.5 password</h3><pre><code class="hljs">pipeline &#123;  agent any  parameters &#123;    password(      name: &#39;PASSWORD&#39;,      defaultValue: &#39;SECRET&#39;,      description: &#39;A  secret password&#39;    )  &#125;&#125;</code></pre><h3 id="3-3-6-imageTag"><a href="#3-3-6-imageTag" class="headerlink" title="3.3.6 imageTag"></a>3.3.6 imageTag</h3><pre><code class="hljs">pipeline &#123;  agent any  parameters &#123;    imageTag(      name: &#39;DOCKER_IMAGE&#39;,      description: &#39;&#39;,      image: &#39;hexo/nginx&#39;, filter: &#39;.*&#39;,      defaultTag: &#39;&#39;,      registry: &#39;https://registry.sword.org&#39;,      credentialId: &#39;harbor-account&#39;,      tagOrder: &#39;NATURAL&#39;    ) &#125;&#125;</code></pre><h3 id="3-3-7-gitParameter"><a href="#3-3-7-gitParameter" class="headerlink" title="3.3.7 gitParameter"></a>3.3.7 gitParameter</h3><pre><code class="hljs">pipeline &#123;  agent any  parameters &#123;    gitParameter(      branch: &#39;&#39;,      branchFilter: &#39;origin/(.*)&#39;,      defaultValue: &#39;&#39;,      description: &#39;Branch for build and deploy&#39;,      name: &#39;BRANCH&#39;,      quickFilterEnabled: false,      selectedValue: &#39;NONE&#39;,      sortMode: &#39;NONE&#39;,      tagFilter: &#39;*&#39;,      type: &#39;PT_BRANCH&#39;    )  &#125;  stage(&#39;git&#39;) &#123;    steps &#123;      git branch: &quot;$BRANCH&quot;,      credentialsId: &#39;gitlab-key&#39;,      url: &#39;git@git-server:root/env.git&#39;    &#125;  &#125;&#125;</code></pre><h2 id="3-4-triggers"><a href="#3-4-triggers" class="headerlink" title="3.4 triggers"></a>3.4 triggers</h2><p>triggers指令用于设置流水线任务的自动触发执行，支持Webhook、Cron、 pollSCM和upstream等方式，作用域为顶层pipeline</p><h3 id="3-4-1-Cron"><a href="#3-4-1-Cron" class="headerlink" title="3.4.1 Cron"></a>3.4.1 Cron</h3><p>该指令用于流水线定时构建，适用于构建时间较长或需要定期在某个时间段执行构建的流水线，如周一到周五每隔四个小时执行一次</p><pre><code class="hljs">pipeline &#123;  agent any  triggers &#123;    cron(&#39;H */4 * * 1-5&#39;)    cron(&#39;H/12 * * * *&#39;)    cron(&#39;H * * * *&#39;)  &#125;  stages &#123;    stage(&#39;Example&#39;) &#123;      steps &#123;        echo &#39;Hello World&#39;      &#125;    &#125;  &#125;&#125;</code></pre><ul><li>注:H意为Hash，而非HOURS，可解决多个流水线在同一时间同时运行所产生的系统负载</li></ul><h3 id="3-4-2-Upstream"><a href="#3-4-2-Upstream" class="headerlink" title="3.4.2 Upstream"></a>3.4.2 Upstream</h3><p>该指令用于根据上游任务的执行结果决定是否触发该流水线，目前支持的状态有SUCCESS、UNSTABLE、FAILURE、NOT_BUILT、ABORTED等</p><pre><code class="hljs">pipeline &#123;  agent any    triggers &#123;      upstream(upstreamProjects: &#39;job01&#39;, threshold: hudson.model.Result.SUCCESS)    &#125;  stages &#123;    stage(&#39;Example&#39;) &#123;      steps &#123;        echo &#39;Hello World&#39;      &#125;    &#125;  &#125;&#125;</code></pre><h3 id="3-4-3-pollSCM"><a href="#3-4-3-pollSCM" class="headerlink" title="3.4.3 pollSCM"></a>3.4.3 pollSCM</h3><p>该指令用于定时检查源码变更的触发，如果发生变更则触发构建</p><pre><code class="hljs">triggers &#123;   pollSCM(&#39;H */4 * * 1-5&#39;) &#125;</code></pre><h2 id="3-5-tools"><a href="#3-5-tools" class="headerlink" title="3.5 tools"></a>3.5 tools</h2><p>该指令用于设置任务构建所需工具，并设置到PATH环境变量，但当agent none时tools声明将会被忽略，支持jdk、maven、gradle等工具，且需在Jenkins控制台全局工具配置处进行配置</p><h2 id="3-6-input"><a href="#3-6-input" class="headerlink" title="3.6 input"></a>3.6 input</h2><p>该指令用于实现流水线交互式操作，如选择要部署的环境、是否继续执行某个阶段等，只能作用于stage，支持如下选项：</p><ul><li>message，必选，需要用户进行input的提示信息，如“是否发布到生产环境?”</li><li>id，可选，input的标识符，默认为stage名称</li><li>ok，可选，确认按钮的显示信息，如“确定”、“允许”等</li><li>submitter，可选，允许提交input操作的用户或组，为空则表示任何登录用户均可提交</li><li>parameters，提供一个参数列表供input使用</li></ul><hr><pre><code class="hljs">pipeline &#123;  agent any    stages &#123;      stage(&#39;阶段1&#39;) &#123;        input &#123;          message &quot;你好，请输入登录用户&quot;          ok &quot;确认&quot;          submitter &quot;alice,bob&quot;            parameters &#123;              string(name: &#39;PERSON&#39;, defaultValue: &#39;张三&#39;, description: &#39;请输入公司账号&#39;)            &#125;        &#125;        steps &#123;          echo &quot;你好, $&#123;PERSON&#125;, 打卡成功&quot;        &#125;      &#125;    &#125;&#125;</code></pre><h2 id="3-7-when"><a href="#3-7-when" class="headerlink" title="3.7 when"></a>3.7 when</h2><p>该指令用于流水线根据给定的条件决定是否应该执行该stage，必须包含至少一个条件，若包含多个条件，则需所有子条件必须都返回True才能执行，还可结合not、allOf、anyOf语法达到更灵活的条件匹配。需要注意的是，正常情况下when判断是在agent、input、options命令之后才执行的，但也可通过将beforeAgent、beforeInput、beforeOptions参数设为true来设置提前执行。when指令只能作用于stage，常用的内置条件如下：</p><h3 id="3-7-1-branch"><a href="#3-7-1-branch" class="headerlink" title="3.7.1 branch"></a>3.7.1 branch</h3><p>当前构建分支与给定分支匹配时执行该stage，只适用于多分支流水线</p><pre><code class="hljs">when &#123;     branch &#39;master&#39; &#125;when &#123;     branch pattern: &quot;release-\\d+&quot;, comparator: &quot;REGEXP&quot;&#125;</code></pre><h3 id="3-7-2-changelog"><a href="#3-7-2-changelog" class="headerlink" title="3.7.2 changelog"></a>3.7.2 changelog</h3><p>匹配提交的变更日志决定是否构建</p><pre><code class="hljs">when &#123;     changelog &#39;.*^\\[DEPENDENCY\\] .+$&#39; &#125;</code></pre><h3 id="3-7-3-environment"><a href="#3-7-3-environment" class="headerlink" title="3.7.3 environment"></a>3.7.3 environment</h3><p>指定的环境变量和给定的变量匹配时执行该stage</p><pre><code class="hljs">when &#123;     environment name: &#39;DEPLOY_TO&#39;, value: &#39;production&#39; &#125;</code></pre><h3 id="3-7-4-equals"><a href="#3-7-4-equals" class="headerlink" title="3.7.4 equals"></a>3.7.4 equals</h3><p>期望值和实际值相同时执行该stage</p><pre><code class="hljs">when &#123;   equals expected: 2, actual: currentBuild.number&#125;</code></pre><h3 id="3-7-5-expression"><a href="#3-7-5-expression" class="headerlink" title="3.7.5 expression"></a>3.7.5 expression</h3><p>指定的Groovy表达式为True时执行该stage</p><pre><code class="hljs">when &#123;   expression &#123;     return params.DEBUG_BUILD   &#125; &#125;</code></pre><h3 id="3-7-6-tag"><a href="#3-7-6-tag" class="headerlink" title="3.7.6 tag"></a>3.7.6 tag</h3><p>TAG_NAME的值和给定的条件匹配时执行该stage，匹配规则如下：</p><ul><li>not，嵌套条件出现错误时执行该stage，必须包含一个条件</li><li>allOf，所有的嵌套条件都正确时执行该stage，必须包含至少一个条件</li><li>anyOf，至少有一个嵌套条件为True时执行该stage</li></ul><hr><pre><code class="hljs">when &#123;   tag &quot;release-*&quot; &#125;when &#123;   tag pattern: &quot;release-\\d+&quot;, comparator: &quot;REGEXP&quot;&#125;when &#123;   not &#123;     branch &#39;master&#39;   &#125; &#125;when &#123;   allOf &#123;     branch &#39;master&#39;;     environment name: &#39;DEPLOY_TO&#39;, value: &#39;production&#39;   &#125; &#125;when &#123;   anyOf &#123;     branch &#39;master&#39;; branch &#39;staging&#39;   &#125; &#125;</code></pre><h1 id="4-parallel"><a href="#4-parallel" class="headerlink" title="4.parallel"></a>4.parallel</h1><p>Parallel，即并发，用于定义并行运行的stage，每个stage只能有一个steps或parallel，且并行的stage由于没有相关的steps就不能包含agent或tools，嵌套的stages也不能定义并行运行。此外，通过对并行stage设置failFast true，以达到其中一个并行stage执行失败时强制终止其他并行stage的目的</p><pre><code class="hljs">pipeline &#123;  agent any    stages &#123;      stage(&#39;非并行阶段&#39;) &#123;        steps &#123;          echo &#39;非并行阶段&#39;        &#125;      &#125;      stage(&#39;并行阶段&#39;) &#123;        failFast true          parallel &#123;            stage(&#39;并行阶段1&#39;) &#123;              steps &#123;                echo &quot;并行阶段1&quot;              &#125;            &#125;            stage(&#39;并行阶段2&#39;) &#123;              steps &#123;                echo &quot;并行阶段2&quot;              &#125;            &#125;          &#125;      &#125;    &#125;&#125;</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="http://www.ioo.cool/posts/fa563716f116">http://www.ioo.cool/posts/fa563716f116</a></li><li><a href="https://www.jenkins.io/zh/doc/book/pipeline/jenkinsfile">https://www.jenkins.io/zh/doc/book/pipeline/jenkinsfile</a></li><li><a href="https://blog.csdn.net/lazycheerup/article/details/130401651">https://blog.csdn.net/lazycheerup/article/details/130401651</a></li><li><a href="https://blog.csdn.net/zhou920786312/article/details/125955651">https://blog.csdn.net/zhou920786312/article/details/125955651</a></li><li><a href="https://blog.csdn.net/qq_34556414/article/details/120663772">https://blog.csdn.net/qq_34556414/article/details/120663772</a></li><li><a href="https://blog.csdn.net/u012060033/article/details/127907588">https://blog.csdn.net/u012060033/article/details/127907588</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>CICD</tag>
      
      <tag>DevOps</tag>
      
      <tag>Jenkins</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Openstack集群基于RDO工具部署</title>
    <link href="/linux/Openstack-RDO/"/>
    <url>/linux/Openstack-RDO/</url>
    
    <content type="html"><![CDATA[<h1 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h1><ul><li>172.100.100.180 controler</li><li>172.100.100.181 compute01</li><li>172.100.100.182 compute02</li></ul><hr><h1 id="1-配置系统环境"><a href="#1-配置系统环境" class="headerlink" title="1.配置系统环境"></a>1.配置系统环境</h1><h2 id="1-1-配置hosts"><a href="#1-1-配置hosts" class="headerlink" title="1.1 配置hosts"></a>1.1 配置hosts</h2><pre><code class="hljs">sudo vi /etc/hosts172.100.100.180 controler172.100.100.181 compute01172.100.100.182 compute02</code></pre><h2 id="1-2-配置字符集"><a href="#1-2-配置字符集" class="headerlink" title="1.2 配置字符集"></a>1.2 配置字符集</h2><pre><code class="hljs">sudo vi /etc/environmentLANG=en_US.utf-8LC_ALL=en_US.utf-8</code></pre><h2 id="1-3-关闭防火墙"><a href="#1-3-关闭防火墙" class="headerlink" title="1.3 关闭防火墙"></a>1.3 关闭防火墙</h2><h2 id="1-4-禁用selinux"><a href="#1-4-禁用selinux" class="headerlink" title="1.4 禁用selinux"></a>1.4 禁用selinux</h2><h2 id="1-5-禁用NetworkManager"><a href="#1-5-禁用NetworkManager" class="headerlink" title="1.5 禁用NetworkManager"></a>1.5 禁用NetworkManager</h2><pre><code class="hljs">sudo systemctl stop NetworkManagersudo systemctl disable NetworkManager</code></pre><h2 id="1-6-控制节点配置外网网卡"><a href="#1-6-控制节点配置外网网卡" class="headerlink" title="1.6 控制节点配置外网网卡"></a>1.6 控制节点配置外网网卡</h2><h1 id="2-配置集群免密登录"><a href="#2-配置集群免密登录" class="headerlink" title="2.配置集群免密登录"></a>2.配置集群免密登录</h1><h1 id="3-配置集群时间同步"><a href="#3-配置集群时间同步" class="headerlink" title="3.配置集群时间同步"></a>3.配置集群时间同步</h1><pre><code class="hljs">sudo yum install -y ntp ntpdate</code></pre><h1 id="4-安装OpenStack库"><a href="#4-安装OpenStack库" class="headerlink" title="4.安装OpenStack库"></a>4.安装OpenStack库</h1><pre><code class="hljs">sudo yum update -ysudo yum install -y centos-release-openstack-trainsudo yum update -y</code></pre><h1 id="5-控制节点安装RDO"><a href="#5-控制节点安装RDO" class="headerlink" title="5.控制节点安装RDO"></a>5.控制节点安装RDO</h1><pre><code class="hljs">sudo yum install -y openstack-packstack</code></pre><h1 id="6-控制节点初始化集群"><a href="#6-控制节点初始化集群" class="headerlink" title="6.控制节点初始化集群"></a>6.控制节点初始化集群</h1><h2 id="6-1-生成应答文件"><a href="#6-1-生成应答文件" class="headerlink" title="6.1 生成应答文件"></a>6.1 生成应答文件</h2><pre><code class="hljs">packstack --gen-answer-file=openstack.txt</code></pre><h2 id="6-2-Allinone部署"><a href="#6-2-Allinone部署" class="headerlink" title="6.2 Allinone部署"></a>6.2 Allinone部署</h2><pre><code class="hljs">sudo packstack --allinone --default-password=Openstack_2023 \--os-neutron-l2-agent=openvswitch --ntp-servers=ntp.ntsc.ac.cn \--os-neutron-ovs-bridge-interfaces=br-ex:eth0 --provision-demo=n \--os-neutron-ovs-bridge-mappings=extnet:br-ex --os-aodh-install=n \--os-neutron-ml2-type-drivers=vxlan,vlan,gre,flat,geneve,local \--os-neutron-ml2-tenant-network-types=vxlan,vlan,gre,geneve,local \--os-neutron-ml2-mechanism-drivers=openvswitch,l2population \--os-ceilometer-install=n --os-swift-install=n</code></pre><h2 id="6-3-多节点部署"><a href="#6-3-多节点部署" class="headerlink" title="6.3 多节点部署"></a>6.3 多节点部署</h2><h3 id="6-3-1-生成应答文件"><a href="#6-3-1-生成应答文件" class="headerlink" title="6.3.1 生成应答文件"></a>6.3.1 生成应答文件</h3><pre><code class="hljs">packstack --default-password=Openstack_2023 --gen-answer-file=openstack.txt</code></pre><h3 id="6-3-2-配置应答文件"><a href="#6-3-2-配置应答文件" class="headerlink" title="6.3.2 配置应答文件"></a>6.3.2 配置应答文件</h3><pre><code class="hljs">vi openstack.txt# 设置是否安装OpenStack对象存储组件swift，默认为y，生产环境设为n，不安装CONFIG_SWIFT_INSTALL=n# 设置是否安装Openstack计量组件，可不安装CONFIG_CEILOMETER_INSTALL=n# 设置是否安装OpenStack遥测告警组件aodh，默认为y，生产环境设为n，不安装CONFIG_AODH_INSTALL=n# 设置NTP时间同步服务器CONFIG_NTP_SERVERS=ntp.ntsc.ac.cn# 设置控制节点IPCONFIG_CONTROLLER_HOST=172.100.100.180# 设置计算节点IPCONFIG_COMPUTE_HOSTS=172.100.100.181,172.100.100.182# 设置网络节点IPCONFIG_NETWORK_HOSTS=172.100.100.180# 设置块存储节点IPCONFIG_STORAGE_HOST=172.100.100.180# 设置块存储大小，默认为20G，存储路径为/var/lib/cinderCONFIG_CINDER_VOLUMES_SIZE=180G# 设置是否安装OpenStack主防火墙，用于控制进出网络的网络包CONFIG_NEUTRON_FWAAS=y# 设置是否安装OpenStack VPNCONFIG_NEUTRON_VPNAAS=y# 设置二层网络模块网络类型驱动CONFIG_NEUTRON_ML2_TYPE_DRIVERS=vxlan,gre,flat,vlan,geneve,local# 设置二层网络模块租户网络类型CONFIG_NEUTRON_ML2_TENANT_NETWORK_TYPES=vxlan,gre,vlan,geneve,local# 设置二层网络模块网络机制驱动，即设备实现机制CONFIG_NEUTRON_ML2_MECHANISM_DRIVER=openvswitch,l2population# 设置设置flat二层网络CONFIG_NEUTRON_ML2_FLAT_NETWORKS=extnet# 设置二层网络代理架构，OVN不支持LBaaS、VPNaaS和FWaaSCONFIG_NEUTRON_L2_AGENT=openvswitch# 设置flat二层网络关联的网桥CONFIG_NEUTRON_OVS_BRIDGE_MAPPINGS=extnet:br-ex# 设置桥接网卡，即将br-ex网卡桥接到eth0网卡用于外网连接CONFIG_NEUTRON_OVS_BRIDGE_IFACES=br-ex:eth0# 设置是否部署演示环境，设为否CONFIG_PROVISION_DEMO=n</code></pre><h3 id="6-3-3-备份应答文件"><a href="#6-3-3-备份应答文件" class="headerlink" title="6.3.3 备份应答文件"></a>6.3.3 备份应答文件</h3><pre><code class="hljs">grep -vE &quot;^#|^$&quot; openstack.txt &gt; openstack.txt.bak</code></pre><h2 id="6-4-控制节点初始化集群"><a href="#6-4-控制节点初始化集群" class="headerlink" title="6.4 控制节点初始化集群"></a>6.4 控制节点初始化集群</h2><pre><code class="hljs">packstack --answer-file=openstack.txt</code></pre><h1 id="7-配置Dashboard"><a href="#7-配置Dashboard" class="headerlink" title="7.配置Dashboard"></a>7.配置Dashboard</h1><h2 id="7-1-配置Dashboard访问地址"><a href="#7-1-配置Dashboard访问地址" class="headerlink" title="7.1 配置Dashboard访问地址"></a>7.1 配置Dashboard访问地址</h2><pre><code class="hljs">sudo cp /etc/httpd/conf.d/15-horizon_vhost.conf /etc/httpd/conf.d/15-horizon_vhost.conf.baksudo sed -i &#39;s/ServerAlias 172.100.100.180/192.168.100.180/g&#39; /etc/httpd/conf.d/15-horizon_vhost.conf</code></pre><h2 id="7-2-配置控制台访问地址"><a href="#7-2-配置控制台访问地址" class="headerlink" title="7.2 配置控制台访问地址"></a>7.2 配置控制台访问地址</h2><pre><code class="hljs">sudo cp /etc/nova/nova.conf /etc/nova/nova.conf.baksudo sed -i &#39;s/172.100.100.180:6080/192.168.100.180:6080/g&#39; /etc/nova/nova.confserver_proxyclient_address</code></pre><h2 id="7-3-重启httpd、nova服务"><a href="#7-3-重启httpd、nova服务" class="headerlink" title="7.3 重启httpd、nova服务"></a>7.3 重启httpd、nova服务</h2><pre><code class="hljs">sudo systemctl restart httpd.servicesudo systemctl restart openstack-nova-novncproxy.service </code></pre><h1 id="8-验证Openstack"><a href="#8-验证Openstack" class="headerlink" title="8.验证Openstack"></a>8.验证Openstack</h1><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://juejin.cn/post/7480036622810808370">https://juejin.cn/post/7480036622810808370</a></li><li><a href="https://www.cnblogs.com/fengdejiyixx/p/14776814.html">https://www.cnblogs.com/fengdejiyixx/p/14776814.html</a></li><li><a href="https://blog.csdn.net/CN_TangZheng/article/details/104543185">https://blog.csdn.net/CN_TangZheng/article/details/104543185</a></li><li><a href="https://m.dandelioncloud.cn/article/details/1613550192089595906">https://m.dandelioncloud.cn/article/details/1613550192089595906</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>虚拟化</tag>
      
      <tag>私有云</tag>
      
      <tag>Openstack</tag>
      
      <tag>公有云</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux系统内核优化</title>
    <link href="/linux/KernelOptimization/"/>
    <url>/linux/KernelOptimization/</url>
    
    <content type="html"><![CDATA[<p>Linux系统内核参数默认值的设定通常偏向稳定保守，目的是以性能换取稳定，适用于通用场景，并不符合高并发量的生产环境。因此，根据实际业务特性与需求优化内核参数，是系统稳定高效运行的强有力的保障，可充分发挥服务器硬件计算能力、提高资源利用以及节省硬件成本。但由于内核程序极其庞大与复杂，参数的数量繁多，某个功能的新增或修改很有可能影响其他功能，甚至导致整个系统的崩溃。所以，一定要结合实际的场景进行优化，切忌过度优化，优化的参数需要具体了解其功能，以免出现不必要的问题</p><h1 id="x2F-proc"><a href="#x2F-proc" class="headerlink" title="&#x2F;proc"></a>&#x2F;proc</h1><p>&#x2F;proc，虚拟文件系统，用于内核参数的调整，动态存储于内存，原理是将内核运行的关键数据结构以文件方式呈现于该虚拟文件系统目录中的特定文件，相当于将不可见的数据结构以可视化的方式呈现出来，调试内核参数即可通过实时观察这些文件来判断其作用。这些文件只是类似于接口的存在，并不是真实文件，故其文件大小都为0，对其的的调用只是内核数据结构的映射。文件目录如下：</p><ul><li>cmdline，系统启动时输入给内核命令行参数</li><li>cpuinfo，CPU的硬件信息，如型号、家族、缓存大小等</li><li>devices，主设备号及设备组的列表，当前加载的各种设备，如块设备、字符设备</li><li>dma，使用的DMA通道</li><li>filesystems，当前内核支持的文件系统，用于为mount命令指定文件系统</li><li>interrupts ，中断的使用及触发次数，调试中断时很有用</li><li>ioports，即IO端口，当前在用的已注册的I&#x2F;O端口范围</li><li>kcore，伪文件，以core文件格式给出系统物理内存映象，可用GDB查探当前内核的任意数据结构，对应dmesg命令，可取代系统调用syslog记录内核日志</li><li>kallsym，内核符号表，文件保存了内核输出的符号定义, modules(X)使用该文件动态地连接和捆绑可装载的模块</li><li>loadavg，平均负载，给出了在过去的 1、 5、15分钟里在运行队列里的任务数、总作业数以及正在运行的作业总数</li><li>locks，内核锁</li><li>meminfo，物理内存、交换空间等信息，系统内存占用情况，对应df命令</li><li>misc，杂项</li><li>modules，已加载的模块列表，对应lsmod命令</li><li>mounts，已加载的文件系统的列表，对应mount命令，无参数</li><li>partitions，系统识别的分区表</li><li>slabinfo，sla池信息</li><li>stat，全面统计状态表，CPU内存的利用率等都是从这里提取数据，对应ps命令</li><li>swaps，对换空间的利用情况</li><li>sys，存放硬件设备的驱动程序信息，可通过&#x2F;sys&#x2F;block优化磁盘I&#x2F;O</li><li>version，指明当前正在运行的内核版本</li></ul><p>Linux系统内核参数按照功能大体分为网络相关参数、文件系统参数、内存参数以及内核参数几类，</p><pre><code class="hljs"># 设置系统进程最大打开文件数，即系统当前最大文件句柄数，系统级别的限制，默认值通常与物理内存有关，配合单个进程最大打开文件数ulimit的open file值（默认1024），达到高并发业务的需求fs.file-max = 6553560# 设置是否启用非本地IP地址socket监听，作为网关、反向代理或负载均衡双机热备高可用绑定虚拟VIP时须开启net.ipv4.ip_nonlocal_bind = 1# 设置是否启用IPv4转发，作为路由网关、反向代理与负载均衡开启客户端IP透传时须开启net.ipv4.ip_forward = 1 # 设置是否启用重用，即是否将TIME_WAIT状态的socket重新用于新的TCP链接，适用于高并发场景，默认为0net.ipv4.tcp_tw_reuse = 1# 设置是否将TIME-WAIT状态的socket重新用于新的TCP连接，适用于高并发场景，快速回收TIME-WAIT状态的连接net.ipv4.tcp_tw_recycle = 1# 设置启用keepalive长连接时，TCP发送keepalive消息的频度，默认为2小时，设为10分钟可更快清理无效链接net.ipv4.tcp_keepalive_time = 600# 设置启用keepalive长连接时，超时探测包的发送次数net.ipv4.tcp_keepalive_probes = 3# 设置启用keepalive长连接时，探测包的发送间隔，单位为秒net.ipv4.tcp_keepalive_intvl = 15 # 设置是否开启TCP时间戳，默认为0net.ipv4.tcp_timestamps = 1</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.cnblogs.com/eddie1127/p/11806372.html">https://www.cnblogs.com/eddie1127/p/11806372.html</a></li></ul><p><a href="https://www.cnblogs.com/soymilk2019/p/13725248.html">https://www.cnblogs.com/soymilk2019/p/13725248.html</a><br><a href="https://blog.csdn.net/weixin_42255494/article/details/116497513">https://blog.csdn.net/weixin_42255494/article/details/116497513</a><br><a href="https://blog.csdn.net/alwaysbefine/article/details/123858239">https://blog.csdn.net/alwaysbefine/article/details/123858239</a><br><a href="https://www.lxlinux.net/311.html">https://www.lxlinux.net/311.html</a></p>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>性能优化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux系统性能监控</title>
    <link href="/linux/Monitor/"/>
    <url>/linux/Monitor/</url>
    
    <content type="html"><![CDATA[<p>系统性能，即操作系统完成计算任务的有效性、稳定性、安全性和响应速度，特别是在高并发的场景下，系统性能将会面临非常巨大的挑战，甚至造成业务崩溃乃至于经济损失的严重后果。Linux系统性能的影响因素有CPU、内存、磁盘IO、网络等，对这些指标进行监控，了解其变化趋势，就对整个系统的运行状态有了一个全局的掌控，从而可及时介入故障的处理，以保障业务的稳定性</p><h1 id="1-CPU"><a href="#1-CPU" class="headerlink" title="1.CPU"></a>1.CPU</h1><p>CPU，操作系统稳定运行的根本，其速度与性能很大程度上决定了系统整体的性能，通常CPU数量越多、主频越高，服务器性能也就相对越好，衡量指标包括平均负载、CPU使用率、上下文切换、CPU缓存命中率等</p><h2 id="1-1-平均负载"><a href="#1-1-平均负载" class="headerlink" title="1.1 平均负载"></a>1.1 平均负载</h2><p>load average，平均负载，表示系统在1分钟、5分钟、15分钟内CPU负载的整体趋势，即指单位时间内系统处于可运行状态和不可中断状态的平均进程数，也即平均活跃进程数，与CPU使用率无关。其大小一般不大于系统CPU个数，否则将说明系统负载过高，可将单核CPU通俗地理解为单向行驶的公路桥，正在通行的车辆数为4时负载数为0.5，车辆数为8时负载为1，车辆数为12时负载为1.5，此时车辆通行已经需要排队</p><pre><code class="hljs"># CPU个数[sword@engine ~]$ cat /proc/cpuinfo |grep cores|uniqcpu cores       : 2[sword@cloud-server ~]$ uptime 22:30:46 up 117 days, 49 min,  0 users,  load average: 0.56, 0.38, 0.38</code></pre><ul><li>注：平均负载一般大于CPU数量的70%的时候就应该排查高负载的原因，以免影响正常的业务运行</li></ul><h2 id="1-2-CPU使用率"><a href="#1-2-CPU使用率" class="headerlink" title="1.2 CPU使用率"></a>1.2 CPU使用率</h2><h1 id="2-内存"><a href="#2-内存" class="headerlink" title="2.内存"></a>2.内存</h1><p>内存，加载进程数据供CPU运算，内存分配过小则进程将被阻塞，应用将变得缓慢，甚至失去响应，内存分配过大也将会导致资源的浪费</p><h1 id="2-1-查询某用户系统负载"><a href="#2-1-查询某用户系统负载" class="headerlink" title="2.1 查询某用户系统负载"></a>2.1 查询某用户系统负载</h1><pre><code class="hljs">top -u username</code></pre><ul><li>交互式命令：P，按%CPU使用率排序； T，按MITE+排行； M，按%MEM排行</li></ul><h1 id="2-2-以进程号查询某进程内存占用量"><a href="#2-2-以进程号查询某进程内存占用量" class="headerlink" title="2.2 以进程号查询某进程内存占用量"></a>2.2 以进程号查询某进程内存占用量</h1><pre><code class="hljs">pmap -d pid</code></pre><h1 id="2-3-某用户所有进程按内存占用量由大到小排序"><a href="#2-3-某用户所有进程按内存占用量由大到小排序" class="headerlink" title="2.3 某用户所有进程按内存占用量由大到小排序"></a>2.3 某用户所有进程按内存占用量由大到小排序</h1><pre><code class="hljs">ps -e -o &#39;pid,comm,args,pcpu,rsz,vsz,stime,user,uid&#39; | grep username |  sort -nrk5ps aux|head -1;ps aux|grep -(www.111cn.net)v PID|sort -rn -k +4|head</code></pre><h1 id="3-磁盘IO"><a href="#3-磁盘IO" class="headerlink" title="3.磁盘IO"></a>3.磁盘IO</h1><p>磁盘I&#x2F;O，内存与磁盘之间的输入输出操作，即程序读取本地文件将磁盘的数据拷贝到内存以及修改或创建本地文件把修改后的数据拷贝到磁盘的操作。由于磁盘的性能与CPU、内存的性能相差很大，所以磁盘IO也是制约程序性能的关键因素，毕竟程序所有的操作最后阶段几乎都是落盘。磁盘性能的衡量指标如下：</p><ul><li><p>使用率，即磁盘处理I&#x2F;O的时间百分比，过高的使用率通常意味着磁盘I&#x2F;O存在性能瓶颈</p></li><li><p>饱和度，即磁盘处理I&#x2F;O的繁忙程度，饱和度过高表示磁盘存在严重的性能瓶颈，达到100%时，磁盘将无法接受新的I&#x2F;O请求</p></li><li><p>IOPS，Input&#x2F;Output Per Second，即磁盘每秒I&#x2F;O请求数，数据库、大量小文件等这类随机读写比较多的场景可作为重要的衡量依据</p></li><li><p>吞吐量，即磁盘每秒的I&#x2F;O请求大小，多媒体等顺序读写较多的场景可作为重要的衡量依据</p></li><li><p>响应时间，即I&#x2F;O请求从发出到收到响应的间隔时间</p></li></ul><h2 id="3-1-磁盘IO"><a href="#3-1-磁盘IO" class="headerlink" title="3.1 磁盘IO"></a>3.1 磁盘IO</h2><h2 id="3-2-进程IO"><a href="#3-2-进程IO" class="headerlink" title="3.2 进程IO"></a>3.2 进程IO</h2><h1 id="4-网络"><a href="#4-网络" class="headerlink" title="4.网络"></a>4.网络</h1><p>Linux运行的应用一般都基于网络，网络速度直接影响客户的感官，低速的、不稳定的网络将导致网络应用程序的访问阻塞，衡量指标如下：</p><ul><li><p>带宽，表示链路的最大传输速率，单位通常为b&#x2F;s，即比特&#x2F;秒</p></li><li><p>吞吐量，表示单位时间内成功传输的数据量，单位为b&#x2F;s或B&#x2F;s，受带宽限制，吞吐量&#x2F;带宽即是网络使用率</p></li><li><p>延时，表示从网络请求发出到收到远端响应所需时间，如TCP建立连接所需时间、RTT数据包往返所需时间等</p></li><li><p>PPS，即Packet Per Second，表示以网络包为单位的传输速率，通常用于评估网络转发能力，如硬件交换机可以达到线性转发即PPS可以达到或者接近理论最大值，而Linux服务器的转发则易受网络包大小的影响</p></li></ul><p>此外，网络的可用性（网络能否正常通信）、并发连接数（TCP连接数量）、丢包率（丢包百分比）、重传率（重新传输的网络包比例）等也是常用的性能指标</p><h2 id="4-3-网络IO"><a href="#4-3-网络IO" class="headerlink" title="4.3 网络IO"></a>4.3 网络IO</h2><p>网络IO，网卡与内存之间的输入输出，即程序接受网络数据时网卡将数据拷贝到内存以及程序发送网络数据时将数据从内存拷贝到网卡的操作</p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://mp.weixin.qq.com/s/5MhVQ3FJedVGAhIGycbs3g">https://mp.weixin.qq.com/s/5MhVQ3FJedVGAhIGycbs3g</a></li><li><a href="https://mp.weixin.qq.com/s/dTplQPAJweA5NEd8JmjGYA">https://mp.weixin.qq.com/s/dTplQPAJweA5NEd8JmjGYA</a></li><li><a href="https://mp.weixin.qq.com/s/j69dSs6D4wauwem1qfIW8Q">https://mp.weixin.qq.com/s/j69dSs6D4wauwem1qfIW8Q</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>监控告警</tag>
      
      <tag>性能优化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Jenkins配置任务构建钉钉通知</title>
    <link href="/linux/JenkinsDingtalkNotification/"/>
    <url>/linux/JenkinsDingtalkNotification/</url>
    
    <content type="html"><![CDATA[<h1 id="1-钉钉群创建机器人"><a href="#1-钉钉群创建机器人" class="headerlink" title="1.钉钉群创建机器人"></a>1.钉钉群创建机器人</h1><p>【群设置】 —&gt; 【机器人】 —&gt; 【添加机器人】 —&gt; 【自定义】</p><hr><p><img src="/img/wiki/jenkins/jenkins-006-001.jpg" alt="jenkins-006-001"></p><p><img src="/img/wiki/jenkins/jenkins-006-002.jpg" alt="jenkins-006-002"></p><h1 id="2-jenkins安装Ding-Talk插件"><a href="#2-jenkins安装Ding-Talk插件" class="headerlink" title="2.jenkins安装Ding Talk插件"></a>2.jenkins安装Ding Talk插件</h1><h1 id="3-jenkins配置钉钉机器人"><a href="#3-jenkins配置钉钉机器人" class="headerlink" title="3.jenkins配置钉钉机器人"></a>3.jenkins配置钉钉机器人</h1><ul><li>【系统管理】 —&gt; 【钉钉】</li></ul><p><img src="/img/wiki/jenkins/jenkins-006-003.jpg" alt="jenkins-006-003"></p><p><img src="/img/wiki/jenkins/jenkins-006-004.jpg" alt="jenkins-006-004"></p><h1 id="4-项目配置钉钉机器人通知"><a href="#4-项目配置钉钉机器人通知" class="headerlink" title="4.项目配置钉钉机器人通知"></a>4.项目配置钉钉机器人通知</h1><h2 id="4-1-freestyle项目"><a href="#4-1-freestyle项目" class="headerlink" title="4.1 freestyle项目"></a>4.1 freestyle项目</h2><p><img src="/img/wiki/jenkins/jenkins-006-005.jpg" alt="jenkins-006-005"></p><h3 id="自定义消息格式"><a href="#自定义消息格式" class="headerlink" title="自定义消息格式"></a>自定义消息格式</h3><pre><code class="hljs">### $&#123;PROJECT_NAME&#125;项目构建构建$&#123;JOB_STATUS&#125;!---------- 项目: $&#123;PROJECT_NAME&#125;- 构建号: $&#123;BUILD_ID&#125;- 构建人: $&#123;EXECUTOR_NAME&#125;- 项目地址: $&#123;PROJECT_URL&#125;- 工作目录: $&#123;PROJECT_URL&#125;ws- 任务地址: $&#123;BUILD_URL&#125;- 构建日志: $&#123;BUILD_URL&#125;console- 持续时间: $&#123;JOB_DURATION&#125;</code></pre><h2 id="4-2-pipeline项目"><a href="#4-2-pipeline项目" class="headerlink" title="4.2 pipeline项目"></a>4.2 pipeline项目</h2><pre><code class="hljs">def project = &quot;hexo&quot;def app_name = &quot;poetry&quot;pipeline &#123;  agent any  stages &#123;      stage(&#39;TestAgent&#39;) &#123;      steps &#123;        sh &quot;&quot;&quot;        date        pwd        &quot;&quot;&quot;      &#125;    &#125;  &#125;    post &#123;      always &#123;        emailext (         subject: &#39;【Jenkins项目自动化构建通知】：$PROJECT_NAME - $BUILD_NUMBER - $BUILD_STATUS!&#39;,        body: &#39;$&#123;FILE,path=&quot;/home/jenkins/email.html&quot;&#125;&#39;,        to: &#39;523343553@qq.com&#39;        )      &#125;              failure &#123;        dingtalk (          robot: &#39;dingtalk-jenkins&#39;,          type:&#39;MARKDOWN&#39;,          atAll: true,          text: [&quot;### $&#123;currentBuild.projectName&#125;项目构建$&#123;currentBuild.currentResult&#125;!&quot;,                 &quot;---------&quot;,                 &quot;- 项目: $&#123;JOB_NAME&#125;&quot;,                 &quot;- 构建号: $&#123;BUILD_ID&#125;&quot;,                 &quot;- 构建人: $&#123;env.BUILD_USER&#125;&quot;,                 &quot;- 项目地址: $&#123;JOB_URL&#125;&quot;,                 &quot;- 工作目录: $&#123;BUILD_URL&#125;ws&quot;,                 &quot;- 任务地址: $&#123;BUILD_URL&#125;&quot;,                 &quot;- 构建日志: $&#123;BUILD_URL&#125;console&quot;,                 &quot;- 持续时间: $&#123;currentBuild.durationString&#125;&quot;                ]          )      &#125;    &#125;&#125;</code></pre><h1 id="5-构建任务，测试钉钉通知"><a href="#5-构建任务，测试钉钉通知" class="headerlink" title="5.构建任务，测试钉钉通知"></a>5.构建任务，测试钉钉通知</h1><p><img src="/img/wiki/jenkins/jenkins-006-006.jpg" alt="jenkins-006-006"></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://jenkinsci.github.io/dingtalk-plugin">https://jenkinsci.github.io/dingtalk-plugin</a></li><li><a href="https://blog.csdn.net/IT_ZRS/article/details/125674756">https://blog.csdn.net/IT_ZRS/article/details/125674756</a></li><li><a href="https://blog.csdn.net/heian_99/article/details/124816190">https://blog.csdn.net/heian_99/article/details/124816190</a></li><li><a href="https://blog.csdn.net/qq_42157883/article/details/124215072">https://blog.csdn.net/qq_42157883/article/details/124215072</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>CICD</tag>
      
      <tag>DevOps</tag>
      
      <tag>Jenkins</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux系统启动过程详解</title>
    <link href="/linux/Init/"/>
    <url>/linux/Init/</url>
    
    <content type="html"><![CDATA[<p>Linux系统的启动是指计算机加电到显示用户登陆提示的整个过程，最终将计算机带入工作状态，是一个复杂而有序的多阶段系统事件，对操作系统管理计算机资源的理解、系统性能的优化以及系统启动故障的排查非常有帮助</p><p><img src="/img/wiki/linux/boot.png" alt="boot"></p><h1 id="1-硬件引导"><a href="#1-硬件引导" class="headerlink" title="1.硬件引导"></a>1.硬件引导</h1><p>计算机打开电源后，首先是BIOS开机自检，按照BIOS中设置的启动设备启动系统，通常为硬盘，然后由启动设备的主引导扇区所存储的引导加载程序（BootLoader）读取引导程序GRUB和硬盘分区表</p><h1 id="2-加载内核"><a href="#2-加载内核" class="headerlink" title="2.加载内核"></a>2.加载内核</h1><p>GRUB引导程序加载内核和镜像文件到内存，进行操作系统内核的运行。之后，由Linux操作系统全面接管硬件</p><h1 id="3-运行init"><a href="#3-运行init" class="headerlink" title="3.运行init"></a>3.运行init</h1><p>内核加载完毕后，Linux操作系统将会启动第一个守护进程init，进程PID为1，所有的进程都是其子进程，之后再读取进程运行等级文件&#x2F;etc&#x2F;inittab，以确定所需要启动的程序与服务。Linux系统运行级别，runlevel，代表了Linux的运行状态，分为7个级别，具体如下：</p><ul><li>级别0，系统停机状态，默认运行级别不能设为0，否则不能正常启动</li><li>级别1，单用户工作状态，root权限，用于系统维护，禁止远程登录</li><li>级别2，多用户状态，没有NFS</li><li>级别3，完全的多用户状态，有NFS，登录后进入控制台命令行模式，最为常用</li><li>级别4，系统未使用，保留级别</li><li>级别5，X11控制台，登录后进入图形GUI模式</li><li>级别6，系统正常关闭并重启，默认运行级别不能设为6，否则不能正常启动</li></ul><h1 id="4-系统初始化"><a href="#4-系统初始化" class="headerlink" title="4.系统初始化"></a>4.系统初始化</h1><p>init进程通过调用&#x2F;etc&#x2F;rc.d&#x2F;rc.sysinit这个bash shell脚本来完成系统的初始化工作，如激活交换分区、加载硬件模块、设置系统时间、挂载文件系统、启动磁盘检查等。此外，还会加载并启动一些必要的守护进程和服务，以便在系统启动的后续阶段能够正常运行</p><h1 id="5-建立终端"><a href="#5-建立终端" class="headerlink" title="5.建立终端"></a>5.建立终端</h1><p>rc.sysinit执行完毕后，返回init，此时基本系统环境已完成设置，各种守护进程也已启动。init接着将会打开6个终端，分别为tty1、tty2、tty3、tty4、tty5和tty6，用户可以通过这些终端进行命令行登录或远程登录</p><h1 id="6-用户登录"><a href="#6-用户登录" class="headerlink" title="6.用户登录"></a>6.用户登录</h1><p>用户登录方式分为三种，即命令行登陆、SSH登陆和图形界面登陆，Linux系统通过账号验证程序login对登录用户进行验证</p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.runoob.com/linux/linux-system-boot.html">https://www.runoob.com/linux/linux-system-boot.html</a></li><li><a href="https://servu.nwafu.edu.cn/servers_mgr/booting.html">https://servu.nwafu.edu.cn/servers_mgr/booting.html</a></li><li><a href="https://blog.csdn.net/zhangkunls/article/details/132769955">https://blog.csdn.net/zhangkunls/article/details/132769955</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>操作系统</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Jenkins配置任务构建邮件通知</title>
    <link href="/linux/JenkinsEmailNotification/"/>
    <url>/linux/JenkinsEmailNotification/</url>
    
    <content type="html"><![CDATA[<p>DevOps聚合了开发、测试、运维这些前后端部门，其中任一环节出现问题，都将导致整个流程的失败。所以，将Jenkins任务的构建结果通知到相关责任人，收到信息后自行判断是否己相关，以便快速介入处理。这样，就将这些部门所有的相关责任人串联起来，也使得整个项目的各个环节的联结更加紧密</p><h1 id="1-发送邮箱开启SMTP服务，获取认证码"><a href="#1-发送邮箱开启SMTP服务，获取认证码" class="headerlink" title="1.发送邮箱开启SMTP服务，获取认证码"></a>1.发送邮箱开启SMTP服务，获取认证码</h1><h1 id="2-jenkins安装邮箱插件Email-Extension"><a href="#2-jenkins安装邮箱插件Email-Extension" class="headerlink" title="2.jenkins安装邮箱插件Email Extension"></a>2.jenkins安装邮箱插件Email Extension</h1><h1 id="3-jenkins配置邮件通知及系统管理员邮箱"><a href="#3-jenkins配置邮件通知及系统管理员邮箱" class="headerlink" title="3.jenkins配置邮件通知及系统管理员邮箱"></a>3.jenkins配置邮件通知及系统管理员邮箱</h1><p><img src="/img/wiki/jenkins/jenkins-005-001.jpg" alt="jenkins-005-001"></p><p><img src="/img/wiki/jenkins/jenkins-005-002.jpg" alt="jenkins-005-002"></p><p><img src="/img/wiki/jenkins/jenkins-005-003.jpg" alt="jenkins-005-003"></p><h1 id="4-jenkins配置邮件扩展"><a href="#4-jenkins配置邮件扩展" class="headerlink" title="4.jenkins配置邮件扩展"></a>4.jenkins配置邮件扩展</h1><p><img src="/img/wiki/jenkins/jenkins-005-004.jpg" alt="jenkins-005-004"></p><h1 id="5-jenkins项目配置邮件通知"><a href="#5-jenkins项目配置邮件通知" class="headerlink" title="5.jenkins项目配置邮件通知"></a>5.jenkins项目配置邮件通知</h1><h2 id="5-1-freestyle项目"><a href="#5-1-freestyle项目" class="headerlink" title="5.1 freestyle项目"></a>5.1 freestyle项目</h2><p><img src="/img/wiki/jenkins/jenkins-005-005.jpg" alt="jenkins-005-005"></p><p><img src="/img/wiki/jenkins/jenkins-005-006.jpg" alt="jenkins-005-006"></p><p><img src="/img/wiki/jenkins/jenkins-005-007.jpg" alt="jenkins-005-007"></p><h3 id="5-1-1-邮件主题"><a href="#5-1-1-邮件主题" class="headerlink" title="5.1.1 邮件主题"></a>5.1.1 邮件主题</h3><pre><code class="hljs">【Jenkins项目自动化构建通知】：$PROJECT_NAME - Build - $BUILD_NUMBER - $BUILD_STATUS!</code></pre><h3 id="5-1-2-邮件正文模版"><a href="#5-1-2-邮件正文模版" class="headerlink" title="5.1.2 邮件正文模版"></a>5.1.2 邮件正文模版</h3><pre><code class="hljs">&lt;!DOCTYPE html&gt;    &lt;html&gt;    &lt;head&gt;    &lt;meta charset=&quot;UTF-8&quot;&gt;    &lt;title&gt;$&#123;ENV, var=&quot;JOB_NAME&quot;&#125;-$&#123;BUILD_NUMBER&#125;&lt;/title&gt;    &lt;/head&gt;    &lt;body leftmargin=&quot;8&quot; marginwidth=&quot;0&quot; topmargin=&quot;8&quot; marginheight=&quot;4&quot;    offset=&quot;0&quot;&gt;     &lt;table width=&quot;95%&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot;  style=&quot;font-size: 11pt; font-family: Tahoma, Arial, Helvetica, sans-serif&quot;&gt;        &lt;tr&gt;            本邮件由Jenkins系统自动发出，无需回复！&lt;br/&gt;                    大家好，以下为$&#123;PROJECT_NAME &#125;项目构建信息，请相关负责人关注：&lt;/br&gt;         &lt;td&gt;&lt;font color=&quot;#CC0000&quot;&gt;构建结果 - $&#123;BUILD_STATUS&#125;&lt;/font&gt;&lt;/td&gt;       &lt;/tr&gt;        &lt;tr&gt;            &lt;td&gt;&lt;br /&gt;            &lt;b&gt;&lt;font color=&quot;#0B610B&quot;&gt;构建信息&lt;/font&gt;&lt;/b&gt;            &lt;hr size=&quot;2&quot; width=&quot;100%&quot; align=&quot;center&quot; /&gt;&lt;/td&gt;        &lt;/tr&gt;        &lt;tr&gt;            &lt;td&gt;                &lt;ul&gt;                    &lt;li&gt;项目名称: $&#123;PROJECT_NAME&#125;&lt;/li&gt;                    &lt;li&gt;项目描述: $&#123;JOB_DESCRIPTION&#125;&lt;/li&gt;                    &lt;li&gt;构建编号: $&#123;BUILD_NUMBER&#125;&lt;/li&gt;                    &lt;li&gt;触发原因: $&#123;CAUSE&#125;&lt;/li&gt;                    &lt;li&gt;构建状态: $&#123;BUILD_STATUS&#125;&lt;/li&gt;                &lt;li&gt;项目地址: &lt;a href=&quot;$&#123;PROJECT_URL&#125;&quot;&gt;$&#123;PROJECT_URL&#125;&lt;/a&gt;&lt;/li&gt;                &lt;li&gt;工作目录: &lt;a href=&quot;$&#123;PROJECT_URL&#125;ws&quot;&gt;$&#123;PROJECT_URL&#125;ws&lt;/a&gt;&lt;/li&gt;                &lt;li&gt;任务地址: &lt;a href=&quot;$&#123;BUILD_URL&#125;&quot;&gt;$&#123;BUILD_URL&#125;&lt;/a&gt;&lt;/li&gt;                &lt;li&gt;构建日志: &lt;a href=&quot;$&#123;BUILD_URL&#125;console&quot;&gt;$&#123;BUILD_URL&#125;console&lt;/a&gt;&lt;/li&gt;            &lt;/ul&gt;            &lt;/td&gt;        &lt;/tr&gt;    &lt;tr&gt;         &lt;td&gt;&lt;b&gt;&lt;font color=&quot;#0B610B&quot;&gt;变更集&lt;/font&gt;&lt;/b&gt;        &lt;hr size=&quot;2&quot; width=&quot;100%&quot; align=&quot;center&quot; /&gt;&lt;/td&gt;        &lt;/tr&gt;        &lt;td&gt;$&#123;JELLY_SCRIPT,template=&quot;html&quot;&#125;&lt;br/&gt;            &lt;hr size=&quot;2&quot; width=&quot;100%&quot; align=&quot;center&quot; /&gt;&lt;/td&gt;            &lt;/tr&gt;    &lt;/table&gt;    &lt;/body&gt;    &lt;/html&gt;</code></pre><h2 id="5-2-pipeline项目"><a href="#5-2-pipeline项目" class="headerlink" title="5.2 pipeline项目"></a>5.2 pipeline项目</h2><h3 id="5-2-1-创建邮件通知模版文件"><a href="#5-2-1-创建邮件通知模版文件" class="headerlink" title="5.2.1 创建邮件通知模版文件"></a>5.2.1 创建邮件通知模版文件</h3><pre><code class="hljs">vi email.html&lt;!DOCTYPE html&gt;    &lt;html&gt;    &lt;head&gt;    &lt;meta charset=&quot;UTF-8&quot;&gt;    &lt;title&gt;$&#123;ENV, var=&quot;JOB_NAME&quot;&#125;-$&#123;BUILD_NUMBER&#125;&lt;/title&gt;    &lt;/head&gt;    &lt;body leftmargin=&quot;8&quot; marginwidth=&quot;0&quot; topmargin=&quot;8&quot; marginheight=&quot;4&quot;    offset=&quot;0&quot;&gt;     &lt;table width=&quot;95%&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot;  style=&quot;font-size: 11pt; font-family: Tahoma, Arial, Helvetica, sans-serif&quot;&gt;        &lt;tr&gt;            本邮件由Jenkins系统自动发出，无需回复！&lt;br/&gt;                    大家好，以下为$&#123;PROJECT_NAME &#125;项目构建信息，请相关负责人关注：&lt;/br&gt;         &lt;td&gt;&lt;font color=&quot;#CC0000&quot;&gt;构建结果 - $&#123;BUILD_STATUS&#125;&lt;/font&gt;&lt;/td&gt;       &lt;/tr&gt;        &lt;tr&gt;            &lt;td&gt;&lt;br /&gt;            &lt;b&gt;&lt;font color=&quot;#0B610B&quot;&gt;构建信息&lt;/font&gt;&lt;/b&gt;            &lt;hr size=&quot;2&quot; width=&quot;100%&quot; align=&quot;center&quot; /&gt;&lt;/td&gt;        &lt;/tr&gt;        &lt;tr&gt;            &lt;td&gt;                &lt;ul&gt;                    &lt;li&gt;项目名称: $&#123;PROJECT_NAME&#125;&lt;/li&gt;                    &lt;li&gt;项目描述: $&#123;JOB_DESCRIPTION&#125;&lt;/li&gt;                    &lt;li&gt;构建编号: $&#123;BUILD_NUMBER&#125;&lt;/li&gt;                    &lt;li&gt;触发原因: $&#123;CAUSE&#125;&lt;/li&gt;                    &lt;li&gt;构建状态: $&#123;BUILD_STATUS&#125;&lt;/li&gt;                &lt;li&gt;项目地址: &lt;a href=&quot;$&#123;PROJECT_URL&#125;&quot;&gt;$&#123;PROJECT_URL&#125;&lt;/a&gt;&lt;/li&gt;                &lt;li&gt;工作目录: &lt;a href=&quot;$&#123;PROJECT_URL&#125;ws&quot;&gt;$&#123;PROJECT_URL&#125;ws&lt;/a&gt;&lt;/li&gt;                &lt;li&gt;任务地址: &lt;a href=&quot;$&#123;BUILD_URL&#125;&quot;&gt;$&#123;BUILD_URL&#125;&lt;/a&gt;&lt;/li&gt;                &lt;li&gt;构建日志: &lt;a href=&quot;$&#123;BUILD_URL&#125;console&quot;&gt;$&#123;BUILD_URL&#125;console&lt;/a&gt;&lt;/li&gt;            &lt;/ul&gt;        &lt;/td&gt;     &lt;/tr&gt;    &lt;tr&gt;        &lt;td&gt;&lt;b&gt;&lt;font color=&quot;#0B610B&quot;&gt;变更集&lt;/font&gt;&lt;/b&gt;        &lt;hr size=&quot;2&quot; width=&quot;100%&quot; align=&quot;center&quot; /&gt;&lt;/td&gt;        &lt;/tr&gt;        &lt;td&gt;$&#123;JELLY_SCRIPT,template=&quot;html&quot;&#125;&lt;br/&gt;            &lt;hr size=&quot;2&quot; width=&quot;100%&quot; align=&quot;center&quot; /&gt;&lt;/td&gt;            &lt;/tr&gt;    &lt;/table&gt;    &lt;/body&gt;    &lt;/html&gt;  </code></pre><h3 id="5-2-2-创建pipeline文件"><a href="#5-2-2-创建pipeline文件" class="headerlink" title="5.2.2 创建pipeline文件"></a>5.2.2 创建pipeline文件</h3><pre><code class="hljs">def project = &quot;hexo&quot;def app_name = &quot;poetry&quot;def git_auth_id = &quot;sword-cloud&quot;def registry = &quot;registry.sword.org&quot;pipeline &#123;  agent &#123;    kubernetes &#123;      label &quot;jenkins-slave&quot;      customWorkspace &#39;/home/jenkins/workspace/hexo&#39;        yaml &#39;&#39;&#39;apiVersion: v1kind: Podmetadata:  name: jenkins-slave  namespace: devopsspec:  containers:    - name: jnlp      image: registry.sword.org/jenkins-slave:4.13.3-1-jdk11      imagePullPolicy: IfNotPresent      env:        - name: &quot;workDir&quot;          value: &quot;/home/jenkins&quot;        - name: &quot;TZ&quot;          value: &quot;Asia/Shanghai&quot;      resources:        limits:          cpu: 500m          memory: 300Mi        requests:          cpu: 300m          memory: 200Mi      volumeMounts:        - name: docker-cmd          mountPath: /usr/bin/docker        - name: docker-sock          mountPath: /var/run/docker.sock        - name: jenkins-slave-data          mountPath: /home/jenkins        - name: localtime          mountPath: /etc/localtime  volumes:    - name: docker-cmd      hostPath:        path: /usr/bin/docker    - name: docker-sock      hostPath:        path: /var/run/docker.sock    - name: jenkins-slave-data      persistentVolumeClaim:        claimName: jenkins-slave-data    - name: localtime      hostPath:        path: /etc/localtime  securityContext:    runAsGroup: 0    runAsUser: 1000  serviceAccountName: &quot;jenkins&quot;&#39;&#39;&#39;        &#125;    &#125;    stages &#123;      stage(&#39;Test&#39;) &#123;        steps &#123;          sh &quot;&quot;&quot;          pwd          hostname          date          &quot;&quot;&quot;        &#125;      &#125;    &#125;    post &#123;          always &#123;                emailext (         subject: &#39;【Jenkins项目自动化构建通知】：$PROJECT_NAME - $BUILD_NUMBER - $BUILD_STATUS!&#39;,        body: &#39;$&#123;FILE,path=&quot;/home/jenkins/email.html&quot;&#125;&#39;,        to: &#39;523343553@qq.com&#39;        )      &#125;            &#125; &#125;</code></pre><h1 id="6-构建任务，测试邮件通知"><a href="#6-构建任务，测试邮件通知" class="headerlink" title="6.构建任务，测试邮件通知"></a>6.构建任务，测试邮件通知</h1><p><img src="/img/wiki/jenkins/jenkins-005-008.jpg" alt="jenkins-005-008"></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.cnblogs.com/qinziteng/p/16974541.html">https://www.cnblogs.com/qinziteng/p/16974541.html</a></li><li><a href="https://www.cnblogs.com/rb2010/p/16195448.html">https://www.cnblogs.com/rb2010/p/16195448.html</a></li><li><a href="https://blog.csdn.net/fullbug/article/details/53024562">https://blog.csdn.net/fullbug/article/details/53024562</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>CICD</tag>
      
      <tag>DevOps</tag>
      
      <tag>Jenkins</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Jenkins基于Kubernetes搭建Hexo博客CICD平台</title>
    <link href="/linux/JenkinsRunningHexo/"/>
    <url>/linux/JenkinsRunningHexo/</url>
    
    <content type="html"><![CDATA[<h1 id="发布流程"><a href="#发布流程" class="headerlink" title="发布流程"></a>发布流程</h1><ul><li>1.本地VSCode编写hexo博客</li><li>2.运行hexo -g -d发布博客，将静态博客文件提交到Kubernetes集群的Git私有仓库</li><li>3.Gitweb钩子触发Kubernetes集群部署的Jenkins的构建触发器，拉取托管在Git私有仓库的Jenkinsfile作为构建依据</li><li>4.Jenkins开始任务构建，连接kubernetes集群拉起代理Pod，执行任务构建</li><li>5.Jenkins代理Pod拉取Git私有仓库代码，通过Dockerfile文件将静态博客文件构建为镜像，再将镜像push到Docker私有镜像仓库</li><li>6.执行kubelet set命令，将博客的deployment镜像更新为新构建的镜像，完成整个发布流程</li></ul><hr><h1 id="1-部署Jenkins"><a href="#1-部署Jenkins" class="headerlink" title="1.部署Jenkins"></a>1.部署Jenkins</h1><h2 id="1-1-创建存储资源文件"><a href="#1-1-创建存储资源文件" class="headerlink" title="1.1 创建存储资源文件"></a>1.1 创建存储资源文件</h2><pre><code class="hljs">vi jenkins-data.yamlapiVersion: v1kind: PersistentVolumemetadata:  # 设置PV名称  name: jenkins-data  # 设置PV标签，用于PVC的定向绑定  labels:    app: jenkins-dataspec:  # 设置存储类别  storageClassName: nfs  # 设置访问模式  accessModes:    - ReadWriteMany  # 设置PV的存储空间  capacity:    storage: 10Gi  # 设置PV的回收策略  persistentVolumeReclaimPolicy: Retain  nfs:    path: /home/project/kubernetes/devops/jenkins    server: 192.168.100.200---apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: jenkins-data  namespace: devopsspec:  # 设置PVC存储类别，用于指定存储类型  storageClassName: nfs  # 设置访问模式，匹配相同模式的PV  accessModes:  - ReadWriteMany  # 设置PVC所申请存储空间的大小  resources:    requests:      storage: 10Gi  selector:     matchLabels:      app: jenkins-data</code></pre><h2 id="1-2-创建应用资源文件"><a href="#1-2-创建应用资源文件" class="headerlink" title="1.2 创建应用资源文件"></a>1.2 创建应用资源文件</h2><pre><code class="hljs">vi jenkins-deployment.yamlapiVersion: v1kind: ServiceAccountmetadata:  name: jenkins  namespace: devops  labels:    name: jenkins---kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata:  name: jenkins  namespace: devopsrules:- apiGroups: [&quot;&quot;]  resources: [&quot;pods&quot;,&quot;events&quot;]  verbs: [&quot;create&quot;,&quot;delete&quot;,&quot;get&quot;,&quot;list&quot;,&quot;patch&quot;,&quot;update&quot;,&quot;watch&quot;]- apiGroups: [&quot;&quot;]  resources: [&quot;pods/exec&quot;]  verbs: [&quot;create&quot;,&quot;delete&quot;,&quot;get&quot;,&quot;list&quot;,&quot;patch&quot;,&quot;update&quot;,&quot;watch&quot;]- apiGroups: [&quot;&quot;]  resources: [&quot;pods/log&quot;]  verbs: [&quot;get&quot;,&quot;list&quot;,&quot;watch&quot;]- apiGroups: [&quot;&quot;]  resources: [&quot;secrets&quot;,&quot;events&quot;]  verbs: [&quot;get&quot;]---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata:  name: jenkins  namespace: devopssubjects:- kind: ServiceAccount  name: jenkinsroleRef:  apiGroup: rbac.authorization.k8s.io  kind: Role  name: jenkins---kind: DeploymentapiVersion: apps/v1metadata:  name: jenkins  namespace: devopsspec:  replicas: 1  selector:    matchLabels:      app: jenkins  template:    metadata:      labels:        app: jenkins    spec:      serviceAccountName: jenkins      containers:      - name: jenkins        image: registry.cn-hangzhou.aliyuncs.com/swords/jenkins:v2.400-jdk11        imagePullPolicy: IfNotPresent        securityContext:                               runAsUser:          privileged: true        env:        - name: &quot;JAVA_OPTS&quot;          value: -Dhudson.security.csrf.GlobalCrumbIssuerConfiguration.DISABLE_CSRF_PROTECTION=true        ports:        - containerPort: 8080          name: web          protocol: TCP        - containerPort: 50000          name: agent          protocol: TCP        resources:          limits:            cpu: 2000m            memory: 1Gi          requests:            cpu: 50m            memory: 512Mi        livenessProbe:          httpGet:            path: /login            port: 8080          initialDelaySeconds: 60          timeoutSeconds: 5          failureThreshold: 3        readinessProbe:          httpGet:            path: /login            port: 8080          initialDelaySeconds: 60          timeoutSeconds: 5          failureThreshold: 3        volumeMounts:        - name: jenkins-data          mountPath: /var/jenkins_home        - name: localtime          mountPath: /etc/localtime      volumes:      - name: jenkins-data        persistentVolumeClaim:          claimName: jenkins-data      - name: localtime        hostPath:          path: /etc/localtime      imagePullSecrets:        - name: regcred---apiVersion: v1kind: Servicemetadata:  name: jenkins  namespace: devops  labels:    app: jenkinsspec:  type: NodePort  ports:  - name: web    port: 8080    targetPort: 8080    nodePort: 38080  - name: agent    port: 50000    targetPort: 50000    nodePort: 50000  selector:    app: jenkins</code></pre><h2 id="1-3-部署Jenkins"><a href="#1-3-部署Jenkins" class="headerlink" title="1.3 部署Jenkins"></a>1.3 部署Jenkins</h2><pre><code class="hljs">kubectl apply -f jenkins-data.yamlkubectl apply -f jenkins-deployment.yaml</code></pre><h1 id="2-部署Git服务器"><a href="#2-部署Git服务器" class="headerlink" title="2.部署Git服务器"></a>2.部署Git服务器</h1><h2 id="2-1-创建存储资源文件"><a href="#2-1-创建存储资源文件" class="headerlink" title="2.1 创建存储资源文件"></a>2.1 创建存储资源文件</h2><pre><code class="hljs">vi gitea-data.yamlapiVersion: v1kind: PersistentVolumemetadata:  # 设置PV名称  name: gitea-data  # 设置PV标签，用于PVC的定向绑定  labels:    app: gitea-dataspec:  # 设置存储类别  storageClassName: nfs  # 设置访问模式  accessModes:    - ReadWriteMany  # 设置PV的存储空间  capacity:    storage: 10Gi  # 设置PV的回收策略  persistentVolumeReclaimPolicy: Retain  nfs:    path: /home/project/kubernetes/devops/gitea    server: 192.168.100.200---apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: gitea-data  namespace: devopsspec:  # 设置PVC存储类别，用于指定存储类型  storageClassName: nfs  # 设置访问模式，匹配相同模式的PV  accessModes:  - ReadWriteMany  # 设置PVC所申请存储空间的大小  resources:    requests:      storage: 10Gi  selector:     matchLabels:      app: gitea-data</code></pre><h2 id="2-2-创建应用资源文件"><a href="#2-2-创建应用资源文件" class="headerlink" title="2.2 创建应用资源文件"></a>2.2 创建应用资源文件</h2><pre><code class="hljs">vi gitea-deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata:  name: gitea-deployment  namespace: devops  labels:    app: giteaspec:  replicas: 1  selector:    matchLabels:      app: gitea  template:    metadata:      labels:        app: gitea    spec:      containers:      - name: gitea        image: gitea/gitea        imagePullPolicy: IfNotPresent        ports:        - containerPort: 3000          name: gitea-http        - containerPort: 22          name: gitea-ssh        resources:          limits:            cpu: 500m            memory: 200Mi          requests:            cpu: 200m            memory: 100Mi        volumeMounts:        - mountPath: /data          name: gitea-data        - name: localtime          mountPath: /etc/localtime      volumes:      - name: gitea-data        persistentVolumeClaim:          claimName: gitea-data      - name: localtime        hostPath:          path: /etc/localtime---kind: ServiceapiVersion: v1metadata:  name: gitea-service  namespace: devopsspec:  selector:    app: gitea  type: NodePort  ports:  - name: gitea-http    port: 3000    targetPort: gitea-http    nodePort: 30000  - name: gitea-ssh    port: 22    targetPort: gitea-ssh    nodePort: 30022</code></pre><h2 id="2-3-部署Gitea"><a href="#2-3-部署Gitea" class="headerlink" title="2.3 部署Gitea"></a>2.3 部署Gitea</h2><pre><code class="hljs">kubectl apply -f gitea-data.yamlkubectl apply -f gitea-deployment.yaml</code></pre><h1 id="3-配置Jenkins"><a href="#3-配置Jenkins" class="headerlink" title="3.配置Jenkins"></a>3.配置Jenkins</h1><pre><code class="hljs">apiVersion: apps/v1kind: Deploymentmetadata:  name: hexo  namespace: devopsspec:  selector:    matchLabels:      app: hexo  replicas: 1  template:    metadata:      labels:        app: hexo    spec:      containers:        - name: hexo          image: registry.cn-hangzhou.aliyuncs.com/swords/nginx:ubuntu18          imagePullPolicy: IfNotPresent          ports:            - containerPort: 80              name: hexo          resources:            limits:              cpu: 100m              memory: 100M            requests:              cpu: 100m              memory: 64M          volumeMounts:            - name: nginx-conf              mountPath: /etc/nginx/nginx.conf              subPath: nginx.conf      volumes:        - name: nginx-conf          configMap:            name: nginx.conf      imagePullSecrets:        - name: regcred---apiVersion: v1kind: Servicemetadata:  name: hexo-service  namespace: devopsspec:  type: NodePort  sessionAffinity: ClientIP  selector:    app: hexo  ports:    - port: 80      targetPort: 80</code></pre><h2 id="3-1-创建任务"><a href="#3-1-创建任务" class="headerlink" title="3.1 创建任务"></a>3.1 创建任务</h2><pre><code class="hljs">def project = &quot;hexo&quot;pipeline &#123;   agent &#123;     kubernetes &#123;       label &quot;jenkins-slave&quot;       cloud &quot;Kubernetes&quot;       customWorkspace &#39;/home/jenkins/workspace/hexo&#39;       yaml &#39;&#39;&#39;         apiVersion: v1         kind: Pod         metadata:           name: jenkins-slave           namespace: devops         spec:           containers:             - name: jnlp               image: registry.cn-hangzhou.aliyuncs.com/swords/jenkins-inbound-agent:latest-jdk17               imagePullPolicy: IfNotPresent               env:                 - name: &quot;workDir&quot;                   value: &quot;/home/jenkins&quot;                 - name: &quot;TZ&quot;                   value: &quot;Asia/Shanghai&quot;               resources:                 limits:                   cpu: 500m                   memory: 300Mi                 requests:                   cpu: 300m                   memory: 200Mi               volumeMounts:                 - name: docker-cmd                   mountPath: /usr/bin/docker                 - name: docker-sock                   mountPath: /var/run/docker.sock                 - name: jenkins-slave-data                   mountPath: /home/jenkins                 - name: localtime                   mountPath: /etc/localtime           volumes:             - name: docker-cmd               hostPath:                 path: /usr/bin/docker             - name: docker-sock               hostPath:                 path: /var/run/docker.sock             - name: jenkins-slave-data               persistentVolumeClaim:                 claimName: jenkins-slave-data             - name: localtime               hostPath:                 path: /etc/localtime           securityContext:             runAsGroup: 0             runAsUser: 1000           serviceAccountName: &quot;jenkins&quot;       &#39;&#39;&#39;     &#125;   &#125;  stages &#123;       stage(&#39;TestAgent&#39;) &#123;      steps &#123;        sh &quot;&quot;&quot;          date          pwd          whoami        &quot;&quot;&quot;      &#125;    &#125;    stage(&#39;CleanWorkspace&#39;) &#123;      steps &#123;        sh &quot;&quot;&quot;        rm -rf *        &quot;&quot;&quot;      &#125;    &#125;    stage(&#39;CheckoutGitea&#39;) &#123;      steps &#123;        checkout scmGit(          branches: [[name: &#39;*/master&#39;]],           extensions: [],          userRemoteConfigs: [            [credentialsId: &#39;gitea-cloud&#39;, url: &#39;http://192.168.100.200:3200/sword/hexo.git&#39;]          ]        )      &#125;    &#125;    stage(&#39;DockerImage&#39;) &#123;      steps &#123;        sh &quot;&quot;&quot;          docker build -t registry.cn-hangzhou.aliyuncs.com/geekers/hexo:v$&#123;BUILD_NUMBER&#125; .          docker push registry.cn-hangzhou.aliyuncs.com/geekers/hexo:v$&#123;BUILD_NUMBER&#125;          docker rmi registry.cn-hangzhou.aliyuncs.com/geekers/hexo:v$&#123;BUILD_NUMBER&#125;        &quot;&quot;&quot;      &#125;    &#125;    stage(&#39;Deploy&#39;) &#123;      steps &#123;        sh &quot;&quot;&quot;          /home/jenkins/kubectl -n devops set image deployments/hexo *=&quot;registry.cn-hangzhou.aliyuncs.com/geekers/$&#123;project&#125;:v$&#123;BUILD_NUMBER&#125;&quot;        &quot;&quot;&quot;      &#125;    &#125;  &#125;  post &#123;     always &#123;      emailext (         subject: &#39;【Jenkins项目自动化构建通知】：$PROJECT_NAME - $BUILD_NUMBER - $BUILD_STATUS!&#39;,        body: &#39;$&#123;FILE,path=&quot;/home/jenkins/email.html&quot;&#125;&#39;,        to: &#39;523343553@qq.com&#39;        )    &#125;            failure &#123;      dingtalk (        robot: &#39;dingtalk-jenkins&#39;,        type:&#39;MARKDOWN&#39;,        atAll: true,        text: [&quot;### $&#123;currentBuild.projectName&#125;项目构建$&#123;currentBuild.currentResult&#125;!&quot;,               &quot;---------&quot;,               &quot;- 项目: $&#123;JOB_NAME&#125;&quot;,               &quot;- 构建号: $&#123;BUILD_ID&#125;&quot;,               &quot;- 构建人: $&#123;env.BUILD_USER&#125;&quot;,               &quot;- 项目地址: $&#123;JOB_URL&#125;&quot;,               &quot;- 工作目录: $&#123;BUILD_URL&#125;ws&quot;,               &quot;-任务地址: $&#123;BUILD_URL&#125;&quot;,               &quot;- 构建日志: $&#123;BUILD_URL&#125;console&quot;,               &quot;- 持续时间: $&#123;currentBuild.durationString&#125;&quot;              ]      )    &#125;  &#125;&#125;</code></pre><h2 id="3-2-配置Cloud"><a href="#3-2-配置Cloud" class="headerlink" title="3.2 配置Cloud"></a>3.2 配置Cloud</h2><h2 id="3-2-1-配置镜像仓库"><a href="#3-2-1-配置镜像仓库" class="headerlink" title="3.2.1 配置镜像仓库"></a>3.2.1 配置镜像仓库</h2><h2 id="3-3-配置CSF安全认证"><a href="#3-3-配置CSF安全认证" class="headerlink" title="3.3 配置CSF安全认证"></a>3.3 配置CSF安全认证</h2><h1 id="4-配置Git仓库"><a href="#4-配置Git仓库" class="headerlink" title="4.配置Git仓库"></a>4.配置Git仓库</h1><h2 id="4-1-创建Hexo仓库"><a href="#4-1-创建Hexo仓库" class="headerlink" title="4.1 创建Hexo仓库"></a>4.1 创建Hexo仓库</h2><h2 id="4-2-创建Web钩子"><a href="#4-2-创建Web钩子" class="headerlink" title="4.2 创建Web钩子"></a>4.2 创建Web钩子</h2><h2 id="4-3-测试Git仓库"><a href="#4-3-测试Git仓库" class="headerlink" title="4.3 测试Git仓库"></a>4.3 测试Git仓库</h2><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.lanweihong.com/posts/45278">https://blog.lanweihong.com/posts/45278</a></li><li><a href="http://www.liuhaihua.cn/archives/516810.html">http://www.liuhaihua.cn/archives/516810.html</a></li><li><a href="https://blog.csdn.net/sanhewuyang/article/details/121189389">https://blog.csdn.net/sanhewuyang/article/details/121189389</a></li><li><a href="https://blog.csdn.net/weixin_43458965/article/details/129121351">https://blog.csdn.net/weixin_43458965/article/details/129121351</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Git</tag>
      
      <tag>CICD</tag>
      
      <tag>DevOps</tag>
      
      <tag>Hexo</tag>
      
      <tag>Jenkins</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes集群Helm工具详解</title>
    <link href="/linux/KubernetesHelm/"/>
    <url>/linux/KubernetesHelm/</url>
    
    <content type="html"><![CDATA[<p>Helm，意为舵轮，是Kubernetes集群的包管理器，用于yaml文件的统一管理及高效复用，实现了应用级别的版本管理，即是将部署应用的yaml文件像Linux的包管理器一样进行封装，从仓库里快速查找、下载和安装，快速的发布中间件、数据库、公共组件等应用，类似于RedHat的yum、Python的pip。helm大大简化了Kubernetes应用的部署方式，将yaml文件存入chart包一键执行即可，不再需要kubectl一个个地执行，是查找、共享、打包、升级、回滚和使用Kubernetes构建软件的最佳方式</p><hr><h1 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h1><h2 id="1-Chart"><a href="#1-Chart" class="headerlink" title="1.Chart"></a>1.Chart</h2><p>chart，应用描述，即一系列用于描述Kubernetes资源文件的集合，包含镜像、服务、依赖以及资源定义等，类似于apt的dpkg包或yum的rpm包</p><h2 id="2-Release"><a href="#2-Release" class="headerlink" title="2.Release"></a>2.Release</h2><p>release，基于Chart的部署实体，执行后将会在Kubernetes集群创建真实运行的资源对象，如Tomcat Chart</p><h2 id="3-Repository"><a href="#3-Repository" class="headerlink" title="3.Repository"></a>3.Repository</h2><p>Repository，用于发布和存储Chart的存储库</p><h2 id="4-helm"><a href="#4-helm" class="headerlink" title="4.helm"></a>4.helm</h2><p>helm，命令行客户端工具，用于Kubernetes集群应用chart的创建、打包、发布和管理</p><hr><h1 id="1-helm的安装与配置"><a href="#1-helm的安装与配置" class="headerlink" title="1.helm的安装与配置"></a>1.helm的安装与配置</h1><h2 id="1-1-下载helm"><a href="#1-1-下载helm" class="headerlink" title="1.1 下载helm"></a>1.1 下载helm</h2><pre><code class="hljs">wget -O helm.tar.gz https://get.helm.sh/helm-v3.6.3-linux-amd64.tar.gz</code></pre><h2 id="1-2-安装helm"><a href="#1-2-安装helm" class="headerlink" title="1.2 安装helm"></a>1.2 安装helm</h2><pre><code class="hljs">tar -xzvf helm.tar.gzcd linux-amd64sudo mv helm /usr/local/bin</code></pre><h2 id="1-3-配置helm仓库"><a href="#1-3-配置helm仓库" class="headerlink" title="1.3 配置helm仓库"></a>1.3 配置helm仓库</h2><h3 id="1-3-1-查看仓库"><a href="#1-3-1-查看仓库" class="headerlink" title="1.3.1 查看仓库"></a>1.3.1 查看仓库</h3><pre><code class="hljs">helm repo list</code></pre><h3 id="1-3-2-添加仓库"><a href="#1-3-2-添加仓库" class="headerlink" title="1.3.2 添加仓库"></a>1.3.2 添加仓库</h3><pre><code class="hljs">helm repo add stable http://mirror.azure.cn/kubernetes/charts</code></pre><h3 id="1-3-3-更新仓库"><a href="#1-3-3-更新仓库" class="headerlink" title="1.3.3 更新仓库"></a>1.3.3 更新仓库</h3><pre><code class="hljs">helm repo update</code></pre><h3 id="1-3-4-删除仓库"><a href="#1-3-4-删除仓库" class="headerlink" title="1.3.4 删除仓库"></a>1.3.4 删除仓库</h3><pre><code class="hljs">helm repo remove stable</code></pre><h2 id="1-4-安装chart"><a href="#1-4-安装chart" class="headerlink" title="1.4 安装chart"></a>1.4 安装chart</h2><h3 id="1-4-1-下载"><a href="#1-4-1-下载" class="headerlink" title="1.4.1 下载"></a>1.4.1 下载</h3><h1 id="2-搭建私有仓库"><a href="#2-搭建私有仓库" class="headerlink" title="2.搭建私有仓库"></a>2.搭建私有仓库</h1><h2 id="2-1-安装nginx"><a href="#2-1-安装nginx" class="headerlink" title="2.1 安装nginx"></a>2.1 安装nginx</h2><pre><code class="hljs">sudo mkdir -p /web/helm/chartssudo vi /etc/nginx/conf.d/helm.confserver &#123;  listen       80;  server_name  localhost;  location /charts &#123;    root  /web/helm;    autoindex on;    charset utf-8;    autoindex_exact_size off;    autoindex_localtime on;    access_log  /var/log/nginx/helm_access.log  main;    error_log  /var/log/nginx/helm_error.log;  &#125;&#125;</code></pre><h2 id="2-2-创建helm应用并打包"><a href="#2-2-创建helm应用并打包" class="headerlink" title="2.2 创建helm应用并打包"></a>2.2 创建helm应用并打包</h2><pre><code class="hljs">helm create nginxhelm package nginx</code></pre><h2 id="2-3-生成charts库index文件"><a href="#2-3-生成charts库index文件" class="headerlink" title="2.3 生成charts库index文件"></a>2.3 生成charts库index文件</h2><pre><code class="hljs">mkdir swordhubmv nginx-0.1.0.tgz swordhub/helm repo index swordhub/ --url http://engine.sword.org/charts</code></pre><h2 id="2-4-charts库文件上传"><a href="#2-4-charts库文件上传" class="headerlink" title="2.4 charts库文件上传"></a>2.4 charts库文件上传</h2><pre><code class="hljs">sudo cp swordhub/* /web/helm/charts</code></pre><h2 id="2-5-helm添加私有仓库swordhub"><a href="#2-5-helm添加私有仓库swordhub" class="headerlink" title="2.5 helm添加私有仓库swordhub"></a>2.5 helm添加私有仓库swordhub</h2><pre><code class="hljs">helm repo add swordhub http://engine.sword.org/charts</code></pre><h2 id="2-6-验证私有库中swordhub"><a href="#2-6-验证私有库中swordhub" class="headerlink" title="2.6 验证私有库中swordhub"></a>2.6 验证私有库中swordhub</h2><pre><code class="hljs">helm install test swordhub/nginx --namespace test</code></pre><h1 id="3-chart应用编排"><a href="#3-chart应用编排" class="headerlink" title="3.chart应用编排"></a>3.chart应用编排</h1><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.51cto.com/u_13760351/2898442">https://blog.51cto.com/u_13760351/2898442</a></li><li><a href="https://www.cnblogs.com/lvzhenjiang/p/14878279.html">https://www.cnblogs.com/lvzhenjiang/p/14878279.html</a></li><li><a href="https://blog.csdn.net/u011127242/article/details/118197361">https://blog.csdn.net/u011127242/article/details/118197361</a></li><li><a href="https://blog.csdn.net/qq_35745940/article/details/120693245">https://blog.csdn.net/qq_35745940/article/details/120693245</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
      <tag>Helm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Jenkins基于Kubernetes集群配置动态agent节点</title>
    <link href="/linux/JenkinsAgentOnKubernetes/"/>
    <url>/linux/JenkinsAgentOnKubernetes/</url>
    
    <content type="html"><![CDATA[<h1 id="1-配置代理节点持久化存储"><a href="#1-配置代理节点持久化存储" class="headerlink" title="1.配置代理节点持久化存储"></a>1.配置代理节点持久化存储</h1><h2 id="1-1-创建PV、PVC"><a href="#1-1-创建PV、PVC" class="headerlink" title="1.1 创建PV、PVC"></a>1.1 创建PV、PVC</h2><pre><code class="hljs">vi jenkins-slave-data.yamlapiVersion: v1kind: PersistentVolumemetadata:  # 设置PV名称  name: jenkins-slave-data  # 设置PV标签，用于PVC的定向绑定  labels:    app: jenkins-slave-dataspec:  # 设置存储类别  storageClassName: nfs  # 设置访问模式  accessModes:    - ReadWriteMany  # 设置PV的存储空间  capacity:    storage: 10Gi  # 设置PV的回收策略  persistentVolumeReclaimPolicy: Retain  nfs:    path: /home/project/kubernetes/devops/jenkins-slave    server: 192.168.100.200---apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: jenkins-slave-data  namespace: devopsspec:  # 设置PVC存储类别，用于指定存储类型  storageClassName: nfs  # 设置访问模式，匹配相同模式的PV  accessModes:  - ReadWriteMany  # 设置PVC所申请存储空间的大小  resources:    requests:      storage: 10Gi  selector:     matchLabels:      app: jenkins-slave-data</code></pre><h2 id="1-2-部署PV、PVC"><a href="#1-2-部署PV、PVC" class="headerlink" title="1.2 部署PV、PVC"></a>1.2 部署PV、PVC</h2><pre><code class="hljs">kubectl apply -f jenkins-slave-data.yaml</code></pre><h1 id="2-jenkins安装kubernetes插件"><a href="#2-jenkins安装kubernetes插件" class="headerlink" title="2.jenkins安装kubernetes插件"></a>2.jenkins安装kubernetes插件</h1><h1 id="3-登录jenkins，创建kubernetes云"><a href="#3-登录jenkins，创建kubernetes云" class="headerlink" title="3.登录jenkins，创建kubernetes云"></a>3.登录jenkins，创建kubernetes云</h1><p>【系统管理】 -&gt; 【节点管理】 -&gt; 【Clouds】 -&gt; 【新增一个云】</p><p><img src="/img/wiki/jenkins/jenkins-003-001.jpg" alt="jenkins-003-001"></p><p><img src="/img/wiki/jenkins/jenkins-003-002.jpg" alt="jenkins-003-002"></p><p><img src="/img/wiki/jenkins/jenkins-003-003.jpg" alt="jenkins-003-003"></p><p><img src="/img/wiki/jenkins/jenkins-003-004.jpg" alt="jenkins-003-004"></p><h1 id="4-部署动态agent节点"><a href="#4-部署动态agent节点" class="headerlink" title="4.部署动态agent节点"></a>4.部署动态agent节点</h1><h2 id="4-1-Pod模版部署方式"><a href="#4-1-Pod模版部署方式" class="headerlink" title="4.1 Pod模版部署方式"></a>4.1 Pod模版部署方式</h2><h3 id="4-1-1-配置POd模版"><a href="#4-1-1-配置POd模版" class="headerlink" title="4.1.1 配置POd模版"></a>4.1.1 配置POd模版</h3><p><img src="/img/wiki/jenkins/jenkins-003-005.jpg" alt="jenkins-003-005"></p><p><img src="/img/wiki/jenkins/jenkins-003-006.jpg" alt="jenkins-003-006"></p><h3 id="4-1-2-配置Pod容器"><a href="#4-1-2-配置Pod容器" class="headerlink" title="4.1.2 配置Pod容器"></a>4.1.2 配置Pod容器</h3><p><img src="/img/wiki/jenkins/jenkins-003-007.jpg" alt="jenkins-003-007"></p><p><img src="/img/wiki/jenkins/jenkins-003-008.jpg" alt="jenkins-003-008"></p><p><img src="/img/wiki/jenkins/jenkins-003-009.jpg" alt="jenkins-003-009"></p><p><img src="/img/wiki/jenkins/jenkins-003-010.jpg" alt="jenkins-003-010"></p><h2 id="4-2-pipeline部署方式"><a href="#4-2-pipeline部署方式" class="headerlink" title="4.2 pipeline部署方式"></a>4.2 pipeline部署方式</h2><h3 id="4-2-1-创建ServiceAccount"><a href="#4-2-1-创建ServiceAccount" class="headerlink" title="4.2.1 创建ServiceAccount"></a>4.2.1 创建ServiceAccount</h3><pre><code class="hljs">vi jenkins-serviceAccount.yamlapiVersion: v1kind: ServiceAccountmetadata:  name: jenkins  namespace: devops  labels:    name: jenkins---kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata:  name: jenkins  namespace: devopsrules:- apiGroups: [&quot;&quot;]  resources: [&quot;pods&quot;,&quot;events&quot;,&quot;deployments&quot;]  verbs: [&quot;create&quot;,&quot;delete&quot;,&quot;get&quot;,&quot;list&quot;,&quot;patch&quot;,&quot;update&quot;,&quot;watch&quot;]- apiGroups: [&quot;&quot;]  resources: [&quot;pods/exec&quot;]  verbs: [&quot;create&quot;,&quot;delete&quot;,&quot;get&quot;,&quot;list&quot;,&quot;patch&quot;,&quot;update&quot;,&quot;watch&quot;]- apiGroups: [&quot;&quot;]  resources: [&quot;pods/log&quot;]  verbs: [&quot;get&quot;,&quot;list&quot;,&quot;watch&quot;]- apiGroups: [&quot;&quot;]  resources: [&quot;secrets&quot;,&quot;events&quot;]  verbs: [&quot;get&quot;]---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata:  name: jenkins  namespace: devopssubjects:- kind: ServiceAccount  name: jenkinsroleRef:  apiGroup: rbac.authorization.k8s.io  kind: Role  name: jenkins</code></pre><h3 id="4-2-2-创建Jenkinsfile"><a href="#4-2-2-创建Jenkinsfile" class="headerlink" title="4.2.2 创建Jenkinsfile"></a>4.2.2 创建Jenkinsfile</h3><pre><code class="hljs">vi Jenkinsfilepipeline &#123;  agent &#123;    kubernetes &#123;      label &quot;jenkins-slave&quot;      customWorkspace &#39;/home/jenkins/workspace/hexo&#39;      yaml &#39;&#39;&#39;        apiVersion: v1        kind: Pod        metadata:          name: jenkins-slave          namespace: devops        spec:          containers:            - name: jnlp              image: registry.sword.org/jenkins-slave:4.13.3-1-jdk11              imagePullPolicy: IfNotPresent              env:                - name: &quot;workDir&quot;                  value: &quot;/home/jenkins&quot;                - name: &quot;TZ&quot;                  value: &quot;Asia/Shanghai&quot;              resources:                limits:                  cpu: 500m                  memory: 300Mi                requests:                  cpu: 300m                  memory: 200Mi              volumeMounts:                - name: docker-cmd                  mountPath: /usr/bin/docker                - name: docker-sock                  mountPath: /var/run/docker.sock                - name: jenkins-slave-data                  mountPath: /home/jenkins                - name: localtime                  mountPath: /etc/localtime          volumes:            - name: docker-cmd              hostPath:                path: /usr/bin/docker            - name: docker-sock              hostPath:                path: /var/run/docker.sock            - name: jenkins-slave-data              persistentVolumeClaim:                claimName: jenkins-slave-data            - name: localtime              hostPath:                path: /etc/localtime          securityContext:            runAsGroup: 0            runAsUser: 1000          serviceAccountName: &quot;jenkins&quot;      &#39;&#39;&#39;    &#125;  &#125;  stages &#123;    stage(&#39;TestAgent&#39;) &#123;      steps &#123;        sh &quot;&quot;&quot;        hostname        docker images        &quot;&quot;&quot;      &#125;    &#125;  &#125;&#125;</code></pre><h1 id="5-构建任务，测试动态agent"><a href="#5-构建任务，测试动态agent" class="headerlink" title="5.构建任务，测试动态agent"></a>5.构建任务，测试动态agent</h1><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.freesion.com/article/6009773987">https://www.freesion.com/article/6009773987</a></li><li><a href="https://www.cnblogs.com/panwenbin-logs/p/17299430.html">https://www.cnblogs.com/panwenbin-logs/p/17299430.html</a></li><li><a href="https://blog.csdn.net/weixin_44802620/article/details/125179310">https://blog.csdn.net/weixin_44802620/article/details/125179310</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>CICD</tag>
      
      <tag>DevOps</tag>
      
      <tag>Jenkins</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Jenkins配置agent节点</title>
    <link href="/linux/JenkinsAgent/"/>
    <url>/linux/JenkinsAgent/</url>
    
    <content type="html"><![CDATA[<p>Jenkins主节点负责任务调度，也具备运行构建任务的功能，但为保障构整个建流程的可用性一般不进行任务构建，而是配置以专用于任务执行的代理节点。代理节点分为静态节点和动态节点，静态节点是固定的虚机或容器，动态节点则是随着任务的构建自动创建以及任务的完成自动销毁的节点</p><hr><h1 id="1-配置代理节点服务器"><a href="#1-配置代理节点服务器" class="headerlink" title="1.配置代理节点服务器"></a>1.配置代理节点服务器</h1><h1 id="1-1-安装jdk，配置java环境"><a href="#1-1-安装jdk，配置java环境" class="headerlink" title="1.1 安装jdk，配置java环境"></a>1.1 安装jdk，配置java环境</h1><h1 id="1-2-创建jenkins用户并设置密码"><a href="#1-2-创建jenkins用户并设置密码" class="headerlink" title="1.2 创建jenkins用户并设置密码"></a>1.2 创建jenkins用户并设置密码</h1><pre><code class="hljs">sudo adduser</code></pre><h1 id="1-3-配置jenkins-sudo权限"><a href="#1-3-配置jenkins-sudo权限" class="headerlink" title="1.3 配置jenkins sudo权限"></a>1.3 配置jenkins sudo权限</h1><h1 id="2-登录jenkins，创建代理节点"><a href="#2-登录jenkins，创建代理节点" class="headerlink" title="2.登录jenkins，创建代理节点"></a>2.登录jenkins，创建代理节点</h1><h1 id="2-1-创建代理节点登录凭据"><a href="#2-1-创建代理节点登录凭据" class="headerlink" title="2.1 创建代理节点登录凭据"></a>2.1 创建代理节点登录凭据</h1><p>【系统管理】 -&gt; 【凭据】 -&gt; 【全局凭据】 -&gt; 【新增凭据】</p><p><img src="/img/wiki/jenkins/jenkins-002-001.jpg" alt="jenkins-002-001"></p><h1 id="2-2-创建代理节点"><a href="#2-2-创建代理节点" class="headerlink" title="2.2 创建代理节点"></a>2.2 创建代理节点</h1><p>【系统管理】 -&gt; 【节点管理】 -&gt; 【新增节点】</p><p><img src="/img/wiki/jenkins/jenkins-002-002.jpg" alt="jenkins-002-002"></p><p><img src="/img/wiki/jenkins/jenkins-002-003.jpg" alt="jenkins-002-003"></p><p><img src="/img/wiki/jenkins/jenkins-002-004.jpg" alt="jenkins-002-004"></p><p><img src="/img/wiki/jenkins/jenkins-002-005.jpg" alt="jenkins-002-005"></p><h1 id="3-测试agent节点任务构建"><a href="#3-测试agent节点任务构建" class="headerlink" title="3.测试agent节点任务构建"></a>3.测试agent节点任务构建</h1><h2 id="3-1-配置任务运行节点"><a href="#3-1-配置任务运行节点" class="headerlink" title="3.1 配置任务运行节点"></a>3.1 配置任务运行节点</h2><p><img src="/img/wiki/jenkins/jenkins-002-006.jpg" alt="jenkins-002-006"></p><h2 id="3-2-配置测试任务"><a href="#3-2-配置测试任务" class="headerlink" title="3.2 配置测试任务"></a>3.2 配置测试任务</h2><p><img src="/img/wiki/jenkins/jenkins-002-007.jpg" alt="jenkins-002-007"></p><p><img src="/img/wiki/jenkins/jenkins-002-008.jpg" alt="jenkins-002-008"></p><h2 id="3-3-构建任务，测试代理服务器"><a href="#3-3-构建任务，测试代理服务器" class="headerlink" title="3.3 构建任务，测试代理服务器"></a>3.3 构建任务，测试代理服务器</h2><p><img src="/img/wiki/jenkins/jenkins-002-009.jpg" alt="jenkins-002-009"></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.freesion.com/article/85671159445">https://www.freesion.com/article/85671159445</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>CICD</tag>
      
      <tag>DevOps</tag>
      
      <tag>Jenkins</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kafka集群基于Kubernetes部署</title>
    <link href="/linux/KafkaOnKubernetes/"/>
    <url>/linux/KafkaOnKubernetes/</url>
    
    <content type="html"><![CDATA[<h1 id="1-部署nfs"><a href="#1-部署nfs" class="headerlink" title="1.部署nfs"></a>1.部署nfs</h1><h1 id="2-部署StorageClass"><a href="#2-部署StorageClass" class="headerlink" title="2.部署StorageClass"></a>2.部署StorageClass</h1><h1 id="3-部署zookeeper集群"><a href="#3-部署zookeeper集群" class="headerlink" title="3.部署zookeeper集群"></a>3.部署zookeeper集群</h1><pre><code class="hljs">vi zookeeper.yamlapiVersion: v1kind: Servicemetadata:  name: zookeeper  namespace: logging  labels:    app: zookeeperspec:  ports:    - port: 2181      name: client  selector:    app: zookeeper---apiVersion: v1kind: Servicemetadata:  name: zookeepers  namespace: logging  labels:    app: zookeeperspec:  ports:    - port: 2888      name: server    - port: 3888      name: leader-election  clusterIP: None  selector:    app: zookeeper---apiVersion: policy/v1beta1kind: PodDisruptionBudgetmetadata:  name: zookeeper-pdb  namespace: loggingspec:  selector:    matchLabels:      app: zookeeper  maxUnavailable: 1---apiVersion: apps/v1kind: StatefulSetmetadata:  name: zookeeper  namespace: loggingspec:  selector:    matchLabels:      app: zookeeper  serviceName: zookeepers  replicas: 3  updateStrategy:    type: RollingUpdate  podManagementPolicy: Parallel  template:    metadata:      labels:        app: zookeeper    spec:      containers:        - name: zookeeper          imagePullPolicy: IfNotPresent          image: registry.cn-hangzhou.aliyuncs.com/swords/zookeeper:v3.6.0          resources:            limits:              cpu: 500m              memory: 512Mi            requests:              cpu: 200m              memory: 300M          ports:            - containerPort: 2181              name: client            - containerPort: 2888              name: server            - containerPort: 3888              name: leader-election          command:            - bash            - -x            - -c            - |              SERVERS=3 &amp;&amp;              HOST=`hostname -s` &amp;&amp;              DOMAIN=`hostname -d` &amp;&amp;              CLIENT_PORT=2181 &amp;&amp;              SERVER_PORT=2888 &amp;&amp;              ELECTION_PORT=3888 &amp;&amp;              PROMETHEUS_PORT=7000 &amp;&amp;              ZOO_DATA_DIR=/var/lib/zookeeper/data &amp;&amp;              ZOO_DATA_LOG_DIR=/var/lib/zookeeper/datalog &amp;&amp;              &#123;                echo &quot;clientPort=$&#123;CLIENT_PORT&#125;&quot;                echo &#39;tickTime=2000&#39;                echo &#39;initLimit=300&#39;                echo &#39;syncLimit=10&#39;                echo &#39;maxClientCnxns=2000&#39;                echo &#39;maxSessionTimeout=60000000&#39;                echo &quot;dataDir=$&#123;ZOO_DATA_DIR&#125;&quot;                echo &quot;dataLogDir=$&#123;ZOO_DATA_LOG_DIR&#125;&quot;                echo &#39;autopurge.snapRetainCount=10&#39;                echo &#39;autopurge.purgeInterval=1&#39;                echo &#39;preAllocSize=131072&#39;                echo &#39;snapCount=3000000&#39;                echo &#39;leaderServes=yes&#39;                echo &#39;standaloneEnabled=false&#39;                echo &#39;4lw.commands.whitelist=*&#39;                echo &#39;metricsProvider.className=org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider&#39;                echo &quot;metricsProvider.httpPort=$&#123;PROMETHEUS_PORT&#125;&quot;              &#125; &gt; /conf/zoo.cfg &amp;&amp;              &#123;                echo &quot;zookeeper.root.logger=CONSOLE&quot;                echo &quot;zookeeper.console.threshold=INFO&quot;                echo &quot;TZ=Asia/Shanghai&quot;                echo &quot;log4j.rootLogger=\$&#123;zookeeper.root.logger&#125;&quot;                echo &quot;log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender&quot;                echo &quot;log4j.appender.CONSOLE.Threshold=\$&#123;zookeeper.console.threshold&#125;&quot;                echo &quot;log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout&quot;                echo &quot;log4j.appender.CONSOLE.layout.ConversionPattern=%d&#123;ISO8601&#125; [myid:%X&#123;myid&#125;] - %-5p [%t:%C&#123;1&#125;@%L] - %m%n&quot;              &#125; &gt; /conf/log4j.properties &amp;&amp;              echo &#39;JVMFLAGS=&quot;-Xms128M -Xmx4G -XX:+UseG1GC -XX:+CMSParallelRemarkEnabled&quot;&#39; &gt; /conf/java.env &amp;&amp;              if [[ $HOST =~ (.*)-([0-9]+)$ ]]; then                  NAME=$&#123;BASH_REMATCH[1]&#125;                  ORD=$&#123;BASH_REMATCH[2]&#125;              else                  echo &quot;Failed to parse name and ordinal of Pod&quot;                  exit 1              fi &amp;&amp;              mkdir -p $&#123;ZOO_DATA_DIR&#125; &amp;&amp;              mkdir -p $&#123;ZOO_DATA_LOG_DIR&#125; &amp;&amp;              export MY_ID=$((ORD+1)) &amp;&amp;              echo $MY_ID &gt; $ZOO_DATA_DIR/myid &amp;&amp;              for (( i=1; i&lt;=$SERVERS; i++ )); do                  echo &quot;server.$i=$NAME-$((i-1)).$DOMAIN:$SERVER_PORT:$ELECTION_PORT&quot; &gt;&gt; /conf/zoo.cfg;              done &amp;&amp;              chown -Rv zookeeper &quot;$ZOO_DATA_DIR&quot; &quot;$ZOO_DATA_LOG_DIR&quot; &quot;$ZOO_LOG_DIR&quot; &quot;$ZOO_CONF_DIR&quot; &amp;&amp;              zkServer.sh start-foreground          readinessProbe:            exec:              command:                - bash                - -c                - &quot;OK=$(echo ruok | nc 127.0.0.1 2181); if [[ \&quot;$OK\&quot; == \&quot;imok\&quot; ]]; then exit 0; else exit 1; fi&quot;            initialDelaySeconds: 10            timeoutSeconds: 5          livenessProbe:            exec:              command:                - bash                - -c                - &quot;OK=$(echo ruok | nc 127.0.0.1 2181); if [[ \&quot;$OK\&quot; == \&quot;imok\&quot; ]]; then exit 0; else exit 1; fi&quot;            initialDelaySeconds: 10            timeoutSeconds: 5          volumeMounts:            - name: data              mountPath: /var/lib/zookeeper      securityContext:        runAsUser: 0        fsGroup: 0      affinity:        podAntiAffinity:          requiredDuringSchedulingIgnoredDuringExecution:          - labelSelector:              matchExpressions:              - key: app                operator: In                values:                - zookeeper            topologyKey: &quot;kubernetes.io/hostname&quot;  volumeClaimTemplates:    - metadata:        name: data      spec:        storageClassName: sc-nfs        accessModes:          - ReadWriteOnce        resources:          requests:            storage: 10Gi</code></pre><h1 id="4-部署kafka集群"><a href="#4-部署kafka集群" class="headerlink" title="4.部署kafka集群"></a>4.部署kafka集群</h1><pre><code class="hljs">vi kafka.yamlapiVersion: v1kind: Servicemetadata:  name: kafka  namespace: logging  labels:    app: kafkaspec:  ports:  - port: 9092    name: server  clusterIP: None  selector:    app: kafka---apiVersion: policy/v1beta1kind: PodDisruptionBudgetmetadata:  name: kafka-pdb  namespace: loggingspec:  selector:    matchLabels:      app: kafka  minAvailable: 2---apiVersion: apps/v1kind: StatefulSetmetadata:  name: kafka  namespace: loggingspec:  selector:    matchLabels:      app: kafka  serviceName: kafka  replicas: 3  template:    metadata:      labels:        app: kafka    spec:      terminationGracePeriodSeconds: 300      imagePullSecrets:      - name: regauth      containers:      - name: kafka        imagePullPolicy: IfNotPresent        image: registry.cn-hangzhou.aliyuncs.com/swords/kafka:v2.12        resources:          limits:            cpu: 500m            memory: 1024Mi          requests:            cpu: 100m            memory: 800Mi        ports:        - containerPort: 9092          name: server        command:        - sh        - -c        - &quot;exec /opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/server.properties --override broker.id=$&#123;HOSTNAME##*-&#125; \          --override listeners=PLAINTEXT://:9092 \          --override zookeeper.connect=zookeeper-0.zookeepers.logging.svc.cluster.local:2181,zookeeper-1.zookeepers.logging.svc.cluster.local:2181 \          --override log.dir=/opt/kafka/logs \          --override auto.create.topics.enable=true \          --override auto.leader.rebalance.enable=true \          --override background.threads=10 \          --override compression.type=producer \          --override delete.topic.enable=false \          --override leader.imbalance.check.interval.seconds=300 \          --override leader.imbalance.per.broker.percentage=10 \          --override log.flush.interval.messages=9223372036854775807 \          --override log.flush.offset.checkpoint.interval.ms=60000 \          --override log.flush.scheduler.interval.ms=9223372036854775807 \          --override log.retention.bytes=-1 \          --override log.retention.hours=72 \          --override log.roll.hours=168 \          --override log.roll.jitter.hours=0 \          --override log.segment.bytes=1073741824 \          --override log.segment.delete.delay.ms=60000 \          --override message.max.bytes=1000012 \          --override min.insync.replicas=1 \          --override num.io.threads=8 \          --override num.network.threads=3 \          --override num.recovery.threads.per.data.dir=1 \          --override num.replica.fetchers=1 \          --override offset.metadata.max.bytes=4096 \          --override offsets.commit.required.acks=-1 \          --override offsets.commit.timeout.ms=5000 \          --override offsets.load.buffer.size=5242880 \          --override offsets.retention.check.interval.ms=600000 \          --override offsets.retention.minutes=1440 \          --override offsets.topic.compression.codec=0 \          --override offsets.topic.num.partitions=50 \          --override offsets.topic.replication.factor=3 \          --override offsets.topic.segment.bytes=104857600 \          --override queued.max.requests=500 \          --override quota.consumer.default=9223372036854775807 \          --override quota.producer.default=9223372036854775807 \          --override replica.fetch.min.bytes=1 \          --override replica.fetch.wait.max.ms=500 \          --override replica.high.watermark.checkpoint.interval.ms=5000 \          --override replica.lag.time.max.ms=10000 \          --override replica.socket.receive.buffer.bytes=65536 \          --override replica.socket.timeout.ms=30000 \          --override request.timeout.ms=30000 \          --override socket.receive.buffer.bytes=102400 \          --override socket.request.max.bytes=104857600 \          --override socket.send.buffer.bytes=102400 \          --override unclean.leader.election.enable=true \          --override zookeeper.session.timeout.ms=6000 \          --override zookeeper.set.acl=false \          --override broker.id.generation.enable=true \          --override connections.max.idle.ms=600000 \          --override controlled.shutdown.enable=true \          --override controlled.shutdown.max.retries=3 \          --override controlled.shutdown.retry.backoff.ms=5000 \          --override controller.socket.timeout.ms=30000 \          --override default.replication.factor=2 \          --override fetch.purgatory.purge.interval.requests=1000 \          --override group.max.session.timeout.ms=300000 \          --override group.min.session.timeout.ms=6000 \          --override inter.broker.protocol.version=2.2.0 \          --override log.cleaner.backoff.ms=15000 \          --override log.cleaner.dedupe.buffer.size=134217728 \          --override log.cleaner.delete.retention.ms=86400000 \          --override log.cleaner.enable=true \          --override log.cleaner.io.buffer.load.factor=0.9 \          --override log.cleaner.io.buffer.size=524288 \          --override log.cleaner.io.max.bytes.per.second=1.7976931348623157E308 \          --override log.cleaner.min.cleanable.ratio=0.5 \          --override log.cleaner.min.compaction.lag.ms=0 \          --override log.cleaner.threads=1 \          --override log.cleanup.policy=delete \          --override log.index.interval.bytes=4096 \          --override log.index.size.max.bytes=10485760 \          --override log.message.timestamp.difference.max.ms=9223372036854775807 \          --override log.message.timestamp.type=CreateTime \          --override log.preallocate=false \          --override log.retention.check.interval.ms=300000 \          --override max.connections.per.ip=2147483647 \          --override num.partitions=4 \          --override producer.purgatory.purge.interval.requests=1000 \          --override replica.fetch.backoff.ms=1000 \          --override replica.fetch.max.bytes=1048576 \          --override replica.fetch.response.max.bytes=10485760 \          --override reserved.broker.max.id=1000 &quot;        env:        - name: KAFKA_HEAP_OPTS          value : &quot;-Xmx512M -Xms512M&quot;        - name: KAFKA_OPTS          value: &quot;-Dlogging.level=INFO&quot;        - name: TZ          value: Asia/Shanghai        volumeMounts:        - name: data          mountPath: /opt/kafka/logs        readinessProbe:          tcpSocket:            port: 9092          timeoutSeconds: 1          initialDelaySeconds: 5      securityContext:        runAsUser: 1000        fsGroup: 1000      affinity:        podAntiAffinity:          requiredDuringSchedulingIgnoredDuringExecution:          - labelSelector:              matchExpressions:              - key: app                operator: In                values:                - kafka            topologyKey: &quot;kubernetes.io/hostname&quot;  volumeClaimTemplates:  - metadata:      name: data    spec:      accessModes: [ &quot;ReadWriteMany&quot; ]      storageClassName: sc-nfs      resources:        requests:          storage: 10Gi</code></pre><h1 id="5-查看kfka集群状态"><a href="#5-查看kfka集群状态" class="headerlink" title="5.查看kfka集群状态"></a>5.查看kfka集群状态</h1><pre><code class="hljs">kubectl -n logging get pod -o wide</code></pre><h1 id="6-验证kafka集群"><a href="#6-验证kafka集群" class="headerlink" title="6.验证kafka集群"></a>6.验证kafka集群</h1><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.orchome.com/1277">https://www.orchome.com/1277</a></li><li><a href="https://www.jianshu.com/p/50a9d38ada27">https://www.jianshu.com/p/50a9d38ada27</a></li><li><a href="https://blog.csdn.net/shanghaibao123/article/details/124351255">https://blog.csdn.net/shanghaibao123/article/details/124351255</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kafka</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>MQ</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes集群调度策略详解</title>
    <link href="/linux/KubernetesShedulerPolicy/"/>
    <url>/linux/KubernetesShedulerPolicy/</url>
    
    <content type="html"><![CDATA[<p>Sheduler，即调度器，负责整个集群的资源调度。默认情况下，Pod被调度到哪个节点运行是由Scheduler组件经过相应的算法计算所得，这个过程不受人工控制，由集群自动完成。但业务实际运行时，自动调度的结果可能并不能满足需求，如需要控制某些Pod到达某些特定的节点上。当然，这样人为控制调度策略将会额外增加资源分配时的计算开销，且复杂的调度规则也将加大管理及维护的成本，具体的配置还是要根据实际情况进行详尽的规划</p><p>kubernetes集群调度方式分为四种：</p><ul><li>自动调度，运行在哪个节点上完全由Scheduler经过一系列的算法自动计算得出</li><li>定向调度，NodeName、NodeSelector</li><li>亲和性&#x2F;反亲和性调度，NodeAffinity、PodAffinity、PodAntiAffinity</li><li>污点&#x2F;容忍调度，Taints&#x2F;Toleration</li></ul><hr><h1 id="1-自动调度"><a href="#1-自动调度" class="headerlink" title="1.自动调度"></a>1.自动调度</h1><p>自动调度是Scheduler默认调度方式，经由一些列算法确定将Pod调度到哪些节点</p><pre><code class="hljs">apiVersion: apps/v1kind: Deploymentmetadata:  name: test-nginxspec:  selector:    matchLabels:      app: test  replicas: 3  template:    metadata:      labels:        app: test    spec:      containers:        - name: nginx          image: registry.sword.org/nginx          imagePullPolicy: IfNotPresent          ports:            - containerPort: 80              name: nginx              protocol: TCP          resources:            limits:              cpu: 200m              memory: 200Mi            requests:              cpu: 100m              memory: 100Mi      imagePullSecrets:        - name: regcred</code></pre><h1 id="2-定向调度"><a href="#2-定向调度" class="headerlink" title="2.定向调度"></a>2.定向调度</h1><p>定向调度，即将Pod调度到指定的节点，带有强制性，也就是说即使所要调度的目标节点不存在或不可调度也要进行调度，调度的结果自然是失败的。定向调度有两种实现方式，即NodeName和NodeSelector</p><h2 id="2-1-NodeName"><a href="#2-1-NodeName" class="headerlink" title="2.1 NodeName"></a>2.1 NodeName</h2><p>NodeName，用于强制约束将Pod调度到指定Name的节点，即跳过Scheduler的调度逻辑直接将Pod调度到指定名称的节点。如匹配的节点不存在，则Pod运行失败</p><pre><code class="hljs">apiVersion: apps/v1kind: Deploymentmetadata:  name: test-nginxspec:  selector:    matchLabels:      app: test  replicas: 3  template:    metadata:      labels:        app: test    spec:      containers:        - name: nginx          image: registry.sword.org/nginx          imagePullPolicy: IfNotPresent          ports:            - containerPort: 80              name: nginx              protocol: TCP          resources:            limits:              cpu: 200m              memory: 200Mi            requests:              cpu: 100m              memory: 100Mi      imagePullSecrets:        - name: regcred      nodeName: worker03</code></pre><h2 id="2-2-NodeSelector"><a href="#2-2-NodeSelector" class="headerlink" title="2.2 NodeSelector"></a>2.2 NodeSelector</h2><p>NodeSelector，用于通过kubernetes的label-selector机制将Pod调度到添加了指定标签的节点，即由scheduler使用MatchNodeSelector调度策略匹配指定label的节点进行Pod调度，匹配规则具有强制性。如匹配的节点不存在，则Pod运行失败</p><h3 id="2-2-1-节点新增标签"><a href="#2-2-1-节点新增标签" class="headerlink" title="2.2.1 节点新增标签"></a>2.2.1 节点新增标签</h3><pre><code class="hljs">kubectl label nodes worker01 worker=01kubectl label nodes worker02 worker=02kubectl label nodes worker03 worker=03kubectl label nodes worker02 app=jellyfinkubectl label nodes worker03 app=database</code></pre><h3 id="2-2-2-验证NodeSelector调度算法"><a href="#2-2-2-验证NodeSelector调度算法" class="headerlink" title="2.2.2 验证NodeSelector调度算法"></a>2.2.2 验证NodeSelector调度算法</h3><pre><code class="hljs">apiVersion: apps/v1kind: Deploymentmetadata:  name: jellyfin-deploymentspec:  selector:    matchLabels:      app: jellyfin-server  replicas: 1  template:    metadata:      labels:        app: jellyfin-server    spec:      containers:        - name: jellyfin          image: registry.cn-hangzhou.aliyuncs.com/geekers/jellyfin:v10.8.5          imagePullPolicy: IfNotPresent          ports:            - containerPort: 80              name: tcp-jellyfin              protocol: TCP          resources:            limits:              cpu: 500m              memory: 512M            requests:              cpu: 300m              memory: 500M          volumeMounts:          - mountPath: /etc/localtime            name: localtime          - mountPath: /media/cloud            name: nfs-sword          - mountPath: /config            name: jellyfin-config          - mountPath: /cache            name: jellyfin-cache      volumes:      - hostPath:          path: /etc/localtime        name: localtime      - name: nfs-sword        persistentVolumeClaim:          claimName: nfs-sword      - name: jellyfin-config        persistentVolumeClaim:          claimName: jellyfin-ceph-config      - name: jellyfin-cache        persistentVolumeClaim:          claimName: jellyfin-ceph-cache      imagePullSecrets:        - name: regcred       nodeSelector:        app: jellyfin</code></pre><h3 id="2-2-3-删除节点标签，验证NodeSelector调度算法"><a href="#2-2-3-删除节点标签，验证NodeSelector调度算法" class="headerlink" title="2.2.3 删除节点标签，验证NodeSelector调度算法"></a>2.2.3 删除节点标签，验证NodeSelector调度算法</h3><pre><code class="hljs"># 查看节点标签kubectl get nodes worker02 --show-labels# 删除节点的app标签kubectl label nodes worker02 app-# 删除任一pod，重新调度kubectl delete pod jellyfin-deployment-545d877bbc-c65pz</code></pre><h1 id="3-亲和性调度"><a href="#3-亲和性调度" class="headerlink" title="3.亲和性调度"></a>3.亲和性调度</h1><p>亲和性调度，即Affinity，是基于NodeSelector调度策略的扩展，通过配置节点标签的匹配规则优先选择满足条件的Node进行调度，若匹配失败，则再调度到不满足条件的节点上，从而使得调度更加灵活。Affinity分为三类，即nodeAffinity、podAffinity、podAntiAffinity：</p><ul><li>nodeAffinity，node亲和性，即以node为目标决定pod可以调度到哪些Node上</li><li>podAffinity，pod亲和性，即以正在运行的pod为参照目标，实现新创建的pod跟参照pod部署在同一区域</li><li>podAntiAffinity，pod反亲和性，即以正在运行的pod为参照目标，实现新创建的pod不跟参照pod部署在同一区域</li></ul><h2 id="3-1-使用场景"><a href="#3-1-使用场景" class="headerlink" title="3.1 使用场景"></a>3.1 使用场景</h2><ul><li>亲和性，若两个应用频繁交互，则有必要利用亲和性调度策略将两者尽可能调度到同一节点，以减少因网络通信带来的性能损耗</li><li>反亲和性，若应用涉及多副本部署，则有必要采用反亲和性调度策略将各个应用实例打散分布到各个不同的节点，以提高服务的高可用性</li></ul><h2 id="3-2-标签匹配表达式"><a href="#3-2-标签匹配表达式" class="headerlink" title="3.2 标签匹配表达式"></a>3.2 标签匹配表达式</h2><p>亲和性调度算法依据标签匹配表达式列出符合要求的节点列表，从而更灵活地完成调度，表达式可用的关系运算符有In、NotIn、Exists、 DoesNotExist、Gt、Lt，用法如下：</p><h3 id="3-2-1-Exists"><a href="#3-2-1-Exists" class="headerlink" title="3.2.1 Exists"></a>3.2.1 Exists</h3><pre><code class="hljs">- matchExpressions:  # 匹配存在标签key为test的节点   - key: test    operator: Exists</code></pre><h3 id="3-2-2-In"><a href="#3-2-2-In" class="headerlink" title="3.2.2 In"></a>3.2.2 In</h3><pre><code class="hljs">- matchExpressions:  # 匹配标签key为test，且value是&quot;01&quot;或&quot;02&quot;的节点   - key: test    operator: In    values:     - [&quot;01&quot;,&quot;02&quot;]</code></pre><h3 id="3-2-3-Gt"><a href="#3-2-3-Gt" class="headerlink" title="3.2.3 Gt"></a>3.2.3 Gt</h3><pre><code class="hljs">- matchExpressions:  # 匹配标签key为test，且value大于&quot;01&quot;的节点   - key: test    operator: Gt    values:     - &quot;01&quot;</code></pre><hr><ul><li>注：其余运算符的运算逻辑基本为上面三个的反例</li></ul><h2 id="3-3-nodeAffinity"><a href="#3-3-nodeAffinity" class="headerlink" title="3.3 nodeAffinity"></a>3.3 nodeAffinity</h2><h3 id="3-3-1-硬亲和性策略"><a href="#3-3-1-硬亲和性策略" class="headerlink" title="3.3.1 硬亲和性策略"></a>3.3.1 硬亲和性策略</h3><pre><code class="hljs">apiVersion: apps/v1kind: Deploymentmetadata:  name: redis-deploymentspec:  selector:    matchLabels:      app: redis-server  replicas: 1  template:    metadata:      labels:        app: redis-server    spec:      containers:        - name: redis          image: registry.sword.org/redis          imagePullPolicy: IfNotPresent          command: [ &quot;/usr/bin/redis-server&quot; ]          args: [&quot;--requirepass&quot;,&quot;$(requirepass)&quot;]          envFrom:          - configMapRef:              name: redis-conf          env:            - name: requirepass              valueFrom:                secretKeyRef:                  name: redis-auth                  key: requirepass          ports:            - containerPort: 6379              name: redis-tcp              protocol: TCP          resources:            limits:              cpu: 100m              memory: 100M            requests:              cpu: 50m              memory: 64M      imagePullSecrets:        - name: regcred      # 设置亲和性       affinity:        # 设置node亲和性        nodeAffinity:          # 设置硬亲和性策略，即必须满足的匹配规则          requiredDuringSchedulingIgnoredDuringExecution:            # 设置节点选择列表            nodeSelectorTerms:            # 设置标签匹配表达式            - matchExpressions:              - key: app                # 设置标签匹配规则，即匹配标签key为app且value值不为jellyfin的节点                operator: NotIn                values:                 - jellyfin</code></pre><h3 id="3-3-2-软亲和性策略"><a href="#3-3-2-软亲和性策略" class="headerlink" title="3.3.2 软亲和性策略"></a>3.3.2 软亲和性策略</h3><pre><code class="hljs">apiVersion: apps/v1kind: Deploymentmetadata:  name: gitea-deployment  namespace: devopsspec:  selector:    matchLabels:      app: gitea  replicas: 1  template:    metadata:      labels:        app: gitea    spec:      containers:        - name: gitea          image: gitea/gitea          imagePullPolicy: IfNotPresent          ports:            - containerPort: 3000              name: gitea-http            - containerPort: 22              name: gitea-ssh            resources:            limits:              cpu: 300m              memory: 256M            requests:              cpu: 200m              memory: 200M          volumeMounts:          - mountPath: /data            name: gitea-data          - name: localtime            mountPath: /etc/localtime      volumes:      - name: gitea-data        persistentVolumeClaim:          claimName: gitea-data      - name: localtime        hostPath:          path: /etc/localtime      imagePullSecrets:        - name: regcred      # 设置亲和性       affinity:        # 设置node亲和性        nodeAffinity:          # 设置软亲和性策略，即优先调度到满足指定规则的节点，若无满足规则的节点再调度到其他节点          preferredDuringSchedulingIgnoredDuringExecution:          # 设置策略权重，取值范围为1-100          - weight: 1            # 设置节点选择器            preference:               # 设置标签匹配表达式              matchExpressions:              - key: worker                # 设置标签匹配规则，即匹配标签key为worker且value值大于01的节点                operator: Gt                values:                 - &quot;01&quot;</code></pre><h3 id="3-3-3-注意事项"><a href="#3-3-3-注意事项" class="headerlink" title="3.3.3 注意事项"></a>3.3.3 注意事项</h3><ul><li>若同时定义了nodeSelector和nodeAffinity，则须两个条件都得到满足才能调度到指定的Node</li><li>若nodeAffinity指定了多个nodeSelectorTerms，则只需其中一个能够匹配成功即可</li><li>若nodeSelectorTerms有多个matchExpressions，则节点须全部满足才能匹配成功</li><li>若Pod所在的节点在Pod运行期间标签发生了改变，不再符合该Pod节点的亲和性需求，则将忽略此变化，不再重新调度</li></ul><h2 id="3-4-podAffinity"><a href="#3-4-podAffinity" class="headerlink" title="3.4 podAffinity"></a>3.4 podAffinity</h2><pre><code class="hljs"># 查看Pod标签kubectl get pod redis --show-labels</code></pre><h3 id="3-4-1-硬亲和性策略"><a href="#3-4-1-硬亲和性策略" class="headerlink" title="3.4.1 硬亲和性策略"></a>3.4.1 硬亲和性策略</h3><pre><code class="hljs">apiVersion: apps/v1kind: Deploymentmetadata:  name: cloudreve-deploymentspec:  selector:    matchLabels:      app: cloudreve-server  replicas: 1  template:    metadata:      labels:        app: cloudreve-server    spec:      containers:        - name: cloudreve          image: registry.sword.org/cloudreve          imagePullPolicy: IfNotPresent          ports:            - containerPort: 5212              name: cloudreve-tcp              protocol: TCP          resources:            limits:              cpu: 200m              memory: 512M            requests:              cpu: 100m              memory: 300M          volumeMounts:          - mountPath: /cloudreve/conf.ini            name: cloudreve-conf            subPath: conf.ini          - mountPath: /home/sword            name: nfs-sword      volumes:      - configMap:          name: cloudreve-conf        name: cloudreve-conf      - name: nfs-sword        persistentVolumeClaim:          claimName: nfs-sword      imagePullSecrets:        - name: regcred      # 设置亲和性       affinity:        # 设置pod亲和性        podAffinity:          # 设置硬亲和性策略，即必须满足的匹配规则          requiredDuringSchedulingIgnoredDuringExecution:            # 设置Pod标签选择器          - labelSelector:            # 设置Pod标签匹配表达式              matchExpressions:              - key: app                # 设置Pod标签匹配规则，即匹配标签key为app且value值为redis-server的Pod作为调度目标                operator: In                values:                 - &quot;redis-server&quot;              - key: app                # 设置Pod标签匹配规则，即匹配标签key为app且value值不为jellyfin的Pod作为调度目标                operator: NotIn                values:                 - &quot;jellyfin-server&quot;            # 设置调度作用域            topologyKey: kubernetes.io/hostname        # 设置node亲和性        nodeAffinity:          # 设置软亲和性策略，即优先调度到满足指定规则的节点，若无满足规则的节点再调度到其他节点          preferredDuringSchedulingIgnoredDuringExecution:          # 设置策略权重，取值范围为1-100          - weight: 1            # 设置节点选择器            preference:               # 设置标签匹配表达式              matchExpressions:              - key: app                # 设置标签匹配规则，即匹配标签key为app且value值为database的节点                operator: In                values:                 - &quot;database&quot;</code></pre><h3 id="3-4-2-软亲和性策略"><a href="#3-4-2-软亲和性策略" class="headerlink" title="3.4.2 软亲和性策略"></a>3.4.2 软亲和性策略</h3><pre><code class="hljs">apiVersion: apps/v1kind: Deploymentmetadata:  name: hexo-deployment  namespace: devopsspec:  selector:    matchLabels:      app: hexo-server  replicas: 1  template:    metadata:      labels:        app: hexo-server    spec:      containers:        - name: hexo          image: registry.sword.org/nginx          imagePullPolicy: IfNotPresent          ports:            - containerPort: 80              name: hexo-nginx          resources:            limits:              cpu: 100m              memory: 100M            requests:              cpu: 100m              memory: 64M          volumeMounts:            - name: nginx-conf              mountPath: /etc/nginx/nginx.conf              subPath: nginx.conf      volumes:        - name: nginx-conf          configMap:            name: nginx.conf      imagePullSecrets:        - name: regcred      # 设置亲和性       affinity:        # 设置Pod亲和性        podAffinity:          # 设置软亲和性策略，即优以满足指定规则的Pod为目标进行调度，若无满足规则的Pod则按照集群自动调度          preferredDuringSchedulingIgnoredDuringExecution:            # 设置策略权重值，范围为1-100            - weight: 1              # 设置标签列表              podAffinityTerm:                # 设置Pod标签选择器                labelSelector:                  # 设置Pod标签匹配表达式                  matchExpressions:                    - key: app                      # 设置Pod标签匹配规则，即匹配标签key为app且value值为jenkins的Pod作为调度目标                      operator: In                      values:                       - jenkins                # 设置调度作用域，以节点区分区域                topologyKey: kubernetes.io/hostname        # 设置node亲和性        nodeAffinity:          # 设置硬亲和性策略，即必须满足的匹配规则          requiredDuringSchedulingIgnoredDuringExecution:            # 设置节点选择列表            nodeSelectorTerms:            # 设置标签匹配表达式            - matchExpressions:              - key: app                # 设置标签匹配规则，即匹配没有标签key为app的节点                operator: DoesNotExist</code></pre><h2 id="3-5-PodAntiAffinity"><a href="#3-5-PodAntiAffinity" class="headerlink" title="3.5 PodAntiAffinity"></a>3.5 PodAntiAffinity</h2><h3 id="3-5-1-硬反亲和性策略"><a href="#3-5-1-硬反亲和性策略" class="headerlink" title="3.5.1 硬反亲和性策略"></a>3.5.1 硬反亲和性策略</h3><pre><code class="hljs">apiVersion: apps/v1kind: Deploymentmetadata:  name: nginx-deploymentspec:  selector:    matchLabels:      app: nginx-server  replicas: 3  template:    metadata:      labels:        app: nginx-server    spec:      containers:        - name: nginx          image: registry.sword.org/nginx          imagePullPolicy: IfNotPresent          ports:            - containerPort: 80              name: nginx              protocol: TCP          resources:            limits:              cpu: 200m              memory: 200Mi            requests:              cpu: 100m              memory: 100Mi          volumeMounts:            - name: nginx-conf              mountPath: /etc/nginx/nginx.conf              subPath: nginx.conf            - name: nginx-logs              mountPath: /var/log/nginx            - name: nfs-sword              mountPath: /home/sword      volumes:        - name: nginx-conf          configMap:            name: nginx-conf        - name: nginx-logs          hostPath:            path: /var/log/nginx        - name: nfs-sword          persistentVolumeClaim:            claimName: nfs-sword      imagePullSecrets:        - name: regcred      # 设置亲和性       affinity:        # 设置pod反亲和性        podAntiAffinity:          # 设置硬反亲和策略，即必须满足的匹配规则          requiredDuringSchedulingIgnoredDuringExecution:            # 设置Pod标签选择器          - labelSelector:            # 设置Pod标签匹配表达式              matchExpressions:              - key: app                # 设置Pod标签匹配规则，即匹配标签key为app且value值为nginx-servers的Pod作为反向                # 调度目标，也即是使得副本Pod之间互斥，相互之间不在同一节点上，将之打散以保障其可用性                operator: In                values:                 - nginx-server            # 设置调度作用域            topologyKey: kubernetes.io/hostname        # 设置Pod亲和性        podAffinity:          # 设置软亲和性策略，即优以满足指定规则的Pod为目标进行调度，若无满足规则的Pod则按照集群自动调度          preferredDuringSchedulingIgnoredDuringExecution:            # 设置策略权重值，范围为1-100            - weight: 1              # 设置标签列表              podAffinityTerm:                # 设置Pod标签选择器                labelSelector:                  # 设置Pod标签匹配表达式                  matchExpressions:                    - key: app                      # 设置Pod标签匹配规则，即匹配标签key为test且value值为00的pod作为调度目标                      operator: NotIn                      values:                       - [&quot;jellyfin&quot;,&quot;cloudreve&quot;]                    - key: app                      # 设置Pod标签匹配规则，即匹配标签key为test且value值为00的pod作为调度目标                      operator: In                      values:                       - redis                 # 设置调度作用域，以节点区分区域                topologyKey: kubernetes.io/hostname</code></pre><h3 id="3-5-2-软亲和性策略"><a href="#3-5-2-软亲和性策略" class="headerlink" title="3.5.2 软亲和性策略"></a>3.5.2 软亲和性策略</h3><pre><code class="hljs">apiVersion: apps/v1kind: Deploymentmetadata:  name: webos-deployment  spec:  selector:    matchLabels:      app: webos-server  replicas: 1  template:    metadata:      labels:        app: webos-server    spec:      containers:        - name: webos          image: registry.cn-hangzhou.aliyuncs.com/geekers/webos          imagePullPolicy: IfNotPresent          ports:            - containerPort: 8088              protocol: TCP          resources:              limits:                cpu: 500m                memory: 300Mi              requests:                cpu: 300m                memory: 200Mi          volumeMounts:            - name: webos-root              mountPath: /webos/api/rootPath            - name: webos-apps              mountPath: /webos/web/apps            - name: webos-data              mountPath: /home/webos      volumes:        - name: webos-root          persistentVolumeClaim:            claimName: webos-root        - name: webos-apps          persistentVolumeClaim:            claimName: webos-apps        - name: webos-data          persistentVolumeClaim:            claimName: webos-data      imagePullSecrets:        - name: regcred      # 设置亲和性       affinity:        # 设置Pod反亲和性        podAntiAffinity:          # 设置软亲和性策略，即优以满足指定规则的Pod为目标进行反向调度          preferredDuringSchedulingIgnoredDuringExecution:            # 设置策略权重值，范围为1-100            - weight: 1              # 设置标签列表              podAffinityTerm:                # 设置Pod标签选择器                labelSelector:                  # 设置Pod标签匹配表达式                  matchExpressions:                    - key: app                      # 设置Pod标签匹配规则，即匹配标签key为app且value值为jellyfin的pod作为调度目标                      operator: In                      values:                       - &quot;jellyfin-server&quot;                # 设置调度作用域                topologyKey: kubernetes.io/hostname        podAffinity:          # 设置软亲和性策略，即优以满足指定规则的Pod为目标进行调度，若无满足规则的Pod则按照集群自动调度          preferredDuringSchedulingIgnoredDuringExecution:            # 设置策略权重值，范围为1-100            - weight: 1              # 设置标签列表              podAffinityTerm:                # 设置Pod标签选择器                labelSelector:                  # 设置Pod标签匹配表达式                  matchExpressions:                    - key: app                      # 设置Pod标签匹配规则，即匹配标签key为test且value值为00的pod作为调度目标                      operator: In                      values:                       - &quot;cloud-server&quot;                # 设置调度作用域，以节点区分区域                topologyKey: kubernetes.io/hostname        # 设置node亲和性        nodeAffinity:          # 设置软亲和性策略，即优先调度到满足指定规则的节点，若无满足规则的节点再调度到其他节点          preferredDuringSchedulingIgnoredDuringExecution:          # 设置策略权重，取值范围为1-100          - weight: 1            # 设置节点选择器            preference:               # 设置标签匹配表达式              matchExpressions:              - key: app                # 设置标签匹配规则，即匹配具有标签key为app的节点                operator: Exists</code></pre><h1 id="4-污点和容忍调度"><a href="#4-污点和容忍调度" class="headerlink" title="4.污点和容忍调度"></a>4.污点和容忍调度</h1><p>Kubernetes作为容器化云平台已经实现了资源的容器隔离，但目前容器隔离还不能做到宿主机资源100%的隔离，且平台层面对业务线占用资源的限制和隔离也大大增加了跨部门协同合作的沟通成本。污点和容忍配合的调度策略可妥善的解决此问题，即是通过对节点配置Taints和Tolerations，将资源需求不同的业务线Pod调度到各自独立的节点组，从而避免同一业务线的Pod被分配到不合适的节点，从而实现了不同业务线的物理隔离</p><ul><li>污点，即Taints，节点避免被调度的属性，即不允许将Pod调度到配置了污点的节点，甚至还能将已经存在的Pod驱逐出去</li><li>容忍，即Tolerations，Pod对污点忽略的属性，即允许Pod被调度到有污点的节点</li></ul><h2 id="4-1-污点配置"><a href="#4-1-污点配置" class="headerlink" title="4.1 污点配置"></a>4.1 污点配置</h2><p>污点的格式为:key&#x3D;value:effect, 其中，key和value是污点的标签；effect描述污点的作用，支持三种选项：</p><ul><li>NoSchedule，不会将Pod调度到具有该污点的Node上，但不影响当前Node上已存在的Pod</li><li>PreferNoSchedule，尽量避免将Pod调度到具有该污点的Node上，除非没有其他节点可调度</li><li>NoExecute，不会将Pod调度到具有该污点的Node上，且还会将Node上已存在的Pod驱离</li></ul><h3 id="4-1-1-设置污点"><a href="#4-1-1-设置污点" class="headerlink" title="4.1.1 设置污点"></a>4.1.1 设置污点</h3><pre><code class="hljs">kubectl taint nodes master01 master=master01:NoExecutekubectl taint nodes worker01 worker=worker01:PreferNoSchedule</code></pre><ul><li>注：kubeadm引导的集群master节点已经被打上key为node-role.kubernetes.io&#x2F;master且effect为NoSchedule的污点</li></ul><h3 id="4-1-2-删除污点"><a href="#4-1-2-删除污点" class="headerlink" title="4.1.2 删除污点"></a>4.1.2 删除污点</h3><pre><code class="hljs">kubectl taint nodes node01 key1=value1:NoSchedule-</code></pre><h2 id="4-2-容忍配置"><a href="#4-2-容忍配置" class="headerlink" title="4.2 容忍配置"></a>4.2 容忍配置</h2><p>Pod容忍属性匹配污点的原则是key、effect相同，且满足运算符为Exists或Equal时value相等，此外还存在两种特例：</p><ul><li>若容忍的key为空且运算符为Exists，将会匹配所有的key、value和effect，即容忍所有污点</li><li>若一个key和一个空的effect匹配此key的所有effect</li></ul><h3 id="4-2-1-容忍所有污点"><a href="#4-2-1-容忍所有污点" class="headerlink" title="4.2.1 容忍所有污点"></a>4.2.1 容忍所有污点</h3><pre><code class="hljs">apiVersion: apps/v1kind: Deploymentmetadata:  name: test-nginxspec:  selector:    matchLabels:      app: test  replicas: 3  template:    metadata:      labels:        app: test    spec:      containers:        - name: nginx          image: registry.sword.org/nginx          imagePullPolicy: IfNotPresent          ports:            - containerPort: 80              name: nginx              protocol: TCP          resources:            limits:              cpu: 100m              memory: 100M            requests:              cpu: 50m              memory: 50M      imagePullSecrets:        - name: regcred      tolerations:      - operator: &quot;Exists&quot;</code></pre><h3 id="4-2-2-容忍标签所有的污点"><a href="#4-2-2-容忍标签所有的污点" class="headerlink" title="4.2.2 容忍标签所有的污点"></a>4.2.2 容忍标签所有的污点</h3><pre><code class="hljs">apiVersion: apps/v1kind: Deploymentmetadata:  name: test-nginxspec:  selector:    matchLabels:      app: test  replicas: 3  template:    metadata:      labels:        app: test    spec:      containers:        - name: nginx          image: registry.sword.org/nginx          imagePullPolicy: IfNotPresent          ports:            - containerPort: 80              name: nginx              protocol: TCP          resources:            limits:              cpu: 100m              memory: 100M            requests:              cpu: 50m              memory: 50M      imagePullSecrets:        - name: regcred      tolerations:      - key: &quot;worker&quot;        operator: &quot;Exists&quot; </code></pre><h3 id="4-2-3-精确匹配容忍的污点"><a href="#4-2-3-精确匹配容忍的污点" class="headerlink" title="4.2.3 精确匹配容忍的污点"></a>4.2.3 精确匹配容忍的污点</h3><pre><code class="hljs">apiVersion: apps/v1kind: Deploymentmetadata:  name: test-nginxspec:  selector:    matchLabels:      app: test  replicas: 3  template:    metadata:      labels:        app: test    spec:      containers:        - name: nginx          image: registry.sword.org/nginx          imagePullPolicy: IfNotPresent          ports:            - containerPort: 80              name: nginx              protocol: TCP          resources:            limits:              cpu: 100m              memory: 100M            requests:              cpu: 50m              memory: 50M      imagePullSecrets:        - name: regcred      tolerations:      - key: &quot;master&quot;        operator: &quot;Equal&quot;         value: &quot;master01&quot;        effect: &quot;NoExecute&quot;</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.jianshu.com/p/4e9c81607f95">https://www.jianshu.com/p/4e9c81607f95</a></li><li><a href="https://blog.csdn.net/qq_52397471/article/details/129093055">https://blog.csdn.net/qq_52397471/article/details/129093055</a></li><li><a href="https://cloud.tencent.com/developer/article/1815807?from=15425">https://cloud.tencent.com/developer/article/1815807?from=15425</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python模块与包</title>
    <link href="/linux/PythonModule/"/>
    <url>/linux/PythonModule/</url>
    
    <content type="html"><![CDATA[<p>Python为提高代码的维护性与复用性，引入了模块与包的概念，即通过模块将相关的代码拆分到不同的代码文件以提高代码的可维护性与逻辑性，通过包目录化的组织模块以提高代码的可维护性与层次感，通过互相调用以提高代码的复用性。Python这种内置电池的哲学思维，使得代码的编写不必要从零开始，直接导入模块或包即可完成调用，大大编程的繁复性，降低了学习门槛，促成了Python的广泛应用</p><h1 id="模块"><a href="#模块" class="headerlink" title="模块"></a>模块</h1><p>模块，Module，是一组包含Python定义和语句的文件，后缀名为.py，可被相互调用以实现其功能。模块分为内置模块、自定义模块和第三方模块：自定义模块即为自己编写的模块，可以通过申请成为Python内置的标准模块，发布到网络供他人使用即为第三方模块。</p><h1 id="包"><a href="#包" class="headerlink" title="包"></a>包</h1><p>包，Package，模块的集合，是Python用于按目录分层次组织模块的方法，定义了由模块及子包，以及子包下的子包所组成的Python应用环境，是比模块又高一级的封装，采用“包名.子包名…..模块名”的调用形式，解决了模块重名的问题。包类似于文件系统的文件目录，其文件夹下必须存在__init__.py文件, 用于标识当前文件夹是一个Python包，内容可为空</p><hr><h1 id="1-创建模块"><a href="#1-创建模块" class="headerlink" title="1.创建模块"></a>1.创建模块</h1><p>创建模块，即为编写自定义模块，实际上就是编写一个后缀名为.py的Pyhton脚本，模块名应符合标识符命名规范</p><pre><code class="hljs">vi power.py#!/usr/bin/python3# -*- coding: utf-8 -*-def pow(a,b):    return a ** b</code></pre><h1 id="2-导入模块"><a href="#2-导入模块" class="headerlink" title="2.导入模块"></a>2.导入模块</h1><p>导入模块，即是将模块代码加载到内存以供调用，使用import关键，调用模块所定义函数的方式为：模块名.函数名</p><h2 id="2-1-导入自定义模块"><a href="#2-1-导入自定义模块" class="headerlink" title="2.1 导入自定义模块"></a>2.1 导入自定义模块</h2><pre><code class="hljs">&gt;&gt;&gt; import power&gt;&gt;&gt; power.pow(2,3)8</code></pre><h2 id="2-2-导入内置模块"><a href="#2-2-导入内置模块" class="headerlink" title="2.2 导入内置模块"></a>2.2 导入内置模块</h2><pre><code class="hljs">&gt;&gt;&gt; import math&gt;&gt;&gt; math.pow(2,3)8.0</code></pre><h2 id="2-3-导入第三方模块"><a href="#2-3-导入第三方模块" class="headerlink" title="2.3 导入第三方模块"></a>2.3 导入第三方模块</h2><p>Python第三方模块存储于网络上，所以要使用第三方模块首先要将其下载到本地，然后才能进行导入、调用</p><h3 id="2-3-1-导入国内python镜像源"><a href="#2-3-1-导入国内python镜像源" class="headerlink" title="2.3.1 导入国内python镜像源"></a>2.3.1 导入国内python镜像源</h3><pre><code class="hljs">vi ~/.pip/pip.conf[global]index-url = http://mirrors.aliyun.com/pypi/simple/[install]trusted-host=mirrors.aliyun.com</code></pre><h3 id="2-3-2-安装第三方模块"><a href="#2-3-2-安装第三方模块" class="headerlink" title="2.3.2 安装第三方模块"></a>2.3.2 安装第三方模块</h3><pre><code class="hljs">sudo pip install requests</code></pre><h2 id="2-4-导入部分模块"><a href="#2-4-导入部分模块" class="headerlink" title="2.4 导入部分模块"></a>2.4 导入部分模块</h2><p>Python语言from关键字用于从某个对象导入指定的部分到当前命名空间，而不会将整个对象导入，但需注意名字冲突</p><pre><code class="hljs"># 导入datetime库的date与datetime类&gt;&gt;&gt; from datetime import date,datetime# 导入datetime库的date类，并重命名为cdate，注意名字冲突&gt;&gt;&gt; from datetime import date as cdate&gt;&gt;&gt; date.today()datetime.date(2024, 1, 25)&gt;&gt;&gt; now = date.today()&gt;&gt;&gt; print(now)2024-01-25&gt;&gt;&gt; from datetime import date as cdate&gt;&gt;&gt; now = cdate.today()&gt;&gt;&gt; print(now)2024-01-25</code></pre><h1 id="3-搜索路径"><a href="#3-搜索路径" class="headerlink" title="3.搜索路径"></a>3.搜索路径</h1><p>Python语言import关键字用于导入的模块，一个模块只会被导入一次，不会重复导入，以节省资源。Python解释器根据内置函数sys.path的设置Python环境变量PYTHONPATH按顺序搜索模块文件，若找不到模块文件则会报错，默认的搜索顺一般为：当前执行脚本目录（自定义） —&gt;&gt; Python安装目录（内置） —&gt;&gt; Python安装目录site-packages目录（第三方）。也可自定义Python环境变量：</p><pre><code class="hljs">set PYTHONPATH=/usr/local/lib/python</code></pre><h1 id="4-模块内置函数"><a href="#4-模块内置函数" class="headerlink" title="4.模块内置函数"></a>4.模块内置函数</h1><h2 id="4-1-dir"><a href="#4-1-dir" class="headerlink" title="4.1 dir()"></a>4.1 dir()</h2><p>dir() 函数，返回值为排好序的字符串列表，其内容是模块里定义过的名字，包括模块、变量和函数</p><pre><code class="hljs">&gt;&gt;&gt; import a&gt;&gt;&gt; import math&gt;&gt;&gt; dir(a)&gt;&gt;&gt; dir()</code></pre><h2 id="4-2-reload"><a href="#4-2-reload" class="headerlink" title="4.2 reload()"></a>4.2 reload()</h2><p>reload() 函数，用于重新执行模块顶层部分的代码，也即重新导入之前导入过的模块，语法为：reload(module_name)</p><pre><code class="hljs">&gt;&gt;&gt; reload(a)</code></pre><h1 id="5-包"><a href="#5-包" class="headerlink" title="5.包"></a>5.包</h1><p>包，package，由若干Python模块或子包构成的层级目录结构，功能完备的包或模块的集合也称为库。包实质上是Python语言管理与组织模块和库的手段，是Python库的组织形式与表现形式，以便增强模块调用的层次感与逻辑性。事实上，Python语言并没有库这个概念，是种通俗的说法，通常所说的库即可以是模块也可以是包</p><h2 id="5-1-创建包"><a href="#5-1-创建包" class="headerlink" title="5.1 创建包"></a>5.1 创建包</h2><pre><code class="hljs"># 创建包目录mkdir -p package_test &amp;&amp; cd package_test</code></pre><h3 id="5-1-1-创建模块"><a href="#5-1-1-创建模块" class="headerlink" title="5.1.1 创建模块"></a>5.1.1 创建模块</h3><pre><code class="hljs"># 创建模块m1vi m1.py#!/usr/bin/python3# -*- coding: utf-8 -*-print(&quot;Hello&quot;)</code></pre><hr><pre><code class="hljs"># 创建模块m2vi m2.py#!/usr/bin/python3# -*- coding: utf-8 -*-print(&quot;World&quot;)</code></pre><h3 id="5-1-2-创建初始化模块"><a href="#5-1-2-创建初始化模块" class="headerlink" title="5.1.2 创建初始化模块"></a>5.1.2 创建初始化模块</h3><pre><code class="hljs">vi __init__.py#!/usr/bin/python3# -*- coding: utf-8 -*-if __name__ == &#39;__main__&#39;:    print(&quot;作为主程序运行&quot;)else:    print(&quot;package_test包初始化&quot;)</code></pre><h3 id="5-1-3-创建调用包的脚本"><a href="#5-1-3-创建调用包的脚本" class="headerlink" title="5.1.3 创建调用包的脚本"></a>5.1.3 创建调用包的脚本</h3><pre><code class="hljs">cd ..vi test.py#!/usr/bin/python3# -*- coding: utf-8 -*-# 导入包import package_test# 调用包的模块package_test.m1()package_test.m2()</code></pre><h3 id="5-1-3-调用包"><a href="#5-1-3-调用包" class="headerlink" title="5.1.3 调用包"></a>5.1.3 调用包</h3><pre><code class="hljs">./testpackage_test包初始化HelloWorld</code></pre><h1 id="6-内置库"><a href="#6-内置库" class="headerlink" title="6.内置库"></a>6.内置库</h1><p>Python能大行其道的原因之一就是数量庞大且功能丰富的内置库，无需额外安装与配置，直接调用即可，是Python学习的必修课</p><h2 id="6-1-datetime"><a href="#6-1-datetime" class="headerlink" title="6.1 datetime"></a>6.1 datetime</h2><p>datetime，Python语言日期和时间处理的标准库</p><h3 id="6-1-1-当前日期与时间"><a href="#6-1-1-当前日期与时间" class="headerlink" title="6.1.1 当前日期与时间"></a>6.1.1 当前日期与时间</h3><pre><code class="hljs">&gt;&gt;&gt; import datetime&gt;&gt;&gt; now = datetime.datetime.now()&gt;&gt;&gt; print(now)2024-01-24 14:25:41.623668</code></pre><ul><li>注：datetime.datetime.now()表示调用datetime库的datetime模块的now()方法，该方法（函数），返回值类行为datetime，用于获取当前日期与时间</li></ul><h3 id="6-1-2-格式化日期与时间"><a href="#6-1-2-格式化日期与时间" class="headerlink" title="6.1.2 格式化日期与时间"></a>6.1.2 格式化日期与时间</h3><p>strftime()方法用于将datetime类型转换为字符串类型，已实现日期时间的格式化操作</p><pre><code class="hljs">&gt;&gt;&gt; now = datetime.datetime.now()&gt;&gt;&gt; date = now.strftime(&quot;%Y-%m-%d&quot;)&gt;&gt;&gt; print(date)2024-01-24&gt;&gt;&gt; time = now.strftime(&quot;%H:%M:%S&quot;)&gt;&gt;&gt; print(time)14:30:32</code></pre><ul><li>注：strftime()方法%Y表示4位数的年份，%m表示两位数的月份，%d表示两位数的日期，%H表示24小时制的小时，%M表示分钟，%S表示秒</li></ul><h3 id="6-1-3-字符串转换为日期时间"><a href="#6-1-3-字符串转换为日期时间" class="headerlink" title="6.1.3 字符串转换为日期时间"></a>6.1.3 字符串转换为日期时间</h3><p>strftime()方法用于将字符串类型的数据转换为日期时间类型</p><pre><code class="hljs">&gt;&gt;&gt; datetime = datetime.datetime.strptime(date,&quot;%Y-%m-%d&quot;)&gt;&gt;&gt; print(datetime)2024-01-24 00:00:00</code></pre><h3 id="6-1-4-日期时间运算"><a href="#6-1-4-日期时间运算" class="headerlink" title="6.1.4 日期时间运算"></a>6.1.4 日期时间运算</h3><h2 id="6-2-os模块"><a href="#6-2-os模块" class="headerlink" title="6.2 os模块"></a>6.2 os模块</h2><p>os模块，跨平台的操作系统接口模块，具备大量的操作系统相关功能的函数，主要包括系统相关、目录及文件操作、路径操作、命令执行和进程管理等</p><h3 id="6-2-1-系统相关"><a href="#6-2-1-系统相关" class="headerlink" title="6.2.1 系统相关"></a>6.2.1 系统相关</h3><p>os模块系统相关的功能主要体现在获取操作系统环境变量</p><pre><code class="hljs">import os# 获取当前操作系统名称，Windows平台返回&quot;nt&quot;，Linux返回&quot;posix&quot;print(os.name)# 获取当前操作系统路径的分隔符，Windows平台为&quot;\&quot;，Linux为&quot;/&quot;print(os.sep)# 获取当前操作系统环境变量的分隔符，Windows平台为&quot;;&quot;，Linux为&quot;:&quot;print(os.pathsep)# 获取登陆系统的用户名print(os.getlogin)</code></pre><h3 id="6-2-2-文件及目录操作"><a href="#6-2-2-文件及目录操作" class="headerlink" title="6.2.2 文件及目录操作"></a>6.2.2 文件及目录操作</h3><h2 id="6-3-tarfile模块"><a href="#6-3-tarfile模块" class="headerlink" title="6.3 tarfile模块"></a>6.3 tarfile模块</h2><p>tarfile模块，Python内置模块之一，用于tar归档文件的压缩、解压处理，类似于文件操作。Linux系统常见压缩文件格式如下：</p><ul><li>tar，Linux系统打包归档工具，没有压缩功能，打包为tar包之后才可以其他压缩程序进行压缩，最常用的压缩方式为gzip，压缩率最高的方式为bzip2</li><li>gz，即gzip压缩方式，通常只能压缩一个文件</li><li>tgz，即tar.gz，先以tar打包，再用gz压缩得到的文件</li><li>zip，不同于gzip，可分别打包压缩多个文件，压缩率低于tar</li><li>rar，打包压缩文件，最初用于DOS，基于window操作系统。压缩率比zip高，但速度慢，随机访问的速度也慢</li></ul><h3 id="6-3-1-语法格式"><a href="#6-3-1-语法格式" class="headerlink" title="6.3.1 语法格式"></a>6.3.1 语法格式</h3><h4 id="6-3-1-1-打开-x2F-创建压缩包"><a href="#6-3-1-1-打开-x2F-创建压缩包" class="headerlink" title="6.3.1.1 打开&#x2F;创建压缩包"></a>6.3.1.1 打开&#x2F;创建压缩包</h4><pre><code class="hljs">tarfile.open(name=None, mode=&#39;r&#39;, fileobj=None, bufsize=10240, **kwargs)</code></pre><p>参数详解：</p><p>1.name，打开的文件名或路径<br>2.bufsize：用于指定数据块的大小，默认为20*512字节<br>3.mode，文件打开方式，默认为’r’，具体如下：</p><ul><li><p>‘r’或’r:*’，自动解压并打开文件</p></li><li><p>‘r:’，只打开文件不解压</p></li><li><p>‘r:gz’，gzip格式解压并打开文件</p></li><li><p>‘r:bz2’，bz2格式解压并打开文件</p></li><li><p>‘r:xz’，lzma格式解压并打开文件</p></li><li><p>‘x’or ‘x:’，仅创建打包文件，不压缩</p></li><li><p>‘x:gz’，gzip方式压缩并打包文件</p></li><li><p>‘x:bz2’，bzip2方式压缩并打包文件</p></li><li><p>‘x:xz’，lzma方式压缩并打包文件</p></li><li><p>‘a’ or ‘a:’，打开文件，并以不压缩的方式追加内容。如果文件不存在，则新建</p></li><li><p>‘w’ or ‘w:’，以不压缩的方式写入，即只归档不压缩</p></li><li><p>‘w:gz’，gzip的方式压缩并写入</p></li><li><p>‘w:bz2’，以bzip2的方式压缩并写入</p></li><li><p>‘w:xz’，lzma的方式压缩并写入</p></li></ul><h4 id="6-3-1-2-压缩包添加文件"><a href="#6-3-1-2-压缩包添加文件" class="headerlink" title="6.3.1.2 压缩包添加文件"></a>6.3.1.2 压缩包添加文件</h4><p>压缩文件的方式主要是将模式改变，在rwx的基础上加上各个压缩的方式，如r:gz、w:bz2、x:xz等样式</p><p>add(name, arcname)</p><p>参数详解：</p><ul><li>1.name，压缩文件的文件名或文件路径</li><li>2.arcname，压缩文件文件名的别名，非必要参数，压缩别名一定不要以路径分隔符为结尾，否则只会创建一个文件夹</li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hexo搭建博客</title>
    <link href="/geek/Hexo/"/>
    <url>/geek/Hexo/</url>
    
    <content type="html"><![CDATA[<p>Hexo，基于Nodejs设计的快速、简洁且高效的开源静态博客框架，无需后端数据库的支持使得更加易于部署和维护，可直接将编辑好的Markdown文件解析为静态网页并可托管于GitHub和Coding。Hexo具有大量靓丽的主题，所以相比于Docsify更适合作为博客使用，缺点是需要一点点技术，但也很简单</p><h1 id="1-安装Git、Nodejs"><a href="#1-安装Git、Nodejs" class="headerlink" title="1.安装Git、Nodejs"></a>1.安装Git、Nodejs</h1><h1 id="2-安装hexo"><a href="#2-安装hexo" class="headerlink" title="2.安装hexo"></a>2.安装hexo</h1><pre><code class="hljs">sudo npm install -g hexo-clisudo ln -s /usr/local/nodejs/lib/node_modules/hexo-cli/bin/hexo /usr/local/bin</code></pre><h1 id="3-初始化hexo"><a href="#3-初始化hexo" class="headerlink" title="3.初始化hexo"></a>3.初始化hexo</h1><pre><code class="hljs">sudo hexo init /web/hexosudo npm install</code></pre><hr><ul><li>_config.yml，hexo主配置文件</li><li>db.json，hexo文件存储，以后的博客文章文件应该就是存储在这里</li><li>node_modules，node的模块</li><li>package.json，hexo的插件</li><li>package-lock.json，node的相关依赖</li><li>public，Markdown文件渲染的静态网页目录，可在主配置文件自定义</li><li>scaffolds，hexo命令的模板文件,默认为post</li><li>source，hexo命令写作的源Markdown文件所在目录，将Markdown文件手动拷贝至此目录即可不用hexo命令新建博客</li><li>themes，hexo主题文件所在目录</li></ul><h1 id="4-配置hexo主题"><a href="#4-配置hexo主题" class="headerlink" title="4.配置hexo主题"></a>4.配置hexo主题</h1><h1 id="5-启动本地服务器"><a href="#5-启动本地服务器" class="headerlink" title="5.启动本地服务器"></a>5.启动本地服务器</h1><pre><code class="hljs">hexo -s</code></pre><h1 id="6-Hexo写文章"><a href="#6-Hexo写文章" class="headerlink" title="6.Hexo写文章"></a>6.Hexo写文章</h1><pre><code class="hljs">hexo new 测试</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://hexo.io/zh-cn/docs">https://hexo.io/zh-cn/docs</a></li><li><a href="https://www.modb.pro/db/127951">https://www.modb.pro/db/127951</a></li><li><a href="https://www.jianshu.com/p/785d727810b3">https://www.jianshu.com/p/785d727810b3</a></li><li><a href="https://www.cnblogs.com/wcisns/p/18706913">https://www.cnblogs.com/wcisns/p/18706913</a></li><li><a href="https://blog.shipengx.com/archives/afb81b2.html">https://blog.shipengx.com/archives/afb81b2.html</a></li><li><a href="https://blog.csdn.net/Mo_0214/article/details/137501214">https://blog.csdn.net/Mo_0214/article/details/137501214</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>极客</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>极客</tag>
      
      <tag>Nodejs</tag>
      
      <tag>Git</tag>
      
      <tag>博客</tag>
      
      <tag>Hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python函数</title>
    <link href="/linux/PythonFunction/"/>
    <url>/linux/PythonFunction/</url>
    
    <content type="html"><![CDATA[<p>函数，可重复调用的具有某种特定功能的代码块，通过调用即可实现其功能，面向对象编程中也被称为方法。函数分为内置函数与自定义函数两类：内置函数，Python库自带的已经定义好的函数，直接调用即可，如input、print、max等；自定义函数，Python无法提供的对应各自业务需求的需要开发的函数。函数的引入使得程序更加模块化，不再需要编写大量的重复代码</p><h1 id="1-函数定义"><a href="#1-函数定义" class="headerlink" title="1.函数定义"></a>1.函数定义</h1><p>定义函数，即创建函数，由def关键字实现，语法格式为：</p><p>def 函数名(参数列表):<br>    函数体<br>    return [返回值]</p><ul><li><p>def，用于定义函数的关键字，其后为函数标识符和圆括号()，最后的“:”代表函数的起始</p></li><li><p>函数名，即为符合Python语法的标识符，建议以能够体现出函数的功能来命名</p></li><li><p>参数，即需要函数加工运算的一个或多个的初始输入数据，多个参数以“,”隔离，且参数值和参数名称是按函数声明中定义的顺序进行匹配，也可没有参数</p></li><li><p>函数体，即用于实现某种功能的代码块，须进行缩进。Python编码规范建议一个函数只做一件事，函数体建议不超过20行，否则说明这个函数不止做了一件事情，应该将之拆分为更小的函数，或调用其他函数来实现部分功能</p></li><li><p>return，用于设置函数的返回值，表示函数的结束，可选参数，即返回值视具体情况而定是否设置，无return语句表示返回None</p><p>  def my_function():<br>  print(“Hello world”)</p></li></ul><h1 id="2-函数调用"><a href="#2-函数调用" class="headerlink" title="2.函数调用"></a>2.函数调用</h1><p>函数定义之后即可进行调用，调用方式为：函数名+(参数列表)</p><pre><code class="hljs">def my_function():    print(&quot;Hello world&quot;)# 调用函数my_function my_function()</code></pre><h1 id="3-函数参数"><a href="#3-函数参数" class="headerlink" title="3.函数参数"></a>3.函数参数</h1><p>定义函数时其参数就被确定下来，函数的接口也就固定了，调用函数只需要知道如何传递正确的参数，以及函数将返回什么样的值即可，从而使得函数内部的复杂逻辑被封装起来，调用者无需了解。也即是说，参数决定了函数的调用方式</p><h2 id="3-1-位置参数"><a href="#3-1-位置参数" class="headerlink" title="3.1 位置参数"></a>3.1 位置参数</h2><p>位置参数，也称必传参数、顺序参数，是Python最常见的参数传递方式，调用时必须按照函数定义时参数的顺序，依次传递参数的值，且一一对应，个数不多不少，否则将出现语法错误</p><pre><code class="hljs">&gt;&gt;&gt; def add(a,b):...     return a+b... &gt;&gt;&gt; add(1,2)3&gt;&gt;&gt; </code></pre><h2 id="3-2-默认参数"><a href="#3-2-默认参数" class="headerlink" title="3.2 默认参数"></a>3.2 默认参数</h2><p>函数定义时，可为参数指定默认值，这个参数就被称为默认参数，而不再是位置参数了。调用函数时，若没有传递该参数的值就将使用默认值，也可给默认参数传递自定义的值。默认参数简化了函数的调用，在为最常用的情况提供简便调用的同时，也可在特殊情况传递新的值</p><pre><code class="hljs">&gt;&gt;&gt; def sq(x,n = 2):...     return x**n... &gt;&gt;&gt; sq(3)9&gt;&gt;&gt; sq(3,3)27&gt;&gt;&gt;</code></pre><ul><li>注：默认参数必须定义在位置参数后面，且尽量指向不可变的对象，如整型、字符串、元组，可变对象如列表、字典由于本身就是指向变量的地址，则调用时将会更改其内部元素的数值</li></ul><h2 id="3-3-关键字参数"><a href="#3-3-关键字参数" class="headerlink" title="3.3 关键字参数"></a>3.3 关键字参数</h2><p>调用函数时，使用参数的名称和相应的值来传递参数，这样明确指定参数的值即无需依赖位置，从而可以改变传递参数的顺序，还可以使用默认参数只传递部分参数而不是全部</p><pre><code class="hljs">&gt;&gt;&gt; def info(name,age):...     print(&quot;Name is&quot;,name)...     print(&quot;Age is&quot;,age)... &gt;&gt;&gt; info(age = 15,name = &#39;LiLei&#39;)Name is LiLeiAge is 15&gt;&gt;&gt; </code></pre><h2 id="3-4-不定长参数"><a href="#3-4-不定长参数" class="headerlink" title="3.4 不定长参数"></a>3.4 不定长参数</h2><p>不定长参数表示函数接受任意数量的位置参数，作为元组或字典进行传递，通过在参数名前加上“ * ”定义，即“ *args ”或“ **kwargs ”</p><h3 id="3-4-1-args"><a href="#3-4-1-args" class="headerlink" title="3.4.1 *args"></a>3.4.1 *args</h3><p>一个星号的不定长参数表示接收任意参数，调用时会将实际参数作为元组传入形式参数，若参数是列表则会将整个列表当做一个参数传入</p><pre><code class="hljs">&gt;&gt;&gt; def add(*num):...     total = 0...     for n in num:...         total +=n...     return total... &gt;&gt;&gt; add(1,2,3,4)10</code></pre><h3 id="3-4-2-kwargs"><a href="#3-4-2-kwargs" class="headerlink" title="3.4.2 **kwargs"></a>3.4.2 **kwargs</h3><p>两个星的不定长参数表示接受任意数量的键值对作为参数，调用的时会将实际参数作为字典传入形式参数</p><pre><code class="hljs">&gt;&gt;&gt; def info(**stu):...     for k,v in stu.items():...         print(k,&quot;:&quot;,v)... &gt;&gt;&gt; info(name=&quot;Lilei&quot;,age=15,city=&quot;Beijing&quot;)name : Lileiage : 15city : Beijing</code></pre><ul><li>注：不定长参数必须定义在所有位置参数和默认参数后面</li></ul><h1 id="4-变量作用域"><a href="#4-变量作用域" class="headerlink" title="4.变量作用域"></a>4.变量作用域</h1><p>变量作用域是指变量的有效范围，即变量的访问权限，取决变量所定义的位置，两种最基本的变量作用域为全局作用域和局部作用域：全局作用域变量定义在函数外部，可在整个程序范围内调用；局部作用域变量定义在函数内部，只能在其被声明的函数内部访问，调用函数时局部变量都将被加入到作用域中</p><pre><code class="hljs"># 定义全局变量total&gt;&gt;&gt; total = 0&gt;&gt;&gt; def plus(m,n):        # 定义局部变量total...     total = m + n...     print(&quot;函数内部变量total=&quot;,total)...     return total... &gt;&gt;&gt; plus(10,20)函数内部变量total= 3030&gt;&gt;&gt; print(&quot;函数全局变量total=&quot;,total)函数全局变量total= 0</code></pre><h1 id="5-回调函数"><a href="#5-回调函数" class="headerlink" title="5.回调函数"></a>5.回调函数</h1><p>回调函数，即将函数作为参数的函数，也就是说其参数是另一个函数，用于降低函数调用之间的耦合性，适用于事件处理、异步编程、定时器与事件循环、钩子函数等场景</p><pre><code class="hljs">&gt;&gt;&gt; def area(r):...     circle = 3.14 * r * r...     return circle... &gt;&gt;&gt; area(3)28.259999999999998&gt;&gt;&gt; def r_circle(a):...     r = a * 2...     return r... &gt;&gt;&gt; r_circle(area(3))56.519999999999996</code></pre><h1 id="6-递归函数"><a href="#6-递归函数" class="headerlink" title="6.递归函数"></a>6.递归函数</h1><p>递归函数，即函数体内调用其自身的函数，每次递归都会将原始问题分解为规模较小的子问题，如此反复执行，直到达到为防止无限循环而设置的终止条件，也即是原始问题的解决方案。理论上，所有的递归函数都可以写为循环的方式，但循环的逻辑不如递归简单而清晰。需要注意的是，递归函数可能会导致栈溢出，建议深度不超过100层</p><pre><code class="hljs"># 计算1到100的和，相比起循环方式，递归函数的语意显然更为简洁&gt;&gt;&gt; def sum_num(n):...     if n &lt;= 0:...         return 0...     return n+sum_num(n-1)... &gt;&gt;&gt; sum_num(100)5050</code></pre><h1 id="7-函数式编程"><a href="#7-函数式编程" class="headerlink" title="7.函数式编程"></a>7.函数式编程</h1><p>函数式编程，即将计算作为纯函数数据处理的编程范式，并尽可能避免或最小化可变状态和副作用，也就是只依赖于输入参数并返回输出结果，而避免出现可变状态和副作用。纯函数通常易于测试、可组合和并发执行，使得代码更具可读性、可维护性和可重用性，编程过程中可进行尝试</p><h2 id="7-1-匿名函数"><a href="#7-1-匿名函数" class="headerlink" title="7.1 匿名函数"></a>7.1 匿名函数</h2><p>匿名函数，Anonymous functions，即无需用def关键字这样标准的显式地定义的函数，而是由Lambda关键字创建。lambda函数是Python语言用于快速地定义一次性的、小型的、功能简单的函数，即用即删，非常轻量化，通常搭配内置函数</p><h3 id="7-1-1-语法格式"><a href="#7-1-1-语法格式" class="headerlink" title="7.1.1 语法格式"></a>7.1.1 语法格式</h3><pre><code class="hljs">lambda 参数列表: 表达式</code></pre><ul><li>lambda函数的函数体只是一个表达式，而非代码块，相对简单，因此只能封装有限的逻辑</li><li>lambda函数的输入即为参数列表的值，不能写return，返回值即为表达式的计算值</li><li>lambda函数有自己独立的命名空间，且不能访问自由参数列表之外或全局命名空间的参数</li></ul><h3 id="7-1-2-定义匿名函数"><a href="#7-1-2-定义匿名函数" class="headerlink" title="7.1.2 定义匿名函数"></a>7.1.2 定义匿名函数</h3><pre><code class="hljs">&gt;&gt;&gt; area = lambda l,w: l * w</code></pre><h3 id="7-1-3-调用匿名函数"><a href="#7-1-3-调用匿名函数" class="headerlink" title="7.1.3 调用匿名函数"></a>7.1.3 调用匿名函数</h3><pre><code class="hljs">&gt;&gt;&gt; area(2,5)10</code></pre><h2 id="7-2-推导式"><a href="#7-2-推导式" class="headerlink" title="7.2 推导式"></a>7.2 推导式</h2><p>Python语言的推导式用于从数据序列构建另一个新的数据序列，可快速生成容器类型的数据结构，如列表、字典、集合等</p><h3 id="7-2-1-列表推导式"><a href="#7-2-1-列表推导式" class="headerlink" title="7.2.1 列表推导式"></a>7.2.1 列表推导式</h3><p>语法格式：<br>    [表达式 for 变量 in 序列] 或 [表达式 for in 序列 if 条件]</p><ul><li>表达式，列表生成元素的表达式，可以是有返回值的函数</li><li>for语句，迭代列表，并将变量传入表达式</li><li>if语句，过滤列表中不符合条件的值</li><li>序列，列表、字典、集合和元组等容器型数据结构</li></ul><hr><pre><code class="hljs"># 列表range(1,10)执行for循环，并将每个元素带入表达式x*x以生成新列表&gt;&gt;&gt; list = [x*x for x in range(1,10)]&gt;&gt;&gt; print(list)[1, 4, 9, 16, 25, 36, 49, 64, 81]# 上述推导式新增循环条件，即只对能被2整除的元素进行循环&gt;&gt;&gt; [x * x for x in range(1,11) if x % 2 ==0][4, 16, 36, 64, 100]# 多重循环，即同时循环x、y两个变量，循环序列为字符串&gt;&gt;&gt; [x + y for x in &#39;123&#39; for y in &#39;abc&#39;][&#39;1a&#39;, &#39;1b&#39;, &#39;1c&#39;, &#39;2a&#39;, &#39;2b&#39;, &#39;2c&#39;, &#39;3a&#39;, &#39;3b&#39;, &#39;3c&#39;]# 元组型、集合型数据列表推导式的结果仍然是列表型&gt;&gt;&gt; num1 = (1,2,3)&gt;&gt;&gt; num2 = &#123;1,2,3&#125;&gt;&gt;&gt; [x + y for x in num1 for y in num2][2, 3, 4, 3, 4, 5, 4, 5, 6]# 字典stu进行列表推导式，即将字典的Value转换为列表&gt;&gt;&gt; stu = &#123;&#39;name&#39;: &#39;Lilei&#39;,&#39;age&#39;: 15,&#39;no&#39;: 10000&#125;&gt;&gt;&gt; [stu[key] for key in stu][&#39;Lilei&#39;, 15, 10000]</code></pre><h3 id="7-2-2-字典推导式"><a href="#7-2-2-字典推导式" class="headerlink" title="7.2.2 字典推导式"></a>7.2.2 字典推导式</h3><p>字典推导式的语法格式基本与列表推导式一致，即是将其他类型的容器数据结构转换为字典类型</p><pre><code class="hljs">&gt;&gt;&gt; &#123;x: x**2 for x in (1,2,3)&#125;&#123;1: 1, 2: 4, 3: 9&#125;# 元组stu执行for循环，并将每个元素的值作为字典推导式的Key，每个元素的长度作为字典推导式的Value&gt;&gt;&gt; stu = (&quot;Lilei&quot;,&quot;John&quot;,&quot;Bob&quot;)&gt;&gt;&gt; print(stu)&gt;&gt;&gt; &#123;&#39;Lilei&#39;: 5, &#39;John&#39;: 4, &#39;Bob&#39;: 3&#125;</code></pre><h3 id="7-2-3-集合推导式"><a href="#7-2-3-集合推导式" class="headerlink" title="7.2.3 集合推导式"></a>7.2.3 集合推导式</h3><p>集合推导式的语法格式基本与列表推导式一致，即是将其他类型的容器数据结构转换为集合类型</p><pre><code class="hljs"># 由于集合是无序的数据类型，所以字典推导式的结果随机排布&gt;&gt;&gt; str = &quot;Python&quot;&gt;&gt;&gt; &#123;ch for ch in str&#125;&#123;&#39;n&#39;, &#39;P&#39;, &#39;y&#39;, &#39;o&#39;, &#39;h&#39;, &#39;t&#39;&#125;</code></pre><ul><li>注：元组没有推导式，可用类型转换函数tuple()快速生成，如tuple(x for x in range(1,10))</li></ul><h2 id="7-3-迭代器"><a href="#7-3-迭代器" class="headerlink" title="7.3 迭代器"></a>7.3 迭代器</h2><p>迭代器，Python语言可被标记遍历位置的可迭代对象，是一种访问集合元素的方式，表示一连串的元素流对象。其工作流程是从第一个元素开始不断地被next()函数调用并返回下一个元素，直到没有元素时抛出StopIteration错误。迭代器可用for语句循环遍历，整个遍历过程被迭代的元素流有序序列的长度都不被知晓，不像是列表或元组那样一次性将所有元素加载到内存，而是通过next()函数动态地返回下一次将要处理的数据，因此将会节省不少的内存和空间。迭代器由两个方法组成，即iter()和next()，前者用于创建迭代器，后者用于循环迭代器</p><p>总之，迭代器可用于遍历可迭代对象，如列表、元组等数据类型，但可由next()方法动态调用而非一次性加载所有元素到内存，所以性能较优</p><pre><code class="hljs">&gt;&gt;&gt; list = [1,2,3,4]&gt;&gt;&gt; it = iter(list)&gt;&gt;&gt; for x in it:...     print(x)... 1234</code></pre><h2 id="7-4-生成器"><a href="#7-4-生成器" class="headerlink" title="7.4 生成器"></a>7.4 生成器</h2><p>生成器，Python语言快速创建迭代器的机制，使用关键字yield定义，写法类似于函数，但不像普通函数用return关键字返回某个值以供调用并结束程序，而是执行yield语句返回迭代器对象，并保存当前位置，下次执行next()方法或for循环时从当前位置继续运行。生成器实质上是一种特殊的迭代器，具有迭代器的性质，即循环计算可迭代对象的元素值而非一次性加载，且自动实现迭代器的功能而不必再调用iter()和next()方法</p><h3 id="7-4-1-创建生成器"><a href="#7-4-1-创建生成器" class="headerlink" title="7.4.1 创建生成器"></a>7.4.1 创建生成器</h3><pre><code class="hljs">&gt;&gt;&gt; def countdown(n):...     while n &gt; 0:            # yield关键字标识该函数为生成器函数...         yield n...         n -= 1...# 创建生成器对象&gt;&gt;&gt; gen = countdown(10)# 通过next()方法迭代生成器对象，每次调用都从上次的位置开始&gt;&gt; print(next(gen))10&gt;&gt;&gt; print(next(gen))9&gt;&gt;&gt; print(next(gen))8&gt;&gt;&gt; print(next(gen))5# 通过for循环遍历生成器对象，开始位置为上次的迭代位置，直到全部遍历为止&gt;&gt;&gt; for num in gen:...     print(num)... 4321</code></pre><h3 id="7-4-2-生成器表达式"><a href="#7-4-2-生成器表达式" class="headerlink" title="7.4.2 生成器表达式"></a>7.4.2 生成器表达式</h3><p>功能简单的生成器可写成简洁的表达式，语法类似于推导式，且比推导式更为节省内存</p><pre><code class="hljs">&gt;&gt;&gt; gen = (i*i for i in range(5))&gt;&gt;&gt; next(gen)0&gt;&gt;&gt; next(gen)1&gt;&gt;&gt; next(gen)4&gt;&gt;&gt; for i in gen:...     print(i)... 916</code></pre><h2 id="7-5-装饰器"><a href="#7-5-装饰器" class="headerlink" title="7.5 装饰器"></a>7.5 装饰器</h2><p>装饰器，Python语言用于为已经存在的函数或类动态地增加额外的功能，且不改变原函数或类的代码，其参数是被装饰的函数或类，返回值为带有装饰功能的函数。装饰器适用于切面需求的场景，如日志、缓存、性能测试、事务处理及权限校验等，以抽离大量与函数功能本身无关的重复代码并继续重用，简化程序开发流程，使得代码更具健壮性</p><h3 id="7-5-1-装饰器定义与调用"><a href="#7-5-1-装饰器定义与调用" class="headerlink" title="7.5.1 装饰器定义与调用"></a>7.5.1 装饰器定义与调用</h3><p>装饰器函数通常以@符号为标识，并置于要修饰的函数或类之前</p><pre><code class="hljs"># 定义函数logs()，其参数为函数func，返回值为函数wrapper，函数具体功能为：输出函数logs()的参数（其参数是函数，并输出函数名）&gt;&gt;&gt; def logs(func):...     def wrapper(*args,**kwargs):...         print(f&quot;Calling function: &#123;func.__name__&#125;&quot;)...         return func(*args,**kwargs)...     return wrapper... # 调用函数logs()作为装饰函数，用于装饰函数add()&gt;&gt;&gt; @logs... def add(a,b):...     return a + b... &gt;&gt;&gt; add(2,3)Calling function: add5</code></pre><ul><li>logs()作为装饰器函数用于输出功能函数add()的函数名，是装饰器函数日志功能场景的实现。当然，功能函数本身也能实现这个效果，但若多个功能函数（如减法乘法除法函数等）都需要这一功能时，每个功能函数就都需再增加重复的代码，装饰器则只需直接@引用logs()函数即可，显得更加简洁与清晰，同时将功能函数与日志记录分离，逻辑性更强</li></ul><h3 id="7-5-2-带参数的装饰器"><a href="#7-5-2-带参数的装饰器" class="headerlink" title="7.5.2 带参数的装饰器"></a>7.5.2 带参数的装饰器</h3><p>普通的不带参数的装饰器其实定义了两层函数，如logs()最外层为装饰器本身，内层wrapper()函数为装饰器的返回函数，带参数的装饰器即是在此基础上再定义一层函数用于接收参数</p><pre><code class="hljs">&gt;&gt;&gt; def limit(max_times):...     def logs(func):...         def wrapper(*args,**kwargs):...             if wrapper.max_time &lt; max_times:...                 result = func(*args,**kwargs)...                 wrapper.max_time +=1...                 print(f&quot;Calling function: &#123;func.__name__&#125;&quot;)...                 return result...             else:...                 print(f&quot;函数 &#39;&#123;func.__name__&#125;&#39; 已达到最大执行次数.&quot;)...                 return None...         wrapper.max_time = 0...         return wrapper...     return logs... ...&gt;&gt;&gt; @limit(3)... def add(a,b):...     return a + b... ...&gt;&gt;&gt; add(1,2)Calling function: add3&gt;&gt;&gt; add(1,2)Calling function: add3&gt;&gt;&gt; add(1,2)Calling function: add3&gt;&gt;&gt; add(1,2)函数 &#39;add&#39; 已达到最大执行次数.</code></pre><h3 id="7-5-3-类装饰器"><a href="#7-5-3-类装饰器" class="headerlink" title="7.5.3 类装饰器"></a>7.5.3 类装饰器</h3><p>装饰器也可由类构建</p><pre><code class="hljs">&gt;&gt;&gt; class RecordTime(object):...     def __init__(self,func):...         self._func = func...     def __call__(self):...         start_time = time.time()...         self._func()...         end_time = time.time()...         print(&#39;spend is &#123;&#125;&#39;.format(end_time - start_time))... &gt;&gt;&gt; @RecordTime... def func():...     print(&#39;func...&#39;)...     time.sleep(3)... &gt;&gt;&gt; func()func...spend is 3.000955820083618</code></pre>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes集群监控探针详解</title>
    <link href="/linux/KubernetesProbe/"/>
    <url>/linux/KubernetesProbe/</url>
    
    <content type="html"><![CDATA[<p>Probe，即探针，Kubernetes集群监控容器运行状态的机制，是由kubelet组件对容器执行的定期检查，以监测容器是否正常运行、是否能够正常响应请求以及容器内部的应用程序是否正常运行，并根据探测到的状态做出相应的动作。默认情况下，集群只能检查容器主进程运行状况，而无法判断应用是否能正常提供服务，无法反映真实的健康状态。探针弥补了这一不足，实现了应用的健康检查，完善了集群自我修复能力，从而保障了业务的连续性与稳定性。Kubernetes集群提供了三种探针，即startupProbe、livenessProbe和readinessProbe</p><h1 id="探测流程"><a href="#探测流程" class="headerlink" title="探测流程"></a>探测流程</h1><p>容器启动时，三种探针依次执行，根据每次的探测结果来做相应的动作，探测结果有三种，即Success，表示通过检测；Failure，表示未通过检测；Unknown，表示检测没有正常进行</p><h2 id="1-启动探针检测"><a href="#1-启动探针检测" class="headerlink" title="1.启动探针检测"></a>1.启动探针检测</h2><p>startupProbe，启动探针，容器启动时首先执行，若探测失败则将重启容器，只会在容器启动时执行一次</p><h2 id="2-存活探针定期检测"><a href="#2-存活探针定期检测" class="headerlink" title="2.存活探针定期检测"></a>2.存活探针定期检测</h2><p>livenessProbe，存活探针，定期检测容器是否处于运行状态，若探测失败则由kubelet销毁该容器，并根据容器的重启策略restartPolicy做相应的处理。若未配置存活探针，则默认状态为Success，即探针返回的值永远为Success</p><h2 id="3-就绪探针定期检测"><a href="#3-就绪探针定期检测" class="headerlink" title="3.就绪探针定期检测"></a>3.就绪探针定期检测</h2><p>readinessProbe，就绪探针，定期检测容器是否已准备好接收请求，用于判断应用是否健康，即能否正常提供服务，若探测失败则将从Service的EndPoints列表中剔除该Pod的IP:Port，直到探针再次成功</p><ul><li>注：以上流程为三种探针都做配置的情况</li></ul><h1 id="探测方式"><a href="#探测方式" class="headerlink" title="探测方式"></a>探测方式</h1><p>Kubernetes探针的探测机制支持三种健康检查方法，即命令行exec，httpGet和tcpSocket，其中exec通用性最强，适用与大部分场景，tcpSocket适用于TCP业务，httpGet适用于web业务</p><ul><li>exec，自定义健康检查，在容器中执行指定的命令，若执行成功，退出码为0，则表明容器健康</li><li>httpGet，通过容器的IP地址、端口号及路径调用HTTP Get方法，若响应的状态码大于等于200且小于400，则表明容器健康</li><li>tcpSocket，通过容器IP地址和端口号执行TCP检查，若能够建立TCP连接，则表明容器健康</li></ul><h1 id="配置参数"><a href="#配置参数" class="headerlink" title="配置参数"></a>配置参数</h1><ul><li>initialDelaySeconds，启动liveness、readiness探针前的等待时长，单位为秒，默认为0</li><li>periodSeconds，探针检测的频率，单位为秒，默认为1</li><li>timeoutSeconds，探针检测的超时时长，单位为秒，默认为1</li><li>successThreshold，探针需要通过的最小连续成功检查次数，通过即为探测成功，默认为1</li><li>failureThreshold，将探针标记为失败的重试次数，liveness探针失败将导致Pod重启，readiness探针失败则将标记Pod为未就绪（unready）状态，默认为1</li></ul><h1 id="1-startupProbe"><a href="#1-startupProbe" class="headerlink" title="1.startupProbe"></a>1.startupProbe</h1><pre><code class="hljs">apiVersion: apps/v1kind: Deploymentmetadata:  name: test-nginxspec:  selector:    matchLabels:      app: test  replicas: 3  template:    metadata:      labels:        app: test    spec:      containers:        - name: nginx          image: registry.cn-hangzhou.aliyuncs.com/swords/nginx          imagePullPolicy: IfNotPresent          ports:            - containerPort: 80              name: nginx              protocol: TCP          resources:            limits:              cpu: 200m              memory: 200Mi            requests:              cpu: 100m              memory: 100Mi          startupProbe:            httpGet:              path: /              port: 80            initialDelaySeconds: 10            periodSeconds: 5      imagePullSecrets:        - name: regcred</code></pre><h1 id="2-livenessProbe"><a href="#2-livenessProbe" class="headerlink" title="2.livenessProbe"></a>2.livenessProbe</h1><pre><code class="hljs">apiVersion: apps/v1kind: Deploymentmetadata:  name: test-nginxspec:  selector:    matchLabels:      app: test  replicas: 3  template:    metadata:      labels:        app: test    spec:      containers:        - name: nginx          image: registry.cn-hangzhou.aliyuncs.com/swords/nginx          imagePullPolicy: IfNotPresent          ports:            - containerPort: 80              name: nginx              protocol: TCP          resources:            limits:              cpu: 200m              memory: 200Mi            requests:              cpu: 100m              memory: 100Mi          livenessProbe:            httpGet:              path: /              port: 80            initialDelaySeconds: 10            periodSeconds: 5      imagePullSecrets:        - name: regcred</code></pre><h1 id="3-readinessProbe"><a href="#3-readinessProbe" class="headerlink" title="3.readinessProbe"></a>3.readinessProbe</h1><pre><code class="hljs">apiVersion: apps/v1kind: Deploymentmetadata:  name: test-nginxspec:  selector:    matchLabels:      app: test  replicas: 3  template:    metadata:      labels:        app: test    spec:      containers:        - name: nginx          image: registry.cn-hangzhou.aliyuncs.com/swords/nginx          imagePullPolicy: IfNotPresent          ports:            - containerPort: 80              name: nginx              protocol: TCP          resources:            limits:              cpu: 200m              memory: 200Mi            requests:              cpu: 100m              memory: 100Mi          livenessProbe:            httpGet:              path: /              port: 80            initialDelaySeconds: 10            periodSeconds: 5          readinessProbe:            tcpSocket:              port: 80            initialDelaySeconds: 5            periodSeconds: 5      imagePullSecrets:        - name: regcred---apiVersion: v1kind: Servicemetadata:  name: test-nginxspec:  selector:    app: test  ports:  - protocol: TCP    port: 80    targetPort: 80    </code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/m0_71518373/article/details/127781889">https://blog.csdn.net/m0_71518373/article/details/127781889</a></li><li><a href="https://blog.csdn.net/ApexPredator/article/details/131170506">https://blog.csdn.net/ApexPredator/article/details/131170506</a></li><li><a href="https://blog.csdn.net/weixin_40579389/article/details/129930165">https://blog.csdn.net/weixin_40579389/article/details/129930165</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python流程控制</title>
    <link href="/linux/PythonCondition/"/>
    <url>/linux/PythonCondition/</url>
    
    <content type="html"><![CDATA[<p>流程控制，即代码执行方式与顺序的控制，使得程序按照一定的结构进行执行，一般分为三种结构，即顺序结构、条件判断结构和循环控制结构。初学者建议以伪代码或流程图的方式将程序拆解，以培养编程语言的逻辑思维，厘清程序的控制逻辑</p><h1 id="1-顺序结构"><a href="#1-顺序结构" class="headerlink" title="1.顺序结构"></a>1.顺序结构</h1><p>顺序结构，Python默认结构，即是按照代码的位置自上而下的执行，每行代码只执行一次</p><pre><code class="hljs">&gt;&gt;&gt; print(&#39;&lt;- 欢迎使用幂运算器 -&gt;&#39;)&lt;- 欢迎使用幂运算器 -&gt;&gt;&gt;&gt; n = float(input(&#39;请输入底数:&#39;))请输入底数:9&gt;&gt;&gt; m = float(input(&#39;请输入指数:&#39;))请输入指数:3&gt;&gt;&gt; result = n ** m&gt;&gt;&gt; print(f&#39;&#123;n&#125;的&#123;m&#125;次幂等于&#123;result&#125;&#39;)9.0的3.0次幂等于729.0</code></pre><h1 id="2-条件结构"><a href="#2-条件结构" class="headerlink" title="2.条件结构"></a>2.条件结构</h1><p>条件判断，Python程序的逻辑实现，即是依据一条或多条逻辑判断语句的执行结果（True或False）决定执行的代码块，关键字为if、elif和else</p><h2 id="2-1-单分支判断结构"><a href="#2-1-单分支判断结构" class="headerlink" title="2.1 单分支判断结构"></a>2.1 单分支判断结构</h2><pre><code class="hljs">&gt;&gt;&gt; m = int(input(&#39;请输入数字:&#39;))请输入数字:-3&gt;&gt;&gt; if m &lt; 0:...     n = -m... &gt;&gt;&gt; print(f&#39;&#123;m&#125;的绝对值为&#123;n&#125;&#39;)-3的绝对值为3</code></pre><h2 id="2-2-双分支判断结构"><a href="#2-2-双分支判断结构" class="headerlink" title="2.2 双分支判断结构"></a>2.2 双分支判断结构</h2><pre><code class="hljs">&gt;&gt;&gt; age = int(input(&#39;请输入您的年龄: &#39;))请输入您的年龄: 24&gt;&gt;&gt; if age &gt;= 18:...     print(&#39;您已成年&#39;)... else:...     print(&#39;您未成年&#39;)... 您已成年</code></pre><h2 id="2-3-多分支判断结构"><a href="#2-3-多分支判断结构" class="headerlink" title="2.3 多分支判断结构"></a>2.3 多分支判断结构</h2><pre><code class="hljs">&gt;&gt;&gt; score = float(input(&#39;请输入您的分数: &#39;))请输入您的分数: 81&gt;&gt;&gt; if score &gt; 85:...     print(&#39;优&#39;)... elif score &gt; 70:...     print(&#39;良&#39;)... elif score &gt; 60:...     print(&#39;中&#39;)... else:...     print(&#39;不及格&#39;)... 良</code></pre><h2 id="2-4-嵌套判断结构"><a href="#2-4-嵌套判断结构" class="headerlink" title="2.4 嵌套判断结构"></a>2.4 嵌套判断结构</h2><pre><code class="hljs">&gt;&gt;&gt; username = input(&#39;请输入用户名: &#39;)请输入用户名: admin&gt;&gt;&gt; password = input(&#39;请输入密码: &#39;)请输入密码: admin123&gt;&gt;&gt; if username == &#39;admin&#39;:...     if password == &#39;admin123&#39;:...         print(&#39;登录成功!&#39;)...     else:...         print(&#39;密码错误!&#39;)... else:...     print(&#39;用户不存在!&#39;)... 登录成功!</code></pre><h1 id="3-循环结构"><a href="#3-循环结构" class="headerlink" title="3.循环结构"></a>3.循环结构</h1><p>循环结构，Python程序的控制实现，即是控制代码或代码块循环往复执行直到满足退出条件才能退出，关键字为for、while、break、continue和pass</p><h2 id="3-1-while循环"><a href="#3-1-while循环" class="headerlink" title="3.1 while循环"></a>3.1 while循环</h2><p>Python语言while语句表示某条件下循环执行某段程序，直到满足退出的条件才退出循环，可翻译为“当…就…，否则就结束循环”，用于处理重复执行的任务，基本形式为：</p><pre><code class="hljs">while 判断条件(condition)：    执行语句(statements)</code></pre><hr><h3 id="3-1-1-遍历循环"><a href="#3-1-1-遍历循环" class="headerlink" title="3.1.1 遍历循环"></a>3.1.1 遍历循环</h3><pre><code class="hljs">&gt;&gt;&gt; n = 100&gt;&gt;&gt; sum =0&gt;&gt;&gt; counter = 1&gt;&gt;&gt; while counter &lt;=n:...     sum = sum + counter...     counter += 1... &gt;&gt;&gt; print(&quot;1到 %d 之和为: %d&quot; % (n,sum))1到 100 之和为: 5050</code></pre><h3 id="3-1-2-else语句"><a href="#3-1-2-else语句" class="headerlink" title="3.1.2 else语句"></a>3.1.2 else语句</h3><pre><code class="hljs">&gt;&gt;&gt; while i &lt; num:...     print(i)...     i +=1... else:...     print(&quot;执行完毕，结束循环！&quot;)... 0123456789执行完毕，结束循环！</code></pre><h3 id="3-1-3-无限循环"><a href="#3-1-3-无限循环" class="headerlink" title="3.1.3 无限循环"></a>3.1.3 无限循环</h3><p>若while循环的判断语句永远为true，循环将会一直进行下去，即为无限循环</p><pre><code class="hljs">&gt;&gt;&gt; var = 1&gt;&gt;&gt; while var == 1:...     n = int(input(&quot;请输入一个数字:&quot;))...     print(&quot;你输入的数字为 %d&quot; %n)... 请输入一个数字:3你输入的数字为 3请输入一个数字:2你输入的数字为 2请输入一个数字:67你输入的数字为 67请输入一个数字:</code></pre><h2 id="3-2-for循环"><a href="#3-2-for循环" class="headerlink" title="3.2 for循环"></a>3.2 for循环</h2><p>Python语言for循环通常用于遍历可迭代对象，如字符串、列表或字典，即每次从这些对象中取出一个元素进行操作，直到全部遍历完成才退出循环，语法格式如下：</p><pre><code class="hljs">for 目标 in 迭代对象    循环体</code></pre><h3 id="3-2-1-元素循环"><a href="#3-2-1-元素循环" class="headerlink" title="3.2.1 元素循环"></a>3.2.1 元素循环</h3><pre><code class="hljs">&gt;&gt;&gt; for l in &#39;Python&#39;:...     print(&quot;当前字母为：%s&quot; %l)... 当前字母为：P当前字母为：y当前字母为：t当前字母为：h当前字母为：o当前字母为：n</code></pre><h3 id="3-2-2-索引循环"><a href="#3-2-2-索引循环" class="headerlink" title="3.2.2 索引循环"></a>3.2.2 索引循环</h3><pre><code class="hljs">&gt;&gt;&gt; database = [&#39;mysql&#39;,&#39;redis&#39;,&#39;mongodb&#39;]&gt;&gt;&gt; for index in range(len(database)):...     print(&quot;当前数据库为：%s&quot; %database[index])... 当前数据库为：mysql当前数据库为：redis当前数据库为：mongodb</code></pre><h2 id="3-3-嵌套循环"><a href="#3-3-嵌套循环" class="headerlink" title="3.3 嵌套循环"></a>3.3 嵌套循环</h2><p>嵌套循环即是循环体内再定义另一个循环，但建议循环嵌套不超过3层，否则将会降低程序执行效率</p><pre><code class="hljs">#!/usr/bin/python3# -*- coding: utf-8 -*-week = [&#39;一&#39;,&#39;二&#39;,&#39;三&#39;,&#39;四&#39;,&#39;五&#39;]for w in week:    print(f&quot;今天是周&#123;w&#125;&quot;)    for i in range(1,9):        if i &gt; 6:            print(f&quot;已经学习&#123;i&#125;小时了，可以休息了&quot;)        else:            print(f&quot;学习不足&#123;i&#125;小时，继续努力&quot;)</code></pre><h2 id="3-4-循环控制"><a href="#3-4-循环控制" class="headerlink" title="3.4 循环控制"></a>3.4 循环控制</h2><p>循环控制语句用于在循环中进行特定操作，如提前终止循环或跳过当前迭代、条件判断之后再执行特定操作等等</p><h3 id="3-4-1-终止循环"><a href="#3-4-1-终止循环" class="headerlink" title="3.4.1 终止循环"></a>3.4.1 终止循环</h3><p>Python语言break语句用于终止循环，即循环条件没有False或序列还没被完全迭代完成的情况下停止执行循环语句，可用于while和for循环。嵌套循环的情况下，break语句将停止执行最深层的循环，并开始执行下一行代码</p><pre><code class="hljs">&gt;&gt;&gt; for w in &#39;Python&#39;:...     if w == &#39;h&#39;:...         break...     print(&quot;当前字母为：&quot;,w)... 当前字母为： P当前字母为： y当前字母为： t</code></pre><h3 id="3-4-2-跳过循环"><a href="#3-4-2-跳过循环" class="headerlink" title="3.4.2 跳过循环"></a>3.4.2 跳过循环</h3><p>Python语言continue语句用于跳出本次循环，而不是整个循环，也即是跳过当前循环的剩余语句，然后继续进行下一轮循环，可用于while和for循环</p><pre><code class="hljs">&gt;&gt;&gt; for w in &#39;Python&#39;:...     if w == &#39;h&#39;:...         continue...     print(&quot;当前字母为：&quot;,w)... 当前字母为： P当前字母为： y当前字母为： t当前字母为： o当前字母为： n</code></pre><h3 id="3-4-3-pass语句"><a href="#3-4-3-pass语句" class="headerlink" title="3.4.3 pass语句"></a>3.4.3 pass语句</h3><p>Python语言pass语句是空语句，即不做任务操作，一般用作站位语句，保持程序结构的完整性</p><pre><code class="hljs">&gt;&gt;&gt; for w in &#39;Python&#39;:...     if w == &#39;h&#39;:...         pass...     print(&quot;当前字母为：&quot;,w)... 当前字母为： P当前字母为： y当前字母为： t当前字母为： h当前字母为： o当前字母为： n</code></pre><h3 id="3-4-4-if-x2F-else语句"><a href="#3-4-4-if-x2F-else语句" class="headerlink" title="3.4.4 if&#x2F;else语句"></a>3.4.4 if&#x2F;else语句</h3><p>if&#x2F;else语句用于循环中的条件判断，即根据条件判断的结构执行不同的操作</p><pre><code class="hljs">#!/usr/bin/python3# -*- coding: utf-8 -*-video = &#123;&#39;电影&#39;: 268,&#39;电视剧&#39;: 32,&#39;纪录片&#39;: 18&#125;for key,value in video.items():    if value &gt; 30:       print(f&quot;&#123;key&#125;观看数量达标&quot;)    else:       print(f&quot;&#123;key&#125;观看过少&quot;)</code></pre>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prometheus监控系统部署</title>
    <link href="/linux/Prometheus/"/>
    <url>/linux/Prometheus/</url>
    
    <content type="html"><![CDATA[<p>Prometheus是基于Go语言构建的由SoundCloud开发的开源监控告警系统，基本原理是通过HTTP协议周期性拉取被监控组件的状态，任意组件只要提供对应的HTTP接口即可接入，而不再需要任何SDK或其他集成过程</p><h1 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h1><h2 id="1-Prometheus"><a href="#1-Prometheus" class="headerlink" title="1.Prometheus"></a>1.Prometheus</h2><p>Prometheus Server，Prometheus监控系统的核心部分，负责实现监控数据的收集、存储及数据查询</p><h2 id="2-Exporters"><a href="#2-Exporters" class="headerlink" title="2.Exporters"></a>2.Exporters</h2><p>Exporter，监控数据采集端，即通过通过http方式暴露的API，Prometheus Server周期性地访问Exporter提供的Endpoint端点，以获取需要采集的监控数据。一个Exporter即是一个监控实例Instance，分布于不同节点的实例组即构成任务Job，如node_exporter系统监控实例所组成的服务器资源监控任务、mysql_exporter监控实例所组成的数据库资源监控任务等。Exporter分为两大类：</p><ul><li>直接采集，直接内置了对Prometheus监控的支持，如cAdvisor、Kubernetes等</li><li>间接采集，原有监控目标不支持prometheus，需要通过prometheus提供的客户端库编写监控采集程序，如Mysql Exporter、JMX Exporter等</li></ul><h2 id="3-AlertManager"><a href="#3-AlertManager" class="headerlink" title="3.AlertManager"></a>3.AlertManager</h2><p>告警管理，处理prometheus基于PromQL创建的告警规则所生成的告警信息，可集成邮件、Slack或webhook自定义报警</p><h2 id="4-PushGateway"><a href="#4-PushGateway" class="headerlink" title="4.PushGateway"></a>4.PushGateway</h2><p>推送服务，用于被监控端无法直接提供http形式的exporter，将内部网络数据主动push到gateway，prometheus再从中将监控数据以pull方式拉取过来</p><h2 id="5-Web-UI"><a href="#5-Web-UI" class="headerlink" title="5.Web UI"></a>5.Web UI</h2><p>Prometheus系统内置的简单Web控制台，用于查询指标、查看配置信息或Service Discovery等，比较简陋，通常是配合Grafana展示监控指标</p><h1 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h1><h2 id="1-拉取监控数据"><a href="#1-拉取监控数据" class="headerlink" title="1.拉取监控数据"></a>1.拉取监控数据</h2><p>Prometheus server定期从配置好的jobs或者exporters中拉取metrics，或从push gateway拉取metrics，当然也可从其他Prometheus server拉取metrics</p><h2 id="2-存储监控数据"><a href="#2-存储监控数据" class="headerlink" title="2.存储监控数据"></a>2.存储监控数据</h2><p>Prometheus server在本地存储收集到的metrics，并运行定义好的alert.rules，通过一定规则进行清理和整理数据，并把得到的结果存储到新的时间序列中，同时向Alertmanager推送警报</p><h2 id="3-可视化监控数据"><a href="#3-可视化监控数据" class="headerlink" title="3.可视化监控数据"></a>3.可视化监控数据</h2><p>Prometheus通过PromQL和其他可视化的展现收集的数据，支持很多方式的图表可视化，如Grafana、自带的Promdash以及自身提供的模板引擎等。Promenade还提供HTTP API的查询方式，自定义所需要的输出</p><h1 id="监控指标"><a href="#监控指标" class="headerlink" title="监控指标"></a>监控指标</h1><h2 id="1-服务器监控"><a href="#1-服务器监控" class="headerlink" title="1.服务器监控"></a>1.服务器监控</h2><h3 id="1-1-CPU"><a href="#1-1-CPU" class="headerlink" title="1.1 CPU"></a>1.1 CPU</h3><p>CPU整体使用量、用户态百分比、内核态百分比，以及单个CPU的使用量、等待队列长度、I&#x2F;O等待百分比、CPU消耗最多的进程、上下文切换次数、缓存命中率等</p><h3 id="1-2-内存"><a href="#1-2-内存" class="headerlink" title="1.2 内存"></a>1.2 内存</h3><p>内存整体使用量、剩余量、内存占用最高的进程、交换分区大小、缺页异常等</p><h3 id="1-3-网络I-x2F-O"><a href="#1-3-网络I-x2F-O" class="headerlink" title="1.3 网络I&#x2F;O"></a>1.3 网络I&#x2F;O</h3><p>单个网卡的上行流量、下行流量、网络延迟、丢包率等</p><h3 id="1-4-磁盘I-x2F-O"><a href="#1-4-磁盘I-x2F-O" class="headerlink" title="1.4 磁盘I&#x2F;O"></a>1.4 磁盘I&#x2F;O</h3><p>硬盘读写速率、IOPS、磁盘用量、读写延迟等</p><h2 id="2-网络监控"><a href="#2-网络监控" class="headerlink" title="2.网络监控"></a>2.网络监控</h2><h3 id="2-1-网络性能监控"><a href="#2-1-网络性能监控" class="headerlink" title="2.1 网络性能监控"></a>2.1 网络性能监控</h3><p>网络监测、网络实时流量监控（网络延迟、访问量、成功率）、历史数据统计、汇总和历史数据分析等指标</p><h3 id="2-2-网络检测"><a href="#2-2-网络检测" class="headerlink" title="2.2 网络检测"></a>2.2 网络检测</h3><p>网络攻击，如DDoS攻击，通过分析异常流量来确定网络攻击行为</p><h3 id="2-3-设备监控"><a href="#2-3-设备监控" class="headerlink" title="2.3 设备监控"></a>2.3 设备监控</h3><p>数据中心内的多种网络设备监控，如路由器、防火墙和交换机等硬件设备，通过snmp协议收集监控数据</p><h2 id="3-存储监控"><a href="#3-存储监控" class="headerlink" title="3.存储监控"></a>3.存储监控</h2><h3 id="3-1-块存储"><a href="#3-1-块存储" class="headerlink" title="3.1 块存储"></a>3.1 块存储</h3><p>存储块的读写速率、IOPS、读写延迟，磁盘用量等</p><h3 id="3-2-文件存储"><a href="#3-2-文件存储" class="headerlink" title="3.2 文件存储"></a>3.2 文件存储</h3><p>文件存储的文件系统inode、读写速度、目录权限等</p><h3 id="3-3-分布式存储"><a href="#3-3-分布式存储" class="headerlink" title="3.3 分布式存储"></a>3.3 分布式存储</h3><p>分布式系统监控，不同的存储系统有不同的指标，如ceph的OSD、MON运行状态，各种状态pg的数量及集群IOPS信息等</p><h3 id="3-4-存储设备"><a href="#3-4-存储设备" class="headerlink" title="3.4 存储设备"></a>3.4 存储设备</h3><p>对于构建在x86服务器上的存储设备，设备监控通过每个存储节点上的采集器统一收集磁盘、SSD、网卡等设备信息。存储厂商通常以黑盒方式提供商业存储设备，自带监控功能，可监控设备的运行状态，性能和容量</p><h2 id="4-中间件监控"><a href="#4-中间件监控" class="headerlink" title="4.中间件监控"></a>4.中间件监控</h2><h3 id="4-1-消息中间件"><a href="#4-1-消息中间件" class="headerlink" title="4.1 消息中间件"></a>4.1 消息中间件</h3><p>RabbitMQ、Kafka等</p><h3 id="4-2-Web服务中间件"><a href="#4-2-Web服务中间件" class="headerlink" title="4.2 Web服务中间件"></a>4.2 Web服务中间件</h3><p>Tomcat、Jetty等</p><h3 id="4-3-缓存中间件"><a href="#4-3-缓存中间件" class="headerlink" title="4.3 缓存中间件"></a>4.3 缓存中间件</h3><p>Redis、Memcached等</p><h3 id="4-4-数据库中间件"><a href="#4-4-数据库中间件" class="headerlink" title="4.4 数据库中间件"></a>4.4 数据库中间件</h3><p>MySQL、PostgreSQL等</p><h2 id="5-应用程序监控"><a href="#5-应用程序监控" class="headerlink" title="5.应用程序监控"></a>5.应用程序监控</h2><p>APM主要是针对应用程序的监控，包括应用程序的运行状态监控，性能监控，日志监控及调用链跟踪等。调用链跟踪是指追踪整个请求过程（从用户发送请求，通常指浏览器或者应用客户端）到后端API服务以及API服务和关联的中间件，或者其他组件之间的调用，构建出一个完整的调用拓扑结构，不仅如此，APM 还可以监控组件内部方法的调用层次（Controller–&gt;service–&gt;Dao）获取每个函数的执行耗时，从而为性能调优提供数据支撑</p><hr><h1 id="1-下载安装包"><a href="#1-下载安装包" class="headerlink" title="1.下载安装包"></a>1.下载安装包</h1><pre><code class="hljs">wget https://github.com/prometheus/prometheus/releases/download/v2.45.0/prometheus-2.45.0.linux-amd64.tar.gz</code></pre><h1 id="2-安装Prometheus"><a href="#2-安装Prometheus" class="headerlink" title="2.安装Prometheus"></a>2.安装Prometheus</h1><pre><code class="hljs">tar -xzvf prometheus-2.45.0.linux-amd64.tar.gzsudo mv prometheus-2.45.0.linux-amd64 /usr/local/prometheussudo mkdir -p /usr/local/prometheus/data</code></pre><h1 id="3-创建启动脚本"><a href="#3-创建启动脚本" class="headerlink" title="3.创建启动脚本"></a>3.创建启动脚本</h1><pre><code class="hljs">sudo vi /lib/systemd/system/prometheus.service [Unit]Description=Prometheus ServerDocumentation=https://prometheus.io/After=network.target[Service]Type=simpleUser=rootGroup=rootWorkingDirectory=/usr/local/prometheusExecStart=/usr/local/prometheus/prometheus --web.listen-address=0.0.0.0:9090 --config.file=/usr/local/prometheus/prometheus.yml --web.enable-lifecycleExecReload=/bin/kill -s HUP $MAINPIDExecStop=/bin/kill -s QUIT $MAINPIDRestart=on-failure[Install]WantedBy=multi-user.target</code></pre><h1 id="4-启动Prometheus"><a href="#4-启动Prometheus" class="headerlink" title="4.启动Prometheus"></a>4.启动Prometheus</h1><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start prometheus.servicesudo systemctl enable prometheus.service</code></pre><h1 id="5-验证Prometheus"><a href="#5-验证Prometheus" class="headerlink" title="5.验证Prometheus"></a>5.验证Prometheus</h1><pre><code class="hljs">curl http://127.0.0.1:9090/metrics</code></pre><h1 id="6-部署Grafana"><a href="#6-部署Grafana" class="headerlink" title="6.部署Grafana"></a>6.部署Grafana</h1><h2 id="6-1-安装Grafana"><a href="#6-1-安装Grafana" class="headerlink" title="6.1 安装Grafana"></a>6.1 安装Grafana</h2><pre><code class="hljs">sudo docker run -it -d -p 3000:3000 --name grafana grafana</code></pre><h2 id="6-2-登录Grafana，更改默认密码"><a href="#6-2-登录Grafana，更改默认密码" class="headerlink" title="6.2 登录Grafana，更改默认密码"></a>6.2 登录Grafana，更改默认密码</h2><ul><li>地址：<a href="http://ip:3000/">http://ip:3000</a></li><li>账号密码：admin&#x2F;admin</li></ul><h2 id="6-3-配置数据源"><a href="#6-3-配置数据源" class="headerlink" title="6.3 配置数据源"></a>6.3 配置数据源</h2><p>Configuration — &gt; Data Sources —&gt; Add data source —&gt; Prometheus</p><p><img src="/img/wiki/prometheus/prometheus01.jpg" alt="prometheus01"></p><h2 id="6-3-导入监控模板"><a href="#6-3-导入监控模板" class="headerlink" title="6.3 导入监控模板"></a>6.3 导入监控模板</h2><p>Dashboards — &gt; New — &gt; Import — &gt; 模版ID：3662</p><p><img src="/img/wiki/prometheus/prometheus02.jpg" alt="prometheus02"></p><h2 id="6-4-验证Grafana"><a href="#6-4-验证Grafana" class="headerlink" title="6.4 验证Grafana"></a>6.4 验证Grafana</h2><p><img src="/img/wiki/prometheus/prometheus03.jpg" alt="prometheus03"></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://andyoung.blog.csdn.net/article/details/122040410">https://andyoung.blog.csdn.net/article/details/122040410</a></li><li><a href="https://blog.csdn.net/qq_43164571/article/details/112655017">https://blog.csdn.net/qq_43164571/article/details/112655017</a></li><li><a href="https://blog.csdn.net/weixin_43883625/article/details/129756413">https://blog.csdn.net/weixin_43883625/article/details/129756413</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>云原生</tag>
      
      <tag>Prometheus</tag>
      
      <tag>监控告警</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes集群故障处理</title>
    <link href="/linux/KubernetesTroubleshooting/"/>
    <url>/linux/KubernetesTroubleshooting/</url>
    
    <content type="html"><![CDATA[<h1 id="1-worker节点重启后一直notready状态"><a href="#1-worker节点重启后一直notready状态" class="headerlink" title="1.worker节点重启后一直notready状态"></a>1.worker节点重启后一直notready状态</h1><h2 id="故障背景"><a href="#故障背景" class="headerlink" title="故障背景"></a>故障背景</h2><p>某个worker节点服务器重启，重启后处于notready状态</p><h2 id="故障现象"><a href="#故障现象" class="headerlink" title="故障现象"></a>故障现象</h2><p>集群master节点正常，docker运行正常，机器负载正常，kubelet反复启动</p><h2 id="故障原因"><a href="#故障原因" class="headerlink" title="故障原因"></a>故障原因</h2><p>刚接触kubernets不太熟悉，排查半天不得要领，直到查看kubelet日志发现有报错：”Failed to run kubelet” err&#x3D;”failed to run Kubelet: misconfiguration: kubelet cgroup driver: “systemd” is different from docker cgroup driver:”cgroupfs”“，百度后是由docker默认的cgroup驱动cgroupfs与kubernets默认的驱动systemd不一致所导致，将docker驱动配置为systemd，两者保持一致即可</p><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><h3 id="1-配置docker的cgroup驱动为systemd"><a href="#1-配置docker的cgroup驱动为systemd" class="headerlink" title="1.配置docker的cgroup驱动为systemd"></a>1.配置docker的cgroup驱动为systemd</h3><pre><code class="hljs">sudo vi /etc/docker/daemon.json&#123;  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],  &quot;registry-mirrors&quot;: [&quot;https://10.254.100.200&quot;]&#125;</code></pre><h3 id="2-重启docker"><a href="#2-重启docker" class="headerlink" title="2.重启docker"></a>2.重启docker</h3><pre><code class="hljs">sudo systemctl restart docker.service</code></pre><hr><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://bbs.deepin.org/zh/post/198565">https://bbs.deepin.org/zh/post/198565</a></li><li><a href="https://blog.csdn.net/qq_45034687/article/details/129022342">https://blog.csdn.net/qq_45034687/article/details/129022342</a></li></ul><h1 id="2-POD启动失败"><a href="#2-POD启动失败" class="headerlink" title="2.POD启动失败"></a>2.POD启动失败</h1><h2 id="故障背景-1"><a href="#故障背景-1" class="headerlink" title="故障背景"></a>故障背景</h2><p>客户要求集群新加一批机器，node节点添加正常，下午集中监控告警一批POD反复重启</p><h2 id="故障现象-1"><a href="#故障现象-1" class="headerlink" title="故障现象"></a>故障现象</h2><p>1.重启的POD状态为CrashLoopBackOff，退出码为139，且都是同一镜像拉起的容器<br>2.只有调度到新加入的机器才会出现异常，且手动起容器也不行，容器也无日志。其他机器运行正常</p><h2 id="故障原因-1"><a href="#故障原因-1" class="headerlink" title="故障原因"></a>故障原因</h2><p>问题比较奇怪，镜像一直能正常拉起容器，证明镜像没问题，但在新机器就不行，推翻了这个结论。一开始感觉无从下手，有用的信息就只有退出码，程序也不复杂，其实就是一个java程序。百度139的退出码，可能是代码问题，或是基础镜像问题，在其他机器上能正常运行即可排除代码问题。接下来排查基础镜像，发现镜像基于centos6构建，尝试将基础镜像换成centos7构建后即正常运行</p><h2 id="根因探究"><a href="#根因探究" class="headerlink" title="根因探究"></a>根因探究</h2><p>根本原因其实是基础镜像低于2.14版本的glibc所采用的vsyscall内核调用机制由于风险过多，新的内核已经弃用，以可模拟兼容vsyscall的vDSO机制代替，由此产生报错</p><h2 id="解决方案-1"><a href="#解决方案-1" class="headerlink" title="解决方案"></a>解决方案</h2><p>将基础镜像换成centos7重新构建即可</p><hr><h2 id="参考文档-1"><a href="#参考文档-1" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://www.qedev.com/linux/340487.html">https://www.qedev.com/linux/340487.html</a></li><li><a href="https://blog.csdn.net/hakula007/article/details/125786395">https://blog.csdn.net/hakula007/article/details/125786395</a></li><li><a href="https://help.aliyun.com/document_detail/154067.html?spm=a2c6h.12873639.0.0.5b1224a46IyNve">https://help.aliyun.com/document_detail/154067.html?spm=a2c6h.12873639.0.0.5b1224a46IyNve</a></li></ul><h1 id="3-文件上传失败报错"><a href="#3-文件上传失败报错" class="headerlink" title="3.文件上传失败报错"></a>3.文件上传失败报错</h1><h2 id="故障背景-2"><a href="#故障背景-2" class="headerlink" title="故障背景"></a>故障背景</h2><p>转码业务的源数据来源于客户的媒资系统，通过FTP定时扫描获取，客户手动上传需求较少，前期规划与测试可满足平常的业务需求</p><h2 id="故障现象-2"><a href="#故障现象-2" class="headerlink" title="故障现象"></a>故障现象</h2><p>手动上传视频文件失败，测试发现报错信息为：413 Request Entity Too Large</p><h2 id="故障原因-2"><a href="#故障原因-2" class="headerlink" title="故障原因"></a>故障原因</h2><p>初步判断为前端代理nginx的client_max_body_size参数设置过小导致，更改为更大的值后还是报错。因为nginx的后端代理地址为ingress地址，原因即为nginx-ingress-controller配置文件client_max_body_size参数设置过小导致，调大即可</p><h2 id="解决方案-2"><a href="#解决方案-2" class="headerlink" title="解决方案"></a>解决方案</h2><pre><code class="hljs">kubectl -n ingress-nginx edit configmaps ingress-nginx-controllerapiVersion: v1kind: ConfigMapmetadata:  name: nginx-custom-configuration  namespace: kube-system  labels:    k8s-app: nginx-ingress-controllerdata:  proxy-body-size: &quot;4096m&quot;</code></pre><hr><h2 id="参考文档-2"><a href="#参考文档-2" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/28963911">https://zhuanlan.zhihu.com/p/28963911</a></li><li><a href="https://www.cnblogs.com/seablogs/articles/16054315.html">https://www.cnblogs.com/seablogs/articles/16054315.html</a></li></ul><h1 id="4-集群初始化失败报错"><a href="#4-集群初始化失败报错" class="headerlink" title="4.集群初始化失败报错"></a>4.集群初始化失败报错</h1><h2 id="故障背景-3"><a href="#故障背景-3" class="headerlink" title="故障背景"></a>故障背景</h2><p>客户业务集群构建于centos7，自己私有服务器上用Ubuntu18.04搭建1.20版集群</p><h2 id="故障现象-3"><a href="#故障现象-3" class="headerlink" title="故障现象"></a>故障现象</h2><p>kubeadm初始化失败报错，具体信息为：Failed to create pod sandbox: open &#x2F;run&#x2F;systemd&#x2F;resolve&#x2F;resolv.conf: no such file or directory</p><h2 id="故障原因-3"><a href="#故障原因-3" class="headerlink" title="故障原因"></a>故障原因</h2><p>debian系的DNS配置文件由systemd-resolved服务管理，该服务没有开机自启，导致&#x2F;etc&#x2F;resolv.conf未生成，连不上网络</p><h2 id="解决方案-3"><a href="#解决方案-3" class="headerlink" title="解决方案"></a>解决方案</h2><pre><code class="hljs">sudo mkdir -p /run/systemd/resolvesudo ln -s /etc/resolv.conf /run/systemd/resolve/resolv.conf</code></pre><hr><h2 id="参考文档-3"><a href="#参考文档-3" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://github.com/kubernetes/kubeadm/issues/1124">https://github.com/kubernetes/kubeadm/issues/1124</a></li></ul><h1 id="5-PVC不可用"><a href="#5-PVC不可用" class="headerlink" title="5.PVC不可用"></a>5.PVC不可用</h1><h2 id="故障背景-4"><a href="#故障背景-4" class="headerlink" title="故障背景"></a>故障背景</h2><p>集群版本为V1.20.12，创建StorageClass正常未报错</p><h2 id="故障现象-4"><a href="#故障现象-4" class="headerlink" title="故障现象"></a>故障现象</h2><p>StorageClass创建的PVC一直处于Pending状态，具体信息为：waiting for a volume to be created,either by external provisioner “nfs-client” or manually created by system administrator，意思是SC的供应器未就绪，处于不可用状态。查看供应器日志，发现报错信息：Subsets:[]v1.EndpointSubset(nil)’ due to:‘selfLink was empty, can’t make reference’. Will not report event:‘Normal’ ‘LeaderElection’ ‘nfs-client-provisioner-5c4d67788-xwq2j_4cc3307a-bd8a-11ed-938e-5efc4b2ec335 became<br>leader’</p><h2 id="故障原因-4"><a href="#故障原因-4" class="headerlink" title="故障原因"></a>故障原因</h2><p>1.20版本的BUG导致，selfLink was empty在v1.20之前都存在，v1.20之后将会被删除</p><h2 id="解决方案-4"><a href="#解决方案-4" class="headerlink" title="解决方案"></a>解决方案</h2><pre><code class="hljs">sudo vi /etc/kubernetes/manifests/kube-apiserver.yaml# 每个master节点apiserver配置文件添加该参数，之后kubeadm部署的集群会自动加载部署pod- --feature-gates=RemoveSelfLink=false</code></pre><hr><h2 id="参考文档-4"><a href="#参考文档-4" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner/issues/25">https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner/issues/25</a></li></ul><h1 id="6-service创建失败"><a href="#6-service创建失败" class="headerlink" title="6.service创建失败"></a>6.service创建失败</h1><h2 id="故障背景-5"><a href="#故障背景-5" class="headerlink" title="故障背景"></a>故障背景</h2><p>创建service时失败报错</p><h2 id="故障现象-5"><a href="#故障现象-5" class="headerlink" title="故障现象"></a>故障现象</h2><p>创建service时报错，Invalid value: 38096: provided port is not in the valid range. The range of valid ports is 30000-32767</p><h2 id="故障原因-5"><a href="#故障原因-5" class="headerlink" title="故障原因"></a>故障原因</h2><p>kubernetes集群service默认端口是在30000-32767，给service指定的端口超出这个范围，导致报错</p><h2 id="解决方案-5"><a href="#解决方案-5" class="headerlink" title="解决方案"></a>解决方案</h2><pre><code class="hljs">sudo vi /etc/kubernetes/manifests/kube-apiserver.yaml- --service-node-port-range=1-65535</code></pre><ul><li>注：高可用集群的每个apiserver都需要进行修改</li></ul><hr><h2 id="参考文档-5"><a href="#参考文档-5" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://blog.csdn.net/weixin_45290734/article/details/120197991">https://blog.csdn.net/weixin_45290734/article/details/120197991</a></li></ul><h1 id="7-服务器断电重启后docker启动失败报错"><a href="#7-服务器断电重启后docker启动失败报错" class="headerlink" title="7.服务器断电重启后docker启动失败报错"></a>7.服务器断电重启后docker启动失败报错</h1><h2 id="故障背景-6"><a href="#故障背景-6" class="headerlink" title="故障背景"></a>故障背景</h2><p>上次新加的几台机器与其他机器不在同一机房，一次机房断电，随后重启服务器</p><h2 id="故障现象-6"><a href="#故障现象-6" class="headerlink" title="故障现象"></a>故障现象</h2><p>服务器重启后发现所有节点的docker都无法启动，报错信息为：cgroups: cgroup mountpoint does not exist: unknown</p><h2 id="故障原因-6"><a href="#故障原因-6" class="headerlink" title="故障原因"></a>故障原因</h2><p>从报错信息看，是由cgroup挂载点不存在所导致。百度上找到临时解决方案，即新建挂载点目录再重新挂载。原来以为是bug，随后找到一篇github上的帖子，说像是第三方打包或OS的问题，官方无能为力。目前只能以临时方案解决，个人猜测是docker版本问题，因为为了保持集群节点的一致性，docker统一为19版本，可能新版本操作系统有某些限制。测试环境试验最新版本没有这个问题</p><h2 id="解决方案-6"><a href="#解决方案-6" class="headerlink" title="解决方案"></a>解决方案</h2><pre><code class="hljs">sudo mkdir /sys/fs/cgroup/systemdsudo mount -t cgroup -o none,name=systemd cgroup /sys/fs/cgroup/system</code></pre><hr><h2 id="参考文档-6"><a href="#参考文档-6" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://github.com/moby/moby/issues/36016">https://github.com/moby/moby/issues/36016</a></li><li><a href="https://blog.csdn.net/weixin_41481443/article/details/130729906">https://blog.csdn.net/weixin_41481443/article/details/130729906</a></li></ul><h1 id="8-docker监控容器启动失败"><a href="#8-docker监控容器启动失败" class="headerlink" title="8.docker监控容器启动失败"></a>8.docker监控容器启动失败</h1><h2 id="故障背景-7"><a href="#故障背景-7" class="headerlink" title="故障背景"></a>故障背景</h2><p>Centos7维护周期将尽，考虑以RockyLinux9.2进行代替，配置Prometheus监控时docker容器启动失败</p><h2 id="故障现象-7"><a href="#故障现象-7" class="headerlink" title="故障现象"></a>故障现象</h2><p>docker运行cadvisor容器时报错：cadvisor.go:146 Failed to create a Container Manager: mountpoint for cpu not found</p><h2 id="故障原因-7"><a href="#故障原因-7" class="headerlink" title="故障原因"></a>故障原因</h2><p>cadvisor镜像版本太旧，需要升级为v0.46.0以上版本</p><h2 id="解决方案-7"><a href="#解决方案-7" class="headerlink" title="解决方案"></a>解决方案</h2><p>拉取v0.46.0以后版本的cadvisor镜像，重新启动</p><hr><h2 id="参考文档-7"><a href="#参考文档-7" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://github.com/google/cadvisor/issues/1943">https://github.com/google/cadvisor/issues/1943</a></li><li><a href="https://blog.csdn.net/wuxingge/article/details/133071181">https://blog.csdn.net/wuxingge/article/details/133071181</a></li></ul><h1 id="9-集群不可访问"><a href="#9-集群不可访问" class="headerlink" title="9.集群不可访问"></a>9.集群不可访问</h1><h2 id="故障背景-8"><a href="#故障背景-8" class="headerlink" title="故障背景"></a>故障背景</h2><p>Ubuntu18.04搭建本地kubernetes集群，服务器重启之后集群不可访问</p><h2 id="故障现象-8"><a href="#故障现象-8" class="headerlink" title="故障现象"></a>故障现象</h2><p>集群所有节点均不可用，etcd集群未启动，kubelet组件报错：Failed to create pod sandbox: open &#x2F;run&#x2F;systemd&#x2F;resolve&#x2F;resolv.conf: no such file or directory</p><h2 id="故障原因-8"><a href="#故障原因-8" class="headerlink" title="故障原因"></a>故障原因</h2><p>kubelet组件的报错报错信息很明显指向DNS解析，Ubuntu的&#x2F;etc&#x2F;resolv.conf是一个软连接，由系统服务器systemd-resolved生成。systemd-resolved是systemd附带的解析DNS的代理，将本地的DNS请求转发到外部的DNS服务器，也即是DNS解析的过程在本地又新增了一层，影响效率，也不利于管理，建议关闭，手动设置resolv.conf</p><h2 id="解决方案-8"><a href="#解决方案-8" class="headerlink" title="解决方案"></a>解决方案</h2><h3 id="1-禁用systemd-resolved"><a href="#1-禁用systemd-resolved" class="headerlink" title="1.禁用systemd-resolved"></a>1.禁用systemd-resolved</h3><pre><code class="hljs">sudo systemctl stop systemd-resolvedsudo systemctl disable systemd-resolved</code></pre><h3 id="2-创建resolv-conf文件"><a href="#2-创建resolv-conf文件" class="headerlink" title="2.创建resolv.conf文件"></a>2.创建resolv.conf文件</h3><pre><code class="hljs">sudo rm -rf /etc/resolv.confsudo vi /etc/resolv.confnameserver 8.8.8.8</code></pre><h3 id="3-锁定resolv-conf文件"><a href="#3-锁定resolv-conf文件" class="headerlink" title="3.锁定resolv.conf文件"></a>3.锁定resolv.conf文件</h3><pre><code class="hljs">sudo chattr +i /etc/resolv.conf</code></pre><hr><h2 id="参考文档-8"><a href="#参考文档-8" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://www.cnblogs.com/czlong/p/15802384.html">https://www.cnblogs.com/czlong/p/15802384.html</a></li><li><a href="https://www.cnblogs.com/xzlive/p/17139520.html">https://www.cnblogs.com/xzlive/p/17139520.html</a></li></ul><h1 id="10-node节点OOM"><a href="#10-node节点OOM" class="headerlink" title="10.node节点OOM"></a>10.node节点OOM</h1><h2 id="故障背景-9"><a href="#故障背景-9" class="headerlink" title="故障背景"></a>故障背景</h2><p>集群为1.9.9版本，机器也比较旧，资源也很紧张</p><h2 id="故障现象-9"><a href="#故障现象-9" class="headerlink" title="故障现象"></a>故障现象</h2><p>故障群大批量告警，业务方反馈系统某功能异常</p><h2 id="故障原因-9"><a href="#故障原因-9" class="headerlink" title="故障原因"></a>故障原因</h2><p>大批量KubeDeploymentReplicasMismatch告警说明是集群节点异常，kubectl describe nodes命令显示SystemOOM。因为集群只有4个node节点，不好进行驱逐，因此先重启Pod恢复业务。系统OOM即内存超限，导致容器被kill，监控也显示内存突然飙升，所以接下来就要定位是哪个进程导致的内存突增。运行命令dmesg | grep -i “Killed process”查看被kill掉的进程，发现ruby进程所占内存异常</p><h2 id="解决方案-9"><a href="#解决方案-9" class="headerlink" title="解决方案"></a>解决方案</h2><p>ruby是日志采集与投递程序td-agent，排查之后发现是配置文件导致日志不能解析，并一直循环，修改配置并重启之后恢复正常</p>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python运算符</title>
    <link href="/linux/PythonOperator/"/>
    <url>/linux/PythonOperator/</url>
    
    <content type="html"><![CDATA[<p>运算符用于对变量及其值执行操作，运算符操作的变量或值称为运算数，Python运算要求数据类型一致，类型不完全一致时将以高类型为准进行运算，如整数与浮点数计算将会得到精度更高的浮点数</p><h1 id="1-算术运算符"><a href="#1-算术运算符" class="headerlink" title="1.算术运算符"></a>1.算术运算符</h1><p>算术运算符用于数据的基础运算，加减乘除、取余取整、次幂运算等</p><pre><code class="hljs">&gt;&gt;&gt; print(9 + 8.0)17.0&gt;&gt;&gt; print(8 - 9)-1&gt;&gt;&gt; print(8 * 9)72&gt;&gt;&gt; print(9 / 8)1.125# 取余运算&gt;&gt;&gt; print(10 % 20)1# 取整运算&gt;&gt;&gt; print(9 // 8)1# 幂运算，即3的4次幂&gt;&gt;&gt; print(3 ** 4)81</code></pre><h1 id="2-比较运算符"><a href="#2-比较运算符" class="headerlink" title="2.比较运算符"></a>2.比较运算符</h1><p>比较运算符用余比较两个对象之间的关系，若成立则返回布尔值True，否则返回布尔值False</p><pre><code class="hljs">&gt;&gt;&gt; print(9 == 9.00)True&gt;&gt;&gt; print(9 &gt; 9.00)False&gt;&gt;&gt; print(9 &lt; 9.00)False&gt;&gt;&gt; print(9 &gt;= 9.00)True&gt;&gt;&gt; print(9 &lt;= 9.00)True</code></pre><h1 id="3-赋值运算符"><a href="#3-赋值运算符" class="headerlink" title="3.赋值运算符"></a>3.赋值运算符</h1><pre><code class="hljs">&gt;&gt;&gt; x = 2&gt;&gt;&gt; y = 3# 相当于x = x + y&gt;&gt;&gt; x +=y&gt;&gt;&gt; print(x)5# 相当于x = x - y&gt;&gt;&gt; x -= y&gt;&gt;&gt; print(x)2# 相当于x = x * y&gt;&gt;&gt; x *= y&gt;&gt;&gt; print(x)6# 相当于x = x / y&gt;&gt;&gt; x /= y&gt;&gt;&gt; print(x)2.0# 相当于x = x % y&gt;&gt;&gt; x %= y&gt;&gt;&gt; print(x)2.0</code></pre><h1 id="4-逻辑运算符"><a href="#4-逻辑运算符" class="headerlink" title="4.逻辑运算符"></a>4.逻辑运算符</h1><p>逻辑运算符用于判断语句以判断表达式是否成立，常用的逻辑符为and、or和not，对应于数学概念的逻辑运算</p><pre><code class="hljs">&gt;&gt;&gt; print(9 &gt; 8 and 8 &gt; 7)True&gt;&gt;&gt; print(9 &gt; 8 and 8 &lt; 7)False&gt;&gt;&gt; print(9 &gt; 8 or 8 &lt; 7)True&gt;&gt;&gt; print(not 9 &lt; 8)True&gt;&gt;&gt; print(not 9 &gt; 8)False</code></pre><h1 id="5-成员运算符"><a href="#5-成员运算符" class="headerlink" title="5.成员运算符"></a>5.成员运算符</h1><p>成员运算符属于Python所独有，用于判断对象是否某个集合的元素之一，运算速度很快</p><pre><code class="hljs">&gt;&gt;&gt; print(&#39;node02&#39; in list)True&gt;&gt;&gt; print(&#39;node01&#39; not in list)False</code></pre><h1 id="6-位运算符"><a href="#6-位运算符" class="headerlink" title="6.位运算符"></a>6.位运算符</h1><p>位运算符是将数字转换为二进制再进行计算</p><pre><code class="hljs"># a的二进制数为1001&gt;&gt;&gt; a = 9# b的二进制数为1000&gt;&gt;&gt; b = 8# a和b按位与运算，即两个数相应位都为1时则该位的结果为1,否则为0，运算结果为1000，转换为十进制则为8&gt;&gt;&gt; print(a &amp; b)8# a和b按位或运算，即两个数只要对应的二进位有一个为1时结果位就为1，运算结果为1001，转换为十进制则为9&gt;&gt;&gt; print(a | b)9# a按位取反运算，即对每个二进制位取反，也就是把1变为0，把0变为1，计算公式为-x-1，也即-10&gt;&gt;&gt; print(～a)-10</code></pre><h1 id="7-身份运算符"><a href="#7-身份运算符" class="headerlink" title="7.身份运算符"></a>7.身份运算符</h1><p>身份运算符用于比较两个对象的存储单元，即是判断两个变量引用对象是否为同一个，或者说是否为同一块内存空间，而&#x3D;&#x3D;用于判断引用变量的值是否相等</p><pre><code class="hljs">&gt;&gt;&gt; a = (1,2,3)&gt;&gt;&gt; b = [1,2.3]&gt;&gt;&gt; c = a&gt;&gt;&gt; print(a is b)False&gt;&gt;&gt; print(a is c)True&gt;&gt;&gt; print(a is not c)False&gt;&gt;&gt; </code></pre><h1 id="8-运算符优先级"><a href="#8-运算符优先级" class="headerlink" title="8.运算符优先级"></a>8.运算符优先级</h1><p>运算符从最高到最低的优先级为：</p><pre><code class="hljs">**(指数运算) --&gt; ~ + -(位反、一元加号和减号) --&gt; * / % //(乘、除、取模和取整) --&gt; + -(加法减法) --&gt; &gt;&gt; &lt;&lt;(右移、左移) --&gt; &amp;(位与) --&gt; ^ |(位运算) --&gt; &lt;= &lt; &gt; &gt;=(比较运算) --&gt; &lt;&gt; == !=(等于) --&gt; = %= /= //= -= += *= **=(赋值运算) --&gt; is is not(身份运算) --&gt; in not in(成员运算) --&gt; not and or(逻辑运算)</code></pre>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python数据类型</title>
    <link href="/linux/PythonVariables/"/>
    <url>/linux/PythonVariables/</url>
    
    <content type="html"><![CDATA[<p>变量，即程序用于存储计算数据的内存单元，包含变量的标识、名称和数据。变量名即为存储单元的命名，通过给变量名赋值可以随时改变变量的值。Python变量使用前需进行赋值，赋值之后变量才会被创建并存储于内存，且赋值不需要类型声明</p><p>计算机程序的计算数据为适应不同的场景，将数据划分为不同的具备各自特点的类型，以便于高效的处理与展示数据，即数据类型即决定变量类型</p><h1 id="1-数字类型"><a href="#1-数字类型" class="headerlink" title="1.数字类型"></a>1.数字类型</h1><p>数字类型用于存储数学意义上的数值，是不可改变数据类型，即改变数字类型的数据将会重新分配全新的对象。Python支持四种不同的数字类型，即有符号整型int、长整型long（也可代表八进制和十六进制）、浮点型float和复数complex</p><pre><code class="hljs">&gt;&gt;&gt; print(100)100&gt;&gt;&gt; print(0.24)0.24&gt;&gt;&gt; print(-100)-100</code></pre><h2 id="1-1-数字类型转换"><a href="#1-1-数字类型转换" class="headerlink" title="1.1 数字类型转换"></a>1.1 数字类型转换</h2><p>int()函数将变量x转化为整数，即只保留变量的整数部分，变量x可以是整数、浮点数或整数字符串<br>float()函数将变量x转化为浮点数，变量x可以是整数、浮点数或者数字字符串</p><pre><code class="hljs">&gt;&gt;&gt; x = 100&gt;&gt;&gt; int(x)100&gt;&gt;&gt; float(x)100.0&gt;&gt;&gt; y = 0.24&gt;&gt;&gt; float(y)0.24&gt;&gt;&gt; int(y)0# 八进制数，以0o开头&gt;&gt;&gt; print(0o12)10# 十六进制数，以0x开头&gt;&gt;&gt; print(0xff00)65280</code></pre><h1 id="2-布尔类型"><a href="#2-布尔类型" class="headerlink" title="2.布尔类型"></a>2.布尔类型</h1><p>布尔类型，逻辑运算的计算结果，通常用于判断条件是否成立。Python布尔类型只有两个值，即True与False，首字母一定是大写</p><pre><code class="hljs">&gt;&gt;&gt; print(1 &gt; 2)False&gt;&gt;&gt; print(-1 &lt; 2+6)True</code></pre><ul><li>注：None，即空值，Python程序特殊值，是空类型，不是布尔类型，也不能理解为0，因为0是整数类型</li></ul><h1 id="3-字符串类型"><a href="#3-字符串类型" class="headerlink" title="3.字符串类型"></a>3.字符串类型</h1><p>String，即字符串，由数字、字母、下划线等一串字符组成，表示为文本类型的数据，使用单引号’’或双引号””创建，三引号’’’则表示多行字符串。若字符串本身包含单引号或双引号，则用\标识，如‘I&#39;m &quot;OK\ “!’，字符串内容为I’m “OK”!</p><h2 id="3-1-字符串定义"><a href="#3-1-字符串定义" class="headerlink" title="3.1 字符串定义"></a>3.1 字符串定义</h2><pre><code class="hljs"># 空字符串&gt;&gt;&gt; var = &#39;&#39;&gt;&gt;&gt; var = &quot;Hello Python!&quot;&gt;&gt;&gt; print(var)Hello Python!&gt;&gt;&gt; var = &#39;Hello &quot;Python&quot;!&#39;Hello &quot;Python&quot;!# len()函数获取字符串的长度&gt;&gt;&gt; print(len(var))15</code></pre><h2 id="3-2-字符串运算"><a href="#3-2-字符串运算" class="headerlink" title="3.2 字符串运算"></a>3.2 字符串运算</h2><h3 id="3-2-1-字符串拼接"><a href="#3-2-1-字符串拼接" class="headerlink" title="3.2.1 字符串拼接"></a>3.2.1 字符串拼接</h3><p>字符串拼接，即字符串之间的加法运算，运算符为+，运算结果的数据类型还是字符串，作用类似于火车车厢的拼接</p><pre><code class="hljs">&gt;&gt;&gt; str1 = &quot;Hello&quot;&gt;&gt;&gt; str2 = &quot;!&quot;&gt;&gt;&gt; print(str1+str2)Hello!&gt;&gt;&gt; print(str2+str1)!Hello</code></pre><h3 id="3-2-2-字符串连接"><a href="#3-2-2-字符串连接" class="headerlink" title="3.2.2 字符串连接"></a>3.2.2 字符串连接</h3><p>字符串连接，即是字符串的重复，运算符为*，运算结果的数据类型还是字符串，作用相当于字符串的复制</p><pre><code class="hljs">&gt;&gt;&gt; str = &quot;哈哈&quot;&gt;&gt;&gt; print(str*2)哈哈哈哈</code></pre><h3 id="3-2-3-成员运算"><a href="#3-2-3-成员运算" class="headerlink" title="3.2.3 成员运算"></a>3.2.3 成员运算</h3><p>成员运算，即是判断某个字符或字符串是否是其成员，运算结果的数据类型是布尔型</p><pre><code class="hljs">&gt;&gt;&gt; print(&quot;H&quot; in str)False&gt;&gt;&gt; str = &quot;Hello !&quot;&gt;&gt;&gt; print(&quot;H&quot; in str)True&gt;&gt;&gt; print(&quot;H&quot; not in str)False&gt;&gt;&gt; print(&quot;e&quot; not in str)False&gt;&gt;&gt; print(&quot;*&quot; not in str)True</code></pre><h2 id="3-3-字符串索引"><a href="#3-3-字符串索引" class="headerlink" title="3.3 字符串索引"></a>3.3 字符串索引</h2><p>字符串是序列不可变的数据类型，每个字符都是序列的成员，都有属于自己的编号，类似于房间编号，即为索引，通过索引可以取出字符串的成员，取值顺序有两种：</p><ul><li>正索引，从左到右索引，默认从0开始，最大范围为字符串长度减1</li><li>负索引，从右到左索引，默认从-1开始，最大范围为字符串开头</li></ul><hr><p>字符串索引取值由三部分构成，即字符串变量名或字符串本身、英文中括号、索引</p><pre><code class="hljs">&gt;&gt;&gt; str = &quot;Hello!&quot;&gt;&gt;&gt; print(str)Hello!&gt;&gt;&gt; print(str[1])e&gt;&gt;&gt; print(str[-3])l</code></pre><h2 id="3-4-字符串切片"><a href="#3-4-字符串切片" class="headerlink" title="3.4 字符串切片"></a>3.4 字符串切片</h2><p>字符串切片，即是截取字符串的部分成员重新组成一个字符串，也即是获取其子串，语法格式为：字符串[star:end:step]，其中star为开始索引，默认为0；end为结束索引，默认字符串的长度，为空也表示字符串的长度；step为步长，可为正整数或负整数，默认为1，表示依次取值，为2表示跳过一个字符取值，为负整数则表示负索引顺序，依此类推</p><pre><code class="hljs">&gt;&gt;&gt; str = &quot;Kubernetes&quot;&gt;&gt;&gt; print(str[1:6])ubern&gt;&gt;&gt; print(str[1:6:1])ubern&gt;&gt;&gt; print(str[1:6:2])uen&gt;&gt;&gt; print(str[-1:-8:-2])stne&gt;&gt;&gt; print(str[-8:-2])bernet&gt;&gt;&gt; print(str[3:-2])ernet&gt;&gt;&gt; print(str[:-2])Kubernet&gt;&gt;&gt; print(str[3::-2])eu</code></pre><ul><li>注：截取的字符串包含开始索引的字符但不包含结束索引的字符，即左闭右开</li></ul><h2 id="3-5-转义字符"><a href="#3-5-转义字符" class="headerlink" title="3.5 转义字符"></a>3.5 转义字符</h2><p>转义字符，用于表示具有特殊含义或无法常规用普通字符表达的特殊字符，如换行符、制表符等，由反斜杠+特殊字符组成</p><pre><code class="hljs"># 反斜杠\&gt;&gt;&gt; print(&quot;\\&quot;)\print(&quot;\\User\\admin&quot;)\User\admin# 单引号&gt;&gt;&gt; print(&quot;\&#39;&quot;)&#39;# 双引号&gt;&gt;&gt; print(&quot;\&quot;&quot;)&quot;# 响铃&gt;&gt;&gt; print(&quot;\a&quot;)&gt;&gt;&gt; print(&quot;Error\a&quot;)Error# 制表符，即tab键&gt;&gt;&gt; print(&quot;123\t45&quot;)12345# 退格符&gt;&gt;&gt; print(&quot;123\b45&quot;)# 换行符&gt;&gt;&gt; print(&quot;123\n456&quot;)123456# 回车符，光标移动到初始位置，回车符之前的内容将被覆盖&gt;&gt;&gt; print(&quot;123\r45&quot;)453# 行尾续行符&gt;&gt;&gt; str = &#39;123&#39; \...       &#39; 45&#39;&gt;&gt;&gt; print(str)123 45</code></pre><h1 id="4-列表类型"><a href="#4-列表类型" class="headerlink" title="4.列表类型"></a>4.列表类型</h1><p>List，即列表，是可有序可重复的数据集，支持字符、数字、字符串以及嵌套列表，使用方括号[]创建，是Python中使用最频繁的数据类型。Python列表数据结构是表现为一个可变长度的顺序存储架构，每一个位置存放的都是对象的指针</p><h2 id="4-1-列表定义"><a href="#4-1-列表定义" class="headerlink" title="4.1 列表定义"></a>4.1 列表定义</h2><pre><code class="hljs"># 空列表&gt;&gt;&gt; list = []&gt;&gt;&gt; stu = [&#39;John&#39;,&#39;Bob&#39;,&#39;LiLei&#39;]</code></pre><h2 id="4-2-列表运算"><a href="#4-2-列表运算" class="headerlink" title="4.2 列表运算"></a>4.2 列表运算</h2><h3 id="4-2-1-列表连接"><a href="#4-2-1-列表连接" class="headerlink" title="4.2.1 列表连接"></a>4.2.1 列表连接</h3><p>列表连接，即是列表的合并，运算符为+</p><pre><code class="hljs">&gt;&gt;&gt; list1 = [&#39;1&#39;,&#39;2&#39;,&#39;3&#39;]&gt;&gt;&gt; list2 = [&#39;01&#39;,&#39;02&#39;,&#39;03&#39;]&gt;&gt;&gt; print(list1+list2)[&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;01&#39;, &#39;02&#39;, &#39;03&#39;]</code></pre><h3 id="4-2-2-列表乘法"><a href="#4-2-2-列表乘法" class="headerlink" title="4.2.2 列表乘法"></a>4.2.2 列表乘法</h3><p>列表乘法，即是列表的复制，运算符为*</p><pre><code class="hljs">&gt;&gt;&gt; print(list2*2)[&#39;01&#39;, &#39;02&#39;, &#39;03&#39;, &#39;01&#39;, &#39;02&#39;, &#39;03&#39;]&gt;&gt;&gt; print(list2*3)[&#39;01&#39;, &#39;02&#39;, &#39;03&#39;, &#39;01&#39;, &#39;02&#39;, &#39;03&#39;, &#39;01&#39;, &#39;02&#39;, &#39;03&#39;]</code></pre><h3 id="4-2-3-成员运算"><a href="#4-2-3-成员运算" class="headerlink" title="4.2.3 成员运算"></a>4.2.3 成员运算</h3><p>元素运算，即是判断某个元素是否在列表中</p><pre><code class="hljs">&gt;&gt;&gt; print(&#39;01&#39; in list2)True&gt;&gt;&gt; print(&#39;1&#39; in list2)False&gt;&gt;&gt; print(&#39;1&#39; not in list2)True&gt;&gt;&gt; print(&#39;01&#39; not in list2)False</code></pre><h2 id="4-3-列表索引"><a href="#4-3-列表索引" class="headerlink" title="4.3 列表索引"></a>4.3 列表索引</h2><p>列表的每个元素都有各自的编号，也即列表的索引，通过索引可以取出列表的元素，跟字符串一样</p><pre><code class="hljs">&gt;&gt;&gt; list = [&#39;a&#39;,&#39;b&#39;,&#39;3&#39;]&gt;&gt;&gt; print(list[1])b&gt;&gt;&gt; test = list[-1]&gt;&gt;&gt; print(test)3# 创建嵌套列表，即列表的某个元素或某几个元素的数据类型为列表&gt;&gt;&gt; list = [&#39;Lilei&#39;,[&#39;no&#39;,&#39;001&#39;],[&#39;age&#39;,&#39;18&#39;]]# 嵌套列表2次索引取值语法为：列表[索引][索引]，&gt;&gt;&gt; print(list[2][1])18</code></pre><h2 id="4-4-列表切片"><a href="#4-4-列表切片" class="headerlink" title="4.4 列表切片"></a>4.4 列表切片</h2><p>列表切片的语法与字符串切片一致，也是通过索引</p><pre><code class="hljs">&gt;&gt;&gt; list = [&quot;Lilei&quot;,[&quot;No&quot;,&quot;001&quot;],&quot;F&quot;,&quot;18&quot;,[&quot;170CM&quot;,&quot;60KG&quot;]]&gt;&gt;&gt; print(list[1:4])[[&#39;No&#39;, &#39;001&#39;], &#39;F&#39;, &#39;18&#39;]&gt;&gt;&gt; print(list[1:5])[[&#39;No&#39;, &#39;001&#39;], &#39;F&#39;, &#39;18&#39;, [&#39;170CM&#39;, &#39;60KG&#39;]]&gt;&gt;&gt; print(list[-2::2])[&#39;18&#39;]&gt;&gt;&gt; print(list[:3:22])[&#39;Lilei&#39;]&gt;&gt;&gt; print(list[:3:-2])[[&#39;170CM&#39;, &#39;60KG&#39;]]</code></pre><h2 id="4-5-列表遍历"><a href="#4-5-列表遍历" class="headerlink" title="4.5 列表遍历"></a>4.5 列表遍历</h2><pre><code class="hljs">&gt;&gt;&gt; list = [&#39;node01&#39;,&#39;node02&#39;,&#39;node03&#39;]&gt;&gt;&gt; for i in list:...     print(i)... node01node02node03</code></pre><h1 id="5-元组类型"><a href="#5-元组类型" class="headerlink" title="5.元组类型"></a>5.元组类型</h1><p>Tuple，即元组，是有序不可变的数据集，相当于只读列表，使用圆括号()创建，相比于列表，由于创建时间和占用空间更小所以处理速度更快，更适合多线程环境，代码也更安全</p><h2 id="5-1-元组定义"><a href="#5-1-元组定义" class="headerlink" title="5.1 元组定义"></a>5.1 元组定义</h2><pre><code class="hljs">&gt;&gt;&gt; tuple = (&quot;node01&quot;,&quot;node02&quot;,&quot;nopde03&quot;)&gt;&gt;&gt; print(tuple)(&#39;node01&#39;, &#39;node02&#39;, &#39;nopde03&#39;)</code></pre><h2 id="5-2-元组运算"><a href="#5-2-元组运算" class="headerlink" title="5.2 元组运算"></a>5.2 元组运算</h2><h3 id="5-2-1-元组连接"><a href="#5-2-1-元组连接" class="headerlink" title="5.2.1 元组连接"></a>5.2.1 元组连接</h3><p>元组连接，即是元组的合并，运算符为+</p><pre><code class="hljs">&gt;&gt;&gt; tuple1 = (&quot;master01&quot;,&quot;master02&quot;,&quot;master03&quot;)&gt;&gt;&gt; tuple2 = (&quot;node01&quot;,&quot;node02&quot;,&quot;node03&quot;)&gt;&gt;&gt; tuple3 = tuple&gt;&gt;&gt; print(tuple1+tuple2)(&#39;master01&#39;, &#39;master02&#39;, &#39;master03&#39;, &#39;node01&#39;, &#39;node02&#39;, &#39;node03&#39;)&gt;&gt;&gt; print(tuple2+tuple1)(&#39;node01&#39;, &#39;node02&#39;, &#39;node03&#39;, &#39;master01&#39;, &#39;master02&#39;, &#39;master03&#39;)</code></pre><h3 id="5-2-2-元组乘法"><a href="#5-2-2-元组乘法" class="headerlink" title="5.2.2 元组乘法"></a>5.2.2 元组乘法</h3><p>元组乘法，即是元组的复制，运算符为*</p><pre><code class="hljs"># 定义单元素元组时需要加上一个逗号，否则()就将表示改变运算优先级的圆括号，而不是元组类型&gt;&gt;&gt; tuple = (&quot;master&quot;,)&gt;&gt;&gt; print(tuple*3)(&#39;master&#39;, &#39;master&#39;, &#39;master&#39;)</code></pre><h3 id="5-2-3-成员运算"><a href="#5-2-3-成员运算" class="headerlink" title="5.2.3 成员运算"></a>5.2.3 成员运算</h3><pre><code class="hljs">&gt;&gt;&gt; tuple = (&quot;node01&quot;,&quot;node02&quot;,&quot;node03&quot;)&gt;&gt;&gt; print(&quot;node03&quot; in tuple)True&gt;&gt;&gt; print(&quot;node00&quot; in tuple)False&gt;&gt;&gt; print(&quot;node00&quot; not in tuple)True&gt;&gt;&gt; print(&quot;node01&quot; not in tuple)False</code></pre><h2 id="5-3-元组索引"><a href="#5-3-元组索引" class="headerlink" title="5.3 元组索引"></a>5.3 元组索引</h2><p>元组的索引用法与列表一致</p><pre><code class="hljs">&gt;&gt;&gt; print(tuple[2])node03&gt;&gt;&gt; print(tuple[-2])node02</code></pre><h2 id="5-4-元组切片"><a href="#5-4-元组切片" class="headerlink" title="5.4 元组切片"></a>5.4 元组切片</h2><p>元组的切片用法与列表一致</p><pre><code class="hljs">&gt;&gt;&gt; tuple = (&quot;Lilei&quot;,[&quot;No&quot;,&quot;001&quot;],&quot;F&quot;,&quot;18&quot;,[&quot;170CM&quot;,&quot;60KG&quot;])&gt;&gt;&gt; print(tuple[1:4])([&#39;No&#39;, &#39;001&#39;], &#39;F&#39;, &#39;18&#39;)&gt;&gt;&gt; print(tuple[1:5])([&#39;No&#39;, &#39;001&#39;], &#39;F&#39;, &#39;18&#39;, [&#39;170CM&#39;, &#39;60KG&#39;])&gt;&gt;&gt; print(tuple[-2:2])()&gt;&gt;&gt; print(tuple[:3:2])(&#39;Lilei&#39;, &#39;F&#39;)&gt;&gt;&gt; print(tuple[:3:-2])([&#39;170CM&#39;, &#39;60KG&#39;],)</code></pre><h2 id="5-5-元组遍历"><a href="#5-5-元组遍历" class="headerlink" title="5.5 元组遍历"></a>5.5 元组遍历</h2><pre><code class="hljs">&gt;&gt;&gt; tuple = (&#39;node01&#39;,&#39;node02&#39;,&#39;node03&#39;)&gt;&gt;&gt; for i in tuple:...     print(i)... node01node02node03</code></pre><h1 id="6-集合类型"><a href="#6-集合类型" class="headerlink" title="6.集合类型"></a>6.集合类型</h1><p>Set，即集合，是无序不重复且可变的数据集，使用花括号{}创建，个元素以逗号,分隔，是数学概念集合的Python实现。由于集合的底层存储是基于哈希方式的实现，所以具有查找速度快的优势，且其数据元素不可重复，特别适用于需要大量查找和去重的应用场景，如爬虫不重复的链接地址</p><h2 id="6-1-集合定义"><a href="#6-1-集合定义" class="headerlink" title="6.1 集合定义"></a>6.1 集合定义</h2><pre><code class="hljs">&gt;&gt;&gt; set = &#123;&quot;node01&quot;,&quot;node01&quot;,&quot;node01&quot;&#125;&gt;&gt;&gt; print(set)&#123;&#39;node01&#39;&#125;&gt;&gt;&gt; set = &#123;&quot;node01&quot;,&quot;node02&quot;,&quot;node03&quot;&#125;&gt;&gt;&gt; print(set)&#123;&#39;node03&#39;, &#39;node02&#39;, &#39;node01&#39;&#125;&gt;&gt;&gt; len(set)3</code></pre><h2 id="6-2-集合运算"><a href="#6-2-集合运算" class="headerlink" title="6.2 集合运算"></a>6.2 集合运算</h2><h3 id="6-2-1-交集运算"><a href="#6-2-1-交集运算" class="headerlink" title="6.2.1 交集运算"></a>6.2.1 交集运算</h3><pre><code class="hljs">&gt;&gt;&gt; set1 = &#123;1,2,3&#125;&gt;&gt;&gt; set2 = &#123;2,3,4&#125;&gt;&gt;&gt; set1 &amp; set2&#123;2, 3&#125;</code></pre><h3 id="6-2-1-并集运算"><a href="#6-2-1-并集运算" class="headerlink" title="6.2.1 并集运算"></a>6.2.1 并集运算</h3><pre><code class="hljs">&gt;&gt;&gt; set1 | set2&#123;1, 2, 3, 4&#125;</code></pre><h3 id="6-2-3-差集运算"><a href="#6-2-3-差集运算" class="headerlink" title="6.2.3 差集运算"></a>6.2.3 差集运算</h3><pre><code class="hljs">&gt;&gt;&gt; set1 - set2&#123;1&#125;&gt;&gt;&gt; set2 - set1&#123;4&#125;</code></pre><h3 id="6-2-4-成员运算"><a href="#6-2-4-成员运算" class="headerlink" title="6.2.4 成员运算"></a>6.2.4 成员运算</h3><pre><code class="hljs">&gt;&gt;&gt; set = &#123;&quot;master&quot;,&quot;node01&quot;,&quot;node02&quot;&#125;&gt;&gt;&gt; print(&quot;node03&quot; in set)False&gt;&gt;&gt; print(&quot;node03&quot; not in set)True&gt;&gt;&gt; print(&quot;node02&quot; not in set)False&gt;&gt;&gt; print(&quot;node02&quot; in set)True</code></pre><h3 id="6-2-5-比较运算"><a href="#6-2-5-比较运算" class="headerlink" title="6.2.5 比较运算"></a>6.2.5 比较运算</h3><p>集合的比较运算就是数学中集合的包含关系的体现，&#x3D;&#x3D;表示相等，!&#x3D;表示不等，&lt;&#x3D;表示子集，&lt;表示真子集</p><pre><code class="hljs">&gt;&gt;&gt; set1 = &#123;1,2,3,4,5,6&#125;&gt;&gt;&gt; set2 = &#123;1,2,3,4,5&#125;&gt;&gt;&gt; set3 = set1&gt;&gt;&gt; print(set1==set2,set1==set3,set2!=set1)False True True&gt;&gt;&gt; print(set2&lt;set1,set3&lt;=set2,set1&lt;set3)True False False</code></pre><h2 id="6-3-集合遍历"><a href="#6-3-集合遍历" class="headerlink" title="6.3 集合遍历"></a>6.3 集合遍历</h2><p>集合是无序数据类型，所以没有索引，不能通过引用索引来访问集合的元素，只能进行循环遍历</p><pre><code class="hljs">set = &#123;&quot;master&quot;,&quot;node01&quot;,&quot;node02&quot;&#125;&gt;&gt;&gt; for x in set:...     print(x)... masternode01node02</code></pre><h1 id="7-字典类型"><a href="#7-字典类型" class="headerlink" title="7.字典类型"></a>7.字典类型</h1><p>Dict，即字典，是有序可变的键值对数据集，由索引(key)和它对应的值value组成，通过唯一的Key进行存取（重复Key则会被覆盖），使用花括号{}创建，元素以逗号,隔开，元素的k&#x2F;v由冒号:连接，是典型的映射类数据。字典可精确描述不定长、可变、散列的数据类型，底层存储基于hash散列算法实现，具有非常快的查取和插入速度，根据key的值计算value的地址，可用于关键字存储、提取值</p><h2 id="7-1-字典定义"><a href="#7-1-字典定义" class="headerlink" title="7.1 字典定义"></a>7.1 字典定义</h2><pre><code class="hljs">&gt;&gt;&gt; dict = &#123;&#125;&gt;&gt;&gt; print(dict)&#123;&#125;&gt;&gt;&gt; dict = &#123;&quot;name&quot;:&quot;LiLei&quot;,&quot;no&quot;:&quot;001&quot;,&quot;age&quot;:&quot;15&quot;&#125;&gt;&gt;&gt; print(dict)&#123;&#39;name&#39;: &#39;LiLei&#39;, &#39;no&#39;: &#39;001&#39;, &#39;age&#39;: &#39;15&#39;&#125;&gt;&gt;&gt; len(dict)3</code></pre><h2 id="7-2-字典访问"><a href="#7-2-字典访问" class="headerlink" title="7.2 字典访问"></a>7.2 字典访问</h2><p>通过方括号内引用字典元素的key来访问字典的元素</p><pre><code class="hljs">&gt;&gt;&gt; name = dict[&quot;name&quot;]&gt;&gt;&gt; print(name)LiLei</code></pre><h2 id="7-3-字典修改"><a href="#7-3-字典修改" class="headerlink" title="7.3 字典修改"></a>7.3 字典修改</h2><h3 id="7-3-1-元素添加"><a href="#7-3-1-元素添加" class="headerlink" title="7.3.1 元素添加"></a>7.3.1 元素添加</h3><pre><code class="hljs">&gt;&gt;&gt; dict[&quot;sex&quot;] = &quot;F&quot;&gt;&gt;&gt; print(dict)&#123;&#39;name&#39;: &#39;LiLei&#39;, &#39;no&#39;: &#39;001&#39;, &#39;age&#39;: &#39;15&#39;, &#39;sex&#39;: &#39;F&#39;&#125;</code></pre><h3 id="7-3-2-元素修改"><a href="#7-3-2-元素修改" class="headerlink" title="7.3.2 元素修改"></a>7.3.2 元素修改</h3><pre><code class="hljs">&gt;&gt;&gt; print(dict)&#123;&#39;name&#39;: &#39;LiLei&#39;, &#39;no&#39;: &#39;001&#39;, &#39;age&#39;: &#39;15&#39;, &#39;sex&#39;: &#39;F&#39;&#125;&gt;&gt;&gt; dict[&quot;age&quot;] = 16&gt;&gt;&gt; print(dict)&#123;&#39;name&#39;: &#39;LiLei&#39;, &#39;no&#39;: &#39;001&#39;, &#39;age&#39;: 16, &#39;sex&#39;: &#39;F&#39;&#125;</code></pre><h3 id="7-3-3-元素删除"><a href="#7-3-3-元素删除" class="headerlink" title="7.3.3 元素删除"></a>7.3.3 元素删除</h3><p>字典的元素删除使用del关键字</p><pre><code class="hljs">&gt;&gt;&gt; dict = &#123;&quot;name&quot;:&quot;Lilei&quot;,&quot;no&quot;:&quot;15&quot;,&quot;age&quot;:&quot;15&quot;&#125;&gt;&gt;&gt; print(dict)&#123;&#39;name&#39;: &#39;Lilei&#39;, &#39;no&#39;: &#39;15&#39;, &#39;age&#39;: &#39;15&#39;&#125;&gt;&gt;&gt; del dict[&quot;age&quot;]&gt;&gt;&gt; print(dict)&#123;&#39;name&#39;: &#39;Lilei&#39;, &#39;no&#39;: &#39;15&#39;&#125;</code></pre><h2 id="7-4-字典遍历"><a href="#7-4-字典遍历" class="headerlink" title="7.4 字典遍历"></a>7.4 字典遍历</h2><p>字典遍历可通过key、value、key&#x2F;value进行循环</p><h3 id="7-4-1-key循环遍历"><a href="#7-4-1-key循环遍历" class="headerlink" title="7.4.1 key循环遍历"></a>7.4.1 key循环遍历</h3><p>默认情况下是循环key，也可指定循环key</p><pre><code class="hljs">&gt;&gt;&gt; for item in dict:...     print(item)  ... namenoage&gt;&gt;&gt; for item in dict.keys():...     print(item)...      namenoage</code></pre><h3 id="7-4-2-value循环遍历"><a href="#7-4-2-value循环遍历" class="headerlink" title="7.4.2 value循环遍历"></a>7.4.2 value循环遍历</h3><blockquote><blockquote><blockquote><p>for item in dict.values():<br>…     print(item)<br>…<br>Lilei<br>15<br>15</p></blockquote></blockquote></blockquote><h4 id="7-4-3-key-x2F-value循环遍历"><a href="#7-4-3-key-x2F-value循环遍历" class="headerlink" title="7.4.3 key&#x2F;value循环遍历"></a>7.4.3 key&#x2F;value循环遍历</h4><pre><code class="hljs">&gt;&gt;&gt; for key,value in dict.items():...     print(key,value)... name Lileino 15age 15</code></pre>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python基础语法</title>
    <link href="/linux/Python/"/>
    <url>/linux/Python/</url>
    
    <content type="html"><![CDATA[<p>Python，开源免费的面向对象的解释型计算机高级程序设计语言，简单易用，应用广泛，功能强大。Python是一门推崇极简主义的编程语言，具有很强的可读性及更有特色语法结构</p><h1 id="发展历史"><a href="#发展历史" class="headerlink" title="发展历史"></a>发展历史</h1><p>Python最初是由就职于荷兰国家数学和计算机科学研究所的荷兰人Guido van Rossum（吉多·范罗苏姆）于1989年底发明，并于1991年发行第一个公开发行版。其诞生极具戏曲性，据Guido自述记载，Python语言是在圣诞节期间为了打发无聊的时间而开发，之所以会选择Python作为名字，是因为Guido是Monty Python戏剧团的忠实粉丝</p><p>Python是在ABC语言的基础上发展而来，设计初衷是成为ABC语言的替代品。ABC语言虽然是一款功能强大的高级语言，但由于不开放的原因，导致并没有得到普及。基于此，Guido在开发Python之初就决定将其开源。Python中不仅添加了许多ABC语言没有的功能，还为其设计了各种丰富而强大的库，利用这些Python库，程序员可以把使用其它语言制作的各类模块（尤其是C语言和 C++）很轻松地“黏连”在一起，因此Python又被称为胶水语言。现在，Python由一个核心开发团队在维护，Guido van Rossum仍然占据着至关重要的作用，指导其进展</p><p>Python因其简单易用、优雅干净、学习成本低及标准库与第三方库众多的特点，自面世之日起即发展迅猛，大受欢迎：</p><ul><li>2004年，Python使用率呈线性增长，不断受到编程者的欢迎和喜爱</li><li>2010年，Python荣膺TIOBE『2010 年度最佳编程语言』桂冠</li><li>2017年，IEEE Spectrum发布的『2017 年度编程语言』排行榜中，Python 位居第1位</li><li>2018年，Python斩获TIOBE『2018 年度最佳编程语言』第1名</li><li>2020年和2021年，Python连续两年摘得TIOBE『年度最佳编程语言』桂冠，且市场份额仍在持续提升。由于人工智能、大数据等行业的发展，使得Python近几年增姿迅猛，甚至超越了C、C++和Java，成为编程语言排行榜冠军</li></ul><h1 id="功能特点"><a href="#功能特点" class="headerlink" title="功能特点"></a>功能特点</h1><h2 id="1-语法简单"><a href="#1-语法简单" class="headerlink" title="1.语法简单"></a>1.语法简单</h2><p>关键字相对较少，结构简单，代码清晰和一个明确定义的语法，阅读、学习、维护更加简单。</p><h2 id="2-封装较好"><a href="#2-封装较好" class="headerlink" title="2.封装较好"></a>2.封装较好</h2><p>Python封装较深，屏蔽了很多底层细节，如Python自动管理内存，即需要时自动分配，不需要时自动释放，优点是使用方便，无需顾虑细枝末节；缺点是令人浅尝辄止，知其然不知其所以然</p><h2 id="3-跨平台"><a href="#3-跨平台" class="headerlink" title="3.跨平台"></a>3.跨平台</h2><p>Python是解释型语言，跨平台性好，易于移植，即可通过不同的解释器将相同的源代码解释成不同平台下的机器码</p><h2 id="4-模块众多"><a href="#4-模块众多" class="headerlink" title="4.模块众多"></a>4.模块众多</h2><p>Python的模块众多，基本实现了所有的常见的功能，从简单的字符串处理，到复杂的3D图形绘制，都可快速完成。且社区发展良好，除了 Python官方提供的核心模块，很多第三方机构也会参与到模块的开发中，其中就有 Google（谷歌）、Facebook（脸书）、Microsoft（微软） 等巨头</p><h2 id="5-面向对象"><a href="#5-面向对象" class="headerlink" title="5.面向对象"></a>5.面向对象</h2><p>面向对象，即Object Oriented，是大多数现代高级语言（即第三代编程语言）都具备的特性，否则在开发中大型程序时会捉襟见肘。但不同于Java这种典型的面向对象的编程语言，强制必须以类和对象的形式来组织代码，Python虽然支持面向对象，但非强制性</p><h2 id="7-扩展性强"><a href="#7-扩展性强" class="headerlink" title="7.扩展性强"></a>7.扩展性强</h2><p>如需运行一段很快的关键代码，或是编写一些不便开放的算法，可用C或C++完成，之后从Python程序中调用，可在一定程度上弥补了运行效率慢的缺点</p><h2 id="8-互动模式"><a href="#8-互动模式" class="headerlink" title="8.互动模式"></a>8.互动模式</h2><p>互动模式的支持，可从终端输入执行代码并获得结果的语言，互动的测试和调试代码片断</p><h2 id="9-GUI编程"><a href="#9-GUI编程" class="headerlink" title="9.GUI编程"></a>9.GUI编程</h2><p>Python支持GUI可以创建和移植到许多系统调用</p><hr><p>Python是解释型语言，需预先编译，而是由解释器逐行对源码进行解释，一边解释一边执行，效率较低，速度远远慢于C&#x2F;C++、Java。因此，计算机的一些底层功能或关键算法，一般都使用C&#x2F;C++实现，只有在应用层面，如网站开发、批处理、小工具等适于解释型语言。此外，编译型语言的源代码被编译成可执行程序的过程就相当于对源码完成了加密，而Python由于没有编译过程，则对源码加密就将相当困难</p><p>相对的，编译型语言开发完成之后则只需通过专门的编译器即可一次性将所有的源代码都转换成特定平台下包含机器码的可执行程序,如Windows下的.exe文件，只要拥有可执行程序即可随时运行，不再需要重新编译，也即是一次编译，无限次运行</p><h1 id="应用领域"><a href="#应用领域" class="headerlink" title="应用领域"></a>应用领域</h1><p>Python应用广发，几乎所有大中型互联网公司都有应用，如国外的Google、Youtube、Dropbox，国内的百度、新浪、搜狐、腾讯、阿里、网易、淘宝、知乎、豆瓣、汽车之家、美团等等，应用领域极为广泛</p><h2 id="1-Web应用开发"><a href="#1-Web应用开发" class="headerlink" title="1.Web应用开发"></a>1.Web应用开发</h2><p>尽管PHP、JS目前依然是Web开发的主流语言，但随着Python的Web开发框架逐渐成熟，如Django、Flask、Tornado、Web2py等，开发、管理复杂的Web程序将变得更加轻松。全球最大的搜索引擎Google在其网络搜索系统中就广泛地使用了Python语言，全球最大的视频网站 Youtube、网络文件同步工具Dropbox以及豆瓣网也是基于Python的</p><h2 id="2-网络爬虫"><a href="#2-网络爬虫" class="headerlink" title="2.网络爬虫"></a>2.网络爬虫</h2><p>Python提供有很多服务于编写网络爬虫的工具，如urllib、Selenium 和 BeautifulSoup等，还提供了一个网络爬虫框架Scrapy，Google等搜索引擎公司大量地使用Python语言编写网络爬虫</p><h2 id="3-游戏开发"><a href="#3-游戏开发" class="headerlink" title="3.游戏开发"></a>3.游戏开发</h2><p>很多游戏使用Python或Lua编写游戏的逻辑，使用C++编写图形显示等高性能模块，如Sid Meier’s Civilization（文明）和 EVE（星战前夜）</p><h2 id="3-自动化运维"><a href="#3-自动化运维" class="headerlink" title="3.自动化运维"></a>3.自动化运维</h2><p>所谓自动化运维，实际上就是利用一些开源的自动化工具来管理服务器，如业界流行的基于Python开发的Ansible，用于运维工程师解决重复性的工作。且Python编写的系统管理脚本，无论是可读性、性能以及代码重度和扩展性等方面，都要优于shell编写的脚本</p><h2 id="4-人工智能"><a href="#4-人工智能" class="headerlink" title="4.人工智能"></a>4.人工智能</h2><p>人工智能的核心是机器学习，机器学习的研究可分为传统机器学习和深度学习，两者被广泛的应用于图像识别、智能驾驶、智能推荐、自然语言处理等应用方向</p><p>目前世界上优秀的人工智能学习框架，如Google的TransorFlow（神经网络框架）、FaceBook的PyTorch（神经网络框架）以及开源社区的 Karas神经网络库等，都是基于Python实现的；微软的 CNTK（认知工具包）完全支持Python；Python擅长科学计算和数据分析，支持各种数学运算，可绘制出更高质量的2D和3D图像</p><h2 id="5-科学计算"><a href="#5-科学计算" class="headerlink" title="5.科学计算"></a>5.科学计算</h2><p>Python在数据分析、可视化方面有相当完善和优秀的库，如NumPy、SciPy、Matplotlib、pandas等，可满足科学计算程序的需求，NASA在1997年以来就大量使用Python进行各种复杂的科学运算</p><hr><p>Python编码规范为PEP 8，Python Enhancement Proposal，即Python增强建议书，8代表的是Python代码的样式指南</p><h1 id="1-头部注释"><a href="#1-头部注释" class="headerlink" title="1.头部注释"></a>1.头部注释</h1><p>头部注释，又称声明编码格式，被系统或解释器调用而不是服务于代码，主要作用是标示Python解释器的位置与脚本的编码格式，几乎在所有主流的编程语言脚本中都是必需元素</p><pre><code class="hljs"># /bin/python3# codeing: utf-8</code></pre><h1 id="2-注释规范"><a href="#2-注释规范" class="headerlink" title="2.注释规范"></a>2.注释规范</h1><p>注释，即是对代码的解释，大大提高了程序代码的可读性与程序调试的效率，程序执行时将会忽略掉注释</p><p>Python单行注释以#开头，多行注释以三个单引号’’’或三个双引号”””将注释内容包含进来</p><pre><code class="hljs">#!/usr/bin/python3# coding:utf-8&quot;&quot;&quot;    @Author:songsong    @Date:2022/12/13    @Filename:3.py    @Host:cloud-server&quot;&quot;&quot;print(&quot;一定要坚持下去&quot;)print(&quot;Python学习&quot;)print(&quot;------------&quot;)&#39;&#39;&#39;    Python注释分为单行注释与多行注释两种，单行注释以#开头，    多行注释有两种写法，即6个单引号或6个双引号&#39;&#39;&#39;# 多行注释并没有强制的规范，保持统一即可print(&quot;人生苦短，我用Python&quot;)</code></pre><h1 id="3-缩进规范"><a href="#3-缩进规范" class="headerlink" title="3.缩进规范"></a>3.缩进规范</h1><p>Python的代码块不使用{}控制类、函数以及逻辑判断，而是使用缩进，且缩进量可自定但所有代码块的缩进量必须保持一致，否则将报错。建议以TAB键或4个空格作为缩进</p><p>正确缩进</p><pre><code class="hljs">#!/bin/python3# coding: utf8a = 2if a &gt; 1:    print(&quot;当前a的值大于1&quot;)else:    print(&quot;当前a的值小于1&quot;)</code></pre><p>错误缩进</p><pre><code class="hljs">#!/bin/python3# coding: utf8a = 2if a &gt; 1:# 错误缩进print(&quot;当前a的值大于1&quot;)else:    print(&quot;当前a的值小于1&quot;)</code></pre><h1 id="4-空格规范"><a href="#4-空格规范" class="headerlink" title="4.空格规范"></a>4.空格规范</h1><ul><li>运算符两侧各一个空格，数量可自定，但两侧要保持一致</li><li>逗号、分号、冒号之前不要加空格其后可以加空格</li><li>函数参数列表中</li></ul><h1 id="5-空行规范"><a href="#5-空行规范" class="headerlink" title="5.空行规范"></a>5.空行规范</h1><ul><li>函数之间和类的方法之间用空行分隔，表示一段新的代码的开始</li><li>类和函数入口之间用一行空行分隔，以突出函数入口的开始</li><li>空行的作用在于分隔两段不同功能或含义的代码，便于日后代码的维护或重构</li></ul><h1 id="6-变量命名规范"><a href="#6-变量命名规范" class="headerlink" title="6.变量命名规范"></a>6.变量命名规范</h1><p>变量，Python程序用来保存计算结果的存储单元，变量名即是</p>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Jenkins持续集成工具的安装与配置</title>
    <link href="/linux/Jenkins/"/>
    <url>/linux/Jenkins/</url>
    
    <content type="html"><![CDATA[<p>DevOps，由Development和Operations组合而成，即开发 &amp;&amp; 运维，是一组用于促进开发、运营和质量保障部门之间沟通、协作与整合的过程、方法与系统的统称</p><h3 id="软件工程模型"><a href="#软件工程模型" class="headerlink" title="软件工程模型"></a>软件工程模型</h3><p>软件系统的工程化贯穿了软件完整的生命周期，实质上就是将各项任务按照预定的框架与规程进行划分与整合，以便于清晰、直观地表达软件开发全过程，明确规定了要完成的主要活动和任务，即是软件工程模型</p><p>软件系统从零开始到最终交付大致包括如下阶段：规划、编码、构建、测试、发布、部署和维护，软件工程模型即是基于这些阶段不断进行改进与规范逐渐发展而来</p><h5 id="1-瀑布式流程"><a href="#1-瀑布式流程" class="headerlink" title="1.瀑布式流程"></a>1.瀑布式流程</h5><p>传统的软件工程开发方式，前期需求确立之后，软件开发人员花费数周和数月编写代码，把所有需求一次性开发完，然后将代码交给QA（质量保障）团队进行测试，最后将最终的发布版交给运维团队去部署。简单来说，就是等一个阶段所有工作完成之后再进入下一个阶段，由各个团队相互衔接完成整个项目的生命周期。该模式下整个软件开发流程其实是线性的孤岛式作业，产品迭代周期长，沟通成本高，灵活性差，周期动辄几周几个月，无法适应当下产品需求快速迭代的场景</p><h5 id="2-敏捷开发"><a href="#2-敏捷开发" class="headerlink" title="2.敏捷开发"></a>2.敏捷开发</h5><p>将开发任务由大拆小，开发、测试协同工作，注重开发敏捷，不重视交付敏捷</p><h5 id="3-DevOps"><a href="#3-DevOps" class="headerlink" title="3.DevOps"></a>3.DevOps</h5><p>开发、测试、运维协同工作，即持续开发 &amp;&amp; 持续交付</p><hr><h1 id="1-jar包部署方式"><a href="#1-jar包部署方式" class="headerlink" title="1.jar包部署方式"></a>1.jar包部署方式</h1><h2 id="1-1-安装jdk，配置java环境"><a href="#1-1-安装jdk，配置java环境" class="headerlink" title="1.1 安装jdk，配置java环境"></a>1.1 安装jdk，配置java环境</h2><h2 id="1-2-下载war包"><a href="#1-2-下载war包" class="headerlink" title="1.2 下载war包"></a>1.2 下载war包</h2><pre><code class="hljs">mkdir -p /usr/share/java /var/lib/jenkins cd /usr/share/java &amp;&amp; wget https://mirrors.tuna.tsinghua.edu.cn/jenkins/war/2.399/jenkins.war</code></pre><h2 id="1-3-创建启动脚本"><a href="#1-3-创建启动脚本" class="headerlink" title="1.3 创建启动脚本"></a>1.3 创建启动脚本</h2><pre><code class="hljs">vi /lib/systemd/system/jenkins.service [Unit]Requires=network.targetAfter=network.targetDescription=Jenkins Continuous Integration Server[Service]User=wwwGroup=wwwType=notifyNotifyAccess=mainRestart=on-failureSuccessExitStatus=143WorkingDirectory=/var/lib/jenkinsEnvironment=&quot;JENKINS_HOME=/var/lib/jenkins&quot;Environment=&quot;JAVA_OPTS=-Djava.awt.headless=true&quot;ExecStart=/usr/bin/java -jar /usr/share/java/jenkins.war --webroot=/var/lib/jenkins[Install]WantedBy=multi-user.target</code></pre><h2 id="1-4-启动Jenkins"><a href="#1-4-启动Jenkins" class="headerlink" title="1.4 启动Jenkins"></a>1.4 启动Jenkins</h2><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start jenkins.servicesudo systemctl enable jenkins.service</code></pre><h2 id="1-5-获取初始密码，解锁jenkins"><a href="#1-5-获取初始密码，解锁jenkins" class="headerlink" title="1.5 获取初始密码，解锁jenkins"></a>1.5 获取初始密码，解锁jenkins</h2><p><img src="/img/wiki/jenkins/jenkins-001-001.jpg" alt="jenkins-001-001"></p><hr><h2 id="1-6-安装推荐插件"><a href="#1-6-安装推荐插件" class="headerlink" title="1.6 安装推荐插件"></a>1.6 安装推荐插件</h2><p><img src="/img/wiki/jenkins/jenkins-001-002.jpg" alt="jenkins-001-002"></p><hr><h1 id="2-docker部署方式"><a href="#2-docker部署方式" class="headerlink" title="2.docker部署方式"></a>2.docker部署方式</h1><pre><code class="hljs">sudo mkdir /web/jenkinssudo chmod -R 777 /web/jenkinssudo docker run -d -it -p 8080:8080 -p 50000:50000 \--privileged=true -v /web/jenkins:/var/jenkins_home \--name jenkins jenkins</code></pre><h1 id="3-kubernetes部署方式"><a href="#3-kubernetes部署方式" class="headerlink" title="3.kubernetes部署方式"></a>3.kubernetes部署方式</h1><h2 id="3-1-安装nfs"><a href="#3-1-安装nfs" class="headerlink" title="3.1 安装nfs"></a>3.1 安装nfs</h2><pre><code class="hljs">sudo mkdir /home/project/kubernetes/jenkinssudo chmod -R 777 /home/project/kubernetes/jenkins</code></pre><h2 id="3-2-配置持久化存储"><a href="#3-2-配置持久化存储" class="headerlink" title="3.2 配置持久化存储"></a>3.2 配置持久化存储</h2><h3 id="3-2-1-创建PV、PVC"><a href="#3-2-1-创建PV、PVC" class="headerlink" title="3.2.1 创建PV、PVC"></a>3.2.1 创建PV、PVC</h3><pre><code class="hljs">vi jenkins-data.yamlapiVersion: v1kind: PersistentVolumemetadata:  # 设置PV名称  name: jenkins-data  # 设置PV标签，用于PVC的定向绑定  labels:    app: jenkins-dataspec:  # 设置存储类别  storageClassName: nfs  # 设置访问模式  accessModes:    - ReadWriteMany  # 设置PV的存储空间  capacity:    storage: 10Gi  # 设置PV的回收策略  persistentVolumeReclaimPolicy: Retain  nfs:    path: /home/project/kubernetes/jenkins    server: 192.168.100.200---apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: jenkins-data  namespace: devopsspec:  # 设置PVC存储类别，用于指定存储类型  storageClassName: nfs  # 设置访问模式，匹配相同模式的PV  accessModes:  - ReadWriteMany  # 设置PVC所申请存储空间的大小  resources:    requests:      storage: 10Gi</code></pre><h3 id="3-2-2-部署jenkins存储"><a href="#3-2-2-部署jenkins存储" class="headerlink" title="3.2.2 部署jenkins存储"></a>3.2.2 部署jenkins存储</h3><pre><code class="hljs">kubectl apply -f jenkins-data.yaml</code></pre><h2 id="3-3-部署jenkins"><a href="#3-3-部署jenkins" class="headerlink" title="3.3 部署jenkins"></a>3.3 部署jenkins</h2><h3 id="3-3-1-创建部署文件"><a href="#3-3-1-创建部署文件" class="headerlink" title="3.3.1 创建部署文件"></a>3.3.1 创建部署文件</h3><pre><code class="hljs">vi jenkins.yamlapiVersion: v1kind: ServiceAccountmetadata:  name: jenkins  namespace: devops  labels:    name: jenkins---kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata:  name: jenkins  namespace: devopsrules:- apiGroups: [&quot;&quot;]  resources: [&quot;pods&quot;,&quot;events&quot;]  verbs: [&quot;create&quot;,&quot;delete&quot;,&quot;get&quot;,&quot;list&quot;,&quot;patch&quot;,&quot;update&quot;,&quot;watch&quot;]- apiGroups: [&quot;&quot;]  resources: [&quot;pods/exec&quot;]  verbs: [&quot;create&quot;,&quot;delete&quot;,&quot;get&quot;,&quot;list&quot;,&quot;patch&quot;,&quot;update&quot;,&quot;watch&quot;]- apiGroups: [&quot;&quot;]  resources: [&quot;pods/log&quot;]  verbs: [&quot;get&quot;,&quot;list&quot;,&quot;watch&quot;]- apiGroups: [&quot;&quot;]  resources: [&quot;secrets&quot;,&quot;events&quot;]  verbs: [&quot;get&quot;]---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata:  name: jenkins  namespace: devopssubjects:- kind: ServiceAccount  name: jenkinsroleRef:  apiGroup: rbac.authorization.k8s.io  kind: Role  name: jenkins---kind: DeploymentapiVersion: apps/v1metadata:  name: jenkins  namespace: devopsspec:  replicas: 1  selector:    matchLabels:      app: jenkins  template:    metadata:      labels:        app: jenkins    spec:      serviceAccountName: jenkins      containers:      - name: jenkins        image: registry.cn-hangzhou.aliyuncs.com/swords/jenkins:v2.400-jdk11        imagePullPolicy: IfNotPresent        securityContext:                               runAsUser:          privileged: true        env:        - name: &quot;JAVA_OPTS&quot;          value: -Dhudson.security.csrf.GlobalCrumbIssuerConfiguration.DISABLE_CSRF_PROTECTION=true        ports:        - containerPort: 8080          name: web          protocol: TCP        - containerPort: 50000          name: agent          protocol: TCP        resources:          limits:            cpu: 2000m            memory: 1Gi          requests:            cpu: 50m            memory: 512Mi        livenessProbe:          httpGet:            path: /login            port: 8080          initialDelaySeconds: 60          timeoutSeconds: 5          failureThreshold: 3        readinessProbe:          httpGet:            path: /login            port: 8080          initialDelaySeconds: 60          timeoutSeconds: 5          failureThreshold: 3        volumeMounts:        - name: jenkins-data          mountPath: /var/jenkins_home        - name: localtime          mountPath: /etc/localtime      volumes:      - name: jenkins-data        persistentVolumeClaim:          claimName: jenkins-data      - name: localtime        hostPath:          path: /etc/localtime      imagePullSecrets:        - name: regcred---apiVersion: v1kind: Servicemetadata:  name: jenkins  namespace: devops  labels:    app: jenkinsspec:  type: NodePort  ports:  - name: web    port: 8080    targetPort: 8080    nodePort: 38080  - name: agent    port: 50000    targetPort: 50000    nodePort: 50000  selector:    app: jenkins</code></pre><h3 id="3-3-2-部署jenkins"><a href="#3-3-2-部署jenkins" class="headerlink" title="3.3.2 部署jenkins"></a>3.3.2 部署jenkins</h3><pre><code class="hljs">kubectl apply -f jenkins.yaml</code></pre><h2 id="3-4-获取随机密码，解锁jenkins"><a href="#3-4-获取随机密码，解锁jenkins" class="headerlink" title="3.4 获取随机密码，解锁jenkins"></a>3.4 获取随机密码，解锁jenkins</h2><pre><code class="hljs">kubectl -n devops logs jenkins-655564d6f5-ld5tl</code></pre><h1 id="4-创建项目，测试任务构建"><a href="#4-创建项目，测试任务构建" class="headerlink" title="4.创建项目，测试任务构建"></a>4.创建项目，测试任务构建</h1><p><img src="/img/wiki/jenkins/jenkins-001-003.jpg" alt="jenkins-001-003"></p><p><img src="/img/wiki/jenkins/jenkins-001-004.jpg" alt="jenkins-001-004"></p><p><img src="/img/wiki/jenkins/jenkins-001-005.jpg" alt="jenkins-001-005"></p><p><img src="/img/wiki/jenkins/jenkins-001-006.jpg" alt="jenkins-001-006"></p><p><img src="/img/wiki/jenkins/jenkins-001-007.jpg" alt="jenkins-001-007"></p><p><img src="/img/wiki/jenkins/jenkins-001-008.jpg" alt="jenkins-001-008"></p><p><img src="/img/wiki/jenkins/jenkins-001-009.jpg" alt="jenkins-001-009"></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://cloud.tencent.com/developer/article/1559679">https://cloud.tencent.com/developer/article/1559679</a></li><li><a href="https://blog.csdn.net/qq_42883074/article/details/126009573">https://blog.csdn.net/qq_42883074/article/details/126009573</a></li><li><a href="https://blog.csdn.net/weixin_38299857/article/details/115384071">https://blog.csdn.net/weixin_38299857/article/details/115384071</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>CICD</tag>
      
      <tag>DevOps</tag>
      
      <tag>Jenkins</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes实战之部署Cloudreve云存储系统</title>
    <link href="/linux/KubernetesRunningCloudreve/"/>
    <url>/linux/KubernetesRunningCloudreve/</url>
    
    <content type="html"><![CDATA[<h1 id="1-部署Mysql数据库"><a href="#1-部署Mysql数据库" class="headerlink" title="1.部署Mysql数据库"></a>1.部署Mysql数据库</h1><h1 id="2-部署nfs"><a href="#2-部署nfs" class="headerlink" title="2.部署nfs"></a>2.部署nfs</h1><h1 id="3-部署StorageClass"><a href="#3-部署StorageClass" class="headerlink" title="3.部署StorageClass"></a>3.部署StorageClass</h1><h1 id="4-配置镜像仓库免密拉取"><a href="#4-配置镜像仓库免密拉取" class="headerlink" title="4.配置镜像仓库免密拉取"></a>4.配置镜像仓库免密拉取</h1><h1 id="4-1-创建镜像仓库认证文件"><a href="#4-1-创建镜像仓库认证文件" class="headerlink" title="4.1 创建镜像仓库认证文件"></a>4.1 创建镜像仓库认证文件</h1><pre><code class="hljs">sudo docker loginsudo docker login registry.sword.orgsudo docker login ccr.ccs.tencentyun.comsudo docker login registry.cn-hangzhou.aliyuncs.com</code></pre><h3 id="4-2-创建镜像拉取密钥"><a href="#4-2-创建镜像拉取密钥" class="headerlink" title="4.2 创建镜像拉取密钥"></a>4.2 创建镜像拉取密钥</h3><pre><code class="hljs">kubectl create secret generic regcred \  --from-file=.dockerconfigjson=/root/.docker/config.json \  --type=kubernetes.io/dockerconfigjson</code></pre><h1 id="5-部署Redis缓存服务"><a href="#5-部署Redis缓存服务" class="headerlink" title="5.部署Redis缓存服务"></a>5.部署Redis缓存服务</h1><h2 id="5-1-创建配置文件"><a href="#5-1-创建配置文件" class="headerlink" title="5.1 创建配置文件"></a>5.1 创建配置文件</h2><pre><code class="hljs">vi redis-conf.yamlapiVersion: v1kind: ConfigMapmetadata:  name: redis-confdata:  maxclients: &quot;1024&quot;  dir: &quot;/var/lib/redis&quot;  dbfilename: &quot;dump.rdb&quot;</code></pre><h2 id="5-2-创建redis持久化存储资源部署文件"><a href="#5-2-创建redis持久化存储资源部署文件" class="headerlink" title="5.2 创建redis持久化存储资源部署文件"></a>5.2 创建redis持久化存储资源部署文件</h2><pre><code class="hljs">vi redis-data.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata:  name: redis-data  namespace: defaultspec:  # 设置PVC StorageClassName  storageClassName: sc-nfs  accessModes:  - ReadWriteMany  resources:    requests:      storage: 10Gi</code></pre><h2 id="5-3-创建redis应用资源部署文件"><a href="#5-3-创建redis应用资源部署文件" class="headerlink" title="5.3 创建redis应用资源部署文件"></a>5.3 创建redis应用资源部署文件</h2><pre><code class="hljs">vi redis-deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata:  name: redis-deployment  spec:  selector:    matchLabels:      app: redis-servers  replicas: 1  template:    metadata:      labels:        app: redis-servers    spec:      containers:        - name: redis          image: registry.cn-hangzhou.aliyuncs.com/swords/redis          imagePullPolicy: IfNotPresent          command: [ &quot;/usr/bin/redis-server&quot; ]          args: [&quot;--requirepass&quot;,&quot;$(requirepass)&quot;,&quot;--dir&quot;,&quot;$(dir)&quot;]          envFrom:          - configMapRef:              name: redis-conf          env:            - name: requirepass              valueFrom:                secretKeyRef:                  name: redis-auth                  key: requirepass          ports:            - containerPort: 6379              name: tcp-redis              protocol: TCP          resources:              limits:                cpu: 500m              requests:                cpu: 200m          volumeMounts:            - name: redis-data              mountPath: /var/lib/redis      volumes:        - name: redis-data          persistentVolumeClaim:            claimName: redis-data      # 设置镜像拉取认证密钥      imagePullSecrets:        - name: regcred---apiVersion: v1kind: Servicemetadata:  name: redis-servicespec:  selector:    app: redis-servers  ports:  - port: 6379    targetPort: 6379</code></pre><h2 id="5-4-部署redis服务"><a href="#5-4-部署redis服务" class="headerlink" title="5.4 部署redis服务"></a>5.4 部署redis服务</h2><pre><code class="hljs"># 部署redis认证密钥kubectl create secret generic redis-auth --from-literal=requirepass=&#39;redis&#39;# 部署配置文件kubectl apply -f redis-conf.yaml# 部署数据持久化存储kubectl apply -f redis-data.yaml# 部署rediskubectl apply -f redis-deployment.yaml</code></pre><h1 id="6-部署Cloudreve"><a href="#6-部署Cloudreve" class="headerlink" title="6.部署Cloudreve"></a>6.部署Cloudreve</h1><h2 id="6-1-创建配置文件"><a href="#6-1-创建配置文件" class="headerlink" title="6.1 创建配置文件"></a>6.1 创建配置文件</h2><pre><code class="hljs">vi conf.ini[System]Debug = falseMode = masterListen = :5212SessionSecret = fVVTsAm06i7YQjE0uX6dEhgNarFd6Bfg0BNceMT6n3jI0l7TGHMlqobQ235NksNuHashIDSalt = l5j19NsR47owEy0qQcL0IUKSgsy1Zc4tMophQ6Rane72he0AIySBWXsYAlRsYomN[Database]Type = mysqlPort = 3306User = cloudrevePassword = cloudreveHost = 192.168.100.120Name = cloudreveTablePrefix = sword_[Redis]Server = redis-service:6379Password = redisDB = 3</code></pre><h2 id="6-2-创建持久化存储资源部署文件"><a href="#6-2-创建持久化存储资源部署文件" class="headerlink" title="6.2 创建持久化存储资源部署文件"></a>6.2 创建持久化存储资源部署文件</h2><pre><code class="hljs">vi cloudreve-data.yamlapiVersion: v1kind: PersistentVolumemetadata:  # 设置PV名称  name: cloudreve-data  # 设置PV标签，用于PVC的定向绑定  labels:    app: cloudreve-dataspec:  # 设置存储类别  storageClassName: nfs  # 设置访问模式  accessModes:    - ReadWriteMany  # 设置PV的存储空间  capacity:    storage: 500Gi  # 设置PV的回收策略  persistentVolumeReclaimPolicy: Retain  nfs:    path: /web/cloudreve/data    server: 192.168.100.200---apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: cloudreve-data  namespace: defaultspec:  # 设置PVC存储类别，用于指定存储类型  storageClassName: nfs  # 设置访问模式，匹配相同模式的PV  accessModes:  - ReadWriteMany  # 设置PVC所申请存储空间的大小  resources:    requests:      storage: 500Gi  selector:     matchLabels:      app: cloudreve-data</code></pre><h2 id="6-3-创建cloudreve应用资源部署文件"><a href="#6-3-创建cloudreve应用资源部署文件" class="headerlink" title="6.3 创建cloudreve应用资源部署文件"></a>6.3 创建cloudreve应用资源部署文件</h2><pre><code class="hljs">vi cloudreve-deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata:  name: cloudreve-deployment  spec:  selector:    matchLabels:      app: cloudreve-servers  replicas: 1  template:    metadata:      labels:        app: cloudreve-servers    spec:      containers:        - name: cloudreve          image: cloudreve/cloudreve          imagePullPolicy: IfNotPresent          ports:            - containerPort: 5212              name: tcp-cloudreve              protocol: TCP          resources:              limits:                cpu: 500m              requests:                cpu: 200m          volumeMounts:            - name: cloudreve-conf              mountPath: /cloudreve/conf.ini              subPath: conf.ini            - name: data              mountPath: /data      volumes:        - name: cloudreve-conf          configMap:            name: cloudreve-conf        - name: data          persistentVolumeClaim:            claimName: cloudreve-data      # 设置镜像拉取认证密钥      imagePullSecrets:        - name: regcred---apiVersion: v1kind: Servicemetadata:  name: cloudreve-servicespec:  type: NodePort  sessionAffinity: ClientIP  selector:    app: cloudreve-servers  ports:  - port: 5212    targetPort: 5212    nodePort: 32512</code></pre><h2 id="6-4-部署"><a href="#6-4-部署" class="headerlink" title="6.4 部署"></a>6.4 部署</h2><pre><code class="hljs"># 部署配置文件cloudrevekubectl create configmap cloudreve-conf --from-file=conf.ini# 部署数据持久化存储kubectl apply -f cloudreve-data.yaml# 部署cloudrevekubectl apply -f cloudreve-deployment.yaml</code></pre><h2 id="6-5-部署HPA"><a href="#6-5-部署HPA" class="headerlink" title="6.5 部署HPA"></a>6.5 部署HPA</h2><pre><code class="hljs">kubectl autoscale deployment cloudreve-deployment --cpu-percent=90 --min=1 --max=3</code></pre><h1 id="7-部署Nginx"><a href="#7-部署Nginx" class="headerlink" title="7.部署Nginx"></a>7.部署Nginx</h1><h2 id="7-1-创建配置文件"><a href="#7-1-创建配置文件" class="headerlink" title="7.1 创建配置文件"></a>7.1 创建配置文件</h2><pre><code class="hljs">vi nginx-conf.yamlapiVersion: v1kind: ConfigMapmetadata:  name: nginx-confdata:  cloudreve.conf: |    server &#123;      listen       80;      server_name  localhost;      location / &#123;                                                         access_log  /var/log/nginx/cloudreve_access.log  main;      error_log  /var/log/nginx/cloudreve_error.log;      proxy_pass http://cloudreve-service:5212;      &#125;      location /wiki &#123;        root  /web;        autoindex on;        charset utf-8;        autoindex_exact_size off;        autoindex_localtime on;        access_log  /var/log/nginx/wiki_access.log  main;        error_log  /var/log/nginx/wiki_error.log;      &#125;    &#125;</code></pre><h2 id="7-2-创建nginx应用资源部署文件"><a href="#7-2-创建nginx应用资源部署文件" class="headerlink" title="7.2 创建nginx应用资源部署文件"></a>7.2 创建nginx应用资源部署文件</h2><pre><code class="hljs">vi nginx-deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata:  name: nginx-deployment  spec:  selector:    matchLabels:      app: nginx-servers  replicas: 1  template:    metadata:      labels:        app: nginx-servers    spec:      containers:        - name: nginx          image: registry.cn-hangzhou.aliyuncs.com/swords/nginx          imagePullPolicy: IfNotPresent          ports:            - containerPort: 80              name: tcp-nginx              protocol: TCP          resources:              limits:                cpu: 500m              requests:                cpu: 200m          volumeMounts:            - name: nginx-conf              mountPath: /etc/nginx/nginx.conf              subPath: nginx.conf            - name: nginx-logs              mountPath: /var/log/nginx      volumes:        - name: nginx-conf          configMap:            name: nginx-conf        - name: nginx-logs          hostPath:            path: /var/log/nginx      # 设置镜像拉取认证密钥      imagePullSecrets:        - name: regcred---apiVersion: v1kind: Servicemetadata:  name: nginx-servicespec:  type: NodePort  selector:    app: nginx-servers  ports:  - port: 80    targetPort: 80    nodePort: 30080</code></pre><h2 id="7-3-部署nginx"><a href="#7-3-部署nginx" class="headerlink" title="7.3 部署nginx"></a>7.3 部署nginx</h2><pre><code class="hljs"># 部署主配置文件kubectl create configmap nginx-conf --from-file=nginx.conf# 部署配置文件kubectl apply -f nginx-confs.yaml# 部署nginx应用kubectl apply -f nginx-deployment.yaml</code></pre><h2 id="7-4-部署HPA"><a href="#7-4-部署HPA" class="headerlink" title="7.4 部署HPA"></a>7.4 部署HPA</h2><pre><code class="hljs">kubectl autoscale deployment nginx-deployment --cpu-percent=90 --min=1 --max=3</code></pre><h1 id="8-验证cloudreve存储系统"><a href="#8-验证cloudreve存储系统" class="headerlink" title="8.验证cloudreve存储系统"></a>8.验证cloudreve存储系统</h1><pre><code class="hljs">kubectl get pvckubectl get svckubectl get pod -o wide</code></pre>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>存储</tag>
      
      <tag>云存储</tag>
      
      <tag>Cloudreve</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes集群部署EFK日志管理平台</title>
    <link href="/linux/KubernetesELK/"/>
    <url>/linux/KubernetesELK/</url>
    
    <content type="html"><![CDATA[<p>EFK是Kubernetes官方推荐的的日志管理系统解决方案，通过Fluentd&#x2F;Fluent Bit或Filebeat之类的节点级代理程序进行日志采集，实时推送给Elasticsearch集群进行存储和分析，而后再经由Kibana进行数据可视化，这样的组合即简称为EFK。Kubernetes二进制包即包含EFK部署所需资源，可作为参考</p><hr><h1 id="1-部署nfs"><a href="#1-部署nfs" class="headerlink" title="1.部署nfs"></a>1.部署nfs</h1><h1 id="2-部署StorageClass"><a href="#2-部署StorageClass" class="headerlink" title="2.部署StorageClass"></a>2.部署StorageClass</h1><h1 id="3-创建命名空间"><a href="#3-创建命名空间" class="headerlink" title="3.创建命名空间"></a>3.创建命名空间</h1><pre><code class="hljs">kubectl create namespace logging</code></pre><h1 id="4-部署elasticsearch"><a href="#4-部署elasticsearch" class="headerlink" title="4.部署elasticsearch"></a>4.部署elasticsearch</h1><h2 id="4-1-创建elasticsearch应用资源部署文件"><a href="#4-1-创建elasticsearch应用资源部署文件" class="headerlink" title="4.1 创建elasticsearch应用资源部署文件"></a>4.1 创建elasticsearch应用资源部署文件</h2><pre><code class="hljs">vi es-statefulset.yamlapiVersion: apps/v1kind: StatefulSetmetadata:  name: es-cluster  namespace: loggingspec:  serviceName: elasticsearch  replicas: 3  selector:    matchLabels:      app: elasticsearch  template:    metadata:      labels:        app: elasticsearch    spec:      containers:      - name: elasticsearch        image: elasticsearch:7.12.1        imagePullPolicy: IfNotPresent        resources:          limits:            cpu: 100m            memory: 1Gi          requests:            cpu: 100m            memory: 1Gi        ports:        - containerPort: 9200          name: rest          protocol: TCP        - containerPort: 9300          name: inter-node          protocol: TCP        volumeMounts:        - name: data          mountPath: /usr/share/elasticsearch/data        env:          - name: cluster.name            value: k8s-logs          - name: node.name            valueFrom:              fieldRef:                fieldPath: metadata.name          - name: discovery.seed_hosts            value: &quot;elasticsearch&quot;          - name: cluster.initial_master_nodes            value: &quot;es-cluster-0,es-cluster-1,es-cluster-2&quot;          - name: ES_JAVA_OPTS            value: &quot;-Xms2g -Xmx2g&quot;      initContainers:      - name: fix-permissions        image: busybox        imagePullPolicy: IfNotPresent        command: [&quot;sh&quot;, &quot;-c&quot;, &quot;chown -R 1000:1000 /usr/share/elasticsearch/data&quot;]        securityContext:          privileged: true        volumeMounts:        - name: data          mountPath: /usr/share/elasticsearch/data      - name: increase-vm-max-map        image: busybox        imagePullPolicy: IfNotPresent        command: [&quot;sysctl&quot;, &quot;-w&quot;, &quot;vm.max_map_count=262144&quot;]        securityContext:          privileged: true      - name: increase-fd-ulimit        image: busybox        imagePullPolicy: IfNotPresent        command: [&quot;sh&quot;, &quot;-c&quot;, &quot;ulimit -n 65536&quot;]        securityContext:          privileged: true      affinity:        # 设置反亲和性，防止有POD被调度到同一节点        podAntiAffinity:          requiredDuringSchedulingIgnoredDuringExecution:          - labelSelector:              matchExpressions:              - key: app                operator: In                values:                - elasticsearch            topologyKey: &quot;kubernetes.io/hostname&quot;  # 设置持久化数据存储  volumeClaimTemplates:  - metadata:      name: data      annotations:        volume.beta.kubernetes.io/storage-class: sc-nfs    spec:      accessModes: [ &quot;ReadWriteOnce&quot; ]      resources:        requests:          storage: 10Gi</code></pre><h2 id="4-2-创建elasticsearch-service资源部署文件"><a href="#4-2-创建elasticsearch-service资源部署文件" class="headerlink" title="4.2 创建elasticsearch service资源部署文件"></a>4.2 创建elasticsearch service资源部署文件</h2><pre><code class="hljs">vi es-service.yamlapiVersion: v1kind: Servicemetadata:  name: elasticsearch  namespace: logging  labels:    app: elasticsearchspec:  selector:    app: elasticsearch  clusterIP: None  ports:    - port: 9200      name: rest    - port: 9300      name: inter-node</code></pre><h2 id="4-3-部署elasticsearch"><a href="#4-3-部署elasticsearch" class="headerlink" title="4.3 部署elasticsearch"></a>4.3 部署elasticsearch</h2><pre><code class="hljs">kubectl apply -f es-service.yamlkubectl apply -f es-statefulset.yaml   </code></pre><h1 id="5-部署kibana"><a href="#5-部署kibana" class="headerlink" title="5.部署kibana"></a>5.部署kibana</h1><h2 id="5-1-创建配置文件"><a href="#5-1-创建配置文件" class="headerlink" title="5.1 创建配置文件"></a>5.1 创建配置文件</h2><pre><code class="hljs">vi kibana.ymlserver.name: kibanaserver.host: &quot;0&quot;i18n.locale: &quot;zh-CN&quot;elasticsearch.hosts: [ &quot;http://elasticsearch:9200&quot; ]monitoring.ui.container.elasticsearch.enabled: true</code></pre><h2 id="5-2-部署配置文件"><a href="#5-2-部署配置文件" class="headerlink" title="5.2 部署配置文件"></a>5.2 部署配置文件</h2><pre><code class="hljs">kubectl -n logging create configmap kibana-yml --from-file=kibana.yml</code></pre><h2 id="5-3-创建kibana应用资源部署文件"><a href="#5-3-创建kibana应用资源部署文件" class="headerlink" title="5.3 创建kibana应用资源部署文件"></a>5.3 创建kibana应用资源部署文件</h2><pre><code class="hljs">vi kibana.yamlapiVersion: v1kind: Servicemetadata:  name: kibana  namespace: logging  labels:    app: kibanaspec:  ports:  - port: 5601    nodePort: 35601  selector:    app: kibana  type: NodePort---apiVersion: apps/v1kind: Deploymentmetadata:  name: kibana  namespace: logging  labels:    app: kibanaspec:  replicas: 1  selector:    matchLabels:      app: kibana  template:    metadata:      labels:        app: kibana    spec:      containers:      - name: kibana        image:  kibana:7.12.1        imagePullPolicy: IfNotPresent        resources:          limits:            cpu: 1000m            memory: 1Gi          requests:            cpu: 500m            memory: 512Mi        env:          - name: ELASTICSEARCH_URL            value: http://elasticsearch:9200        ports:        - containerPort: 5601        volumeMounts:        - name: kibana-yml          mountPath: /usr/share/kibana/config/kibana.yml          subPath: kibana.yml      volumes:      - name: kibana-yml        configMap:          name: kibana-yml</code></pre><h2 id="5-2-部署kibana"><a href="#5-2-部署kibana" class="headerlink" title="5.2 部署kibana"></a>5.2 部署kibana</h2><pre><code class="hljs">kubectl apply -f kibana.yaml</code></pre><h1 id="6-部署logstash"><a href="#6-部署logstash" class="headerlink" title="6.部署logstash"></a>6.部署logstash</h1><h2 id="6-1-创建配置文件"><a href="#6-1-创建配置文件" class="headerlink" title="6.1 创建配置文件"></a>6.1 创建配置文件</h2><pre><code class="hljs">vi logstash.ymlhttp.host: &quot;0.0.0.0&quot;xpack.monitoring.elasticsearch.hosts: [ &quot;http://elasticsearch:9200&quot; ]</code></pre><h1 id="6-2-创建日志解析配置文件"><a href="#6-2-创建日志解析配置文件" class="headerlink" title="6.2 创建日志解析配置文件"></a>6.2 创建日志解析配置文件</h1><pre><code class="hljs">vi logstash-conf.yamlapiVersion: v1kind: ConfigMapmetadata:  name: logstash-conf  namespace: loggingdata:  logstash.conf: |-    input &#123;      kafka &#123;        bootstrap_servers =&gt; [&quot;kafka-0.kafka:9092,kafka-1.kafka:9092,kafka-2.kafka:9092&quot;]        client_id =&gt; &quot;kubernetes&quot;        group_id =&gt; &quot;kubernetes&quot;        auto_offset_reset =&gt; &quot;latest&quot;        consumer_threads =&gt; 3        decorate_events =&gt; false        topics =&gt; [&quot;sshd&quot;,&quot;nginx-access&quot;,&quot;nginx-error&quot;,&quot;container&quot;]        codec =&gt; &quot;json&quot;        &#125;      &#125;    filter &#123;      if &quot;nginx-access&quot; in [tags] &#123;        grok &#123;          match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;IPORHOST:remote_addr&#125; - %&#123;DATA:remote_user&#125; \[%&#123;HTTPDATE:access_time&#125;\] \&quot;%&#123;WORD:http_method&#125; %&#123;DATA:url&#125; HTTP/%&#123;NUMBER:http_version&#125;\&quot; %&#123;NUMBER:response_code&#125; %&#123;NUMBER:bytes_sent&#125; %&#123;NUMBER:body_bytes_sent&#125; \&quot;%&#123;DATA:http_referer&#125;\&quot; \&quot;%&#123;DATA:http_user_agent&#125;\&quot; \&quot;%&#123;DATA:http_x_forwarded_for&#125;\&quot; \&quot;%&#123;DATA:request_time&#125;\&quot; \&quot;%&#123;DATA:upstream_response_time&#125;\&quot; \&quot;%&#123;DATA:upstream_status&#125;\&quot; \&quot;%&#123;HOSTPORT:upstream_addr&#125;\&quot;&quot;&#125;        &#125;        date &#123;          match =&gt; [&quot;access_time&quot;, &quot;dd/MMM/yyyy:HH:mm:ss Z&quot;]          remove_field =&gt; [&quot;timestamp&quot;]          &#125;        if [http_user_agent] != &quot;-&quot; &#123;          useragent &#123;          target =&gt; &quot;http_user&quot;          source =&gt; &quot;http_user_agent&quot;          &#125;        &#125;        urldecode&#123;          all_fields=&gt;true        &#125;        mutate &#123;          convert =&gt; [&quot;request_time&quot;, &quot;float&quot;]          convert =&gt; [&quot;upstream_response_time&quot;, &quot;float&quot;]          convert =&gt; [&quot;body_bytes_sent&quot;, &quot;integer&quot;]          # 剔除掉多余字段          remove_field =&gt; [&quot;_id&quot;,&quot;_score&quot;,&quot;_type&quot;,&quot;message&quot;,&quot;http_user_agent&quot;,&quot;url_tmp&quot;]        &#125;      &#125;      if &quot;nginx-error&quot; in [tags] &#123;        grok &#123;          match =&gt; [ &quot;message&quot; , &quot;(?&lt;timestamp&gt;%&#123;YEAR&#125;[./-]%&#123;MONTHNUM&#125;[./-]%&#123;MONTHDAY&#125;[- ]%&#123;TIME&#125;) \[%&#123;LOGLEVEL:severity&#125;\] %&#123;POSINT:pid&#125;#%&#123;NUMBER&#125;: %&#123;GREEDYDATA:errormessage&#125;(?:, client: (?&lt;clientip&gt;%&#123;IP&#125;|%&#123;HOSTNAME&#125;))(?:, server: %&#123;IPORHOST:server&#125;?)(?:, request: %&#123;QS:request&#125;)?(?:, upstream: (?&lt;upstream&gt;\&quot;%&#123;URI&#125;\&quot;|%&#123;QS&#125;))?(?:, host: %&#123;QS:request_host&#125;)?(?:, referrer: \&quot;%&#123;URI:referrer&#125;\&quot;)?&quot;]        &#125;        mutate &#123;          convert =&gt; [ &quot;[geoip][coordinates]&quot;, &quot;float&quot;]          remove_field =&gt; [&quot;timestamp&quot;,&quot;message&quot;,&quot;_id&quot;,&quot;_score&quot;,&quot;_type&quot;]        &#125;      &#125;      if &quot;sshd&quot; in [tags] &#123;        grok &#123;          match =&gt; &#123;&quot;message&quot; =&gt; &quot;.*sshd\[\d+\]: %&#123;WORD:status&#125; .* %&#123;USER:username&#125; from.*%&#123;IP:clientip&#125;.*&quot;&#125;        &#125;        if [message] !~ &quot;Accepted | Invalid | Failed&quot; &#123;          drop &#123;&#125;        &#125;        mutate &#123;          convert =&gt; [ &quot;[geoip][coordinates]&quot;, &quot;float&quot;]          remove_field =&gt; [&quot;timestamp&quot;,&quot;message&quot;,&quot;_id&quot;,&quot;_score&quot;,&quot;_type&quot;]        &#125;      &#125;      if &quot;container&quot; in [tags] &#123;        mutate &#123;          remove_field =&gt; [&quot;log&quot;,&quot;agent&quot;,&quot;ecs&quot;,&quot;containers&quot;,&quot;host&quot;,&quot;container&quot;]        &#125;      &#125;      if &quot;_geoip_lookup_failure&quot; in [tags] &#123; drop &#123; &#125; &#125;    &#125;    output&#123;      if &quot;nginx-access&quot; in [tags] &#123;        elasticsearch&#123;          hosts =&gt; [&quot;elasticsearch:9200&quot;]          index =&gt; &quot;logstash-nginx-access-%&#123;+YYYY.MM.dd&#125;&quot;        &#125;      &#125;      if &quot;nginx-error&quot; in [tags] &#123;        elasticsearch&#123;          hosts =&gt; [&quot;elasticsearch:9200&quot;]          index =&gt; &quot;logstash-nginx-error-%&#123;+YYYY.MM.dd&#125;&quot;        &#125;      &#125;      if &quot;sshd&quot; in [tags] &#123;        elasticsearch&#123;          hosts =&gt; [&quot;elasticsearch:9200&quot;]          index =&gt; &quot;logstash-sshd-%&#123;+YYYY.MM.dd&#125;&quot;        &#125;      &#125;      if &quot;container&quot; in [tags] &#123;        elasticsearch&#123;          hosts =&gt; [&quot;elasticsearch:9200&quot;]          index =&gt; &quot;logstash-container-%&#123;+YYYY.MM.dd&#125;&quot;        &#125;      &#125;    # stdout &#123; codec =&gt; rubydebug &#125;    &#125;</code></pre><h2 id="6-3-创建logstash应用资源部署文件"><a href="#6-3-创建logstash应用资源部署文件" class="headerlink" title="6.3 创建logstash应用资源部署文件"></a>6.3 创建logstash应用资源部署文件</h2><pre><code class="hljs">vi logstash.yamlapiVersion: apps/v1kind: Deploymentmetadata:  name: logstash-deployment  namespace: loggingspec:  replicas: 1  selector:    matchLabels:      app: logstash  template:    metadata:      labels:        app: logstash    spec:      containers:      - name: logstash        image: registry.cn-hangzhou.aliyuncs.com/swords/logstash:v7.12.1        imagePullPolicy: IfNotPresent        env:        - name: ES_JAVA_OPTS          value: &quot;-Xms512m -Xmx512m&quot;        - name: TZ          value: Asia/Shanghai        ports:        - name: logport          containerPort: 5044          protocol: TCP        command: [&quot;logstash&quot;,&quot;-f&quot;,&quot;/usr/share/logstash/config/logstash.conf&quot;]        resources:          limits:            cpu: 1000m            memory: 8192Mi          requests:            cpu: 100m            memory: 200Mi        volumeMounts:        - name: logstash-conf          mountPath: /usr/share/logstash/config/logstash.conf          subPath: logstash.conf        - name: timezone          mountPath: /etc/localtime        - name: logstash-yml          mountPath: /usr/share/logstash/config/logstash.yml          subPath: logstash.yml      volumes:      - name: logstash-conf        configMap:          name: logstash-conf      - name: timezone        hostPath:          path: /etc/localtime      - name: logstash-yml        configMap:          name: logstash-yml</code></pre><h1 id="6-4-部署logstash"><a href="#6-4-部署logstash" class="headerlink" title="6.4 部署logstash"></a>6.4 部署logstash</h1><pre><code class="hljs">kubectl apply -f logstash-conf.yamlkubectl -n logging create configmap logstash-yml --from-file=logstash.ymlkubectl apply -f logstash.yaml</code></pre><h1 id="7-部署kafka集群"><a href="#7-部署kafka集群" class="headerlink" title="7.部署kafka集群"></a>7.部署kafka集群</h1><h1 id="8-部署日志采集程序"><a href="#8-部署日志采集程序" class="headerlink" title="8.部署日志采集程序"></a>8.部署日志采集程序</h1><h2 id="8-1-部署fluentd"><a href="#8-1-部署fluentd" class="headerlink" title="8.1 部署fluentd"></a>8.1 部署fluentd</h2><h3 id="8-1-1-创建配置文件"><a href="#8-1-1-创建配置文件" class="headerlink" title="8.1.1 创建配置文件"></a>8.1.1 创建配置文件</h3><pre><code class="hljs">vi fluentd-es-configmap.yamlkind: ConfigMapapiVersion: v1metadata:  name: fluentd-config  namespace: loggingdata:  system.conf: |-    &lt;system&gt;      root_dir /tmp/fluentd-buffers/    &lt;/system&gt;  containers.input.conf: |-    &lt;source&gt;      @id fluentd-containers.log      @type tail                              # Fluentd 内置的输入方式，其原理是不停地从源文件中获取新的日志。      path /var/log/containers/*.log          # 挂载的服务器Docker容器日志地址      pos_file /var/log/es-containers.log.pos      tag raw.kubernetes.*                    # 设置日志标签      read_from_head true      &lt;parse&gt;                                 # 多行格式化成JSON        @type multi_format                    # 使用 multi-format-parser 解析器插件        &lt;pattern&gt;          format json                         # JSON解析器          time_key time                       # 指定事件时间的时间字段          time_format %Y-%m-%dT%H:%M:%S.%NZ   # 时间格式        &lt;/pattern&gt;        &lt;pattern&gt;          format /^(?&lt;time&gt;.+) (?&lt;stream&gt;stdout|stderr) [^ ]* (?&lt;log&gt;.*)$/          time_format %Y-%m-%dT%H:%M:%S.%N%:z        &lt;/pattern&gt;      &lt;/parse&gt;    &lt;/source&gt;    &lt;match raw.kubernetes.**&gt;           # 匹配tag为raw.kubernetes.**日志信息      @id raw.kubernetes      @type detect_exceptions           # 使用detect-exceptions插件处理异常栈信息      remove_tag_prefix raw             # 移除 raw 前缀      message log      stream stream      multiline_flush_interval 5      max_bytes 500000      max_lines 1000    &lt;/match&gt;    &lt;filter **&gt;  # 拼接日志      @id filter_concat      @type concat                # Fluentd Filter 插件，用于连接多个事件中分隔的多行日志。      key message      multiline_end_regexp /\n$/  # 以换行符“\n”拼接      separator &quot;&quot;    &lt;/filter&gt;    &lt;filter kubernetes.**&gt;      @id filter_kubernetes_metadata      @type kubernetes_metadata    &lt;/filter&gt;    &lt;filter kubernetes.**&gt;      @id filter_parser      @type parser                # multi-format-parser多格式解析器插件      key_name log                # 在要解析的记录中指定字段名称。      reserve_data true           # 在解析结果中保留原始键值对。      remove_key_name_field true  # key_name 解析成功后删除字段。      &lt;parse&gt;        @type multi_format        &lt;pattern&gt;          format json        &lt;/pattern&gt;        &lt;pattern&gt;          format none        &lt;/pattern&gt;      &lt;/parse&gt;    &lt;/filter&gt;    &lt;filter kubernetes.**&gt;      @type record_transformer      remove_keys $.docker.container_id,$.kubernetes.container_image_id,$.kubernetes.pod_id,$.kubernetes.namespace_id,$.kubernetes.master_url,$.kubernetes.labels.pod-template-hash    &lt;/filter&gt;  #  &lt;filter kubernetes.**&gt;  #    @id filter_log  #    @type grep  #    &lt;regexp&gt;  #      key $.kubernetes.labels.logging  #      pattern ^true$  #    &lt;/regexp&gt;  #  &lt;/filter&gt;  forward.input.conf: |-    &lt;source&gt;      @id forward      @type forward    &lt;/source&gt;  output.conf: |-    &lt;match **&gt;      @id elasticsearch      @type elasticsearch      @log_level info      include_tag_key true      host 192.168.100.158      port 9200      logstash_format true      logstash_prefix logstash-kubernetes  # 设置 index 前缀为 k8s      request_timeout    30s      &lt;buffer&gt;        @type file        path /var/log/fluentd-buffers/kubernetes.system.buffer        flush_mode interval        retry_type exponential_backoff        flush_thread_count 2        flush_interval 5s        retry_forever        retry_max_interval 30        chunk_limit_size 2M        queue_limit_length 8        overflow_action block      &lt;/buffer&gt;    &lt;/match&gt;</code></pre><h3 id="8-1-2-创建fluentd应用资源部署文件"><a href="#8-1-2-创建fluentd应用资源部署文件" class="headerlink" title="8.1.2 创建fluentd应用资源部署文件"></a>8.1.2 创建fluentd应用资源部署文件</h3><pre><code class="hljs">vi fluentd.yamlapiVersion: v1kind: ServiceAccountmetadata:  name: fluentd-es  namespace: logging  labels:    k8s-app: fluentd-es    kubernetes.io/cluster-service: &quot;true&quot;    addonmanager.kubernetes.io/mode: Reconcile---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata:  name: fluentd-es  labels:    k8s-app: fluentd-es    kubernetes.io/cluster-service: &quot;true&quot;    addonmanager.kubernetes.io/mode: Reconcilerules:- apiGroups:  - &quot;&quot;  resources:  - &quot;namespaces&quot;  - &quot;pods&quot;  verbs:  - &quot;get&quot;  - &quot;watch&quot;  - &quot;list&quot;---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata:  name: fluentd-es  labels:    k8s-app: fluentd-es    kubernetes.io/cluster-service: &quot;true&quot;    addonmanager.kubernetes.io/mode: Reconcilesubjects:- kind: ServiceAccount  name: fluentd-es  namespace: logging  apiGroup: &quot;&quot;roleRef:  kind: ClusterRole  name: fluentd-es  apiGroup: &quot;&quot;---apiVersion: apps/v1kind: DaemonSetmetadata:  name: fluentd-es  namespace: logging  labels:    k8s-app: fluentd-es    kubernetes.io/cluster-service: &quot;true&quot;    addonmanager.kubernetes.io/mode: Reconcilespec:  selector:    matchLabels:      k8s-app: fluentd-es  template:    metadata:      labels:        k8s-app: fluentd-es        kubernetes.io/cluster-service: &quot;true&quot;      annotations:        scheduler.alpha.kubernetes.io/critical-pod: &#39;&#39;    spec:      serviceAccountName: fluentd-es      containers:      - name: fluentd-es        image: quay.io/fluentd_elasticsearch/fluentd:v3.1.0        env:        - name: FLUENTD_ARGS          value: --no-supervisor -q        resources:          limits:            memory: 500Mi          requests:            cpu: 100m            memory: 200Mi        volumeMounts:        - name: varlog          mountPath: /var/log        - name: varlibdockercontainers          mountPath: /var/lib/docker/containers          readOnly: true        - name: config-volume          mountPath: /etc/fluent/config.d      # nodeSelector:      # beta.kubernetes.io/fluentd-ds-ready: &quot;true&quot;      tolerations:      - operator: Exists      terminationGracePeriodSeconds: 30      volumes:      - name: varlog        hostPath:          path: /var/log      - name: varlibdockercontainers        hostPath:          path: /var/lib/docker/containers      - name: config-volume        configMap:          name: fluentd-config</code></pre><h3 id="8-1-3-部署fluentd"><a href="#8-1-3-部署fluentd" class="headerlink" title="8.1.3 部署fluentd"></a>8.1.3 部署fluentd</h3><pre><code class="hljs">kubectl apply -f fluentd-es-configmap.yaml kubectl apply -f fluentd.yaml</code></pre><h2 id="8-2-部署filebeat"><a href="#8-2-部署filebeat" class="headerlink" title="8.2 部署filebeat"></a>8.2 部署filebeat</h2><h3 id="8-2-1-创建配置文件"><a href="#8-2-1-创建配置文件" class="headerlink" title="8.2.1 创建配置文件"></a>8.2.1 创建配置文件</h3><pre><code class="hljs">vi filebeat-configmap.yamlapiVersion: v1kind: ConfigMapmetadata:  name: filebeat-conf  namespace: loggingdata:  filebeat.yml: |-    filebeat.inputs:      - type: log        enabled: true        paths:          - /var/log/nginx/*access.log        tags: [&quot;nginx-access&quot;]        fields_under_root: true        fields:          log_topic: nginx-access      - type: log        enabled: true        paths:          - /var/log/nginx/*error.log        tags: [&quot;nginx-error&quot;]        fields_under_root: true        fields:          log_topic: nginx-error      - type: log        enabled: true        paths:          - /var/log/auth.log        tags: [&quot;sshd&quot;]        fields_under_root: true        fields:          log_topic: sshd      - type: log        enabled: true        paths:          - /var/log/containers/*.log        tags: [&quot;container&quot;]        symlinks: true        fields_under_root: true        fields:          log_topic: container    output.kafka:       hosts: [&quot;kafka-0.kafka:9092&quot;,&quot;kafka-1.kafka:9092&quot;,&quot;kafka-2.kafka:9092&quot;]      topic: &#39;%&#123;[log_topic]&#125;&#39;      partition.round_robin:        reachable_only: false      required_acks: 1      compression: gzip      max_message_bytes: 1000000</code></pre><h3 id="8-2-2-创建filebeat部署文件"><a href="#8-2-2-创建filebeat部署文件" class="headerlink" title="8.2.2 创建filebeat部署文件"></a>8.2.2 创建filebeat部署文件</h3><pre><code class="hljs">vi filebeat.yamlapiVersion: v1kind: ServiceAccountmetadata:  name: filebeat  namespace: logging---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata:  name: filebeat  labels:    app: filebeat-clsterrolerules:- apiGroups:  - &quot;&quot;  resources:  - nodes  - events  - namespaces  - pods  verbs:  - get  - watch  - list- apiGroups:  - &quot;&quot;  resourceNames:  - filebeat-prospectors  resources:  - configmaps  verbs:  - get  - update---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  name: filebeat  labels:    app: filebeat-clusterrolebindingroleRef:  apiGroup: &quot;&quot;  kind: ClusterRole  name: filebeatsubjects:- apiGroup: &quot;&quot;  kind: ServiceAccount  name: filebeat  namespace: logging---apiVersion: apps/v1kind: DaemonSetmetadata:  name: filebeat  namespace: logging  labels:    k8s-app: filebeatspec:  selector:    matchLabels:      k8s-app: filebeat  template:    metadata:      labels:        k8s-app: filebeat    spec:      serviceAccountName: filebeat      terminationGracePeriodSeconds: 30      containers:      - name: filebeat        image: registry.cn-hangzhou.aliyuncs.com/swords/filebeat:7.12.1        imagePullPolicy: IfNotPresent        args: [          &quot;-c&quot;, &quot;/etc/filebeat.yml&quot;,          &quot;-e&quot;,        ]        env:        - name: NODE_NAME          valueFrom:            fieldRef:              fieldPath: spec.nodeName        securityContext:          runAsUser: 0        resources:          limits:            cpu: 1000m            memory: 1000Mi          requests:            cpu: 100m            memory: 100Mi        volumeMounts:        - name: config          mountPath: /etc/filebeat.yml          readOnly: true          subPath: filebeat.yml        - name: data          mountPath: /usr/share/filebeat/data        - name: dockerlog          mountPath: /home/docker/docker/containers        - name: varlog          mountPath: /var/log/          readOnly: true        - name: timezone          mountPath: /etc/localtime        - name: varlibdockercontainers          mountPath: /var/lib/docker/containers      tolerations:      - operator: Exists      volumes:      - name: config        configMap:          defaultMode: 0644          name: filebeat-conf      - name: dockerlog        hostPath:          path: /var/log/containers/      - name: varlog        hostPath:          path: /var/log/      - name: data        hostPath:          path: /var/log/filebeat          type: DirectoryOrCreate      - name: timezone        hostPath:          path: /etc/localtime      - name: varlibdockercontainers        hostPath:          path: /var/lib/docker/containers</code></pre><h3 id="8-2-3-部署filebeat"><a href="#8-2-3-部署filebeat" class="headerlink" title="8.2.3 部署filebeat"></a>8.2.3 部署filebeat</h3><pre><code class="hljs">kubectl apply -f filebeat-configmap.yamlkubectl apply -f filebeat.yaml</code></pre><h1 id="9-验证EFK资源"><a href="#9-验证EFK资源" class="headerlink" title="9.验证EFK资源"></a>9.验证EFK资源</h1><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><p><a href="https://i4t.com/4951.html">https://i4t.com/4951.html</a></p></li><li><p><a href="https://www.cnblogs.com/99jianshao/p/15024955.html">https://www.cnblogs.com/99jianshao/p/15024955.html</a></p></li><li><p><a href="https://blog.csdn.net/heian_99/article/details/123405383">https://blog.csdn.net/heian_99/article/details/123405383</a></p></li><li><p><a href="https://blog.csdn.net/filtercomp/article/details/127125138">https://blog.csdn.net/filtercomp/article/details/127125138</a></p></li><li><p><a href="https://blog.csdn.net/qq_45887180/article/details/122431406">https://blog.csdn.net/qq_45887180/article/details/122431406</a></p></li><li><p><a href="https://github.com/kubernetes/kubernetes/tree/release-1.20/cluster/addons/fluentd-elasticsearch">https://github.com/kubernetes/kubernetes/tree/release-1.20/cluster/addons/fluentd-elasticsearch</a></p></li><li><p><a href="https://github.com/opsnull/follow-me-install-kubernetes-cluster/blob/master/08-5.EFK%E6%8F%92%E4%BB%B6.md">https://github.com/opsnull/follow-me-install-kubernetes-cluster/blob/master/08-5.EFK%E6%8F%92%E4%BB%B6.md</a></p></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>ELK</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes集群水平自动伸缩HPA详解</title>
    <link href="/linux/KubernetesHPA/"/>
    <url>/linux/KubernetesHPA/</url>
    
    <content type="html"><![CDATA[<p>HPA，Horizontal Pod Autoscaling，即Pod水平自动伸缩，是通过监控分析RC或Deployment控制器Pod的负载变化动态调整Pod的副本数量的Kubernetes集群资源对象，目的是根据业务整体压力调整集群规模，以应对流量的场景。HPA Controller通过kube-controller-manager组件的启动参数–horizontal-pod-autoscaler-sync-period定义的轮询时间间隔周期性查询指定资源所含Pod的负载，再根据创建时设定的策略判断是否达到阈值，若达到阈值则根据扩缩容策略扩容副本分担压力，直到Pod稳定空闲一段时间后再缩减副本数量，完成自动伸缩流程</p><h1 id="1-安装metrics"><a href="#1-安装metrics" class="headerlink" title="1.安装metrics"></a>1.安装metrics</h1><pre><code class="hljs">wget -O metrics-server.yml https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.5.0/components.yamlkubectl apply -f metrics-server.yaml</code></pre><h1 id="2-创建HPA"><a href="#2-创建HPA" class="headerlink" title="2.创建HPA"></a>2.创建HPA</h1><pre><code class="hljs">kubectl autoscale deployment nginx-deployment --cpu-percent=90 --min=1 --max=6</code></pre><h1 id="3-应用压测，验证HPA"><a href="#3-应用压测，验证HPA" class="headerlink" title="3.应用压测，验证HPA"></a>3.应用压测，验证HPA</h1><pre><code class="hljs">kubectl exec -it nginx -- while true;do curl -I nginx-service;done</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.cnblogs.com/maxzhang1985/p/15989762.html">https://www.cnblogs.com/maxzhang1985/p/15989762.html</a></li><li><a href="https://blog.csdn.net/fly910905/article/details/105375822">https://blog.csdn.net/fly910905/article/details/105375822</a></li><li><a href="https://blog.csdn.net/zenglingmin8/article/details/116837006">https://blog.csdn.net/zenglingmin8/article/details/116837006</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes集群Etcd数据库详解</title>
    <link href="/linux/KubernetesEtcd/"/>
    <url>/linux/KubernetesEtcd/</url>
    
    <content type="html"><![CDATA[<p>Etcd，由CoreOS团队于2013年用Go言编写的分布式、高可用的开源一致性key-value存储系统，现由Cloud Native Computing Foundation负责管理，用于提供可靠的分布式键值存储、配置共享和服务发现、一致性保障等功能，如数据库选主、分布式锁等。Etcd提供的数据TTL失效、数据改变监视、多值、目录监听、分布式锁原子操作等功能可以方便的跟踪并管理集群节点的状态，从而实现了集群环境的服务发现和注册，解决了分布式系统如何管理节点间的状态这一难题，为跨服务器集群的数据存储提供了可靠的方式</p><p>Etcd具有以下特点：</p><ul><li>简单易使用，基于HTTP+JSON的API，curl即可轻松使用</li><li>易部署，由Go语言编写，跨平台，部署和维护简单</li><li>可靠强一致性，使用Raft算法充分保证了分布式系统数据的强一致性，集群中的每个节点都存储有完整的数据</li><li>高可用，具有容错能力，若集群有n个节点，当有(n-1)&#x2F;2节点发生故障时依然能提供服务</li><li>持久化，将数据存储在分层组织的目录中，如同在标准文件系统中，数据更新后将通过WAL格式数据持久化到磁盘，并支持Snapshot快照</li><li>快速，单实例每秒支持1000次写操作，2000+次读操作，极限写性能可达10K QPS</li><li>安全，可选SSL客户认证机制</li></ul><hr><h1 id="体系架构"><a href="#体系架构" class="headerlink" title="体系架构"></a>体系架构</h1><p>用户的请求数据经由HTTP Server转发给Store进行具体的事务处理，如涉及节点的修改，则交由Raft模块进行状态的变更、日志的记录，然后再同步给其余的Etcd节点以确认数据提交，最后进行数据的提交，再次同步</p><h2 id="1-HTTP-Server"><a href="#1-HTTP-Server" class="headerlink" title="1.HTTP Server"></a>1.HTTP Server</h2><p>用于处理用户发送的API请求以及其它Etcd节点的同步与心跳信息请求</p><h2 id="2-Store"><a href="#2-Store" class="headerlink" title="2.Store"></a>2.Store</h2><p>用于处理Etcd支持的各类功能的事务，包括数据索引、节点状态变更、监控与反馈、事件处理与执行等等，是Etcd对用户提供的大多数API功能的具体实现</p><h2 id="3-Raft"><a href="#3-Raft" class="headerlink" title="3.Raft"></a>3.Raft</h2><p>强一致性算法的具体实现，是Etcd的核心，使得Etcd天然地成为一个强一致性、高可用的服务存储目录</p><h2 id="4-WAL"><a href="#4-WAL" class="headerlink" title="4.WAL"></a>4.WAL</h2><p>WAL，Write Ahead Log，即预写式日志，是Etcd的持久化数据存储方式，记录了整个数据库变化的全部历程，即所有内存数据及其状态数据、节点索引数据提交前事先记录日志。这个机制极大的保障了整个集群的数据完整性，可快速从故障中恢复数据、数据回滚及重做。其中，Snapshot是为了防止数据过多而进行的状态快照，Entry表示存储的具体日志内容</p><h1 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h1><h2 id="1-服务注册与发现"><a href="#1-服务注册与发现" class="headerlink" title="1.服务注册与发现"></a>1.服务注册与发现</h2><p>服务注册及健康状况的机制，即在同一个分布式集群中的进程或服务如何才能找到对方并建立连接。用户可在Etcd注册服务，并且对注册的服务配置key TTL，定时保持服务的心跳，从而达到监控健康状态的效果</p><h2 id="2-消息发布与订阅"><a href="#2-消息发布与订阅" class="headerlink" title="2.消息发布与订阅"></a>2.消息发布与订阅</h2><p>分布式系统组件间通信方式最适用的即为消息发布与订阅机制，即构建一个共享配置中心，数据提供者在配置中心发布消息，而消息使用者则从配置中心订阅所需主题，一旦主题有消息发布就会实时通知订阅者，从而实现分布式系统配置的集中式管理与动态更新功能</p><p>具体地说，业务系统将配置信息写入Etcd进行集中管理，启动的时候主动获取一次配置信息，同时在Etcd节点上注册一个Watcher并等待，之后每次配置有更新的时候，Etcd都会实时通知订阅者，以此达到获取最新配置信息的目的</p><p>Kubernetes集群的API Server将群集状态持久化到Etcd，并通过watch API监视集群，从而发布关键的配置更改，如Pod的创建与删除等操作</p><h2 id="3-分布式通知与协调"><a href="#3-分布式通知与协调" class="headerlink" title="3.分布式通知与协调"></a>3.分布式通知与协调</h2><p>分布式系统中，有一种典型的设计模式就是Master+Slave，即主从模式，具体表现为：Slave提供CPU、内存、磁盘以及网络等各种资源，Master用来协调这些节点以使其对外提供一个服务，如分布式存储（HDFS）、分布式计算（Hadoop）等。主从模式为保障业务的可用性通常会启动多个Master节点作为冗余，通过选主的方式将其中一个节点作为主节点提供服务，其余节点处于等待状态。Etcd的机制即可很容易的实现分布式进程的选主功能</p><h2 id="4-分布式锁"><a href="#4-分布式锁" class="headerlink" title="4.分布式锁"></a>4.分布式锁</h2><p>Etcd的Raft算法保持了数据的强一致性，某次操作存储到集群中的值必然是全局一致的，这样的机制很容易实现分布式锁，分为两种方式，即保持独占和控制时序</p><ul><li>保持独占，即所有获取锁的用户最终只有一个用户可以得到。Etcd提供了实现分布式锁原子操作CAS（CompareAndSwap）的API，通过设置prevExist值，可保证在多个节点同时去创建某个目录时，只有一个能成功，创建成功的用户即为获得了锁</li><li>控制时序，即所有想要获得锁的用户都会被安排执行，但是获得锁的顺序是全局唯一的，且决定了执行顺序。Etcd提供了自动创建有序键的API，对一个目录建值时指定为POST动作，同时会自动在目录下生成一个当前最大的值为键以存储这个新的值，即客户端编号，且还可以按顺序列出所有当前目录下的键值。此时这些键的值就是客户端的时序，而这些键中存储的值可代表客户端的编号</li></ul><hr><h1 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h1><h2 id="1-查看集群成员列表"><a href="#1-查看集群成员列表" class="headerlink" title="1.查看集群成员列表"></a>1.查看集群成员列表</h2><pre><code class="hljs">/opt/etcd/bin/etcdctl --endpoints=https://192.168.100.180:2379,https://192.168.100.181:2379,https://192.168.100.182:2379 --cacert=/opt/etcd/ssl/ca.pem --cert=/opt/etcd/ssl/server.pem --key=/opt/etcd/ssl/server-key.pem member list --write-out=table</code></pre><h2 id="2-查看集群成员健康状态"><a href="#2-查看集群成员健康状态" class="headerlink" title="2.查看集群成员健康状态"></a>2.查看集群成员健康状态</h2><pre><code class="hljs">/opt/etcd/bin/etcdctl --endpoints=https://192.168.100.180:2379,https://192.168.100.181:2379,https://192.168.100.182:2379 --cacert=/opt/etcd/ssl/ca.pem --cert=/opt/etcd/ssl/server.pem --key=/opt/etcd/ssl/server-key.pem endpoint health</code></pre><h2 id="3-查看集群成员读写状态"><a href="#3-查看集群成员读写状态" class="headerlink" title="3.查看集群成员读写状态"></a>3.查看集群成员读写状态</h2><pre><code class="hljs">/opt/etcd/bin/etcdctl --endpoints=https://192.168.100.180:2379,https://192.168.100.181:2379,https://192.168.100.182:2379 --cacert=/opt/etcd/ssl/ca.pem --cert=/opt/etcd/ssl/server.pem --key=/opt/etcd/ssl/server-key.pem endpoint status --write-out=table</code></pre><h2 id="4-写入数据"><a href="#4-写入数据" class="headerlink" title="4.写入数据"></a>4.写入数据</h2><pre><code class="hljs">/opt/etcd/bin/etcdctl --endpoints=https://192.168.100.180:2379,https://192.168.100.181:2379,https://192.168.100.182:2379 --cacert=/opt/etcd/ssl/ca.pem --cert=/opt/etcd/ssl/server.pem --key=/opt/etcd/ssl/server-key.pem --endpoints=localhost:2379 put /kubesre 123</code></pre><h2 id="5-查询数据"><a href="#5-查询数据" class="headerlink" title="5.查询数据"></a>5.查询数据</h2><pre><code class="hljs">/opt/etcd/bin/etcdctl --endpoints=https://192.168.100.180:2379,https://192.168.100.181:2379,https://192.168.100.182:2379 --cacert=/opt/etcd/ssl/ca.pem --cert=/opt/etcd/ssl/server.pem --key=/opt/etcd/ssl/server-key.pem --endpoints=localhost:2379 get  /kubesre</code></pre><h2 id="6-按key的前缀查询数据"><a href="#6-按key的前缀查询数据" class="headerlink" title="6.按key的前缀查询数据"></a>6.按key的前缀查询数据</h2><pre><code class="hljs">/opt/etcd/bin/etcdctl --endpoints=https://192.168.100.180:2379,https://192.168.100.181:2379,https://192.168.100.182:2379 --cacert=/opt/etcd/ssl/ca.pem --cert=/opt/etcd/ssl/server.pem --key=/opt/etcd/ssl/server-key.pem --endpoints=localhost:2379 get --prefix / </code></pre><h2 id="7-只显示键值"><a href="#7-只显示键值" class="headerlink" title="7.只显示键值"></a>7.只显示键值</h2><pre><code class="hljs">/opt/etcd/bin/etcdctl --endpoints=https://192.168.100.180:2379,https://192.168.100.181:2379,https://192.168.100.182:2379 --cacert=/opt/etcd/ssl/ca.pem --cert=/opt/etcd/ssl/server.pem --key=/opt/etcd/ssl/server-key.pem --endpoints=localhost:2379 get --prefix / --keys-only --debug </code></pre><h2 id="8-清空集群数据"><a href="#8-清空集群数据" class="headerlink" title="8.清空集群数据"></a>8.清空集群数据</h2><pre><code class="hljs">/opt/etcd/bin/etcdctl --endpoints=https://192.168.100.180:2379,https://192.168.100.181:2379,https://192.168.100.182:2379 --cacert=/opt/etcd/ssl/ca.pem --cert=/opt/etcd/ssl/server.pem --key=/opt/etcd/ssl/server-key.pem del --prefix /</code></pre><hr><p>Kubernetes集群是一个典型的声明式系统，即是通过API将用户的期望状态持久化到Etcd数据库，再由系统各个组件通过不断地读取数据库，最后以异步方式执行该状态。这种面向最终状态的方式很好的降低了运维和排查错误的成本，只需给出每个Pod的最终状态即可让各个组件自动执行，相比每执行一条命令就执行相关操作的面向过程的运维模式，大大的提高了系统整体的可靠性。但这样密集的I&#x2F;O操作，Etcd数据库的性能必然大受影响，甚至成为整个集群的性能瓶颈。所以，Etcd数据库的高性能运行对Kubernetes集群的稳定性至关重要</p><hr><h1 id="1-Etcd数据库备份"><a href="#1-Etcd数据库备份" class="headerlink" title="1.Etcd数据库备份"></a>1.Etcd数据库备份</h1><p>Kubernetes集群所有的对象都存储在Etcd上，定期备份数据对于灾难场景下的快速恢复非常重要。Etcd可通过快照的机制进行数据备份，快照文件包含所有Kubernetes状态和关键信息</p><h2 id="1-1-备份kubernetes相关目录"><a href="#1-1-备份kubernetes相关目录" class="headerlink" title="1.1 备份kubernetes相关目录"></a>1.1 备份kubernetes相关目录</h2><pre><code class="hljs">sudo cp -r /etc/kubernetes /etc/kubernetes_baksudo cp -r /var/lib/etcd /var/lib/etcd_bak# 二进制安装的集群sudo cp -r /var/lib/etcd/default.etcd /var/lib/etcd/default.etcd.baksudo cp -r /var/lib/kubet /var/lib/kubet_bak</code></pre><h2 id="1-2-备份数据库"><a href="#1-2-备份数据库" class="headerlink" title="1.2 备份数据库"></a>1.2 备份数据库</h2><pre><code class="hljs">sudo ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --cacert=/etc/kubernetes/pki/etcd/ca.crt snapshot save /home/sword/etcd_bak/etcd-snap-$(date +%Y%m%d%H%M).db</code></pre><h2 id="1-3-恢复数据库"><a href="#1-3-恢复数据库" class="headerlink" title="1.3 恢复数据库"></a>1.3 恢复数据库</h2><h3 id="1-3-1-停止Api-Server与Etcd服务"><a href="#1-3-1-停止Api-Server与Etcd服务" class="headerlink" title="1.3.1 停止Api Server与Etcd服务"></a>1.3.1 停止Api Server与Etcd服务</h3><pre><code class="hljs"># 二进制安装的集群用systemctl管理即可sudo mv /etc/kubernetes/manifests /etc/kubernetes/manifests_bak# 二进制安装的集群为sudo rm -rf /var/lib/etcd/default.etcdsudo rm -rf /var/lib/etcd</code></pre><h3 id="1-3-2-恢复数据"><a href="#1-3-2-恢复数据" class="headerlink" title="1.3.2 恢复数据"></a>1.3.2 恢复数据</h3><pre><code class="hljs">sudo ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --cacert=/etc/kubernetes/pki/etcd/ca.crt snapshot restore /home/sword/etcd_bak/etcd-snap-202303061755.db --data-dir=/var/lib/etcd</code></pre><h2 id="1-4-启动kube-apiserver和Etcd，验证集群状态"><a href="#1-4-启动kube-apiserver和Etcd，验证集群状态" class="headerlink" title="1.4 启动kube-apiserver和Etcd，验证集群状态"></a>1.4 启动kube-apiserver和Etcd，验证集群状态</h2><pre><code class="hljs"># 二进制安装的集群用systemctl管理即可sudo mv /etc/kubernetes/manifests_bak /etc/kubernetes/manifests</code></pre><ul><li>注：高可用master节点的kubernetes集群恢复数据库的时候要在所有master节点执行恢复的命令</li></ul><hr><h1 id="2-Etcd隔离环境"><a href="#2-Etcd隔离环境" class="headerlink" title="2.Etcd隔离环境"></a>2.Etcd隔离环境</h1><p>Etcd集群的性能和稳定性对网络和磁盘IO非常敏感，任何资源匮乏都会导致心跳超时，使得Kubernetes集群不能调度新的Pod，从而引发故障。故此，在单独的专用的隔离环境上运行Etcd集群十分必要，且磁盘也建议使用更为快速的本地SSD硬盘</p><h2 id="2-1-部署Etcd集群"><a href="#2-1-部署Etcd集群" class="headerlink" title="2.1 部署Etcd集群"></a>2.1 部署Etcd集群</h2><h2 id="2-2-分发Etcd证书"><a href="#2-2-分发Etcd证书" class="headerlink" title="2.2 分发Etcd证书"></a>2.2 分发Etcd证书</h2><pre><code class="hljs">scp /opt/etcd/ssl/*.pem master01:/etc/kubernetes/pki/etcdmv /etc/kubernetes/pki/etcd/server.pem /etc/kubernetes/pki/etcd/etcd-server.pemmv /etc/kubernetes/pki/etcd/server-key.pem /etc/kubernetes/pki/etcd/etcd-server-key.pem</code></pre><h2 id="2-3-创建初始化配置文件"><a href="#2-3-创建初始化配置文件" class="headerlink" title="2.3 创建初始化配置文件"></a>2.3 创建初始化配置文件</h2><pre><code class="hljs">vi kubeadm-config.yamlapiVersion: kubeadm.k8s.io/v1beta2kind: ClusterConfigurationkubernetesVersion: v1.20.12clusterName: kubernetesimageRepository: registry.aliyuncs.com/google_containersetcd:   external:    endpoints:    - https://192.168.100.100:2379    - https://192.168.100.120:2379    - https://192.168.100.200:2379    caFile: /etc/kubernetes/pki/etcd/ca.pem    certFile: /etc/kubernetes/pki/etcd/etcd-server.pem    keyFile: /etc/kubernetes/pki/etcd/etcd-server-key.pemapiServer:  certSANs:  - 192.168.100.100  - 192.168.100.120  - 192.168.100.200  - 192.168.100.150controlPlaneEndpoint: &quot;192.168.100.150:8443&quot;networking:  podSubnet: &quot;172.30.0.0/16&quot;  serviceSubnet: &quot;10.254.0.0/16&quot;---apiVersion: kubeproxy.config.k8s.io/v1alpha1kind: KubeProxyConfigurationmode: ipvs</code></pre><h2 id="2-4-初始化集群"><a href="#2-4-初始化集群" class="headerlink" title="2.4 初始化集群"></a>2.4 初始化集群</h2><pre><code class="hljs">kubeadm init --config=kubeadm-config.yaml</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/lihongbao80/article/details/126508726">https://blog.csdn.net/lihongbao80/article/details/126508726</a></li><li><a href="https://blog.csdn.net/Jerry00713/article/details/126581563">https://blog.csdn.net/Jerry00713/article/details/126581563</a></li><li><a href="https://blog.csdn.net/fengge55/article/details/121797974">https://blog.csdn.net/fengge55/article/details/121797974</a></li><li><a href="https://blog.csdn.net/alwaysbefine/article/details/127500573">https://blog.csdn.net/alwaysbefine/article/details/127500573</a></li><li><a href="https://blog.csdn.net/yujia_666/article/details/120667639">https://blog.csdn.net/yujia_666/article/details/120667639</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
      <tag>Etcd</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CoreDNS搭建DNS服务器</title>
    <link href="/geek/CoreDNS/"/>
    <url>/geek/CoreDNS/</url>
    
    <content type="html"><![CDATA[<h1 id="1-下载coredns二进制安装包"><a href="#1-下载coredns二进制安装包" class="headerlink" title="1.下载coredns二进制安装包"></a>1.下载coredns二进制安装包</h1><pre><code class="hljs">wget https://github.com/coredns/coredns/releases/download/v1.10.1/coredns_1.10.1_linux_amd64.tgz</code></pre><h1 id="2-安装coredns"><a href="#2-安装coredns" class="headerlink" title="2.安装coredns"></a>2.安装coredns</h1><pre><code class="hljs">tar -xzvf coredns_1.10.1_linux_amd64.tgzsudo mv coredns /usr/local/binsudo chmod +x /usr/local/bin/coredns</code></pre><h1 id="3-创建配置文件"><a href="#3-创建配置文件" class="headerlink" title="3.创建配置文件"></a>3.创建配置文件</h1><pre><code class="hljs">sudo mkdir -p /etc/coredns/zones</code></pre><h2 id="3-1-创建主配置文件"><a href="#3-1-创建主配置文件" class="headerlink" title="3.1 创建主配置文件"></a>3.1 创建主配置文件</h2><pre><code class="hljs">sudo vi /etc/coredns/corefile.:53 &#123;    loadbalance    forward . 8.8.8.8 8.8.4.4    cache 120    reload 6s    log    errors    # 配置hosts插件，适用于解析的域名较少的场景    hosts &#123;      192.168.0.41 web.local.com      ttl 60      reload 1m      fallthrough    &#125;    auto sword.org &#123;        directory /etc/coredns/zones        reload 10s    &#125;&#125;org:53 &#123;    hosts /etc/coredns/hostsfile    log&#125;</code></pre><h2 id="3-2-创建配置文件"><a href="#3-2-创建配置文件" class="headerlink" title="3.2 创建配置文件"></a>3.2 创建配置文件</h2><pre><code class="hljs">sudo vi /etc/coredns/hostsfile192.168.100.120 test.sword.org</code></pre><h1 id="4-创建启动脚本"><a href="#4-创建启动脚本" class="headerlink" title="4.创建启动脚本"></a>4.创建启动脚本</h1><pre><code class="hljs">sudo vi /lib/systemd/system/coredns.service[Unit]Description=https://github.com/coredns/deploymentAfter=network.target[Service]WorkingDirectory=/usr/local/binRestart=on-failureExecStart=/usr/local/bin/coredns -conf=/etc/coredns/corefile#ExecReload=/bin/kill -HUP $MAINPIDType=simpleKillMode=control-groupRestartSec=3[Install]WantedBy=multi-user.target</code></pre><h1 id="5-启动coredns服务"><a href="#5-启动coredns服务" class="headerlink" title="5.启动coredns服务"></a>5.启动coredns服务</h1><pre><code class="hljs">sudo systemctl daemon-reload  sudo systemctl start coredns  sudo systemctl enable coredns</code></pre><h1 id="6-配置客户端DNS服务器，测试域名解析"><a href="#6-配置客户端DNS服务器，测试域名解析" class="headerlink" title="6.配置客户端DNS服务器，测试域名解析"></a>6.配置客户端DNS服务器，测试域名解析</h1><h2 id="6-1-Windows客户端"><a href="#6-1-Windows客户端" class="headerlink" title="6.1 Windows客户端"></a>6.1 Windows客户端</h2><h2 id="6-2-MacOS或Linux客户端"><a href="#6-2-MacOS或Linux客户端" class="headerlink" title="6.2 MacOS或Linux客户端"></a>6.2 MacOS或Linux客户端</h2><pre><code class="hljs">sudo vi /etc/resolv.confnameserver 192.168.100.120nameserver 8.8.8.8</code></pre><h2 id="6-3-测试域名解析"><a href="#6-3-测试域名解析" class="headerlink" title="6.3 测试域名解析"></a>6.3 测试域名解析</h2><pre><code class="hljs">ping test.sword.org</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.cnblogs.com/XY-Heruo/p/16783123.html">https://www.cnblogs.com/XY-Heruo/p/16783123.html</a></li><li><a href="https://www.cnblogs.com/menglingqian/p/15872362.html">https://www.cnblogs.com/menglingqian/p/15872362.html</a></li><li><a href="https://www.pudn.com/news/623c277fe28b415bafe2cf2a.html">https://www.pudn.com/news/623c277fe28b415bafe2cf2a.html</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
      <category>极客</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>DNS</tag>
      
      <tag>域名解析</tag>
      
      <tag>极客</tag>
      
      <tag>CoreDNS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>tinyMediaManager配置电影海报刮削器</title>
    <link href="/geek/Jellyfin-tinyMediaManager/"/>
    <url>/geek/Jellyfin-tinyMediaManager/</url>
    
    <content type="html"><![CDATA[<hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://post.smzdm.com/p/ar0nq5dg">https://post.smzdm.com/p/ar0nq5dg</a></li><li><a href="https://zhuanlan.zhihu.com/p/510388110">https://zhuanlan.zhihu.com/p/510388110</a></li><li><a href="https://www.bilibili.com/read/cv18215732">https://www.bilibili.com/read/cv18215732</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>极客</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>极客</tag>
      
      <tag>Jellyfin</tag>
      
      <tag>影音娱乐</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Jellyfin搭建私人家庭影音娱乐中心</title>
    <link href="/geek/Jellyfin/"/>
    <url>/geek/Jellyfin/</url>
    
    <content type="html"><![CDATA[<p>Jellyfin，跨平台的开源免费的媒体管理系统，用于控制和管理媒体、流媒体文件，通过多个应用程序从专用服务器向终端用户设备提供媒体，配合TTM刮削器可实现华丽的家庭私人影院。Jellyfin分为服务端和客户端应用程序，服务端安装运行于Microsoft Windows、MacOS、Linux、NAS等系统的服务器，客户端可安装在智能手机、平板电脑、智能电视、网络机顶盒、电子游戏机或网页浏览器</p><hr><p><img src="/img/wiki/jellyfin/jellyfin.jpg" alt="jellyfin"></p><hr><h1 id="1-容器化部署"><a href="#1-容器化部署" class="headerlink" title="1.容器化部署"></a>1.容器化部署</h1><h2 id="1-1-安装Docker，拉取镜像"><a href="#1-1-安装Docker，拉取镜像" class="headerlink" title="1.1 安装Docker，拉取镜像"></a>1.1 安装Docker，拉取镜像</h2><pre><code class="hljs">sudo docker pull nyanmisaka/jellyfin</code></pre><h2 id="1-2-创建Jellyfin容器"><a href="#1-2-创建Jellyfin容器" class="headerlink" title="1.2 创建Jellyfin容器"></a>1.2 创建Jellyfin容器</h2><pre><code class="hljs">sudo docker run -d -it -p 8096:8096 --restart=always \-v /web/jellyfin/config:/config -v /web/jellyfin/cache:/cache \-v /home:/media --name jellyfin jellyfin</code></pre><h1 id="2-服务器部署"><a href="#2-服务器部署" class="headerlink" title="2.服务器部署"></a>2.服务器部署</h1><h2 id="2-1-安装Jellyfin"><a href="#2-1-安装Jellyfin" class="headerlink" title="2.1 安装Jellyfin"></a>2.1 安装Jellyfin</h2><pre><code class="hljs">curl -s https://repo.jellyfin.org/install-debuntu.sh | sudo bash</code></pre><h2 id="2-2-启动Jellyfin"><a href="#2-2-启动Jellyfin" class="headerlink" title="2.2 启动Jellyfin"></a>2.2 启动Jellyfin</h2><pre><code class="hljs">sudo systemctl start jellyfin.servicesudo systemctl enable jellyfin.service  </code></pre><h1 id="3-配置Nginx反向代理"><a href="#3-配置Nginx反向代理" class="headerlink" title="3.配置Nginx反向代理"></a>3.配置Nginx反向代理</h1><pre><code class="hljs">server &#123;  listen       80;  server_name  localhost;  location / &#123;  access_log  /var/log/nginx/jellyfin_access.log  main;  error_log  /var/log/nginx/jellyfin_error.log;  &#125;&#125;</code></pre><h1 id="4-登陆Jellyfin"><a href="#4-登陆Jellyfin" class="headerlink" title="4.登陆Jellyfin"></a>4.登陆Jellyfin</h1><p><a href="http://ip/">http://ip</a></p><h1 id="5-添加媒体库"><a href="#5-添加媒体库" class="headerlink" title="5.添加媒体库"></a>5.添加媒体库</h1><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://jellyfin.org/docs/">https://jellyfin.org/docs/</a></li><li><a href="https://post.smzdm.com/p/ao97n5wn">https://post.smzdm.com/p/ao97n5wn</a></li><li><a href="https://post.smzdm.com/p/a4x2nenw">https://post.smzdm.com/p/a4x2nenw</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>极客</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>极客</tag>
      
      <tag>Jellyfin</tag>
      
      <tag>影音娱乐</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ansible自动化运维工具Playbook</title>
    <link href="/linux/AnsiblePlaybook/"/>
    <url>/linux/AnsiblePlaybook/</url>
    
    <content type="html"><![CDATA[<p>Playbook，剧本，Ansible用于配置、部署及管理被控节点的任务编排脚本，描述了一系列tasks的执行，类似于shell脚本执行批量任务，由用于编写配置文件的YAML语言编写</p><hr><h1 id="1-语法格式"><a href="#1-语法格式" class="headerlink" title="1.语法格式"></a>1.语法格式</h1><ul><li><p>文件第一行以“—–”开始，表明YAML文件的开始</p></li><li><p>#，表示注释，类似于Shell、python和Ruby</p></li><li><p>YAML文件的列表元素以“-”开头且跟着一个空格，其后为元素内容</p></li><li><p>同一个列表之中的元素应保持相同缩进，否则将被当做错误处理</p></li><li><p>playbook中hosts、variables、roles、tasks等对象的表示方法是以键值中间“:”为分隔表示，且“:”之后要有一个空格</p></li></ul><h1 id="2-核心元素"><a href="#2-核心元素" class="headerlink" title="2.核心元素"></a>2.核心元素</h1><ul><li>Hosts，远程主机组</li><li>Variables，变量，可传递给Tasks、Templates</li><li>Tasks，任务集，由模块调用所定义的操作任务的列表，以“-”开头</li><li>Templates，模版，包含了模块语法的文本文件  </li><li>Handlers，处理器，由特定条件触发的任务集，即某个Task成功执行状态为changed时，触发执行由Notify所指定的Handler，通常用于配置文件创建或更改时service的重启</li></ul><h1 id="3-基础组件"><a href="#3-基础组件" class="headerlink" title="3.基础组件"></a>3.基础组件</h1><pre><code class="hljs">---# 设置playbook或task的名称name: test# 设置所要运行任务的目标主机hosts：nodes# 设置远程主机执行任务的用户，默认为rootremoute_user: user001 # 设置sudo用户sudo_user: user001# 设置任务列表tasks:  # 设置任务名称   - name: task01    # 设置执行任务模块     module: arguments    # 设置该任务执行成功所触发的处理器    notify: handlers001   - name: task02    module: arguments    # 设置模版文件    template：    # 设置条件判断，即满足条件才能执行任务     when: Expression  # 设置处理器  handlers:       # 设置处理器名称      - name: handlers001        # 设置执行任务模块         module: arguments</code></pre><h1 id="4-创建Playbook"><a href="#4-创建Playbook" class="headerlink" title="4.创建Playbook"></a>4.创建Playbook</h1><pre><code class="hljs">vi nginx.yaml---- name: Nginx安装部署  hosts: nodes  remote_user: sword  become: yes  become_user: root  tasks:    - name: 安装Nginx      apt:        name:            - curl          - wget          - nginx        state: latest    - name: 启动nginx      service: name=nginx state=started</code></pre><h1 id="5-语法检查"><a href="#5-语法检查" class="headerlink" title="5.语法检查"></a>5.语法检查</h1><pre><code class="hljs"># 语法错误检查ansible-playbook --syntax-check nginx.yaml# 模拟执行，并不会在主机上真正执行任务ansible-playbook -C</code></pre><h1 id="6-执行Playbook"><a href="#6-执行Playbook" class="headerlink" title="6.执行Playbook"></a>6.执行Playbook</h1><pre><code class="hljs">ansible-playbook nginx.yaml</code></pre><h1 id="7-验证执行结果"><a href="#7-验证执行结果" class="headerlink" title="7.验证执行结果"></a>7.验证执行结果</h1><pre><code class="hljs">ansible nodes -m shell -a &#39;ps -ef|grep nginx|grep -v nginx&#39;</code></pre><h1 id="8-实战：kubernetes高可用集群部署"><a href="#8-实战：kubernetes高可用集群部署" class="headerlink" title="8.实战：kubernetes高可用集群部署"></a>8.实战：kubernetes高可用集群部署</h1><pre><code class="hljs">vi kubernetes.yaml---- name: kubernetes高可用集群部署  hosts: k8s  remote_user: sword  become: yes  become_user: root  tasks:    - name: 安装依赖包      apt:        name:            - curl          - gnupg2          - ca-certificates          - apt-transport-https        state: latest    - name: 导入阿里云apt源密钥      shell: curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add -    - name: 禁用swap      shell: sed -ri &#39;s/.*swap.*/#&amp;/&#39; /etc/fstab &amp;&amp; swapoff -a    - name: 开启路由转发      copy: src=k8s.conf dest=/etc/sysctl.d    - name: 生效配置文件      shell: sysctl -p    - name: 解压docker安装包      unarchive: src=/home/works/Linux/software/docker-19.03.12.tgz dest=/home/sword    - name: 安装docker      shell: mv docker/* /usr/bin &amp;&amp; mkdir -p /etc/docker &amp;&amp; mkdir -p /root/.docker    - name: 配置docker仓库加速器      copy: src=docker/daemon.json dest=/etc/docker    - name: 配置docker仓库认证文件      copy: src=docker/config.json dest=/root/.docker       - name: 创建创建docker服务启动脚本      copy: src=docker/docker.service dest=/lib/systemd/system    - name: 启动docker      service: name=docker state=started enabled=yes    - name: 配置kubernetes阿里云apt源      copy: src=kubernetes.list dest=/etc/apt/sources.list.d    - name: 安装kubeadm、kubelet、kubectl      shell: apt update &amp;&amp; apt install -y kubelet=1.20.12-00 kubeadm=1.20.12-00 &amp;&amp; systemctl enable kubelet    - name: master节点安装haroxy、keepalived      apt:         name:          - haproxy          - keepalived        state: latest      when: ansible_hostname in groups [&#39;master&#39;]     - name: master节点创建haproxy配置文件      copy: src=haproxy.cfg dest=/etc/haproxy      when: ansible_hostname in groups [&#39;master&#39;]     - name: master节点启动haproxy      service: name=haproxy state=started enabled=yes      when: ansible_hostname in groups [&#39;master&#39;]    - name: master节点创建keepalived配置文件      copy: src=keepalived.conf dest=/etc/keepalived      when: ansible_hostname in groups [&#39;master&#39;]    - name: master节点创建haproxy服务状态监控脚本      copy: src=haproxy_check.sh dest=/etc/keepalived      when: ansible_hostname in groups [&#39;master&#39;]    - name: master02节点修改keepalived配置文件      shell: sed -i &#39;s/master01/master02/g&#39; /etc/keepalived/keepalived.conf &amp;&amp; sed -i &#39;s/priority 100/priority 80/g&#39; /etc/keepalived/keepalived.conf      when: ansible_hostname==&quot;master02&quot;    - name: master03节点修改keepalived配置文件      shell: sed -i &#39;s/master01/master03/g&#39; /etc/keepalived/keepalived.conf &amp;&amp; sed -i &#39;s/priority 100/priority 60/g&#39; /etc/keepalived/keepalived.conf      when: ansible_hostname==&quot;master03&quot;    - name: master节点启动keepalived      service: name=keepalived state=started enabled=yes</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://zhuanlan.zhihu.com/p/149499486">https://zhuanlan.zhihu.com/p/149499486</a></li><li><a href="https://blog.csdn.net/qq_42761527/article/details/105984961">https://blog.csdn.net/qq_42761527/article/details/105984961</a></li><li><a href="https://blog.csdn.net/weixin_34162695/article/details/89784000">https://blog.csdn.net/weixin_34162695/article/details/89784000</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Ansible</tag>
      
      <tag>自动化运维</tag>
      
      <tag>云计算</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux系统安装Web虚拟化管理平台Cockpit</title>
    <link href="/geek/Cockpit/"/>
    <url>/geek/Cockpit/</url>
    
    <content type="html"><![CDATA[<p>Cockpit，ReaHat开发的网页版图形化服务器管理工具，简单易用，支持Debian、Redhat、CentOS、Fedora、Atomic、Arch Linux和Ubuntu多种操作系统，具备存储管理、网络配置、检查日志、管理容器等功能，是管理小规模服务器集群的利器</p><hr><p><img src="/img/wiki/cockpit/001.jpg" alt="001"></p><hr><h1 id="1-安装cockpit"><a href="#1-安装cockpit" class="headerlink" title="1.安装cockpit"></a>1.安装cockpit</h1><pre><code class="hljs"># CentOSsudo yum install -y cockpit cockpit-machines cockpit-docker# Ubuntu. /etc/os-release &amp;&amp; sudo apt install -t $&#123;VERSION_CODENAME&#125;-backports cockpit cockpit-machines cockpit-docker</code></pre><h1 id="2-启动cockpit"><a href="#2-启动cockpit" class="headerlink" title="2.启动cockpit"></a>2.启动cockpit</h1><pre><code class="hljs">sudo systemctl start cockpit.servicesudo systemctl enable cockpit.service</code></pre><h1 id="3-访问cockpit"><a href="#3-访问cockpit" class="headerlink" title="3.访问cockpit"></a>3.访问cockpit</h1><p><a href="https://ip:9090/">https://ip:9090</a></p><h1 id="4-配置KVM桥接网络"><a href="#4-配置KVM桥接网络" class="headerlink" title="4.配置KVM桥接网络"></a>4.配置KVM桥接网络</h1><h1 id="5-安装虚拟机"><a href="#5-安装虚拟机" class="headerlink" title="5.安装虚拟机"></a>5.安装虚拟机</h1><p><img src="/img/wiki/cockpit/001.jpg" alt="002"></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.whsir.com/post-6289.html">https://blog.whsir.com/post-6289.html</a></li><li><a href="https://zhuanlan.zhihu.com/p/113187354">https://zhuanlan.zhihu.com/p/113187354</a></li><li><a href="https://cockpit-project.org/running.html">https://cockpit-project.org/running.html</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>极客</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>极客</tag>
      
      <tag>KVM</tag>
      
      <tag>虚拟化</tag>
      
      <tag>Cockpit</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ansible自动化运维工具常用模块</title>
    <link href="/linux/AnsibleModule/"/>
    <url>/linux/AnsibleModule/</url>
    
    <content type="html"><![CDATA[<h1 id="1-command"><a href="#1-command" class="headerlink" title="1.command"></a>1.command</h1><p>默认模块，远程主机执行命令时不经过其Shell处理，不支持管道符、重定向和变量  </p><pre><code class="hljs">ansible hosts -m command -a &#39;uptime&#39;# chdir，执行命令前先切换到指定目录ansible hosts -m command -a &#39;chdir=/root ls -l&#39; </code></pre><h1 id="2-shell"><a href="#2-shell" class="headerlink" title="2.shell"></a>2.shell</h1><p>基础模块，远程主机执行命令时经过其&#x2F;bin&#x2F;sh程序处理</p><pre><code class="hljs">ansible master -m shell -a &#39;chdir=/root ps -ef|grep keepalived|grep -v grep&#39;</code></pre><h1 id="3-yum-x2F-apt"><a href="#3-yum-x2F-apt" class="headerlink" title="3.yum&#x2F;apt"></a>3.yum&#x2F;apt</h1><p>用于管理远程主机的RPM&#x2F;DEB包，执行安装、升级及卸载操作</p><p>参数详解：<br>name，包名字<br>state，对包进行的操作，latest&#x2F;installd&#x2F;present表示安装，removed&#x2F;absent表示卸载 </p><pre><code class="hljs"># 安装最新版本dockeransible hosts -m yum -a &#39;name=docker state=latest&#39;# 安装指定版本软件包ansible hosts -m apt -a &#39;name=docker-20. state=present&#39;# 将所有软件包升级到最新版本ansible hosts -m apt -a &#39;name=* state=latest&#39;# 卸载软件包ansible hosts -m yum -a &#39;name=docker state=absent&#39;</code></pre><h1 id="4-copy"><a href="#4-copy" class="headerlink" title="4.copy"></a>4.copy</h1><p>用于将控制主机的文件复制到受控主机的制定目录，类似于scp命令，但不支持文件夹和变量</p><p>参数详解：<br>src，本地文件路径<br>dest，远程主机路径<br>backup，备份，默认为no，远程主机的同名文件将被覆盖<br>owner，设置受控主机文件属主，默认与控制主机的权限一致<br>group，设置受控主机文件属组，默认与控制主机的权限一致<br>mode，远程主机的文件权限，默认与控制主机的权限一致</p><pre><code class="hljs">ansible hosts -m copy -a &#39;src=/root/docker- dest=/root&#39;# 将本地文件复制到受控主机，若内容不同则执行备份操作ansible hosts -m copy -a &#39;src=/etc/hosts dest=/etc backup=yes&#39;# 将本地文件复制到受控主机，并修改文件权限ansible host -m copy -a &#39;src=/root/docker- dest=/home/sword owner=sword group=sword mode=755&#39;</code></pre><h1 id="5-unarchive"><a href="#5-unarchive" class="headerlink" title="5.unarchive"></a>5.unarchive</h1><p>用于解压缩远程主机的压缩包</p><p>参数详解：<br>src，控制主机的压缩包文件路径，默认上传到远程主机<br>dest，远程主机压缩包解压的路径<br>copy，是否拷贝压缩包文件到远程主机，默认为yes，no则表示在远程主机搜索压缩包文件<br>exec，排除的文件或目录<br>owner，设置受控主机文件属主，默认与控制主机的权限一致<br>group，设置受控主机文件属组，默认与控制主机的权限一致<br>mode，远程主机的文件权限，默认与控制主机的权限一致</p><pre><code class="hljs">ansible hosts -m unarchive -a &#39;src=docker-20.10.16.tgz dest=/root&#39;ansible hosts -m unarchive -a &#39;src=/root/apache-tomcat-10.0.20.tar.gz dest=/usr/local mode=755&#39;</code></pre><h1 id="5-service"><a href="#5-service" class="headerlink" title="5.service"></a>5.service</h1><p>用于管理远程主机上的服务，且远程主机的服务须通过BSD init,、OpenRC、SysV、Solaris SMF、systemd、<br>upstart中的任意一种所管理</p><p>参数详解：<br>name，服务名称<br>state，对服务进行的操作，started表示启动，stopped表示停止，restarted表示重启，reloaded表示重载<br>enabled，yes表示服务设为开机启动，no则相反</p><pre><code class="hljs">ansible hosts -m service -a &#39;name=docker state=started enabled=yes&#39;</code></pre><h1 id="6-tags"><a href="#6-tags" class="headerlink" title="6.tags"></a>6.tags</h1><p>tags，标签</p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/xcndafad/article/details/122138782?spm=1001.2014.3001.5502">https://blog.csdn.net/xcndafad/article/details/122138782?spm=1001.2014.3001.5502</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Ansible</tag>
      
      <tag>自动化运维</tag>
      
      <tag>云计算</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ProxmoxVE虚拟化管理平台部署与配置</title>
    <link href="/geek/ProxmoxVE/"/>
    <url>/geek/ProxmoxVE/</url>
    
    <content type="html"><![CDATA[<h1 id="1-下载镜像包"><a href="#1-下载镜像包" class="headerlink" title="1.下载镜像包"></a>1.下载镜像包</h1><h1 id="2-制作U盘启动盘"><a href="#2-制作U盘启动盘" class="headerlink" title="2.制作U盘启动盘"></a>2.制作U盘启动盘</h1><h1 id="3-安装PVE"><a href="#3-安装PVE" class="headerlink" title="3.安装PVE"></a>3.安装PVE</h1><h1 id="4-删除无效订阅"><a href="#4-删除无效订阅" class="headerlink" title="4.删除无效订阅"></a>4.删除无效订阅</h1><pre><code class="hljs">sed -i.backup -z &quot;s/res === null || res === undefined || \!res || res\n\t\t\t.data.status.toLowerCase() \!== &#39;active&#39;/false/g&quot; /usr/share/javascript/proxmox-widget-toolkit/proxmoxlib.jssystemctl restart pveproxy.service</code></pre><h1 id="5-注释Proxmox企业版更新源"><a href="#5-注释Proxmox企业版更新源" class="headerlink" title="5.注释Proxmox企业版更新源"></a>5.注释Proxmox企业版更新源</h1><pre><code class="hljs">vi /etc/apt/sources.list.d/pve-enterprise.list    # deb https://enterprise.proxmox.com/debian/pve buster pve-enterprise</code></pre><h1 id="6-配置国内apt源"><a href="#6-配置国内apt源" class="headerlink" title="6.配置国内apt源"></a>6.配置国内apt源</h1><h1 id="7-删除local-lvm"><a href="#7-删除local-lvm" class="headerlink" title="7.删除local-lvm"></a>7.删除local-lvm</h1><pre><code class="hljs"># 删除local-lvm分区lvremove pve/data# 将空闲分区分配给rootlvextend -l +100%FREE -r pve/root</code></pre><p><a href="https://fedorapeople.org/groups/virt/virtio-win/direct-downloads/archive-virtio/virtio-win-0.1.221-1/">https://fedorapeople.org/groups/virt/virtio-win/direct-downloads/archive-virtio/virtio-win-0.1.221-1/</a></p><p>最后，在数据中心-存储页面删除local-lvm分区，编辑local分区，内容一项中勾选所有可选项</p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.nasge.com/archives/136.html">https://www.nasge.com/archives/136.html</a></li><li><a href="https://post.smzdm.com/p/awkv4pq4">https://post.smzdm.com/p/awkv4pq4</a></li><li><a href="https://cloud.tencent.com/developer/article/2007992">https://cloud.tencent.com/developer/article/2007992</a></li><li><a href="https://foxi.buduanwang.vip/virtualization/pve/1868.html">https://foxi.buduanwang.vip/virtualization/pve/1868.html</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>极客</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>极客</tag>
      
      <tag>KVM</tag>
      
      <tag>虚拟化</tag>
      
      <tag>ProxmoxVE</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>oVirt虚拟化管理平台的部署与配置</title>
    <link href="/geek/Ovirt/"/>
    <url>/geek/Ovirt/</url>
    
    <content type="html"><![CDATA[<p>oVirt，基于kvm的企业级开源虚拟化解决方案，是RedHat商业版RHEV的开源版，整合了libvirt、gluster、patternfly、ansible等一系列优秀的开源软件，定位是替代vmware、vsphere，相比OpenStack的庞大和复杂，其简洁的部署与维护更具优势</p><hr><h1 id="体系架构"><a href="#体系架构" class="headerlink" title="体系架构"></a>体系架构</h1><p>oVirt集群架构由三个部分构成，即管理节点Engine、主机节点Node以及存储节点</p><h2 id="1-engine"><a href="#1-engine" class="headerlink" title="1.engine"></a>1.engine</h2><p>用于运行UI、认证及虚拟机管理服务，主要负责集群用户和管理员的认证、虚拟机的创建、开关机、网络与存储用户和管理员的认证等。主要包含以下功能组件：</p><ul><li>ovirt-engine，集群管理组件，管理主机节点、虚拟机、存储、网络等</li><li>Postgresql数据库，用于持久化数据</li><li>SPICE客户端，用于访问虚拟机的工具</li></ul><hr><h2 id="2-node"><a href="#2-node" class="headerlink" title="2.node"></a>2.node</h2><p>主机节点，安装有vdsm和libvirt组件的linux发行版，以及一些用来实现网络虚拟化和其它系统服务的组件，用于运行虚拟机。主要包含以下功能组件：</p><ul><li>VDSM，主机代理，用于与engine通信，接收engine的命令并执行虚拟机与存储的相关操作。此外，还监视主机资源，如内存、存储和网络，以及管理虚拟机创建、统计信息积累和日志收集等任务</li><li>libvirt，被vdsm调用，执行虚拟机的各种管理命令 </li><li>Guest Agent，虚拟机代理，运行于虚拟机内部，通过一个虚拟串口与外部通信，向engine提供所需信息</li></ul><hr><h2 id="3-存储节点"><a href="#3-存储节点" class="headerlink" title="3.存储节点"></a>3.存储节点</h2><p>存储节点，用于存储虚机镜像和iso镜像，支持块存储与文件存储，也可以是主机节点本地存储，还支持外部存储，如NFS。此外，通过gluster将主机节点自身的磁盘组成存储池，即为超融合架构，从而实现虚拟机的高可用和冗余</p><hr><h1 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h1><ul><li>192.168.100.100 engine.ovirt</li><li>192.168.100.180 node01.ovirt</li><li>192.168.100.200 node02.ovirt</li></ul><hr><h1 id="1-安装ovirt仓库包"><a href="#1-安装ovirt仓库包" class="headerlink" title="1.安装ovirt仓库包"></a>1.安装ovirt仓库包</h1><pre><code class="hljs">sudo yum install -y http://resources.ovirt.org/pub/yum-repo/ovirt-release43.rpm  </code></pre><h1 id="2-安装engine"><a href="#2-安装engine" class="headerlink" title="2.安装engine"></a>2.安装engine</h1><pre><code class="hljs">sudo yum install -y ovirt-enginesudo yum update -y</code></pre><h1 id="3-配置engine，设置admin登录密码"><a href="#3-配置engine，设置admin登录密码" class="headerlink" title="3.配置engine，设置admin登录密码"></a>3.配置engine，设置admin登录密码</h1><pre><code class="hljs">sudo engine-setup</code></pre><ul><li>注：设置密码之外的步骤直接回车即可</li></ul><h1 id="4-设置engine通过ip访问"><a href="#4-设置engine通过ip访问" class="headerlink" title="4.设置engine通过ip访问"></a>4.设置engine通过ip访问</h1><pre><code class="hljs">sudo vi /etc/ovirt-engine/engine.conf.d/99-custom-sso-setup.conf    SSO_ALTERNATE_ENGINE_FQDNS=&quot;192.168.100.100&quot;SSO_CALLBACK_PREFIX_CHECK=false</code></pre><h2 id="重启ovirt-engine服务"><a href="#重启ovirt-engine服务" class="headerlink" title="重启ovirt-engine服务"></a>重启ovirt-engine服务</h2><pre><code class="hljs">sudo systemctl restart ovirt-engine.service</code></pre><h1 id="5-修改数据中心存储类型为本地"><a href="#5-修改数据中心存储类型为本地" class="headerlink" title="5.修改数据中心存储类型为本地"></a>5.修改数据中心存储类型为本地</h1><p><img src="/img/wiki/ovirt/datacenter-local.jpg" alt="001"></p><hr><h1 id="6-新建Node节点"><a href="#6-新建Node节点" class="headerlink" title="6.新建Node节点"></a>6.新建Node节点</h1><p><img src="/img/wiki/ovirt/host-add.jpg" alt="002"></p><hr><h1 id="7-新建本地存储域"><a href="#7-新建本地存储域" class="headerlink" title="7.新建本地存储域"></a>7.新建本地存储域</h1><pre><code class="hljs"># 创建虚拟机镜像存储目录并授予权限mkdir -p /home/ovirt/images &amp;&amp; chown -R vdsm.root /home/ovirt/images</code></pre><hr><p><img src="/img/wiki/ovirt/storage-local-add.jpg" alt="003"></p><hr><h1 id="8-新建本地ISO存储域"><a href="#8-新建本地ISO存储域" class="headerlink" title="8.新建本地ISO存储域"></a>8.新建本地ISO存储域</h1><pre><code class="hljs"># 创建ISO光盘存储目录并授予权限mkdir -p /home/ovirt/iso &amp;&amp; chown -R vdsm.root /home/ovirt/iso</code></pre><hr><p><img src="/img/wiki/ovirt/storage-iso-add.jpg" alt="004"></p><hr><h1 id="9-上传ISO光盘文件"><a href="#9-上传ISO光盘文件" class="headerlink" title="9.上传ISO光盘文件"></a>9.上传ISO光盘文件</h1><h1 id="10-安装虚拟机"><a href="#10-安装虚拟机" class="headerlink" title="10.安装虚拟机"></a>10.安装虚拟机</h1><h2 id="10-1-Windows安装"><a href="#10-1-Windows安装" class="headerlink" title="10.1 Windows安装"></a>10.1 Windows安装</h2><h2 id="10-2-创建虚拟机"><a href="#10-2-创建虚拟机" class="headerlink" title="10.2 创建虚拟机"></a>10.2 创建虚拟机</h2><h2 id="10-3-虚拟机安装guest客户端"><a href="#10-3-虚拟机安装guest客户端" class="headerlink" title="10.3 虚拟机安装guest客户端"></a>10.3 虚拟机安装guest客户端</h2><pre><code class="hljs">sudo yum -y install epel-release ovirt-guest-agent-commonsudo apt install -y ovirt-guest-agent</code></pre><h2 id="10-4-导入KVM虚拟机"><a href="#10-4-导入KVM虚拟机" class="headerlink" title="10.4 导入KVM虚拟机"></a>10.4 导入KVM虚拟机</h2><h3 id="10-4-1"><a href="#10-4-1" class="headerlink" title="10.4.1"></a>10.4.1</h3><pre><code class="hljs"># 配置nat网络sudo yum install -y vdsm-hook-extnetengine-config -s CustomDeviceProperties=&#39;&#123;type=interface;prop=&#123;extnet=^[a-zA-Z0-9_ ---]+$&#125;&#125;&#39;</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.cnovirt.com/archives/6">https://www.cnovirt.com/archives/6</a></li><li><a href="https://www.freesion.com/article/7685172883">https://www.freesion.com/article/7685172883</a></li><li><a href="https://blog.csdn.net/weixin_34345753/article/details/92266128">https://blog.csdn.net/weixin_34345753/article/details/92266128</a></li><li><a href="http://t.zoukankan.com/bnsdmmL-p-13601026.html">http://t.zoukankan.com/bnsdmmL-p-13601026.html</a></li><li><a href="https://blog.csdn.net/weixin_34281537/article/details/91574467">https://blog.csdn.net/weixin_34281537/article/details/91574467</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>极客</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>极客</tag>
      
      <tag>KVM</tag>
      
      <tag>虚拟化</tag>
      
      <tag>oVirt</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes集群Ingress详解</title>
    <link href="/linux/KubernetesIngress/"/>
    <url>/linux/KubernetesIngress/</url>
    
    <content type="html"><![CDATA[<p>Ingress，即路由，是Kubernetes集群管理外部访问流量的路由规则，作为集群内部service对外暴露的访问接入点，几乎承载着集群内服务访问的所有流量。ingress为集群提供七层负载均衡、SSL安全连接和基于虚拟主机的反向代理功能，弥补了service的不足。通过配置不同的转发规则，将不同URL的外部访问请求转发到集群内部不同的Service，从而实现了HTTP层的业务路由机制，流量路由由Ingress资源上定义的规则控制</p><hr><h1 id="功能组件"><a href="#功能组件" class="headerlink" title="功能组件"></a>功能组件</h1><ul><li><p>Ingress Controller，即Ingress控制器，部署于集群内具体实现反向代理（即负载均衡）的程序，实现七层转发的Edge Router，通过调用apiserver组件动态感知集群中pod的变化而动态更新配置文件并重载。通常以DaemonSets或Deployments的形式部署，DaemonSets部署方式一般是以hostNetwork或者hostPort的形式暴露，Deployments部署形式以NodePort的方式暴露，控制器的多个节点则借助外部负载均衡ExternalLB以实现统一接入。常用的控制器有Nginx、HAProxy、Istio、Traefik ，当前Kubernetes官方维护的是Nginx Ingress Controller</p></li><li><p>Ingress资源对象，即创建具体的转发到service的配置规则</p></li></ul><hr><h1 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h1><h2 id="1-创建ingress对象"><a href="#1-创建ingress对象" class="headerlink" title="1.创建ingress对象"></a>1.创建ingress对象</h2><p>集群用户向API Server提交ingress创建请求，定义域名与service的对应关系</p><h2 id="2-解析负载均衡规则"><a href="#2-解析负载均衡规则" class="headerlink" title="2.解析负载均衡规则"></a>2.解析负载均衡规则</h2><p>ingress-controller通过和kubernetes APIServer交互，监测到到集群中ingress规则的变更，按照用户提交的规则生成负载均衡配置</p><h2 id="3-加载负载均衡配置"><a href="#3-加载负载均衡配置" class="headerlink" title="3.加载负载均衡配置"></a>3.加载负载均衡配置</h2><p>ingress-controller运行的负载均衡器（nginx、haproxy）加载负载均衡配置到配置文件，并动态更新</p><h2 id="4-转发请求流量"><a href="#4-转发请求流量" class="headerlink" title="4. 转发请求流量"></a>4. 转发请求流量</h2><p>外部流量访问域名，ingress-controller将请求直接转发到Service对应的后端Endpoint，不经过kube-proxy的转发， 此时控制器相当于是边缘路由器的功能</p><h1 id="工作模式"><a href="#工作模式" class="headerlink" title="工作模式"></a>工作模式</h1><h2 id="1-LoadBalancer模式"><a href="#1-LoadBalancer模式" class="headerlink" title="1.LoadBalancer模式"></a>1.LoadBalancer模式</h2><p>用于公有云环境，Deployment部署ingress-controller，service type设为LoadBalancer，再为service自动创建绑定了公网IP的负载均衡器，只需将域名解析到负载均衡器的公网IP即可实现集群服务的对外暴露</p><h2 id="2-NodePort模式"><a href="#2-NodePort模式" class="headerlink" title="2.NodePort模式"></a>2.NodePort模式</h2><p>Deployment部署ingres-controller，service type设为NodePort，由于nodeport暴露的端口是随机分配，一般再搭建一套负载均衡器用于流量转发。此模式需为NodePort新增一层NAT，请求量级很大时性能会有一定影响</p><h2 id="3-HostNetwork模式"><a href="#3-HostNetwork模式" class="headerlink" title="3.HostNetwork模式"></a>3.HostNetwork模式</h2><p>DaemonSet部署ingress-controller，pod网络模式设为HostNetwork，与node节点的网络打通，直接使用宿主机的80&#x2F;433端口访问，ingress-controller所在node节点类似于传统架构的边缘节点，如机房入口的nginx服务器。此模式整个请求链路最为简单，性能相对更好，适用于大并发量环境。由于直接绑定node节点的端口，故此每个node只能部署一个ingress-controller pod</p><hr><h1 id="1-下载Nginx-Ingress-Controller资源配置文件"><a href="#1-下载Nginx-Ingress-Controller资源配置文件" class="headerlink" title="1.下载Nginx Ingress Controller资源配置文件"></a>1.下载Nginx Ingress Controller资源配置文件</h1><pre><code class="hljs">wget -O ingress-nginx-deploy.yaml https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.2.1/deploy/static/provider/cloud/deploy.yaml</code></pre><h1 id="2-部署Nginx-Ingress-Controller"><a href="#2-部署Nginx-Ingress-Controller" class="headerlink" title="2.部署Nginx Ingress Controller"></a>2.部署Nginx Ingress Controller</h1><h2 id="2-1-NodePort模式"><a href="#2-1-NodePort模式" class="headerlink" title="2.1 NodePort模式"></a>2.1 NodePort模式</h2><h3 id="2-1-1-配置service"><a href="#2-1-1-配置service" class="headerlink" title="2.1.1 配置service"></a>2.1.1 配置service</h3><pre><code class="hljs">vi ingress-nginx-deploy.yamlapiVersion: v1kind: Servicemetadata:  labels:    app.kubernetes.io/component: controller    app.kubernetes.io/instance: ingress-nginx    app.kubernetes.io/name: ingress-nginx    app.kubernetes.io/part-of: ingress-nginx    app.kubernetes.io/version: 1.2.1  name: ingress-nginx-controller  namespace: ingress-nginxspec:  externalTrafficPolicy: Local  ports:  - appProtocol: http    name: http    port: 80    # 设置绑定到节点的端口号    # nodePort: 32080    protocol: TCP    targetPort: http  - appProtocol: https    name: https    port: 443    # 设置绑定到节点的端口号    # nodePort: 32443    protocol: TCP    targetPort: https  selector:    app.kubernetes.io/component: controller    app.kubernetes.io/instance: ingress-nginx    app.kubernetes.io/name: ingress-nginx  # 修改为NodePort类型  type: NodePort</code></pre><h3 id="2-1-2-部署ingress-controller"><a href="#2-1-2-部署ingress-controller" class="headerlink" title="2.1.2 部署ingress-controller"></a>2.1.2 部署ingress-controller</h3><pre><code class="hljs">kubectl apply -f ingress-nginx-deploy.yaml</code></pre><h3 id="2-1-3-验证ingress-controller"><a href="#2-1-3-验证ingress-controller" class="headerlink" title="2.1.3 验证ingress-controller"></a>2.1.3 验证ingress-controller</h3><pre><code class="hljs">kubectl -n ingress-nginx get pod -o widekubectl -n ingress-nginx get servicekubectl -n ingress-nginx get ingressclasses</code></pre><h3 id="2-1-4-创建ingress"><a href="#2-1-4-创建ingress" class="headerlink" title="2.1.4 创建ingress"></a>2.1.4 创建ingress</h3><pre><code class="hljs">vi nginx-ingress.yamlapiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: nginx-ingress  namespace: defaultspec:  # 设置Ingress控制器名称  ingressClassName: nginx  # 设置ingress主机规则列表，若未指定或无规则匹配，则所有流量都将发送到默认后端  rules:  # 设置虚拟主机域名  - host: worker01    # 设置网络协议http/https    http:      # 设置转发路径集合      paths:      - path: /        # 设置路径匹配规则，Prefix表示基于URL路径前缀按/分割匹配且区分大小写，并按路径元素，即路径中由/分隔符分割的标签列表，逐个匹配；Exact表示精确匹配且区分大小写                 pathType: Prefix        # 设置流量转发的后端endpoint        backend:          service:            # 设置后端关联的service            name: nginx-service            # 设置service的端口            port:               number: 80      - path: /test        # 设置路径匹配规则，Prefix表示基于URL路径前缀按/分割匹配且区分大小写，并按路径元素，即路径中由/分隔符分割的标签列表，逐个匹配；Exact表示精确匹配且区分大小写                 pathType: Prefix        # 设置流量转发的后端endpoint        backend:          service:            # 设置后端关联的service            name: nginx-service            # 设置service的端口            port:               number: 80</code></pre><h3 id="2-1-5-部署ingress，访问ingress验证转发规则"><a href="#2-1-5-部署ingress，访问ingress验证转发规则" class="headerlink" title="2.1.5 部署ingress，访问ingress验证转发规则"></a>2.1.5 部署ingress，访问ingress验证转发规则</h3><pre><code class="hljs">kubectl apply -f nginx-ingress.yaml</code></pre><h2 id="2-2-HostNetwork模式"><a href="#2-2-HostNetwork模式" class="headerlink" title="2.2 HostNetwork模式"></a>2.2 HostNetwork模式</h2><h3 id="2-2-1-配置网络模式"><a href="#2-2-1-配置网络模式" class="headerlink" title="2.2.1 配置网络模式"></a>2.2.1 配置网络模式</h3><pre><code class="hljs">vi ingress-nginx-deploy.yaml# 设置ingress-controller的service type为ClusterIPtype: ClusterIP---apiVersion: apps/v1# 设置ngress-controller部署为DaemonSetkind: DaemonSet  # 设置ingress-controller dnsPolicy   dnsPolicy: ClusterFirstWithHostNet  # 设置ngress-controller网络模式  hostNetwork: true</code></pre><h3 id="2-2-2-部署ingress-controller"><a href="#2-2-2-部署ingress-controller" class="headerlink" title="2.2.2 部署ingress-controller"></a>2.2.2 部署ingress-controller</h3><pre><code class="hljs">kubectl apply -f ingress-nginx-deploy.yaml</code></pre><h3 id="2-2-3-验证ingress-controller"><a href="#2-2-3-验证ingress-controller" class="headerlink" title="2.2.3 验证ingress-controller"></a>2.2.3 验证ingress-controller</h3><pre><code class="hljs">kubectl -n ingress-nginx get pod -o widekubectl -n ingress-nginx get servicekubectl -n ingress-nginx get ingressclasses</code></pre><h3 id="2-2-4-创建ingress"><a href="#2-2-4-创建ingress" class="headerlink" title="2.2.4 创建ingress"></a>2.2.4 创建ingress</h3><pre><code class="hljs">vi nginx-ingress.yamlapiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: nginx-ingress  namespace: defaultspec:  # 设置Ingress控制器名称  ingressClassName: nginx  # 设置ingress主机规则列表，若未指定或无规则匹配，则所有流量都将发送到默认后端  rules:  # 设置虚拟主机域名  - host: worker01    # 设置网络协议http/https    http:      # 设置转发路径集合      paths:      - path: /        # 设置路径匹配规则，Prefix表示基于URL路径前缀按/分割匹配且区分大小写，并按路径元素，即路径中由/分隔符分割的标签列表，逐个匹配；Exact表示精确匹配且区分大小写                 pathType: Prefix        # 设置流量转发的后端endpoint        backend:          service:            # 设置后端关联的service            name: nginx-service            # 设置service的端口            port:               number: 80     # 设置虚拟主机域名  - host: worker02    # 设置网络协议http/https    http:      # 设置转发路径集合      paths:      - path: /        # 设置路径匹配规则，Prefix表示基于URL路径前缀按/分割匹配且区分大小写，并按路径元素，即路径中由/分隔符分割的标签列表，逐个匹配；Exact表示精确匹配且区分大小写                 pathType: Prefix        # 设置流量转发的后端endpoint        backend:          service:            # 设置后端关联的service            name: nginx-service            # 设置service的端口            port:               number: 80</code></pre><h3 id="2-2-5-部署ingress，访问ingress验证转发规则"><a href="#2-2-5-部署ingress，访问ingress验证转发规则" class="headerlink" title="2.2.5 部署ingress，访问ingress验证转发规则"></a>2.2.5 部署ingress，访问ingress验证转发规则</h3><pre><code class="hljs">kubectl apply -f nginx-ingress.yaml</code></pre><h1 id="3-配置SSL证书"><a href="#3-配置SSL证书" class="headerlink" title="3.配置SSL证书"></a>3.配置SSL证书</h1><h2 id="3-1-创建自签名证书"><a href="#3-1-创建自签名证书" class="headerlink" title="3.1 创建自签名证书"></a>3.1 创建自签名证书</h2><pre><code class="hljs">openssl req -x509 -newkey rsa:4096 -keyout hexo.ops.org.key -out hexo.ops.org.crt -days 3650 -nodes -subj &quot;/CN=hexo.ops.org&quot;</code></pre><h2 id="3-2-创建存储证书与私钥的Secret"><a href="#3-2-创建存储证书与私钥的Secret" class="headerlink" title="3.2 创建存储证书与私钥的Secret"></a>3.2 创建存储证书与私钥的Secret</h2><pre><code class="hljs">kubectl create secret tls hexo-ingress-secret --cert=hexo.ops.org.crt --key=hexo.ops.org.key</code></pre><h2 id="3-3-创建ingress"><a href="#3-3-创建ingress" class="headerlink" title="3.3 创建ingress"></a>3.3 创建ingress</h2><pre><code class="hljs">vi  hexo-ingress.yaml apiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: hexospec:  # 设置Ingress控制器名称  ingressClassName: nginx  tls:  # 设置SSL证书域名  - hosts:    - hexo.ops.org    # 设置证书名称    secretName: hexo-ingress-secret  # 设置ingress主机规则列表，若未指定或无规则匹配，则所有流量都将发送到默认后端  rules:  # 设置虚拟主机域名  - host: hexo.ops.org    # 设置网络协议http/https    http:      # 设置转发路径集合      paths:      - path: /        # 设置路径匹配规则，Prefix表示基于URL路径前缀按/分割匹配且区分大小写，并按路径元素，即路径中由/分隔符分割的标签列表，逐个匹配；Exact表示精确匹配且区分大小写                 pathType: Prefix        # 设置流量转发的后端endpoint        backend:          service:            # 设置后端关联的service            name: hexo            # 设置service的端口            port:               number: 80</code></pre><h2 id="3-4-配置本地hosts域名解析，验证https访问"><a href="#3-4-配置本地hosts域名解析，验证https访问" class="headerlink" title="3.4 配置本地hosts域名解析，验证https访问"></a>3.4 配置本地hosts域名解析，验证https访问</h2><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/MssGuo/article/details/123414161">https://blog.csdn.net/MssGuo/article/details/123414161</a></li><li><a href="https://blog.csdn.net/supahero/article/details/121476304">https://blog.csdn.net/supahero/article/details/121476304</a></li><li><a href="https://blog.csdn.net/wendao76/article/details/143573289">https://blog.csdn.net/wendao76/article/details/143573289</a></li><li><a href="https://blog.csdn.net/qq_41582883/article/details/114003552">https://blog.csdn.net/qq_41582883/article/details/114003552</a></li><li><a href="https://blog.csdn.net/Yusheng9527/article/details/124140541">https://blog.csdn.net/Yusheng9527/article/details/124140541</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes集群访问控制机制详解</title>
    <link href="/linux/KubernetesRBAC/"/>
    <url>/linux/KubernetesRBAC/</url>
    
    <content type="html"><![CDATA[<p>Kubernetes集群一切皆API对象，所有操作和组件之间的通信及外部用户命令都是通过调用API Server进行处理。因此，API Server对集群的访问请求进行身份认证与鉴权即可实现整个集群的权限管理，也即是Kubernetes API访问控制</p><hr><h1 id="访问控制流程"><a href="#访问控制流程" class="headerlink" title="访问控制流程"></a>访问控制流程</h1><ul><li>Authentication，即认证，客户端与API Server建立TLS后，API Server身份认证组件将判断该请求的用户是否为能够访问集群的合法用户，若为非<br>法用户，则返回401状态码，并终止该请求</li><li>Authorization，即鉴权，API Server将判断该请求的用户是否有权限进行请求中的操作，若无权限，则返回403的状态码，并终止该请求</li><li>AdmissionControl，即准入控制，API Server的admission控制器将判断该请求是否是一个安全合规的请求，若验证通过，则访问控制流程结束，并<br>将该请求转换为Kubernetes objects相应的变更请求，最终写入到ETCD</li></ul><h1 id="1-Authentication"><a href="#1-Authentication" class="headerlink" title="1.Authentication"></a>1.Authentication</h1><p>Kubernetes集群用户分为两类，即由集群管理的服务账号和普通账户，认证策略即是对这两类用户进行相关认证。普通账户对应于集群用户，由集群管理员分配私钥，私钥保存于~.kube&#x2F;config，执行kubectl命令时自动读取以供API Server认证；服务账户对应于pod，由Kubernetes API自动创建及管理，且与secret资源关联挂载到pod，作为访问API Server的凭证</p><p>认证策略有8种，可以启动一种或多种认证方式，只要有一种认证方式通过即为认证通过，不再对其它方式认证</p><h2 id="1-1-X509客户端证书"><a href="#1-1-X509客户端证书" class="headerlink" title="1.1 X509客户端证书"></a>1.1 X509客户端证书</h2><p>客户端向API Server传递SSL证书即启用客户端证书身份认证，若证书验证通过，则subject中的公共名称，即Common Name，就被作为请求的用户名，<br>该用户即为普通用户</p><pre><code class="hljs"># 使用用户名test00生成一个证书签名请求（CSR），且该用户属于test和dev两个用户组openssl req -new -key test00.pem -out test00-csr.pem -subj &quot;/CN=test/O=test/O=dev&quot;</code></pre><h2 id="1-2-Service-Account，服务账户令牌"><a href="#1-2-Service-Account，服务账户令牌" class="headerlink" title="1.2 Service Account，服务账户令牌"></a>1.2 Service Account，服务账户令牌</h2><p>通常由API Server自动创建并通过ServiceAccount准入控制器挂载到集群中运行的Pod上，允许集群内进程与API Server通信</p><h2 id="1-3-Static-Token，静态令牌"><a href="#1-3-Static-Token，静态令牌" class="headerlink" title="1.3 Static Token，静态令牌"></a>1.3 Static Token，静态令牌</h2><p>token文件是至少包含3列的csv格式文件，即token, user name, user uid，group（可选）,具体格式为：token,user,uid,”group1,group2,group3”</p><h2 id="1-4-Bootstrap-Tokens，引导令牌"><a href="#1-4-Bootstrap-Tokens，引导令牌" class="headerlink" title="1.4 Bootstrap Tokens，引导令牌"></a>1.4 Bootstrap Tokens，引导令牌</h2><p>主要用于新建集群或在现有集群中添加新节点，支持kubeadm，被定义为bootstrap.kubernetes.io&#x2F;token类型的Secret，存储于 kube-system命名空间中，被API Server上的启动引导认证组件（Bootstrap Authenticator）读取</p><h2 id="1-5-OpenID-Connect（OIDC）令牌"><a href="#1-5-OpenID-Connect（OIDC）令牌" class="headerlink" title="1.5 OpenID Connect（OIDC）令牌"></a>1.5 OpenID Connect（OIDC）令牌</h2><p>主要用于OAuth2认证机制，如Azure Active Directory</p><h2 id="1-6-Webhook令牌"><a href="#1-6-Webhook令牌" class="headerlink" title="1.6 Webhook令牌"></a>1.6 Webhook令牌</h2><p>即具有回调机制的认证策略</p><h2 id="1-7-Authenticating-Proxy"><a href="#1-7-Authenticating-Proxy" class="headerlink" title="1.7 Authenticating Proxy"></a>1.7 Authenticating Proxy</h2><p>即认证代理，从请求的头部字段值如X-Remote-User辩识用户，身份认证代理负责设置请求的头部字段值</p><h2 id="1-8-Basic-Authentication"><a href="#1-8-Basic-Authentication" class="headerlink" title="1.8 Basic Authentication"></a>1.8 Basic Authentication</h2><p>即基本身份认证，API Server将于请求头加入Basic BASE64ENCODED(USER:PASSWORD)</p><h1 id="2-Authorization"><a href="#2-Authorization" class="headerlink" title="2.Authorization"></a>2.Authorization</h1><p>客户端请求通过认证之后，API Server将根据所有授权策略匹配该请求的属性，所有部分都必须被某些策略允许才能决定允许或拒绝。客户端请求属性包含请求者用户名、请求对象及请求操作等。授权策略由授权模块定义，若集群配置了多个鉴权模块，则将按顺序进行匹配</p><h2 id="2-1-客户端请求属性"><a href="#2-1-客户端请求属性" class="headerlink" title="2.1 客户端请求属性"></a>2.1 客户端请求属性</h2><h3 id="2-1-1-请求用户及其所属组"><a href="#2-1-1-请求用户及其所属组" class="headerlink" title="2.1.1 请求用户及其所属组"></a>2.1.1 请求用户及其所属组</h3><ul><li>用户，身份验证环节提供并通过的user</li><li>组，通过身份验证的用户所属组列表</li><li>额外信息，由身份验证环节提供的任意字符串键到字符串值的映射</li></ul><h3 id="2-1-2-请求对象"><a href="#2-1-2-请求对象" class="headerlink" title="2.1.2 请求对象"></a>2.1.2 请求对象</h3><p>请求对象即是该次请求所申请的集群资源对象，分为API资源和非资源端点两类</p><h4 id="API资源对象"><a href="#API资源对象" class="headerlink" title="API资源对象"></a>API资源对象</h4><ul><li>API，请求所对应的API资源</li><li>API组，请求API资源的所属组，空字符串表示核心API组</li><li>命名空间，即namespace，请求所对应的API对象所属命名空间</li><li>resource，即该次请求的集群对象的ID或名称，如pod、service等，对于get、update、patch和delete的资源请求，须提供资源名称的子资源</li></ul><h4 id="非资源端点"><a href="#非资源端点" class="headerlink" title="非资源端点"></a>非资源端点</h4><ul><li>请求路径，非资源端点的路径，如&#x2F;api或&#x2F;healthz</li></ul><h3 id="2-1-3-请求操作"><a href="#2-1-3-请求操作" class="headerlink" title="2.1.3 请求操作"></a>2.1.3 请求操作</h3><h4 id="API请求操作，用于API资源对象的请求，即是对所请求的集群资源对象的操作"><a href="#API请求操作，用于API资源对象的请求，即是对所请求的集群资源对象的操作" class="headerlink" title="API请求操作，用于API资源对象的请求，即是对所请求的集群资源对象的操作"></a>API请求操作，用于API资源对象的请求，即是对所请求的集群资源对象的操作</h4><ul><li>POST请求，对应于API资源对象的create</li><li>GET、HEAD请求，对应于单个API资源对象的get或集合资源的ist</li><li>PUT请求，对应于API资源对象的update</li><li>PATCH请求，对应于API资源对象的patch</li><li>DELETE请求，对应于单个API资源对象的delete或集合资源的deletecollection</li></ul><h4 id="HTTP请求操作，用于非资源端点的请求，类似于HTTP请求的方法，如get、post、put、delete等"><a href="#HTTP请求操作，用于非资源端点的请求，类似于HTTP请求的方法，如get、post、put、delete等" class="headerlink" title="HTTP请求操作，用于非资源端点的请求，类似于HTTP请求的方法，如get、post、put、delete等"></a>HTTP请求操作，用于非资源端点的请求，类似于HTTP请求的方法，如get、post、put、delete等</h4><h2 id="2-2-RABC授权策略"><a href="#2-2-RABC授权策略" class="headerlink" title="2.2 RABC授权策略"></a>2.2 RABC授权策略</h2><p>集群的授权策略由授权模块定义，可用的授权模块有6个，其中RBAC模块为集群默认强制开启且最为完善。RBAC，即Role Based Access Control，基于角色的访问控制，由rbac.authorization.k8s.io API资源组驱动鉴权，通过API动态配置策略</p><h3 id="鉴权流程"><a href="#鉴权流程" class="headerlink" title="鉴权流程"></a>鉴权流程</h3><ul><li><p>定义角色资源对象，角色是集群内某些资源（apiGroups、resources）的操作（verbs）权限的集合，即为对象及对其的操作动作。角色分为Role<br>和ClusterRole两类，前者属于命名空间级别，后者属于集群级别</p></li><li><p>定义角色绑定资源对象，即是将角色绑定到某个主体（subject），从而继承了角色的操作权限。主体为某个用户（User）、用户组（Group）、服务账号（ServiceAccount），角色绑定对应于角色，也分为两类，即RoleBinding，将Role绑定到主体；ClusterRoleBinding，将Role和ClusterRole绑定到主体</p></li></ul><h3 id="优势特点"><a href="#优势特点" class="headerlink" title="优势特点"></a>优势特点</h3><p>对集群中的资源和非资源权限均有完整的覆盖，能够做到精细控制整个授权流程完全由几个API对象完成，同其他API对象一样，可用kubectl或API进行操作支持在运行时进行调整，无须重新启动API Server</p><h2 id="2-1-创建用户"><a href="#2-1-创建用户" class="headerlink" title="2.1 创建用户"></a>2.1 创建用户</h2><h3 id="2-1-1-创建普通用户"><a href="#2-1-1-创建普通用户" class="headerlink" title="2.1.1 创建普通用户"></a>2.1.1 创建普通用户</h3><pre><code class="hljs"># 普通用户user001创建私钥openssl genrsa -out user001.key 2048# 根据私钥创建csr(证书签名请求)文件openssl req -new -key user001.key -subj &quot;/CN=user001/O=user00&quot; -out user001.csr# 根据私钥和csr文件生成证书openssl x509 -req -in user001.csr -CA /opt/kubernetes/ssl/ca.pem -CAkey /opt/kubernetes/ssl/ca-key.pem -CAcreateserial -out user001.crt -days 365# 创建集群普通用户user001，将其认证信息写入kubeconfigkubectl config set-credentials user001 --client-certificate=./user001.crt --client-key=./user001.key --embed-certs=true# 设置上下文， 默认会保存在$HOME/.kube/configkubectl config set-context user001@kubernetes --cluster=kubernetes --user=user001# 查看当前上下文kubectl config get-contexts# 切换上下文，即切换user001为当前用户kubectl config use-context user001@kubernetes# 查看当前上下文kubectl config get-contexts# 执行测试命令kubectl get nodes</code></pre><h3 id="2-1-2-创建服务账户"><a href="#2-1-2-创建服务账户" class="headerlink" title="2.1.2 创建服务账户"></a>2.1.2 创建服务账户</h3><pre><code class="hljs">kubectl create serviceaccount sa001# serviceaccount新增secretkubectl patch serviceaccount sa001 -p &#39;&#123;&quot;imagePullSecrets&quot;: [&#123;&quot;name&quot;: &quot;myregistrykey&quot;&#125;]&#125;&#39;# 创建绑定sa的pod，使其认证私有仓库vi nginx-sa001.yamlapiVersion: v1kind: Podmetadata:  name: nginx-sa001spec:  containers:  - name: nginx    image: sword618/nginx    imagePullPolicy: IfNotPresentserviceAccountName: sa001</code></pre><h2 id="2-2-创建角色"><a href="#2-2-创建角色" class="headerlink" title="2.2 创建角色"></a>2.2 创建角色</h2><h3 id="2-2-1-角色配置参数"><a href="#2-2-1-角色配置参数" class="headerlink" title="2.2.1 角色配置参数"></a>2.2.1 角色配置参数</h3><ul><li><p>APIGroup，即角色所作用的API资源组，可选项：“”,“apps”, “autoscaling”, “batch”</p></li><li><p>Resource，即角色所作用的资源对象，可选项：“services”, “endpoints”, “pods”,“secrets”,“configmaps”,“crontabs”,“deployments”,“jobs”,“nodes”,“rolebindings”,“clusterroles”,“daemonsets”,“replicasets”,“statefulsets”,“horizontalpodautoscalers”,“replicationcontrollers”,“cronjobs”</p></li><li><p>Verbs，即角色的动作权限，可选项：“get”, “list”, “watch”, “create”, “update”, “patch”, “delete”, “exec”</p></li></ul><h3 id="2-2-2-创建Role"><a href="#2-2-2-创建Role" class="headerlink" title="2.2.2 创建Role"></a>2.2.2 创建Role</h3><pre><code class="hljs">vi role.yamlkind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata:  namespace: test  name: role-default-allrules:  - apiGroups:    - &quot;*&quot;    resources:    - pods    - deployments    - services    verbs:    - get    - watch    - list    - create    - delete    - update    - patch    - exec---apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata:  namespace: default  name: role-default-logrules:- apiGroups: [&quot;&quot;]  # 设置角色所作用集群对象为子资源  resources: [&quot;pods&quot;, &quot;pods/log&quot;,&quot;pods/status&quot;]  verbs: [&quot;get&quot;, &quot;list&quot;]---apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata:  namespace: default  name: role-default-redis-conf-updaterules:- apiGroups: [&quot;&quot;]  # 设置角色所作用集群对象为某个指定的对象实例  resources: [&quot;configmaps&quot;]  resourceNames: [&quot;redis-conf&quot;]  verbs: [&quot;update&quot;, &quot;get&quot;]</code></pre><h3 id="2-2-3-创建ClusterRole"><a href="#2-2-3-创建ClusterRole" class="headerlink" title="2.2.3 创建ClusterRole"></a>2.2.3 创建ClusterRole</h3><pre><code class="hljs">vi clusterrole.yamlapiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata:  name: clusterrole-readrules:  - apiGroups: [&quot;&quot;]    # 设置角色所作用集群对象为集群所有资源    resources: [&quot;*&quot;]    verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata:  name: clusterrole-nodes-readrules:  - apiGroups: [&quot;&quot;]  # 设置角色所作用集群对象为node  resources: [&quot;nodes&quot;]  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata:  name: clusterrole-nonresourcerules:  # 设置角色所作用集群对象为非资源端点，支持通配符  - nonResourceURLs: [&quot;/healthz&quot;, &quot;/healthz/*&quot;]    verbs: [&quot;get&quot;, &quot;post&quot;]</code></pre><h2 id="2-3-创建角色绑定"><a href="#2-3-创建角色绑定" class="headerlink" title="2.3 创建角色绑定"></a>2.3 创建角色绑定</h2><h3 id="2-3-1-创建RoleBinding"><a href="#2-3-1-创建RoleBinding" class="headerlink" title="2.3.1 创建RoleBinding"></a>2.3.1 创建RoleBinding</h3><pre><code class="hljs">vi user-role.yamlapiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata:  name: user001-role  namespace: default# 设置角色绑定的主体，可指定多个subjects:- kind: User  # 设置角色绑定所作用的用户  name: user001  apiGroup: rbac.authorization.k8s.io- kind: ServiceAccount  name: kube-system-sa-default  # 设置角色绑定所作用的服务账户，即kube-system命名空间的默认服务账户  namespace: kube-system  apiGroup: rbac.authorization.k8s.ioroleRef:  kind: Role  # 设置绑定的角色为Role，引用Role名称  name: role-default-all  apiGroup: rbac.authorization.k8s.io---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata:  name: user002-rolesubjects:- kind: Group  name: user002  apiGroup: rbac.authorization.k8s.io- kind: Group  # 设置角色绑定所作用的服务账户，即所有命名空间的服务账户  name: system:serviceaccounts  apiGroup: rbac.authorization.k8s.ioroleRef:  # 设置绑定的角色为ClusterRole，用于跨命名空间的整个集群的授权  kind: ClusterRole  name: clusterrole-read  apiGroup: rbac.authorization.k8s.io</code></pre><h3 id="2-3-2-创建ClusterRoleBinding"><a href="#2-3-2-创建ClusterRoleBinding" class="headerlink" title="2.3.2 创建ClusterRoleBinding"></a>2.3.2 创建ClusterRoleBinding</h3><pre><code class="hljs">apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  name: user003-rolesubjects:- kind: Group  # 设置角色绑定所作用的用户，即所有已通过认证的用户  name: system:authenticated  apiGroup: rbac.authorization.k8s.io- kind: Group  # 设置角色绑定所作用的服务账户，即default命名空间中属于dev组的所有服务账户  name: system:serviceaccounts:dev  apiGroup: rbac.authorization.k8s.io  namespace: defaultroleRef:   kind: ClusterRole  name: clusterrole-nonresource  apiGroup: rbac.authorization.k8s.io</code></pre><h1 id="3-AdmissionControl"><a href="#3-AdmissionControl" class="headerlink" title="3.AdmissionControl"></a>3.AdmissionControl</h1><p>客户端请求通过认证和鉴权之后，就将进入准入控制流程，作用是拦截掉某些不合规的请求。拦截规则由准入控制器决定，准入控制器由一系列插件<br>构成，由集群管理员配置，若请求被任一控制器拒绝，则整个请求将立即被拒绝，并向客户端返回错误码。此外，准入控制器还能够修改请求参数以完<br>成一些自动化的任务，比如Service Account这个控制器<br>准入控制器插件通过apiserver组件配置，具体配置项是admission_control，其值为一串逗号连接的插件名称，集群默认启用的准入控制器插件查询命令为：kube-apiserver -h | grep enable-admission-plugins</p><h2 id="常用准入控制器"><a href="#常用准入控制器" class="headerlink" title="常用准入控制器"></a>常用准入控制器</h2><ul><li>AlwaysPullImages，修改新创建pod的镜像拉取策略为Always，用于多租户集群，表明私有镜像只能被有凭证的人使用，否则任何用户的pod都可通过已拉取到节点上的镜像</li><li>DefaultStorageClass，用于设定默认存储类</li><li>DefaultIngressClass，用于设定默认Ingress类</li><li>EventRateLimit，用于设定事件速率限制，缓解API Server的压力，即根据命名空间、用户等条件设置相应的API Server的QPS等</li><li>NamespaceAutoProvision，用于自动创建命名空间的集群，即请求的命名空间不存在时自动创建</li><li>NamespaceExists，用于剔除访问集群不存在的命名空间的请求</li><li>NamespaceLifecycle，用于确保处于Termination状态的命令空间不再接收新对象创建请求，并拒绝请求不存在的命名空间，还可以防止删除系统保留的命名空间，如default、kube-system、kube-public等，建议开启</li><li>NodeRestriction，用于设定kubelet修改node与pod的限制条件，即只可修改绑定到node本身的Pod</li><li>PersistentVolumeClaimResize，用于额外验证PVC容量调整的请求，与ExpandPersistentVolumes配合使用</li><li>ServiceAccount，用于实现ServiceAccount的自动化，使用服务账户的集群建议开启</li><li>TaintNodesByCondition，用于给新建节点添加NotReady和NoSchedule污点，防止静态条件的发生，避免Pod在更新节点污点以准确反映其所报告状况之前就被调度到新节点上</li></ul><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.orchome.com/1308">https://www.orchome.com/1308</a></li><li><a href="http://docs.kubernetes.org.cn/51.html">http://docs.kubernetes.org.cn/51.html</a></li><li><a href="https://blog.csdn.net/qq_35745940/article/details/120693490">https://blog.csdn.net/qq_35745940/article/details/120693490</a></li><li><a href="https://blog.csdn.net/weixin_43936969/article/details/106318259">https://blog.csdn.net/weixin_43936969/article/details/106318259</a></li><li><a href="https://kubernetes.io/zh/docs/concepts/security/controlling-access">https://kubernetes.io/zh/docs/concepts/security/controlling-access</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>KVM磁盘管理</title>
    <link href="/linux/KVM-VirtualDisk/"/>
    <url>/linux/KVM-VirtualDisk/</url>
    
    <content type="html"><![CDATA[<p>KVM虚拟机由两部分组成，即配置文件和磁盘文件，配置文件描述了虚拟机的详细参数，路径默认为&#x2F;etc&#x2F;libvirt&#x2F;qemu；磁盘文件即为虚拟机的镜像，支持多种格式，raw和qcow2这两种格式最为常见</p><ul><li><p>raw，意为未被加工的，又被称为原始镜像或裸设备镜像，KVM默认格式，指定多大就创建多大，直接占用指定大小的空间，镜像文件由宿主机的文件系统来管理，可直接挂载，性能优于qcow2，且通用性好，可随意进行格式转换，所以常作为其他格式转换时的中间格式。缺点是不支持内部快照与加密，远程传输网络IO消耗较大，建议转换格式再传输</p></li><li><p>qcow2，主流虚拟机磁盘镜像格式，也是openstack默认格式，空间动态增长，文件及快照较小，性能接近raw，且支持zlib磁盘压缩及AES加密。缺点是性能略低于raw，且镜像文件一旦损坏则文件系统全部不可用</p></li></ul><hr><h1 id="1-虚拟机磁盘扩容"><a href="#1-虚拟机磁盘扩容" class="headerlink" title="1.虚拟机磁盘扩容"></a>1.虚拟机磁盘扩容</h1><h2 id="1-1-虚拟机添加新磁盘"><a href="#1-1-虚拟机添加新磁盘" class="headerlink" title="1.1 虚拟机添加新磁盘"></a>1.1 虚拟机添加新磁盘</h2><h3 id="1-1-1-创建虚拟磁盘"><a href="#1-1-1-创建虚拟磁盘" class="headerlink" title="1.1.1 创建虚拟磁盘"></a>1.1.1 创建虚拟磁盘</h3><pre><code class="hljs">qemu-img create -f qcow2 /home/kvm/servers/node01-001.qcow2 10G</code></pre><h3 id="1-1-2-验证新增磁盘信息"><a href="#1-1-2-验证新增磁盘信息" class="headerlink" title="1.1.2 验证新增磁盘信息"></a>1.1.2 验证新增磁盘信息</h3><pre><code class="hljs">qemu-img info /home/kvm/servers/node01-001.qcow2</code></pre><h3 id="1-1-3-虚拟机node01添加新磁盘"><a href="#1-1-3-虚拟机node01添加新磁盘" class="headerlink" title="1.1.3 虚拟机node01添加新磁盘"></a>1.1.3 虚拟机node01添加新磁盘</h3><pre><code class="hljs"># 临时生效，--live，热添加；vdb，第二块硬盘；--cache，宿主机对虚拟机镜像的读写缓存；--subdriver，硬盘驱动类型# virsh attach-disk node01 /home/kvm/servers/node01-001.qcow2 vdb --live --cache=none --subdriver=qcow2virsh attach-disk node01 /home/kvm/servers/node01-001.qcow2 vdb --live --cache=none --subdriver=qcow2 --config</code></pre><h3 id="1-1-4-登录虚拟机验证新增的硬盘"><a href="#1-1-4-登录虚拟机验证新增的硬盘" class="headerlink" title="1.1.4 登录虚拟机验证新增的硬盘"></a>1.1.4 登录虚拟机验证新增的硬盘</h3><pre><code class="hljs">lsblk</code></pre><h3 id="1-1-5-卸载硬盘"><a href="#1-1-5-卸载硬盘" class="headerlink" title="1.1.5 卸载硬盘"></a>1.1.5 卸载硬盘</h3><pre><code class="hljs">virsh detach-disk node01 vdb</code></pre><h2 id="1-2-虚拟机扩容磁盘"><a href="#1-2-虚拟机扩容磁盘" class="headerlink" title="1.2 虚拟机扩容磁盘"></a>1.2 虚拟机扩容磁盘</h2><h3 id="1-2-1-调整虚拟磁盘容量，只能增大不能减小"><a href="#1-2-1-调整虚拟磁盘容量，只能增大不能减小" class="headerlink" title="1.2.1 调整虚拟磁盘容量，只能增大不能减小"></a>1.2.1 调整虚拟磁盘容量，只能增大不能减小</h3><pre><code class="hljs">qemu-img resize /home/kvm/servers/node01-001.qcow2 +10G</code></pre><h3 id="1-2-2-验证磁盘容量"><a href="#1-2-2-验证磁盘容量" class="headerlink" title="1.2.2 验证磁盘容量"></a>1.2.2 验证磁盘容量</h3><pre><code class="hljs">qemu-img info /home/kvm/servers/node01-001.qcow2</code></pre><h3 id="1-2-3-虚拟机node01在线添加新磁盘"><a href="#1-2-3-虚拟机node01在线添加新磁盘" class="headerlink" title="1.2.3 虚拟机node01在线添加新磁盘"></a>1.2.3 虚拟机node01在线添加新磁盘</h3><pre><code class="hljs">virsh attach-disk node01 /home/kvm/servers/node01-001.qcow2 vdb --live --cache=none --subdriver=qcow2 --config</code></pre><h3 id="1-2-4-登录虚拟机验证新增的硬盘"><a href="#1-2-4-登录虚拟机验证新增的硬盘" class="headerlink" title="1.2.4 登录虚拟机验证新增的硬盘"></a>1.2.4 登录虚拟机验证新增的硬盘</h3><pre><code class="hljs">lsblk</code></pre><h3 id="1-2-5-新硬盘分区后挂载"><a href="#1-2-5-新硬盘分区后挂载" class="headerlink" title="1.2.5 新硬盘分区后挂载"></a>1.2.5 新硬盘分区后挂载</h3><h2 id="1-3-虚拟机扩容非根分区"><a href="#1-3-虚拟机扩容非根分区" class="headerlink" title="1.3 虚拟机扩容非根分区"></a>1.3 虚拟机扩容非根分区</h2><h3 id="1-3-1-登录虚拟机，卸载文件系统"><a href="#1-3-1-登录虚拟机，卸载文件系统" class="headerlink" title="1.3.1 登录虚拟机，卸载文件系统"></a>1.3.1 登录虚拟机，卸载文件系统</h3><pre><code class="hljs">umount /opt</code></pre><h3 id="1-3-2-在线扩容虚拟磁盘"><a href="#1-3-2-在线扩容虚拟磁盘" class="headerlink" title="1.3.2 在线扩容虚拟磁盘"></a>1.3.2 在线扩容虚拟磁盘</h3><pre><code class="hljs">qemu-img resize /home/kvm/servers/node01-001.qcow2 5G</code></pre><h3 id="1-3-3-登录虚拟机重新挂载文件系统"><a href="#1-3-3-登录虚拟机重新挂载文件系统" class="headerlink" title="1.3.3 登录虚拟机重新挂载文件系统"></a>1.3.3 登录虚拟机重新挂载文件系统</h3><pre><code class="hljs">mount  /dev/vdb  /opt/</code></pre><h3 id="1-3-4-更新文件系统，调整元数据"><a href="#1-3-4-更新文件系统，调整元数据" class="headerlink" title="1.3.4 更新文件系统，调整元数据"></a>1.3.4 更新文件系统，调整元数据</h3><pre><code class="hljs"> xfs_growfs  /opt</code></pre><h3 id="1-3-5-验证磁盘扩容"><a href="#1-3-5-验证磁盘扩容" class="headerlink" title="1.3.5 验证磁盘扩容"></a>1.3.5 验证磁盘扩容</h3><pre><code class="hljs">dh -h</code></pre><h2 id="1-4-虚拟机扩容根分区"><a href="#1-4-虚拟机扩容根分区" class="headerlink" title="1.4 虚拟机扩容根分区"></a>1.4 虚拟机扩容根分区</h2><h3 id="1-4-1-关闭虚拟机"><a href="#1-4-1-关闭虚拟机" class="headerlink" title="1.4.1 关闭虚拟机"></a>1.4.1 关闭虚拟机</h3><h3 id="1-4-2-在线扩容虚拟机磁盘"><a href="#1-4-2-在线扩容虚拟机磁盘" class="headerlink" title="1.4.2 在线扩容虚拟机磁盘"></a>1.4.2 在线扩容虚拟机磁盘</h3><pre><code class="hljs">qemu-img  resize  /home/kvm/servers/node01.qcow2 +10G</code></pre><h3 id="1-4-3-登录虚拟机重新分区"><a href="#1-4-3-登录虚拟机重新分区" class="headerlink" title="1.4.3 登录虚拟机重新分区"></a>1.4.3 登录虚拟机重新分区</h3><pre><code class="hljs">fdisk /dev/vda# 删除原有分区，重新创建分区，然后通知内核系统分区已变化partprobe</code></pre><h3 id="1-4-4-重启虚拟机"><a href="#1-4-4-重启虚拟机" class="headerlink" title="1.4.4 重启虚拟机"></a>1.4.4 重启虚拟机</h3><h3 id="1-4-5-登录虚拟机更新文件系统"><a href="#1-4-5-登录虚拟机更新文件系统" class="headerlink" title="1.4.5 登录虚拟机更新文件系统"></a>1.4.5 登录虚拟机更新文件系统</h3><pre><code class="hljs">xfs_growfs /dev/centos/root</code></pre><h3 id="1-4-6-验证磁盘扩容"><a href="#1-4-6-验证磁盘扩容" class="headerlink" title="1.4.6 验证磁盘扩容"></a>1.4.6 验证磁盘扩容</h3><pre><code class="hljs">df -h</code></pre><h1 id="2-虚拟机快照管理"><a href="#2-虚拟机快照管理" class="headerlink" title="2.虚拟机快照管理"></a>2.虚拟机快照管理</h1><p>KVM虚拟机快照用于保存虚拟机在某个时间点内存、磁盘或设备的状态，若有需要可再回滚到这个时间点。快照分为磁盘快照和内存快照两类，两者组合构成了一个系统还原点，记录了虚拟机在某个时间点的全部状态</p><p>磁盘快照根据存储方式的不同，分为内部快照和外部快照</p><ul><li><p>内部快照，只支持qcow2格式的虚拟机镜像，快照及后续变动都保存在原来的qcow2文件内，平常所说的快照通常即为这种</p></li><li><p>外部快照，被保存于另一个单独文件，创建快照时间点之后的数据被记录到一个新的qcow2文件，原镜像文件即成为新镜像文件的backing file（只读），在创建多个快照后，这些文件将形成一个备份链。外部快照同时支持raw和qcow2格式的虚拟机镜像，需要安装qemu-kvm-rhev</p></li></ul><hr><h2 id="2-1-创建虚拟机快照"><a href="#2-1-创建虚拟机快照" class="headerlink" title="2.1 创建虚拟机快照"></a>2.1 创建虚拟机快照</h2><pre><code class="hljs">sudo virsh snapshot-create-as --domain centos7 --name centos7 --description &quot;centos7&quot;</code></pre><h2 id="2-2-查看快照"><a href="#2-2-查看快照" class="headerlink" title="2.2 查看快照"></a>2.2 查看快照</h2><pre><code class="hljs">sudo virsh snapshot-list centos7</code></pre><h2 id="2-3-查看快照信息"><a href="#2-3-查看快照信息" class="headerlink" title="2.3 查看快照信息"></a>2.3 查看快照信息</h2><pre><code class="hljs">sudo virsh snapshot-info centos7 --snapshotname centos7</code></pre><h2 id="2-4-快照回滚"><a href="#2-4-快照回滚" class="headerlink" title="2.4 快照回滚"></a>2.4 快照回滚</h2><pre><code class="hljs">sudo virsh snapshot-revert centos7 centos7</code></pre><h2 id="2-5-快照删除"><a href="#2-5-快照删除" class="headerlink" title="2.5 快照删除"></a>2.5 快照删除</h2><pre><code class="hljs">sudo virsh snapshot-delete centos7 centos7</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.shuzhiduo.com/A/D854PDBQ5E">https://www.shuzhiduo.com/A/D854PDBQ5E</a></li><li><a href="https://blog.51cto.com/u_3646344/2096347">https://blog.51cto.com/u_3646344/2096347</a></li><li><a href="http://koumm.blog.51cto.com/703525/1304196">http://koumm.blog.51cto.com/703525/1304196</a></li><li><a href="https://blog.csdn.net/weixin_30875157/article/details/97096593">https://blog.csdn.net/weixin_30875157/article/details/97096593</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>KVM</tag>
      
      <tag>虚拟化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>KVM网络模式详解</title>
    <link href="/linux/KVM-Network/"/>
    <url>/linux/KVM-Network/</url>
    
    <content type="html"><![CDATA[<p>kvm虚拟机的网络连接模式有两种，即NAT和Bridge模式</p><hr><h1 id="1-NAT"><a href="#1-NAT" class="headerlink" title="1.NAT"></a>1.NAT</h1><p>NAT是KVM默认的网络连接方式，此模式下宿主机配置有虚拟网桥virbr0作为虚拟交换机，并绑定到虚拟网卡virbr0-nic。虚拟机的虚拟网卡vnet0连接virbr0交换机，通过virbr0-nic网卡将数据包转发到宿主机。此时的宿主机已经配置了iptables，成为配置了SNAT规则具备路由功能的路由器，从而将数据包发送到外网进行数据交换</p><h2 id="1-1-查看所有虚拟网络配置"><a href="#1-1-查看所有虚拟网络配置" class="headerlink" title="1.1 查看所有虚拟网络配置"></a>1.1 查看所有虚拟网络配置</h2><pre><code class="hljs">sudo virsh net-list --all</code></pre><h2 id="1-2-修改虚拟网络default"><a href="#1-2-修改虚拟网络default" class="headerlink" title="1.2  修改虚拟网络default"></a>1.2  修改虚拟网络default</h2><h3 id="1-2-1-备份default配置"><a href="#1-2-1-备份default配置" class="headerlink" title="1.2.1 备份default配置"></a>1.2.1 备份default配置</h3><pre><code class="hljs">sudo virsh net-dumpxml default &gt; default.xml</code></pre><h3 id="1-2-2-修改虚拟网络default"><a href="#1-2-2-修改虚拟网络default" class="headerlink" title="1.2.2 修改虚拟网络default"></a>1.2.2 修改虚拟网络default</h3><pre><code class="hljs">sudo virsh net-edit default</code></pre><h3 id="1-2-3-重新定义虚拟网络default"><a href="#1-2-3-重新定义虚拟网络default" class="headerlink" title="1.2.3 重新定义虚拟网络default"></a>1.2.3 重新定义虚拟网络default</h3><pre><code class="hljs">sudo virsh net-define --file /etc/libvirt/qemu/networks/default.xml</code></pre><h3 id="1-2-4-设置虚拟网络default自动启动"><a href="#1-2-4-设置虚拟网络default自动启动" class="headerlink" title="1.2.4 设置虚拟网络default自动启动"></a>1.2.4 设置虚拟网络default自动启动</h3><pre><code class="hljs">sudo virsh net-autostart default</code></pre><h2 id="1-3-创建虚拟机"><a href="#1-3-创建虚拟机" class="headerlink" title="1.3 创建虚拟机"></a>1.3 创建虚拟机</h2><pre><code class="hljs">sudo virt-install --name=centos7 --memory=1024,maxmemory=2048 --vcpus=1,maxvcpus=2 --os-variant=centos7.0 \--location=/home/kvm/images/CentOS-7-x86_64-Minimal-2009.iso --disk /home/kvm/templates/centos7.img,size=30 --network network=default \--graphics=none --console=pty,target_type=serial --extra-args=&#39;console=ttyS0&#39;</code></pre><h2 id="1-4-查看虚拟交换机的连接"><a href="#1-4-查看虚拟交换机的连接" class="headerlink" title="1.4 查看虚拟交换机的连接"></a>1.4 查看虚拟交换机的连接</h2><pre><code class="hljs">sudo brctl show</code></pre><h2 id="1-5-查看虚拟机网络接口类型"><a href="#1-5-查看虚拟机网络接口类型" class="headerlink" title="1.5 查看虚拟机网络接口类型"></a>1.5 查看虚拟机网络接口类型</h2><pre><code class="hljs">sudo virsh domiflist centos7</code></pre><h2 id="1-6-查看虚拟机网卡IP地址"><a href="#1-6-查看虚拟机网卡IP地址" class="headerlink" title="1.6 查看虚拟机网卡IP地址"></a>1.6 查看虚拟机网卡IP地址</h2><pre><code class="hljs">sudo virsh domifaddr centos7</code></pre><h1 id="2-Bridge"><a href="#2-Bridge" class="headerlink" title="2.Bridge"></a>2.Bridge</h1><p>Bridge模式是将宿主机的物理网卡eth0桥接到虚拟交换机virbr1，虚拟机的虚拟网卡vnet0连接virbr1交换机，从而直接将数据包通过宿主机的物理网卡发送到外网进行数据交换</p><h2 id="2-1-centos配置桥接网络"><a href="#2-1-centos配置桥接网络" class="headerlink" title="2.1 centos配置桥接网络"></a>2.1 centos配置桥接网络</h2><h3 id="2-1-1-创建虚拟网桥配置文件"><a href="#2-1-1-创建虚拟网桥配置文件" class="headerlink" title="2.1.1 创建虚拟网桥配置文件"></a>2.1.1 创建虚拟网桥配置文件</h3><pre><code class="hljs">sudo vi /etc/sysconfig/network-scripts/ifcfg-virbr1# 设置设备类型为网桥TYPE=BridgeBOOTPROTO=static# 设置网桥名称NAME=virbr1DEVICE=virbr1ONBOOT=yesIPADDR=192.168.100.100GATEWAY=192.168.100.1DNS1=192.168.100.1DNS2=8.8.8.8</code></pre><h3 id="2-1-2-宿主机物理网卡连接虚拟网桥"><a href="#2-1-2-宿主机物理网卡连接虚拟网桥" class="headerlink" title="2.1.2 宿主机物理网卡连接虚拟网桥"></a>2.1.2 宿主机物理网卡连接虚拟网桥</h3><pre><code class="hljs">sudo cp /etc/sysconfig/network-scripts/ifcfg-enp1s0 /etc/sysconfig/network-scripts/ifcfg-enp1s0.baksudo vi /etc/sysconfig/network-scripts/ifcfg-enp1s0TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=staticDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=enp1s0UUID=77096720-9699-44b0-83d7-582538c24becDEVICE=enp1s0ONBOOT=yesBRIDGE=virbr1# IPADDR=192.168.100.100# NETMASK=255.255.255.0# GATEWAY=192.168.100.1# DNS1=192.168.100.1# DNS2=8.8.8.8</code></pre><h3 id="2-1-3-重启网络服务以生效网络配置"><a href="#2-1-3-重启网络服务以生效网络配置" class="headerlink" title="2.1.3 重启网络服务以生效网络配置"></a>2.1.3 重启网络服务以生效网络配置</h3><pre><code class="hljs">sudo systemctl restart network</code></pre><h3 id="2-1-4-创建虚拟机"><a href="#2-1-4-创建虚拟机" class="headerlink" title="2.1.4 创建虚拟机"></a>2.1.4 创建虚拟机</h3><pre><code class="hljs">sudo virt-install \--name=debian10 --memory=1024,maxmemory=2048 --vcpus=1,maxvcpus=2 --os-variant=debian10 \--location=/home/kvm/images/debian-10.9.0-amd64-netinst.iso --disk /home/kvm/templates/debian10.qcow2,size=30 --network bridge=virbr1 \--graphics=none --console=pty,target_type=serial --extra-args=&#39;console=ttyS0&#39;</code></pre><h3 id="2-1-5-查看虚拟交换机的连接"><a href="#2-1-5-查看虚拟交换机的连接" class="headerlink" title="2.1.5 查看虚拟交换机的连接"></a>2.1.5 查看虚拟交换机的连接</h3><pre><code class="hljs">sudo brctl show</code></pre><h3 id="2-1-6-查看虚拟机网络接口类型"><a href="#2-1-6-查看虚拟机网络接口类型" class="headerlink" title="2.1.6 查看虚拟机网络接口类型"></a>2.1.6 查看虚拟机网络接口类型</h3><pre><code class="hljs">sudo virsh domiflist debian10</code></pre><h3 id="2-1-7-查看虚拟机网卡IP地址"><a href="#2-1-7-查看虚拟机网卡IP地址" class="headerlink" title="2.1.7 查看虚拟机网卡IP地址"></a>2.1.7 查看虚拟机网卡IP地址</h3><pre><code class="hljs">sudo virsh domifaddr debian10</code></pre><h2 id="2-2-debian配置桥接网络"><a href="#2-2-debian配置桥接网络" class="headerlink" title="2.2 debian配置桥接网络"></a>2.2 debian配置桥接网络</h2><pre><code class="hljs">sudo apt install -y bridge-utils</code></pre><h3 id="2-2-1-创建虚拟网桥配置文件"><a href="#2-2-1-创建虚拟网桥配置文件" class="headerlink" title="2.2.1 创建虚拟网桥配置文件"></a>2.2.1 创建虚拟网桥配置文件</h3><pre><code class="hljs">sudo vi /etc/network/interfaces.d/virbr1auto virbr1iface virbr1 inet static  address 192.168.100.100  broadcast 192.168.100.255  netmask 255.255.255.0  gateway 192.168.100.1  # dns-nameservers 192.168.2.254  # If you have muliple interfaces such as eth0 and eth1  # bridge_ports eth0 eth1  bridge_ports enp3s0  bridge_stp off  bridge_waitport 0  bridge_fd 0</code></pre><h2 id="2-3-ubuntu配置桥接网络"><a href="#2-3-ubuntu配置桥接网络" class="headerlink" title="2.3 ubuntu配置桥接网络"></a>2.3 ubuntu配置桥接网络</h2><h3 id="2-3-1-创建虚拟网桥配置文件"><a href="#2-3-1-创建虚拟网桥配置文件" class="headerlink" title="2.3.1 创建虚拟网桥配置文件"></a>2.3.1 创建虚拟网桥配置文件</h3><pre><code class="hljs">sudo cp /etc/netplan/01-network-manager-all.yaml  /etc/netplan/01-network-manager-all.yaml.baksudo vi /etc/netplan/01-network-manager-all.yamlnetwork:  version: 2  renderer: networkd  ethernets:    enp3s0:      dhcp4: no  bridges:    virbr1:      interfaces:        - enp3s0      addresses:        - 192.168.100.240/24      gateway4: 192.168.100.1      nameservers:          addresses: [192.16.100.1, 8.8.8.8]</code></pre><h3 id="2-3-2-应用网卡配置"><a href="#2-3-2-应用网卡配置" class="headerlink" title="2.3.2 应用网卡配置"></a>2.3.2 应用网卡配置</h3><pre><code class="hljs">sudo netplan apply</code></pre><h1 id="3-NAT模式转换为Bridge模式"><a href="#3-NAT模式转换为Bridge模式" class="headerlink" title="3.NAT模式转换为Bridge模式"></a>3.NAT模式转换为Bridge模式</h1><p>NAT网络模式下虚拟机可以访问外网，但外网不可访问虚拟机资源，而Bridge模式支持外网访问，由此需求的情况可将虚拟机的网络从NAT模式转换为Bridge模式</p><h2 id="3-1-关闭虚拟机"><a href="#3-1-关闭虚拟机" class="headerlink" title="3.1 关闭虚拟机"></a>3.1 关闭虚拟机</h2><pre><code class="hljs">sudo virsh shutdown node01</code></pre><h2 id="3-2-修改虚拟机配置文件的网络模式"><a href="#3-2-修改虚拟机配置文件的网络模式" class="headerlink" title="3.2 修改虚拟机配置文件的网络模式"></a>3.2 修改虚拟机配置文件的网络模式</h2><pre><code class="hljs">sudo virsh edit node01# 设置网络类型为bridge&lt;interface type=&#39;bridge&#39;&gt;  &lt;mac address=&#39;52:54:00:4f:2f:3e&#39;/&gt;  # 设置网络名称  &lt;source bridge=&#39;virbr1&#39;/&gt;  &lt;model type=&#39;virtio&#39;/&gt;  &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x03&#39; function=&#39;0x0&#39;/&gt;&lt;/interface&gt;</code></pre><h2 id="3-3-开启虚拟机，配置网卡"><a href="#3-3-开启虚拟机，配置网卡" class="headerlink" title="3.3 开启虚拟机，配置网卡"></a>3.3 开启虚拟机，配置网卡</h2><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.51cto.com/yangshufan/2130263">https://blog.51cto.com/yangshufan/2130263</a></li><li><a href="https://blog.csdn.net/hzhsan/article/details/44098537">https://blog.csdn.net/hzhsan/article/details/44098537</a></li><li><a href="https://www.cnblogs.com/zhangjianchao/p/15329593.html">https://www.cnblogs.com/zhangjianchao/p/15329593.html</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>KVM</tag>
      
      <tag>虚拟化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes集群服务发现</title>
    <link href="/linux/KubernetesServiceDiscover/"/>
    <url>/linux/KubernetesServiceDiscover/</url>
    
    <content type="html"><![CDATA[<p>服务发现，即服务或应用之间相互调用时定位对方IP及端口的过程，通常是通过记录了系统所有服务的信息注册中心来实现，如传统的DNS协议方式、consul服务注册发现中心等。Kubernetes官方推荐使用DNS插件实现集群的服务发现，CoreDNS由于完全遵循官方提供的标准指南而得以广泛应用</p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.jianshu.com/p/33f4701f38b3">https://www.jianshu.com/p/33f4701f38b3</a></li><li><a href="https://www.cnblogs.com/wangguishe/p/16383394.html">https://www.cnblogs.com/wangguishe/p/16383394.html</a></li><li><a href="https://www.qikqiak.com/k8s-book/docs/39.kubedns.html">https://www.qikqiak.com/k8s-book/docs/39.kubedns.html</a></li><li><a href="https://blog.csdn.net/qq_36885515/article/details/127833803">https://blog.csdn.net/qq_36885515/article/details/127833803</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>DNS</tag>
      
      <tag>CoreDNS</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Docsify搭建个人Wiki知识库</title>
    <link href="/geek/Docsify/"/>
    <url>/geek/Docsify/</url>
    
    <content type="html"><![CDATA[<p>Docsify，动态生成文档网站的工具，可将.md文件以Wiki的形式展示，用于制作技术文档、用户手册、wiki等，部署于主机、VPS、Github、静态云存储，特别适于快速搭建小型的文档网站</p><hr><h1 id="1-配置nodejs环境"><a href="#1-配置nodejs环境" class="headerlink" title="1.配置nodejs环境"></a>1.配置nodejs环境</h1><pre><code class="hljs">wget https://nodejs.org/dist/latest/node-v18.8.0-linux-x64.tar.gztar -xzvf node-v18.8.0-linux-x64.tar.gz &amp;&amp; mv node-v18.8.0 /usr/local/nodejssudo ln -s /usr/local/nodejs/bin/node /usr/bin/nodesudo ln -s /usr/local/nodejs/bin/npm /usr/bin/npm</code></pre><h1 id="2-设置npm网络代理"><a href="#2-设置npm网络代理" class="headerlink" title="2.设置npm网络代理"></a>2.设置npm网络代理</h1><pre><code class="hljs">sudo npm config set registry https://registry.npmmirror.com</code></pre><h1 id="3-安装docsify"><a href="#3-安装docsify" class="headerlink" title="3.安装docsify"></a>3.安装docsify</h1><pre><code class="hljs">sudo npm install -g docsify-cli</code></pre><h1 id="4-初始化项目"><a href="#4-初始化项目" class="headerlink" title="4.初始化项目"></a>4.初始化项目</h1><pre><code class="hljs">sudo /usr/local/nodejs/bin/docsify init /web/wiki</code></pre><h1 id="5-配置nginx代理"><a href="#5-配置nginx代理" class="headerlink" title="5.配置nginx代理"></a>5.配置nginx代理</h1><pre><code class="hljs">sudo vi /etc/nginx/nginx.d/wiki.conf        server &#123;  listen       80;  server_name  localhost;  charsetutf-8;  location /wiki &#123;    root  /web;    autoindex on;    index index.html index.htm;    try_files $uri $uri/ /index.html;    # 设置资源不缓存,整个资源库完整后再缓存防止新配置不生效    add_header Cache-Control &quot;no-cache, no-store&quot;;    access_log  /var/log/nginx/wiki_access.log  main;    error_log  /var/log/nginx/wiki_error.log;    &#125;&#125;</code></pre><h1 id="6-访问wiki"><a href="#6-访问wiki" class="headerlink" title="6.访问wiki"></a>6.访问wiki</h1><p><a href="http://ip/">http://ip</a></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://docsify.js.org/#/zh-cn">https://docsify.js.org/#/zh-cn</a></li><li><a href="https://blog.csdn.net/qq_62982856/article/details/129940209">https://blog.csdn.net/qq_62982856/article/details/129940209</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>极客</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>极客</tag>
      
      <tag>Docsify</tag>
      
      <tag>Nodejs</tag>
      
      <tag>Wiki</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes集群Service详解</title>
    <link href="/linux/KubernetesService/"/>
    <url>/linux/KubernetesService/</url>
    
    <content type="html"><![CDATA[<p>Service，即服务，Kubernetes集群最核心的概念之一，定义了逻辑上一组Pod的访问策略，即通过标签选择器选择一组Pod，并为其分配一个访问入口地址，从而实现这个由Pod副本所组成的集群的访问。Service使Kubernetes集群具备了微服务平台的功能，即通过分析、识别并建模业务系统中所有服务为Service，将业务系统转换为由多个提供不同业务能力而又彼此独立的微服务单元，微服务之间通过TCP&#x2F;IP进行通信，从而拥有了强大的分布式能力、弹性扩展能力和容错能力</p><hr><h1 id="1-概念引入"><a href="#1-概念引入" class="headerlink" title="1.概念引入"></a>1.概念引入</h1><p>Kubernetes集群中Pod是应用程序的载体，可以通过Pod的IP来访问应用程序，但Pod会随着集群规模、节点状态、用户缩放等因素动态变化，Pod的名称、运行节点、IP地址也会随之而变化。这样，就给业务系统的内部调用带来很大的不便。故此，引入Service这一概念</p><h1 id="2-工作流程"><a href="#2-工作流程" class="headerlink" title="2.工作流程"></a>2.工作流程</h1><h2 id="2-1-分配虚拟IP"><a href="#2-1-分配虚拟IP" class="headerlink" title="2.1 分配虚拟IP"></a>2.1 分配虚拟IP</h2><p>集群用户向API Server提交Service创建请求，运行在集群所有Node节点的Kube-Proxy组件通过API Serve监测到新增的Service之后为其分配集群虚拟IP，即ClusterIP，若定义时明确指定虚拟IP，则分配指定IP，如未指定则自动分配</p><h2 id="2-2-选择pod副本组"><a href="#2-2-选择pod副本组" class="headerlink" title="2.2 选择pod副本组"></a>2.2 选择pod副本组</h2><p>根据标签选择器选取符合条件的Pod，Controller Manager组件的Endpoints Controller创建Endpoint，即包含这组Pod IP及访问端口的逻辑组合，并将Endpoint信息写入Etcd数据库</p><h2 id="2-3-生成流量转发规则"><a href="#2-3-生成流量转发规则" class="headerlink" title="2.3 生成流量转发规则"></a>2.3 生成流量转发规则</h2><p>Kube-Proxy组件监测到Service、Endpoint的变更，根据监测到的信息设置流量转发规则，即将一个集群虚拟IP及端口号与一个或多个Pod的IP、端口做映射，以作为后端Pod组的访问代理</p><h2 id="2-4-配置服务发现"><a href="#2-4-配置服务发现" class="headerlink" title="2.4 配置服务发现"></a>2.4 配置服务发现</h2><p>集群DNS服务器，如CoreDNS，监测到增的Service，根据Service名称、分配的集群虚拟IP、端口号创建DNS条目</p><h2 id="2-5-Service访问及流量转发"><a href="#2-5-Service访问及流量转发" class="headerlink" title="2.5 Service访问及流量转发"></a>2.5 Service访问及流量转发</h2><p>通过服务名称访问Service时，首先由DNS将服务名称转换成集群虚拟IP与端口号，Kube-Proxy组件根据转发规则对Service的流量计算负载均衡，最后转发到位于后端的Pod</p><h1 id="3-代理模式"><a href="#3-代理模式" class="headerlink" title="3.代理模式"></a>3.代理模式</h1><p>Kube-Proxy组件负责将Service的请求转发到后端的某个Pod实例上，并配置内部实现服务的负载均衡与会话保持规则，实质上就是一个智能的软件负载均衡器。不同于常见的拥有一个独立IP负载均衡器，而是为每个Service分配了一个全局唯一的虚拟IP地址，即Cluster IP，实际上就是用于生成Iptables或IPVS转发规则时使用的IP地址，仅用于实现Kubernetes集群网络的内部通讯，仅能够将规则中定义的转发服务的请求作为目标地址予以响应，且在Service的整个生命周期内都不会改变</p><p>Kube-Proxy将请求代理至相关端点的方式有三种，即userspace、iptables和ipvs</p><h2 id="3-1-userspace"><a href="#3-1-userspace" class="headerlink" title="3.1 userspace"></a>3.1 userspace</h2><p>userspace，即用户空间，指的是Linux操作系统的用户空间。此模式下，Kube-Proxy负责跟踪API Server上的Service和Endpoint对象的变动（创建或删除），并根据此调整Service资源的定义</p><p>此模式下，请求流量到达内核空间后经由套接字送往用户空间Kube-Proxy，而后再由它送回内核空间，并调度至后端Pod。由于请求在内核空间和用户空间来回转发必然会导致效率不高，Kubernetes1.1版本之后即被废弃</p><h2 id="3-2-iptables"><a href="#3-2-iptables" class="headerlink" title="3.2 iptables"></a>3.2 iptables</h2><p>类似于userspace代理模式，iptables代理模式中，Kube-Proxy负责跟踪API Server上Service和Endpoint对象的变动（创建或删除），并据此做出Service资源定义的变动。同时，对于每个Service，都将会创建iptables规则直接捕获到达ClusterIP和Port的流量，并将其重定向至当前Service后端。对于每个Endpoint对象，Service资源会为其创建iptables规则并关联至挑选的后端Pod资源，默认算法是随即调度（random）。此模式自1.2版本开始成为默认的类型</p><p>在创建Service资源时，集群中每个节点上的Kube-Proxy都会收到通知并将其定义为当前节点上的iptables规则，用于转发工作接口接收到的与此Service资源的ClusterIP和端口的相关流量。客户端发来的请求被相关的iptables规则进行调度和目标地址转换（DNAT）后再转发至集群内的Pod对象上</p><p>相对于userspace模式，iptables模式无须将流量在用户空间和内核空间来回切换，因而更加高效和可靠。其缺点是iptables代理模型不会在被挑中的后端Pod资源无响应时自动进行重定向，而userspace模型则可以</p><h2 id="3-3-ipvs"><a href="#3-3-ipvs" class="headerlink" title="3.3 ipvs"></a>3.3 ipvs</h2><p>此模式下，Kube-Proxy跟踪API Server上Service和Endpoint对象的变动，据此来调用netlink接口创建ipvs规则，并确保与API Server中的变动保持同步。与iptables规则的不同之处仅在于其请求流量的调度功能由ipvs实现，其余功能仍由iptables完成。Kubernetes自1.9-alpha版本引入了ipvs代理模型，且自1.11版本起成为默认设置</p><p>类似于iptables模式，ipvs构建于netfilter的钩子函数之上，但使用hash作为底层数据结构并工作于内核空间，因此具有流量转发速度快、规则同步性能好的特性。此外，ipvs还支持众多的调度算法，例如rr、lc、dh、sh、sed和nq等</p><h1 id="4-Service类型"><a href="#4-Service类型" class="headerlink" title="4.Service类型"></a>4.Service类型</h1><h2 id="4-1-ClusterIP"><a href="#4-1-ClusterIP" class="headerlink" title="4.1 ClusterIP"></a>4.1 ClusterIP</h2><p>默认的Service类型，通过集群自动分配的内部IP地址暴露服务，该地址仅在集群内部可达，无法被集群外部的客户端访问</p><pre><code class="hljs">apiVersion: v1kind: Servicemetadata:  # 设置service名字  name: redis-server  namespace: defaultspec:  selector:     # 设置标签选择器，关联指定的pod    app: redis-servers  # 设置service类型，默认为ClusterIP  type: ClusterIP  # 设置clusterIP，若不指定则自动分配  clusterIP: 172.16.66.66  ports:  # 设置service访问端口  - port: 6379    # 设置pod端口    targetPort: 6379</code></pre><h2 id="4-2-NodePort"><a href="#4-2-NodePort" class="headerlink" title="4.2 NodePort"></a>4.2 NodePort</h2><p>基于ClusterIP而设计，依然会为Service分配集群IP地址，并将此作为NodePort的路由目标，工作原理就是将Service的端口映射到每个Node节点的同一个端口，用于将集群外部的用户请求转发至目标Service的ClusterIP和Port</p><ul><li>集群内部Pod访问流量走向为：服务名 — &gt;&gt; 集群虚拟IP:端口号 — &gt;&gt; kube-proxy转发到pod</li><li>集群外部Pod访问流量走向为：节点IP:NodePort — &gt;&gt; 集群虚拟IP:端口号 — &gt;&gt; kube-Proxy转发到Pod</li></ul><hr><pre><code class="hljs">apiVersion: v1kind: Servicemetadata:  name: nginx-server  namespace: defaultspec:  type: NodePort  selector:    app: nginx-servers  ports:  - protocol: TCP    port: 80    targetPort: 80    # 设置绑定到node节点的端口，若不指定则默认分配，取值范围为30000-32767    nodePort: 30080</code></pre><h2 id="4-3-LoadBalancer"><a href="#4-3-LoadBalancer" class="headerlink" title="4.3 LoadBalancer"></a>4.3 LoadBalancer</h2><p>基于NodePort而设计，通过公有云服务商提供的负载均衡器将服务暴露到集群外部，也具有NodePort和ClusterIP。一个LoadBalancer类型的Service会指向关联Kubernetes集群外部的、切实存在的某个负载均衡设备，该设备通过工作节点之上的NodePort向集群内部发送请求流量。优势在于能够把来自集群外部客户端的请求调度至所有节点（或部分节点）的NodePort之上，而非依赖于客户端自行决定连接至哪个节点，从而避免了因客户端指定的节点故障而导致的服务不可用</p><p>目前已不再推荐使用，建议使用Ingress Controller</p><h2 id="4-4-ExternalName"><a href="#4-4-ExternalName" class="headerlink" title="4.4 ExternalName"></a>4.4 ExternalName</h2><p>通过将Service映射至由externalName字段的内容所指定的主机名来暴露服务，此主机名需被DNS服务解析至CNAME类型的记录。实际之，该类型的Service并不是Kubernetes集群定义，而是把集群外部的某服务以DNS CNAME记录的方式映射到集群内，从而让集群内的Pod资源能够访问外部的Service。因此，该类型的Service没有ClusterIP和NodePort，也没有标签选择器用于选择Pod资源，也不会有Enpoints存在</p><h2 id="4-5-HeadLess"><a href="#4-5-HeadLess" class="headerlink" title="4.5 HeadLess"></a>4.5 HeadLess</h2><p>基于ClusterIP而设计，但通过指定clusterIP属性为None而定义Service，即不为服务分配集群虚拟IP，自然也就不能在DNS插件中添加服务相关条目，Kube-Proxy组件不会为其添加转发规则，也就无法利用Kube-Proxy的转发、负载均衡功能</p><p>HeadLess Service依然会根据标签选择器创建Endpoint，并根据Endpoint向DNS插件中添加条目，DNS会将Headless Service的后端直接解析为Pod组的IP列表。这类Service主要供StatefulSet使用，因为StatefulSet能保证其管理的Pod有序，名称地址等特征保持不变</p><pre><code class="hljs">apiVersion: v1kind: Servicemetadata:  name: kafka-serverspec:  selector:    app: kafka-servers  # 设置cluserIP为空  clusterIP: None  ports:  - port: 9092    targetPort: 9092</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://k.i4t.com/kubernetes_service.html">https://k.i4t.com/kubernetes_service.html</a></li><li><a href="https://cloud.tencent.com/developer/article/1981390">https://cloud.tencent.com/developer/article/1981390</a></li><li><a href="https://www.modb.pro/db/388641">https://www.modb.pro/db/388641</a></li><li><a href="https://blog.csdn.net/dkfajsldfsdfsd/article/details/81200411">https://blog.csdn.net/dkfajsldfsdfsd/article/details/81200411</a></li><li><a href="https://blog.csdn.net/bbj1030/article/details/124582208">https://blog.csdn.net/bbj1030/article/details/124582208</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Git服务器的安装与配置</title>
    <link href="/linux/Gitea/"/>
    <url>/linux/Gitea/</url>
    
    <content type="html"><![CDATA[<p>Gitea，由Go语言开发的轻量级开源DevOps平台系统，安装部署简易，高性能低资耗，旨在为个人、小型团队和企业提供简单、快速、安全的代码自托管解决方案。Gitea发源于Gogs，功能强大，从开发计划到产品成型的整个软件生命周期都能够高效而轻松地进行支持，如Git托管、代码审查、团队协作、软件包注册和CI&#x2F;CD。此外，Gitea还具有UI界面、权限管理、团队协作等功能，其开源社区也非常活跃</p><h1 id="1-安装Git"><a href="#1-安装Git" class="headerlink" title="1.安装Git"></a>1.安装Git</h1><pre><code class="hljs">sudo yum -y install git sudo apt -y install git</code></pre><h1 id="2-下载安装包"><a href="#2-下载安装包" class="headerlink" title="2.下载安装包"></a>2.下载安装包</h1><pre><code class="hljs"> curl -o gitea-1.20.6-linux-amd64.xz https://dl.gitea.com/gitea/1.20.6/gitea-1.20.6-linux-amd64.xz</code></pre><h1 id="3-安装Gitea"><a href="#3-安装Gitea" class="headerlink" title="3.安装Gitea"></a>3.安装Gitea</h1><pre><code class="hljs"> xz -d gitea-1.20.6-linux-amd64.xz sudo mkdir -p /usr/local/gitea/&#123;bin,etc,data,logs&#125; sudo mv gitea-1.20.6-linux-amd64 /usr/local/gitea/bin/gitea sudo chmod +x /usr/local/gitea/bin/gitea &amp;&amp; sudo chown -R sword.sword /usr/local/gitea</code></pre><h1 id="4-创建数据库"><a href="#4-创建数据库" class="headerlink" title="4.创建数据库"></a>4.创建数据库</h1><pre><code class="hljs">MariaDB [(none)]&gt; CREATE DATABASE gitea CHARACTER SET &#39;utf8mb4&#39; COLLATE &#39;utf8mb4_bin&#39;;MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON gitea.* TO &#39;gitea&#39;@&#39;%&#39;;MariaDB [(none)]&gt; FLUSH PRIVILEGES;</code></pre><h1 id="5-创建启动脚本"><a href="#5-创建启动脚本" class="headerlink" title="5.创建启动脚本"></a>5.创建启动脚本</h1><pre><code class="hljs">sudo vi /lib/systemd/system/gitea.service[Unit]Description=Gitea (Git with a cup of tea)After=network.target[Service]RestartSec=2sType=simpleUser=swordGroup=swordWorkingDirectory=/usr/local/giteaExecStart=/usr/local/gitea/bin/gitea web --config /usr/local/gitea/etc/app.iniRestart=always[Install]WantedBy=multi-user.target</code></pre><h1 id="6-启动Gitea"><a href="#6-启动Gitea" class="headerlink" title="6.启动Gitea"></a>6.启动Gitea</h1><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start gitea.servicesudo systemctl enable gitea.service</code></pre><h1 id="7-系统初始化"><a href="#7-系统初始化" class="headerlink" title="7.系统初始化"></a>7.系统初始化</h1><p><img src="/img/wiki/git/gitea.jpg" alt="gitea"></p><h1 id="8-验证Gitea"><a href="#8-验证Gitea" class="headerlink" title="8.验证Gitea"></a>8.验证Gitea</h1><p><a href="http://ip:3000/">http://ip:3000</a></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://hongzx.cn/home/blogShow/204">https://hongzx.cn/home/blogShow/204</a></li><li><a href="https://docs.gitea.com/installation/install-from-binary">https://docs.gitea.com/installation/install-from-binary</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Git</tag>
      
      <tag>CICD</tag>
      
      <tag>DevOps</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes集群基于Kube-Prometheus部署监控告警系统</title>
    <link href="/linux/kube-Prometheus/"/>
    <url>/linux/kube-Prometheus/</url>
    
    <content type="html"><![CDATA[<p>Kube-Prometheus，Coreos基于Prometheus Operator开发的专为Kubernetes集群设计的一站式开箱即用监控方案，以定制资源管理应用和组件的方式简便快捷地实现Prometheus实例的部署、管理及监控规则的定义，从而大大降低了Kubernetes集群接入Prometheus监控系统的难度，无需手动编写繁杂的YAML文件进行部署与管理</p><h1 id="Prometheus-Operator"><a href="#Prometheus-Operator" class="headerlink" title="Prometheus Operator"></a>Prometheus Operator</h1><p>Prometheus Operator基于Kubernetes集群的operator操作模式而设计，在不修改Kubernetes自身代码的情况下通过封装业务逻辑和任务自动化代码定制资源管理应用及其组件，从而获得集群内置的自动化功能与扩展功能，达成Prometheus监控系统配置的精简化与自动化</p><h2 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h2><h3 id="Operator"><a href="#Operator" class="headerlink" title="Operator"></a>Operator</h3><p>Operator根据自定义资源（Custom Resource Definition即CRDs）部署和管理Prometheus Server，并监控这些自定义资源事件的变化做相应的处理，是整个系统的控制中心</p><h3 id="Prometheus"><a href="#Prometheus" class="headerlink" title="Prometheus"></a>Prometheus</h3><p>Prometheus声明性地描述Prometheus部署的期望状态</p><h3 id="Prometheus-Server"><a href="#Prometheus-Server" class="headerlink" title="Prometheus Server"></a>Prometheus Server</h3><p>Operator根据自定义资源Prometheus类型中定义的内容而部署Prometheus Server集群，这些自定义资源是用来管理Prometheus Server集群的StatefulSets</p><h2 id="ServiceMonitor"><a href="#ServiceMonitor" class="headerlink" title="ServiceMonitor"></a>ServiceMonitor</h2><p>ServiceMonitor也是自定义资源，描述了一组被Prometheus监控的targets列表，通过Labels选取对应的Service Endpoint，从而使得Prometheus Server通过选取的Service获取Metrics信息</p><h3 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h3><p>Service用于对应Kubernetes集群中的Metrics Server Pod，也即是Prometheus监控的对象，提供给ServiceMonitor选取让Prometheus Server来获取信息，如Node Exporter Service、Mysql Exporter Service等</p><h3 id="Alertmanager"><a href="#Alertmanager" class="headerlink" title="Alertmanager"></a>Alertmanager</h3><p>Alertmanager也是自定义资源类型，由Operator根据资源描述的内容部署Alertmanager集群</p><hr><h1 id="1-部署nfs"><a href="#1-部署nfs" class="headerlink" title="1.部署nfs"></a>1.部署nfs</h1><h1 id="2-下载Kube-Prometheus"><a href="#2-下载Kube-Prometheus" class="headerlink" title="2.下载Kube-Prometheus"></a>2.下载Kube-Prometheus</h1><pre><code class="hljs">wget -O kube-prometheus-0.8.0.tar.gz https://github.com/prometheus-operator/kube-prometheus/archive/refs/tags/v0.8.0.tar.gz</code></pre><h1 id="3-部署CRD"><a href="#3-部署CRD" class="headerlink" title="3.部署CRD"></a>3.部署CRD</h1><pre><code class="hljs">tar -xzvf kube-prometheus-0.8.0.tar.gz &amp;&amp; cd kube-prometheus-0.8.0/manifestskubectl apply -f setup/</code></pre><h1 id="4-部署StorageClass"><a href="#4-部署StorageClass" class="headerlink" title="4.部署StorageClass"></a>4.部署StorageClass</h1><h2 id="4-1-创建provisioner"><a href="#4-1-创建provisioner" class="headerlink" title="4.1 创建provisioner"></a>4.1 创建provisioner</h2><pre><code class="hljs">vi nfs-client-provisioner.yamlapiVersion: v1kind: ServiceAccountmetadata:  name: nfs-client-provisioner  namespace: kube-system---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata:  name: nfs-client-provisioner-runnerrules:  - apiGroups: [&quot;&quot;]    resources: [&quot;persistentvolumes&quot;]    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;]  - apiGroups: [&quot;&quot;]    resources: [&quot;persistentvolumeclaims&quot;]    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;]  - apiGroups: [&quot;storage.k8s.io&quot;]    resources: [&quot;storageclasses&quot;]    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]  - apiGroups: [&quot;&quot;]    resources: [&quot;events&quot;]    verbs: [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata:  name: run-nfs-client-provisionersubjects:  - kind: ServiceAccount    name: nfs-client-provisioner    namespace: monitoringroleRef:   kind: ClusterRole  name: nfs-client-provisioner-runner  apiGroup: rbac.authorization.k8s.io---kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata:  name: leader-locking-nfs-client-provisioner  namespace: kube-systemrules:  - apiGroups: [&quot;&quot;]    resources: [&quot;endpoints&quot;]    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]---kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata:  name: leader-locking-nfs-client-provisioner  namespace: kube-systemsubjects:  - kind: ServiceAccount    name: nfs-client-provisioner    namespace: monitoringroleRef:  kind: Role  name: leader-locking-nfs-client-provisioner  apiGroup: rbac.authorization.k8s.io---apiVersion: apps/v1kind: Deploymentmetadata:  name: nfs-client-provisioner  labels:    app: nfs-client-provisioner  namespace: kube-systemspec:  replicas: 1  strategy:    type: Recreate  selector:    matchLabels:      app: nfs-client-provisioner  template:    metadata:      labels:        app: nfs-client-provisioner    spec:      serviceAccountName: nfs-client-provisioner      containers:        - name: nfs-client-provisioner          image: quay.io/external_storage/nfs-client-provisioner:latest          volumeMounts:            - name: nfs-client-root              mountPath: /persistentvolumes          env:            - name: PROVISIONER_NAME              # 设置nfs provisioner名称，storageclass需保持一致              value: nfs-client            - name: NFS_SERVER              # 设置nfs服务器IP              value: 192.168.100.200            - name: NFS_PATH              # 设置nfs路径              value: /home/project/kubernetes      volumes:        - name: nfs-client-root          nfs:            server: 192.168.100.200            path: /home/project/kubernetes</code></pre><h2 id="4-2-创建默认StorageClass"><a href="#4-2-创建默认StorageClass" class="headerlink" title="4.2 创建默认StorageClass"></a>4.2 创建默认StorageClass</h2><pre><code class="hljs">vi sc-default.yamlapiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: sc-nfs  annotations:    # 设置为集群默认storageclass    storageclass.kubernetes.io/is-default-class: &quot;true&quot;# 设置动态卷供应器名称，对应供应器PROVISIONER_NAMEprovisioner: nfs-clientparameters:  # 设置PVC删除时保留PV数据  archiveOnDelete: &quot;true&quot;</code></pre><h1 id="5-配置持久化存储"><a href="#5-配置持久化存储" class="headerlink" title="5.配置持久化存储"></a>5.配置持久化存储</h1><h2 id="5-1-配置prometheus持久化存储"><a href="#5-1-配置prometheus持久化存储" class="headerlink" title="5.1 配置prometheus持久化存储"></a>5.1 配置prometheus持久化存储</h2><pre><code class="hljs">vi prometheus-prometheus.yamlapiVersion: monitoring.coreos.com/v1kind: Prometheusmetadata:  labels:    app.kubernetes.io/component: prometheus    app.kubernetes.io/name: prometheus    app.kubernetes.io/part-of: kube-prometheus    app.kubernetes.io/version: 2.26.0    prometheus: k8s  name: k8s  namespace: monitoringspec:  alerting:    alertmanagers:    - apiVersion: v2      name: alertmanager-main      namespace: monitoring      port: web  externalLabels: &#123;&#125;  image: quay.io/prometheus/prometheus:v2.26.0  nodeSelector:    kubernetes.io/os: linux  podMetadata:    labels:      app.kubernetes.io/component: prometheus      app.kubernetes.io/name: prometheus      app.kubernetes.io/part-of: kube-prometheus      app.kubernetes.io/version: 2.26.0  podMonitorNamespaceSelector: &#123;&#125;  podMonitorSelector: &#123;&#125;  probeNamespaceSelector: &#123;&#125;  probeSelector: &#123;&#125;  replicas: 2  resources:    requests:      memory: 400Mi  ruleSelector:    matchLabels:      prometheus: k8s      role: alert-rules  securityContext:    fsGroup: 2000    runAsNonRoot: true    runAsUser: 1000  serviceAccountName: prometheus-k8s  serviceMonitorNamespaceSelector: &#123;&#125;  serviceMonitorSelector: &#123;&#125;  version: 2.26.0  # 新增存储配置  storage:    volumeClaimTemplate:      spec:        storageClassName: sc-nfs        accessModes: [&quot;ReadWriteOnce&quot;]        resources:          requests:            storage: 10Gi</code></pre><h2 id="5-2-配置grafana持久化存储"><a href="#5-2-配置grafana持久化存储" class="headerlink" title="5.2 配置grafana持久化存储"></a>5.2 配置grafana持久化存储</h2><h3 id="5-2-1-修改部署文件"><a href="#5-2-1-修改部署文件" class="headerlink" title="5.2.1 修改部署文件"></a>5.2.1 修改部署文件</h3><pre><code class="hljs">vi grafana-deployment.yamlvolumes:  - persistentVolumeClaim:      claimName: grafana-data    name: grafana-storage</code></pre><h3 id="5-2-2-创建PV、PVC"><a href="#5-2-2-创建PV、PVC" class="headerlink" title="5.2.2 创建PV、PVC"></a>5.2.2 创建PV、PVC</h3><pre><code class="hljs">vi grafana-data.yamlapiVersion: v1kind: PersistentVolumemetadata:  # 设置PV名称  name: grafana-data  # 设置PV标签，用于PVC的定向绑定  labels:    app: grafana-dataspec:  # 设置存储类别  storageClassName: nfs  # 设置访问模式  accessModes:    - ReadWriteMany  # 设置PV的存储空间  capacity:    storage: 10Gi  # 设置PV的回收策略  persistentVolumeReclaimPolicy: Retain  nfs:    path: /home/project/kubernetes/grafana    server: 192.168.100.200---apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: grafana-data  namespace: monitoringspec:  # 设置PVC存储类别，用于指定存储类型  storageClassName: nfs  # 设置访问模式，匹配相同模式的PV  accessModes:  - ReadWriteMany  # 设置PVC所申请存储空间的大小  resources:    requests:      storage: 8Gi</code></pre><h3 id="5-2-3-部署grafana存储"><a href="#5-2-3-部署grafana存储" class="headerlink" title="5.2.3 部署grafana存储"></a>5.2.3 部署grafana存储</h3><pre><code class="hljs">kubectl apply -f grafana-data.yaml</code></pre><h1 id="6-配置grafana-service"><a href="#6-配置grafana-service" class="headerlink" title="6. 配置grafana service"></a>6. 配置grafana service</h1><pre><code class="hljs">vi grafana-service.yamlapiVersion: v1kind: Servicemetadata:  labels:    app.kubernetes.io/component: grafana    app.kubernetes.io/name: grafana    app.kubernetes.io/part-of: kube-prometheus    app.kubernetes.io/version: 7.5.4  name: grafana  namespace: monitoringspec:  #   修改为NodePort  type: NodePort  ports:  - name: http    port: 3000    # 设置绑定节点的端口    nodePort: 32000    targetPort: http  selector:    app.kubernetes.io/component: grafana    app.kubernetes.io/name: grafana    app.kubernetes.io/part-of: kube-prometheus</code></pre><h1 id="7-部署Prometheus-Operator"><a href="#7-部署Prometheus-Operator" class="headerlink" title="7.部署Prometheus Operator"></a>7.部署Prometheus Operator</h1><pre><code class="hljs">kubectl apply -f .</code></pre><h1 id="8-验证Prometheus-Operator"><a href="#8-验证Prometheus-Operator" class="headerlink" title="8.验证Prometheus Operator"></a>8.验证Prometheus Operator</h1><h2 id="8-1-监控服务资源运行"><a href="#8-1-监控服务资源运行" class="headerlink" title="8.1 监控服务资源运行"></a>8.1 监控服务资源运行</h2><pre><code class="hljs"># pod运行kubectl -n monitoring get pod -o wide# 存储资源PVCkubectl -n monitoring get pvc# 网络资源servicekubectl -n monitoring get svc</code></pre><h2 id="8-2-访问grafana"><a href="#8-2-访问grafana" class="headerlink" title="8.2 访问grafana"></a>8.2 访问grafana</h2><p><a href="http://worker节点ip:32000/">http://worker节点IP:32000</a></p><ul><li>默认用户名&#x2F;密码：admin&#x2F;admin</li></ul><h2 id="8-3-导入监控大盘模版"><a href="#8-3-导入监控大盘模版" class="headerlink" title="8.3 导入监控大盘模版"></a>8.3 导入监控大盘模版</h2><p>Manage —&gt; Import —&gt; 模版ID：13105</p><p><img src="/img/wiki/prometheus/kubernetes.jpg" alt="kubernetes"></p><h1 id="9-配置告警"><a href="#9-配置告警" class="headerlink" title="9.配置告警"></a>9.配置告警</h1><p>Prometheus的告警功能由Prometheus Server和AlertManager共同完成，Prometheus Server对配置的告警规则周期性的计算，将触发告警条件的规则生成告警通知并发送给AlertManager，AlertManager对告警通知进行分组、去重后再根据路由规则将其路由到不同的receiver，如Email、短信或Webhook等</p><h2 id="9-1-配置邮箱告警"><a href="#9-1-配置邮箱告警" class="headerlink" title="9.1 配置邮箱告警"></a>9.1 配置邮箱告警</h2><h3 id="9-1-1-发送邮箱开启SMTP服务，获取登录授权码"><a href="#9-1-1-发送邮箱开启SMTP服务，获取登录授权码" class="headerlink" title="9.1.1 发送邮箱开启SMTP服务，获取登录授权码"></a>9.1.1 发送邮箱开启SMTP服务，获取登录授权码</h3><h3 id="9-1-2-创建AlertManager配置文件"><a href="#9-1-2-创建AlertManager配置文件" class="headerlink" title="9.1.2 创建AlertManager配置文件"></a>9.1.2 创建AlertManager配置文件</h3><pre><code class="hljs">vi alertmanager.yamlglobal:  # 设置告警恢复时间间隔，即告警不再继续产生的时长  resolve_timeout: 5m  smtp_smarthost: &#39;smtp.139.com:465&#39;  smtp_from: &#39;sxs0618@139.com&#39;  smtp_auth_username: &#39;sxs0618@139.com&#39;  # 设置邮箱授权码  smtp_auth_password: &#39;xxxxxxxxxxxx&#39;  smtp_hello: &#39;test&#39;  smtp_require_tls: false# 设置告警通知模版templates:  - &#39;./*.tmpl&#39;route:  # 设置告警分组的属性依据  group_by: [&#39;alertname&#39;]  # 设置告警发送前的等待时间  group_wait: 30s  # 设置告警发送时间间隔  group_interval: 5m  # 设置分组内相同告警的发送时间间隔  repeat_interval: 3h  # 设置告警接收者，匹配receivers配置项  receiver: emailreceivers:- name: email  email_configs:  - to: &#39;523343553@qq.com&#39;    html: &#39;&#123;&#123; template "email.html" . &#125;&#125;&#39;    send_resolved: true</code></pre><h3 id="9-1-3-创建AlertManager告警模版"><a href="#9-1-3-创建AlertManager告警模版" class="headerlink" title="9.1.3 创建AlertManager告警模版"></a>9.1.3 创建AlertManager告警模版</h3><pre><code class="hljs">mkdir templatevi template/email.tmpl&#123;&#123; define "email.html" &#125;&#125; &#123;&#123; range .Alerts &#125;&#125; =========start==========&lt;br&gt; 告警程序: Prometheus_Alert &lt;br&gt; 告警级别: &#123;&#123; .Labels.severity &#125;&#125; &lt;br&gt;告警类型: &#123;&#123; .Labels.alertname &#125;&#125; &lt;br&gt; 告警实例: &#123;&#123; .Labels.instance &#125;&#125; &lt;br&gt; 告警主题: &#123;&#123; .Annotations.summary &#125;&#125; &lt;br&gt;告警详情: &#123;&#123; .Annotations.description &#125;&#125; &lt;br&gt;触发时间: &#123;&#123; .StartsAt &#125;&#125; &lt;br&gt;=========end==========&lt;br&gt; &#123;&#123; end &#125;&#125; &#123;&#123; end &#125;&#125;</code></pre><h3 id="9-1-4-更新AlertManager配置文件"><a href="#9-1-4-更新AlertManager配置文件" class="headerlink" title="9.1.4 更新AlertManager配置文件"></a>9.1.4 更新AlertManager配置文件</h3><pre><code class="hljs">kubectl -n monitoring delete secrets alertmanager-mainkubectl -n monitoring create secret generic alertmanager-main --from-file=alertmanager.yaml --from-file=./template/email.tmpl</code></pre><h3 id="9-1-5-验证告警"><a href="#9-1-5-验证告警" class="headerlink" title="9.1.5 验证告警"></a>9.1.5 验证告警</h3><h2 id="9-2-配置钉钉告警"><a href="#9-2-配置钉钉告警" class="headerlink" title="9.2 配置钉钉告警"></a>9.2 配置钉钉告警</h2><h3 id="9-2-1-钉钉群创建机器人"><a href="#9-2-1-钉钉群创建机器人" class="headerlink" title="9.2.1 钉钉群创建机器人"></a>9.2.1 钉钉群创建机器人</h3><h3 id="9-2-2-部署钉钉告警插件"><a href="#9-2-2-部署钉钉告警插件" class="headerlink" title="9.2.2 部署钉钉告警插件"></a>9.2.2 部署钉钉告警插件</h3><h4 id="9-2-2-1-创建钉钉告警插件"><a href="#9-2-2-1-创建钉钉告警插件" class="headerlink" title="9.2.2.1 创建钉钉告警插件"></a>9.2.2.1 创建钉钉告警插件</h4><pre><code class="hljs">vi dingtalk-deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata:  name: dingtalk  namespace: monitoringspec:  replicas: 1  selector:    matchLabels:      app: dingtalk  template:    metadata:      name: dingtalk      labels:        app: dingtalk    spec:      containers:      - name: dingtalk        image: registry.cn-hangzhou.aliyuncs.com/kubernetess/prometheus-webhook-dingtalk:v2.1.0        imagePullPolicy: IfNotPresent        resources:          limits:            cpu: 100m            memory: 100Mi          requests:            cpu: 50m            memory: 50Mi        ports:        - containerPort: 8060        volumeMounts:        - name: config          mountPath: /etc/prometheus-webhook-dingtalk      volumes:      - name: config        configMap:          name: dingtalk-config---apiVersion: v1kind: Servicemetadata:  name: dingtalk  namespace: monitoring  labels:    app: dingtalk  annotations:    prometheus.io/scrape: &#39;false&#39;spec:  selector:    app: dingtalk  ports:  - name: dingtalk    port: 8060    protocol: TCP    targetPort: 8060</code></pre><h4 id="9-2-2-2-创建钉钉告警插件配置文件"><a href="#9-2-2-2-创建钉钉告警插件配置文件" class="headerlink" title="9.2.2.2 创建钉钉告警插件配置文件"></a>9.2.2.2 创建钉钉告警插件配置文件</h4><pre><code class="hljs">vi dingtalk-config.yamlapiVersion: v1kind: ConfigMapmetadata:  name: dingtalk-config  namespace: monitoringdata:  config.yml: |-    templates:      - /etc/prometheus-webhook-dingtalk/template.tmpl    targets:      webhook:        url: https://oapi.dingtalk.com/robot/send?access_token=xxxxxxxxx        secret: xxxxxxxxx        mention:          all: true  template.tmpl: |-    &#123;&#123; define "__subject" &#125;&#125;[&#123;&#123; .Status | toUpper &#125;&#125;&#123;&#123; if eq .Status "firing" &#125;&#125;:&#123;&#123; .Alerts.Firing | len &#125;&#125;&#123;&#123; end &#125;&#125;] &#123;&#123; .GroupLabels.SortedPairs.Values | join " " &#125;&#125; &#123;&#123; if gt (len .CommonLabels) (len .GroupLabels) &#125;&#125;(&#123;&#123; with .CommonLabels.Remove .GroupLabels.Names &#125;&#125;&#123;&#123; .Values | join " " &#125;&#125;&#123;&#123; end &#125;&#125;)&#123;&#123; end &#125;&#125;&#123;&#123; end &#125;&#125;    &#123;&#123; define "__alertmanagerURL" &#125;&#125;&#123;&#123; .ExternalURL &#125;&#125;/#/alerts?receiver=&#123;&#123; .Receiver &#125;&#125;&#123;&#123; end &#125;&#125;    &#123;&#123; define "__text_alert_list" &#125;&#125;&#123;&#123; range . &#125;&#125;    **Labels**    &#123;&#123; range .Labels.SortedPairs &#125;&#125; - &#123;&#123; .Name &#125;&#125;: &#123;&#123; .Value | markdown | html &#125;&#125;    &#123;&#123; end &#125;&#125;    **Annotations**    &#123;&#123; range .Annotations.SortedPairs &#125;&#125; - &#123;&#123; .Name &#125;&#125;: &#123;&#123; .Value | markdown | html &#125;&#125;    &#123;&#123; end &#125;&#125;    **Source:** [&#123;&#123; .GeneratorURL &#125;&#125;](&#123;&#123; .GeneratorURL &#125;&#125;)    &#123;&#123; end &#125;&#125;&#123;&#123; end &#125;&#125;    &#123;&#123; define "default.__text_alert_list" &#125;&#125;&#123;&#123; range . &#125;&#125;    ---    **告警级别:** &#123;&#123; .Labels.severity | upper &#125;&#125;    **运营团队:** &#123;&#123; .Labels.team | upper &#125;&#125;    **触发时间:** &#123;&#123; dateInZone "2006.01.02 15:04:05" (.StartsAt) "Asia/Shanghai" &#125;&#125;    **事件信息:**    &#123;&#123; range .Annotations.SortedPairs &#125;&#125; - &#123;&#123; .Name &#125;&#125;: &#123;&#123; .Value | markdown | html &#125;&#125;    &#123;&#123; end &#125;&#125;    **事件标签:**    &#123;&#123; range .Labels.SortedPairs &#125;&#125;&#123;&#123; if and (ne (.Name) "severity") (ne (.Name) "summary") (ne (.Name) "team") &#125;&#125; - &#123;&#123; .Name &#125;&#125;: &#123;&#123; .Value | markdown | html &#125;&#125;    &#123;&#123; end &#125;&#125;&#123;&#123; end &#125;&#125;    &#123;&#123; end &#125;&#125;    &#123;&#123; end &#125;&#125;    &#123;&#123; define "default.__text_alertresovle_list" &#125;&#125;&#123;&#123; range . &#125;&#125;    ---    **告警级别:** &#123;&#123; .Labels.severity | upper &#125;&#125;    **运营团队:** &#123;&#123; .Labels.team | upper &#125;&#125;    **触发时间:** &#123;&#123; dateInZone "2006.01.02 15:04:05" (.StartsAt) "Asia/Shanghai" &#125;&#125;    **结束时间:** &#123;&#123; dateInZone "2006.01.02 15:04:05" (.EndsAt) "Asia/Shanghai" &#125;&#125;    **事件信息:**    &#123;&#123; range .Annotations.SortedPairs &#125;&#125; - &#123;&#123; .Name &#125;&#125;: &#123;&#123; .Value | markdown | html &#125;&#125;    &#123;&#123; end &#125;&#125;    **事件标签:**    &#123;&#123; range .Labels.SortedPairs &#125;&#125;&#123;&#123; if and (ne (.Name) "severity") (ne (.Name) "summary") (ne (.Name) "team") &#125;&#125; - &#123;&#123; .Name &#125;&#125;: &#123;&#123; .Value | markdown | html &#125;&#125;    &#123;&#123; end &#125;&#125;&#123;&#123; end &#125;&#125;    &#123;&#123; end &#125;&#125;    &#123;&#123; end &#125;&#125;    &#123;&#123;/* Default */&#125;&#125;    &#123;&#123; define "default.title" &#125;&#125;&#123;&#123; template "__subject" . &#125;&#125;&#123;&#123; end &#125;&#125;    &#123;&#123; define "default.content" &#125;&#125;#### \[&#123;&#123; .Status | toUpper &#125;&#125;&#123;&#123; if eq .Status "firing" &#125;&#125;:&#123;&#123; .Alerts.Firing | len &#125;&#125;&#123;&#123; end &#125;&#125;\] **[&#123;&#123; index .GroupLabels "alertname" &#125;&#125;](&#123;&#123; template "__alertmanagerURL" . &#125;&#125;)**    &#123;&#123; if gt (len .Alerts.Firing) 0 -&#125;&#125;    &#123;&#123; template "default.__text_alert_list" .Alerts.Firing &#125;&#125;    &#123;&#123;- end &#125;&#125;    &#123;&#123; if gt (len .Alerts.Resolved) 0 -&#125;&#125;    &#123;&#123; template "default.__text_alertresovle_list" .Alerts.Resolved &#125;&#125;    &#123;&#123;- end &#125;&#125;    &#123;&#123;- end &#125;&#125;    &#123;&#123;/* Legacy */&#125;&#125;    &#123;&#123; define "legacy.title" &#125;&#125;&#123;&#123; template "__subject" . &#125;&#125;&#123;&#123; end &#125;&#125;    &#123;&#123; define "legacy.content" &#125;&#125;#### \[&#123;&#123; .Status | toUpper &#125;&#125;&#123;&#123; if eq .Status "firing" &#125;&#125;:&#123;&#123; .Alerts.Firing | len &#125;&#125;&#123;&#123; end &#125;&#125;\] **[&#123;&#123; index .GroupLabels "alertname" &#125;&#125;](&#123;&#123; template "__alertmanagerURL" . &#125;&#125;)**    &#123;&#123; template "__text_alert_list" .Alerts.Firing &#125;&#125;    &#123;&#123;- end &#125;&#125;    &#123;&#123;/* Following names for compatibility */&#125;&#125;    &#123;&#123; define "ding.link.title" &#125;&#125;&#123;&#123; template "default.title" . &#125;&#125;&#123;&#123; end &#125;&#125;    &#123;&#123; define "ding.link.content" &#125;&#125;&#123;&#123; template "default.content" . &#125;&#125;&#123;&#123; end &#125;&#125;</code></pre><h4 id="9-2-2-2-部署钉钉告警插件"><a href="#9-2-2-2-部署钉钉告警插件" class="headerlink" title="9.2.2.2 部署钉钉告警插件"></a>9.2.2.2 部署钉钉告警插件</h4><pre><code class="hljs">kubectl apply -f dingtalk-config.yamlkubectl apply -f dingtalk-deployment.yaml</code></pre><h3 id="9-2-3-修改alertmanager配置文件"><a href="#9-2-3-修改alertmanager配置文件" class="headerlink" title="9.2.3 修改alertmanager配置文件"></a>9.2.3 修改alertmanager配置文件</h3><pre><code class="hljs">vi alertmanager.yamlglobal:  # 设置告警恢复时间间隔，即告警不再继续产生的时长  resolve_timeout: 5m  smtp_smarthost: &#39;smtp.qq.com:465&#39;  smtp_from: &#39;784956466@qq.com&#39;  smtp_auth_username: &#39;784956466@qq.com&#39;  # 设置邮箱授权码  smtp_auth_password: &#39;xxxxxxxxxxxxxxxx&#39;  smtp_hello: &#39;test&#39;  smtp_require_tls: false# 设置告警通知模版templates:  - &#39;./*.tmpl&#39;route:  # 设置告警分组的属性依据  group_by: [&#39;alertname&#39;,&#39;severity&#39;,&#39;namespace&#39;,&#39;node&#39;,&#39;job&#39;,&#39;service&#39;]  # 设置告警发送前的等待时间  group_wait: 30s  # 设置告警发送时间间隔  group_interval: 5m  # 设置分组内相同告警的发送时间间隔  repeat_interval: 3h  # 设置告警接收者，匹配receivers配置项  receiver: email  routes:  - receiver: email    continue: true  - receiver: dingtalk    group_wait: 10s    match:      severity: criticalreceivers:- name: email  email_configs:  - to: &#39;523343553@qq.com&#39;    html: &#39;&#123;&#123; template "email.html" . &#125;&#125;&#39;    send_resolved: true- name: dingtalk  webhook_configs:  - url: &#39;http://dingtalk.monitoring.svc.cluster.local:8060/dingtalk/webhook/send&#39;    send_resolved: true</code></pre><h3 id="9-2-4-更新AlertManager配置文件"><a href="#9-2-4-更新AlertManager配置文件" class="headerlink" title="9.2.4 更新AlertManager配置文件"></a>9.2.4 更新AlertManager配置文件</h3><pre><code class="hljs">kubectl -n monitoring delete secrets alertmanager-mainkubectl -n monitoring create secret generic alertmanager-main --from-file=alertmanager.yaml --from-file=./template/email.tmpl</code></pre><h3 id="9-2-5-验证钉钉告警"><a href="#9-2-5-验证钉钉告警" class="headerlink" title="9.2.5 验证钉钉告警"></a>9.2.5 验证钉钉告警</h3><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://zhuanlan.zhihu.com/p/469202200">https://zhuanlan.zhihu.com/p/469202200</a></li><li><a href="https://www.jianshu.com/p/a55e95b09ff5">https://www.jianshu.com/p/a55e95b09ff5</a></li><li><a href="https://www.cnblogs.com/pollos/articles/17369294.html">https://www.cnblogs.com/pollos/articles/17369294.html</a></li><li><a href="https://blog.csdn.net/qq_43164571/article/details/124802412">https://blog.csdn.net/qq_43164571/article/details/124802412</a></li><li><a href="https://blog.csdn.net/weixin_45444133/article/details/120434811">https://blog.csdn.net/weixin_45444133/article/details/120434811</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
      <tag>Prometheus</tag>
      
      <tag>监控告警</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes集群网络模型详解</title>
    <link href="/linux/KubernetesCNI/"/>
    <url>/linux/KubernetesCNI/</url>
    
    <content type="html"><![CDATA[<p>Kubernetes网络实现是集群的核心部分，宗旨就是在容器化的应用程序之间共享网络资源。集群中同一个节点上部署多个业务应用，而每个应用都需要自己的网络空间。为避免与其他业务网络冲突，需要Pod有自己独立的网络空间，而Pod中应用需要和其他应用进行通信，就需要Pod能够跟不同的应用互相访问。故此，网络系统的设计要解决如下几个问题：</p><ul><li>同一节点上Pod之间的通信</li><li>不同节点上Pod之间的通信</li><li>外部网络和集群内Pod之间的通信</li></ul><hr><p>容器常见的网络标准有两种，即Docker公司提出的CNM（Container Network Model）及CoreOS公司提出的CNI（Container Network Interface）。CNM和CNI并不是网络实现，而只是网络规范和网络体系，也即是一种网络设计模型，从研发的角度来看其实就是一组接口，底层还是由具体的网络插件来实现。CNI由于兼容其他容器技术（如rkt）及上层编排系统，受到了大力推广，目前已经成为Kubernetes所采用的主流网络标准</p><p>CNI网络插件目前主要有两种，即Flannel与Calico</p><hr><h1 id="1-flannel"><a href="#1-flannel" class="headerlink" title="1.flannel"></a>1.flannel</h1><h2 id="1-1-工作原理"><a href="#1-1-工作原理" class="headerlink" title="1.1 工作原理"></a>1.1 工作原理</h2><p>flannel实质上是一种覆盖网络(overlaynetwork)，工作原理就是为集群中的所有node分配一个subnet（子网），容器从该subnet中分配到一个同属一个内网全集群唯一的虚拟IP，这些IP可以在主机间路由，容器间无需NAT和端口映射就可以跨主机通信。在此基础上将TCP数据包封装在另一种网络包里进行路由转发和通信。目前flannel已支持udp、vxlan、host-gw、aws-vpc、gce和alloc路由等数据转发方式，默认的节点间数据通信方式是UDP</p><h2 id="1-2-网络架构"><a href="#1-2-网络架构" class="headerlink" title="1.2 网络架构"></a>1.2 网络架构</h2><h2 id="1-2-1-flanneld"><a href="#1-2-1-flanneld" class="headerlink" title="1.2.1 flanneld"></a>1.2.1 flanneld</h2><p>flanneld进程连接etcd，负责维护集群可分配IP地址的资源池，同时监控etcd中每个Pod的实际地址，并在内存中建立一个Pod路由表</p><h3 id="1-2-2-flannel0网桥"><a href="#1-2-2-flannel0网桥" class="headerlink" title="1.2.2 flannel0网桥"></a>1.2.2 flannel0网桥</h3><p>flanneld进程创建的虚拟网卡，连接docker0和节点物理网卡，根据内存中维护的Pod路由表将docker0发来的数据包封装，利用物理网卡投递到目标flanneld，目标flanneld将数据包解包后发送给本机docker0网卡，从而完成与另一个pod的通信</p><h3 id="1-2-3-etcd"><a href="#1-2-3-etcd" class="headerlink" title="1.2.3 etcd"></a>1.2.3 etcd</h3><p>数据库存储，负责维护节点上划分的子网配置信息，如子网网段、外部IP等</p><h2 id="1-3-工作模式"><a href="#1-3-工作模式" class="headerlink" title="1.3 工作模式"></a>1.3 工作模式</h2><p>Flannel有三种网络实现模式，即UDP、VXLAN、host-gw</p><h3 id="1-3-1-udp模式"><a href="#1-3-1-udp模式" class="headerlink" title="1.3.1 udp模式"></a>1.3.1 udp模式</h3><p>使用设备flannel.0进行封包解包，不是内核原生支持，上下文切换较大，性能差，目前已被弃用</p><h3 id="1-3-2-vxlan模式"><a href="#1-3-2-vxlan模式" class="headerlink" title="1.3.2 vxlan模式"></a>1.3.2 vxlan模式</h3><p>使用flannel.1进行封包解包，内核原生支持，性能较强</p><h3 id="1-3-3-host-gw模式"><a href="#1-3-3-host-gw模式" class="headerlink" title="1.3.3 host-gw模式"></a>1.3.3 host-gw模式</h3><p>无需flannel.1这样的中间设备，直接宿主机当作子网的下一跳地址，性能最强，性能损失大约在10%左右，优于基于VXLAN“隧道”机制的性能损失在20%~30%左右的网络方案</p><h1 id="2-calico"><a href="#2-calico" class="headerlink" title="2.calico"></a>2.calico</h1><h2 id="2-1-工作原理"><a href="#2-1-工作原理" class="headerlink" title="2.1 工作原理"></a>2.1 工作原理</h2><p>calico是一个纯三层的虚拟网络，工作原理是用linux内核将宿主机节点虚拟为一个vRouter（虚拟路由器），这些vRouter组成了一个全连通的网络，集群中所有的容器都是连接这个网络的路由，每个路由节点通过BGP协议将自身的路由信息向整个Calico网络传播，从而将容器的地址信息维护成一张路由表，据此实现了跨主机容器的通信</p><h2 id="2-2-网络架构"><a href="#2-2-网络架构" class="headerlink" title="2.2 网络架构"></a>2.2 网络架构</h2><h3 id="2-2-1-Felix"><a href="#2-2-1-Felix" class="headerlink" title="2.2.1 Felix"></a>2.2.1 Felix</h3><p>Calico Agent，运行在node上，负责为容器设置网络资源，如IP地址、路由规则配置、ACLS规则的配置及下发（ipvs、iptables）等，保证跨主机容器网络互通</p><h3 id="2-2-2-BGP-Client"><a href="#2-2-2-BGP-Client" class="headerlink" title="2.2.2 BGP Client"></a>2.2.2 BGP Client</h3><p>负责把Felix在各Node上设置的路由信息通过BGP协议广播到Calico网络，确保网络的有效性</p><h3 id="2-2-3-Route-Reflector"><a href="#2-2-3-Route-Reflector" class="headerlink" title="2.2.3 Route Reflector"></a>2.2.3 Route Reflector</h3><p>大规模部署时使用，摒弃所有节点互联的mesh模，通过一个或多个BGP Reflector来完成集中式的分级路由分发</p><h3 id="2-2-4-etcd"><a href="#2-2-4-etcd" class="headerlink" title="2.2.4 etcd"></a>2.2.4 etcd</h3><p>数据库存储，负责网络元数据一致性，确保Calico网络状态的一致性</p><h3 id="2-2-5-CalicoCtl"><a href="#2-2-5-CalicoCtl" class="headerlink" title="2.2.5 CalicoCtl"></a>2.2.5 CalicoCtl</h3><p>Calico的命令行管理工具</p><h2 id="2-3-工作模式"><a href="#2-3-工作模式" class="headerlink" title="2.3 工作模式"></a>2.3 工作模式</h2><p>Calico有两种网络实现模式，即IPIP模式和BGP模式</p><h3 id="2-3-1-IPIP模式"><a href="#2-3-1-IPIP模式" class="headerlink" title="2.3.1 IPIP模式"></a>2.3.1 IPIP模式</h3><p>Calico官方默认的工作模式，启动后会在node上创建一个设备tunl0，容器的网络数据会经过该设备被封装一个ip头再转发</p><p>该模式是将IP数据包嵌套在另一个IP包里，即把IP层封装到IP层的一个tunnel（隧道），作用相当于一个基于IP层的网桥。一般来说，普通的网桥是基于mac层的，并不需要IP，而该模式下的网桥则是通过两端的路由做成tunnel，从而将两个本来不通的网络通过点对点连接起来，网络走向：</p><pre><code class="hljs">pod1 cali0 ---&gt; cali*** ---&gt; tunl0 ---&gt; node1 eth0 ---&gt; node2 eth0 ---&gt; tunl0 ---&gt; cali*** ---&gt; pod2 cali0</code></pre><h3 id="2-3-2-BGP模式"><a href="#2-3-2-BGP模式" class="headerlink" title="2.3.2 BGP模式"></a>2.3.2 BGP模式</h3><p>BGP，Border Gateway Protocol，即边界网关协议，是互联网上一个核心的去中心化自治路由协议，通过维护IP路由表或前缀表来实现自治系统（AS）之间的可达性，属于矢量路由协议。BGP不使用传统的内部网关协议（IGP）的指标，而使用基于路径、网络策略或规则集来决定路由，因此更适合称之为矢量性协议而不是路由协议。实质上就是直接使用物理机作为虚拟路由器（vRouter），不再创建额外的隧道</p><p>该模式启动后，将为容器生成veth pair，一端作为容器网卡加入到容器的网络命名空间，并设置IP和掩码，另一端直接暴露在宿主机上，并通过设置路由规则将容器IP暴露到宿主机的通信路由上。同时，calico为每个主机分配了一段子网作为容器可分配的IP范围，这样就可以根据子网的CIDR为每个主机生成比较固定的路由规则</p><p>当容器需要跨主机通信时，主要经过以下步骤</p><p>1.容器流量通过veth pair到达宿主机的网络命名空间上<br>2.根据容器要访问的IP所在的子网CIDR和主机上的路由规则，找到下一跳要到达的宿主机IP<br>3.流量到达下一跳的宿主机后，根据当前宿主机上的路由规则，直接到达对端容器的veth pair插在宿主机的一端，最终进入容器</p><p>此模式网络走向：</p><pre><code class="hljs">pod1 cali0 ---&gt; cali*** ---&gt; node1 eth0 ---&gt;node2 eth0 ---&gt; cali*** ---&gt; pod2 cali0</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.kubernetes.org.cn/2059.html">https://www.kubernetes.org.cn/2059.html</a></li><li><a href="https://www.cnblogs.com/v-fan/p/14452770.html">https://www.cnblogs.com/v-fan/p/14452770.html</a></li><li><a href="https://cloud.tencent.com/developer/article/1804680">https://cloud.tencent.com/developer/article/1804680</a></li><li><a href="https://blog.csdn.net/Thorne_lu/article/details/121391669">https://blog.csdn.net/Thorne_lu/article/details/121391669</a></li><li><a href="https://www.jianshu.com/p/ecf874d74d61">https://www.jianshu.com/p/ecf874d74d61</a></li><li><a href="https://blog.51cto.com/u_10272167/2698291">https://blog.51cto.com/u_10272167/2698291</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>网络</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes基于kubeadm工具构建高可用集群</title>
    <link href="/linux/KubernetesKubeadm/"/>
    <url>/linux/KubernetesKubeadm/</url>
    
    <content type="html"><![CDATA[<p>kubeadm，用于快速构建kubernetes集群的工具，通过kubeadm init及kubeadm join两个命令迅速启动和运行一个可用的集群。kubeadm大大简化了二进制部署方式令人望而生畏的繁琐步骤，降低了部署门槛，对初学者十分友好。当然，二进制部署方式能详细了解集群的架构及工作原理，也利于后期的维护与排障，适用于对kubernetes有深入了解的情况</p><hr><h1 id="高可用方案"><a href="#高可用方案" class="headerlink" title="高可用方案"></a>高可用方案</h1><p>Kubernetes集群的高可用是生产环境的业务系统必不可少的考量，其核心指标是集群内任意一台服务器的宕机都不能影响整个集群的正常工作，具体策略为给Kubernetes集群Master节点的关键组件都提供高可用的功能，从而消除整个集群的单点故障</p><ul><li><p>Etcd数据库，Kubernetes集群唯一带状态的服务，是整个集群的数据中心，分布式架构自带高可用功能，不存在单点故障，推荐的部署方案是3或5个奇数个节点组成冗余的etcd集群</p></li><li><p>kube-apiserver，API Server是整个集群的访问入口，整个集群高可用的关键，通过启动多个实例并配合负载均衡器实现高可用，如Nginx&#x2F;Haproxy、Keepalived等</p></li><li><p>Kube-controller-manager，集群内部的管理控制中心，整个集群只有一个活跃的实例，可同时启动多个实例，然后开启领导者选举功能实现高可用，即–leader-elect&#x3D;true</p></li><li><p>kube-scheduler，集群节点调度中心，将POD调度到合适节点运行，一个集群只能有一个活跃的实例，可同时启动多个实例，然后开启领导者选举功能实现高可用，默认开启</p></li><li><p>kube-dns，集群Service监控及发现中心，用于解析Service与ClusterIP的映射，通过Deployment方式部署，并用anti-affinity将之部署到不同的Node节点</p></li></ul><h1 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h1><ul><li>172.16.100.100  master01</li><li>172.16.100.120  master02</li><li>172.16.100.160  master03</li><li>172.16.100.180  worker01</li><li>172.16.100.200  worker02</li><li>172.16.100.150  VIP</li></ul><h1 id="1-系统环境配置"><a href="#1-系统环境配置" class="headerlink" title="1.系统环境配置"></a>1.系统环境配置</h1><h2 id="1-1-配置hosts"><a href="#1-1-配置hosts" class="headerlink" title="1.1 配置hosts"></a>1.1 配置hosts</h2><pre><code class="hljs">sudo vi /etc/hosts172.16.100.100  master01172.16.100.120  master02172.16.100.160  master03172.16.100.180  worker01172.16.100.200  worker02</code></pre><h2 id="1-2-关闭防火墙"><a href="#1-2-关闭防火墙" class="headerlink" title="1.2 关闭防火墙"></a>1.2 关闭防火墙</h2><h2 id="1-3-禁用selinux"><a href="#1-3-禁用selinux" class="headerlink" title="1.3 禁用selinux"></a>1.3 禁用selinux</h2><h2 id="1-4-关闭swap"><a href="#1-4-关闭swap" class="headerlink" title="1.4 关闭swap"></a>1.4 关闭swap</h2><pre><code class="hljs">sudo swapoff -a &amp;&amp; sudo sed -ri &#39;s/.*swap.*/#&amp;/&#39; /etc/fstab</code></pre><h2 id="1-5-配置系统内核参数"><a href="#1-5-配置系统内核参数" class="headerlink" title="1.5 配置系统内核参数"></a>1.5 配置系统内核参数</h2><h3 id="1-5-1-开启路由转发"><a href="#1-5-1-开启路由转发" class="headerlink" title="1.5.1 开启路由转发"></a>1.5.1 开启路由转发</h3><pre><code class="hljs">sudo vi /etc/sysctl.d/k8s.confnet.ipv4.ip_forward=1net.bridge.bridge-nf-call-iptables=1net.bridge.bridge-nf-call-ip6tables=1</code></pre><h3 id="1-5-2-加载内核参数配置"><a href="#1-5-2-加载内核参数配置" class="headerlink" title="1.5.2 加载内核参数配置"></a>1.5.2 加载内核参数配置</h3><pre><code class="hljs">sudo sysctl -p</code></pre><h2 id="1-6-配置集群免密登录"><a href="#1-6-配置集群免密登录" class="headerlink" title="1.6 配置集群免密登录"></a>1.6 配置集群免密登录</h2><h1 id="2-集群部署docker"><a href="#2-集群部署docker" class="headerlink" title="2.集群部署docker"></a>2.集群部署docker</h1><h2 id="2-1-安装docker"><a href="#2-1-安装docker" class="headerlink" title="2.1 安装docker"></a>2.1 安装docker</h2><pre><code class="hljs"># 配置阿里云YUM源wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo \-O /etc/yum.repos.d/docker-ce.repo# 安装dockersudo yum install -y docker-ce</code></pre><h3 id="2-2-配置镜像加速及cgroup驱动"><a href="#2-2-配置镜像加速及cgroup驱动" class="headerlink" title="2.2 配置镜像加速及cgroup驱动"></a>2.2 配置镜像加速及cgroup驱动</h3><pre><code class="hljs">sudo mkdir -p /etc/docker &amp;&amp; sudo vi /etc/docker/daemon.json&#123;  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],  &quot;registry-mirrors&quot;: [&quot;https://hub-mirror.c.163.com&quot;]&#125;</code></pre><h2 id="2-3-启动docker"><a href="#2-3-启动docker" class="headerlink" title="2.3 启动docker"></a>2.3 启动docker</h2><pre><code class="hljs">sudo systemctl start dockersudo systemctl enable docker</code></pre><h1 id="3-Master节点部署Haproxy、Keepalived"><a href="#3-Master节点部署Haproxy、Keepalived" class="headerlink" title="3.Master节点部署Haproxy、Keepalived"></a>3.Master节点部署Haproxy、Keepalived</h1><pre><code class="hljs">sudo yum install -y haproxy keepalived</code></pre><h2 id="3-1-创建Haproxy配置文件"><a href="#3-1-创建Haproxy配置文件" class="headerlink" title="3.1 创建Haproxy配置文件"></a>3.1 创建Haproxy配置文件</h2><pre><code class="hljs">sudo vi /etc/haproxy/haproxy.cfgglobal  log    127.0.0.1 local2  # chroot    /usr/local/haproxy  pidfile    /var/run/haproxy.pid  user    sword  group    sword  daemon  nbproc    1  maxconn   1024  # node    haproxy-001  stats socket    /var/lib/haproxy/statsdefaults  log    global  mode    http  option    httplog  option    httpclose  option    forwardfor except 127.0.0.0/8                option    dontlognull  option    redispatch  option    abortonclose  http-reuse    safe  retries    3  timeout client    10s  timeout http-request    2s  timeout http-keep-alive 10s  timeout queue    10s  timeout connect    1s  timeout check    2s  timeout server    3slisten monitor  mode    http  bind    :1443  stats    enable  stats    hide-version  stats    refresh 10       　　　　  stats uri    /status  stats realm    Haproxy Manager  stats auth    admin:admin  stats admin if    TRUE frontend    master  mode    tcp  option   tcplog  bind    0.0.0.0:8443  default_backend    api-servers   backend api-servers    mode    tcp    balance    roundrobin    stick-table type ip size 200k expire 30m    stick on src    server master01 172.16.100.100:6443 check port 6443 inter 2000 rise 2 fall 3    server master02 172.16.100.120:6443 check port 6443 inter 2000 rise 2 fall 3    server master03 172.16.100.160:6443 check port 6443 inter 2000 rise 2 fall 3</code></pre><h2 id="3-2-创建Keepalived配置文件"><a href="#3-2-创建Keepalived配置文件" class="headerlink" title="3.2 创建Keepalived配置文件"></a>3.2 创建Keepalived配置文件</h2><h3 id="3-2-1-master01节点配置文件"><a href="#3-2-1-master01节点配置文件" class="headerlink" title="3.2.1 master01节点配置文件"></a>3.2.1 master01节点配置文件</h3><pre><code class="hljs">sudo vi /etc/keepalived/keepalived.confglobal_defs &#123;  notification_email  &#123;    admin@sword.com  &#125;  notification_email_from  smtp_server 127.0.0.1  smtp_connect_timeout 30  router_id master01&#125;vrrp_script check_haproxy &#123;  script &quot;/etc/keepalived/haproxy_check.sh&quot;  interval 2  weight -10&#125;vrrp_instance Haproxy &#123;  state BACKUP  interface eth0  virtual_router_id 51  priority 100  advert_int 1  authentication &#123;    auth_type PASS    auth_pass 123456  &#125;  virtual_ipaddress &#123;    172.16.100.150/24  &#125;  track_script &#123;    check_haproxy  &#125;&#125;</code></pre><h3 id="3-2-2-master02节点配置文件"><a href="#3-2-2-master02节点配置文件" class="headerlink" title="3.2.2 master02节点配置文件"></a>3.2.2 master02节点配置文件</h3><pre><code class="hljs">sudo vi /etc/keepalived/keepalived.confglobal_defs &#123;  notification_email  &#123;    admin@sword.com  &#125;  notification_email_from  smtp_server 127.0.0.1  smtp_connect_timeout 30  router_id master02&#125;vrrp_script check_haproxy &#123;  script &quot;/etc/keepalived/haproxy_check.sh&quot;  interval 2  weight -10&#125;vrrp_instance Haproxy &#123;  state BACKUP  interface eth0  virtual_router_id 51  priority 80  advert_int 1  authentication &#123;    auth_type PASS    auth_pass 123456  &#125;  virtual_ipaddress &#123;    172.16.100.150/24  &#125;  track_script &#123;    check_haproxy  &#125;&#125;</code></pre><h3 id="3-2-3-master03节点配置文件"><a href="#3-2-3-master03节点配置文件" class="headerlink" title="3.2.3 master03节点配置文件"></a>3.2.3 master03节点配置文件</h3><pre><code class="hljs">sudo vi /etc/keepalived/keepalived.confglobal_defs &#123;  notification_email  &#123;    admin@sword.com  &#125;  notification_email_from  smtp_server 127.0.0.1  smtp_connect_timeout 30  router_id master03&#125;vrrp_script check_haproxy &#123;  script &quot;/etc/keepalived/haproxy_check.sh&quot;  interval 2  weight -10&#125;vrrp_instance Haproxy &#123;  state BACKUP  interface eth0  virtual_router_id 51  priority 60  advert_int 1  authentication &#123;    auth_type PASS    auth_pass 123456  &#125;  virtual_ipaddress &#123;    172.16.100.150/24  &#125;  track_script &#123;    check_haproxy  &#125;&#125;</code></pre><h2 id="3-3-创建Haproxy监控脚本"><a href="#3-3-创建Haproxy监控脚本" class="headerlink" title="3.3 创建Haproxy监控脚本"></a>3.3 创建Haproxy监控脚本</h2><pre><code class="hljs">sudo vi /etc/keepalived/haproxy_check.sh#!/bin/bashpid=`ps -ef|grep haproxy|grep -v grep|wc -l`port=`netstat -anp|grep :8443|grep LISTEN|wc -l`if [[ $pid -gt 1 &amp;&amp; $port -gt 0 ]]  then    exit 0  else    pkill keepalivedfi</code></pre><h2 id="3-4-启动Haproxy、Keepalived"><a href="#3-4-启动Haproxy、Keepalived" class="headerlink" title="3.4 启动Haproxy、Keepalived"></a>3.4 启动Haproxy、Keepalived</h2><pre><code class="hljs">sudo systemctl start haproxysudo systemctl enable haproxysudo systemctl start keepalivedsudo systemctl enable keepalived</code></pre><h1 id="4-Master节点部署kubeadm、kubelet、kubectl"><a href="#4-Master节点部署kubeadm、kubelet、kubectl" class="headerlink" title="4.Master节点部署kubeadm、kubelet、kubectl"></a>4.Master节点部署kubeadm、kubelet、kubectl</h1><h2 id="4-1-配置阿里云YUM源"><a href="#4-1-配置阿里云YUM源" class="headerlink" title="4.1 配置阿里云YUM源"></a>4.1 配置阿里云YUM源</h2><pre><code class="hljs">sudo vi /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</code></pre><h2 id="4-2-安装kubeadm、kubelet、kubectl"><a href="#4-2-安装kubeadm、kubelet、kubectl" class="headerlink" title="4.2 安装kubeadm、kubelet、kubectl"></a>4.2 安装kubeadm、kubelet、kubectl</h2><pre><code class="hljs">sudo yum install -y ipvsadm kubelet-1.20.12 kubeadm-1.20.12 kubectl-1.20.12sudo systemctl enable kubelet</code></pre><h1 id="5-Master节点部署Kubernetes"><a href="#5-Master节点部署Kubernetes" class="headerlink" title="5. Master节点部署Kubernetes"></a>5. Master节点部署Kubernetes</h1><h2 id="5-1-master01节点初始化集群"><a href="#5-1-master01节点初始化集群" class="headerlink" title="5.1 master01节点初始化集群"></a>5.1 master01节点初始化集群</h2><pre><code class="hljs">kubeadm init \--kubernetes-version &quot;1.20.12&quot; \--control-plane-endpoint &quot;172.16.100.100:8443&quot; \--apiserver-cert-extra-sans &quot;172.16.100.100,192.168.100.150,master&quot; \--pod-network-cidr &quot;172.30.0.0/16&quot; \--service-cidr &quot;10.254.0.0/16&quot; \--token &quot;dcwrhm.6wi8mn63s10gxrcf&quot; \--token-ttl &quot;0&quot; \--image-repository registry.aliyuncs.com/google_containers \--upload-certs</code></pre><ul><li>kubernetes-version，设置kubernetes版本</li><li>control-plane-endpoint，设置控制节点IP地址</li><li>apiserver-cert-extra-sans，设置API Server服务的附加认证证书，值为IP和DNS，可用于指定外网地址</li><li>pod-network-cidr，设置Pod网段，与CNI网络组件定义的网段一致</li><li>service-cidr，设置service网段</li><li>image-repository，设置默认镜像拉取仓库</li><li>token-tt，设置引导令牌有效期时长默认为24小时，为0表示永不过期，令牌保存于kube-system命名空间名为bootstrap-token-<token-id>的secret</li></ul><hr><ul><li>注：集群初始化成功将自动生成token及添加其余Master、Node节点的命令，按照给出的命令在其余节点执行即可将之加入集群</li></ul><h2 id="5-2-配置kubectl集群认证令牌"><a href="#5-2-配置kubectl集群认证令牌" class="headerlink" title="5.2 配置kubectl集群认证令牌"></a>5.2 配置kubectl集群认证令牌</h2><pre><code class="hljs">mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/configecho &quot;source &lt;(kubectl completion bash)&quot; &gt;&gt; /root/.bash_profilesource /root/.bash_profile</code></pre><h2 id="5-3-其余Master节点初始化集群"><a href="#5-3-其余Master节点初始化集群" class="headerlink" title="5.3 其余Master节点初始化集群"></a>5.3 其余Master节点初始化集群</h2><h3 id="5-3-1-Master节点初始化集群"><a href="#5-3-1-Master节点初始化集群" class="headerlink" title="5.3.1 Master节点初始化集群"></a>5.3.1 Master节点初始化集群</h3><pre><code class="hljs">kubeadm join 172.16.100.100:8443 --token dcwrhm.6wi8mn63s10gxrcf \--discovery-token-ca-cert-hash sha256:231ea9ea085b6eccaac00cfb27b85748d2cc403cd2194434b163896d375126ea \--control-plane --certificate-key e9f4c8e66fd5b9629be14a92fac1230972f46427636be616cbae8573abc6bd5c</code></pre><h3 id="5-3-2-配置kubectl集群认证令牌"><a href="#5-3-2-配置kubectl集群认证令牌" class="headerlink" title="5.3.2  配置kubectl集群认证令牌"></a>5.3.2  配置kubectl集群认证令牌</h3><pre><code class="hljs">mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config</code></pre><h1 id="6-部署Node"><a href="#6-部署Node" class="headerlink" title="6.部署Node"></a>6.部署Node</h1><pre><code class="hljs">kubeadm join 172.16.100.100:8443 --token dcwrhm.6wi8mn63s10gxrcf \--discovery-token-ca-cert-hash sha256:231ea9ea085b6eccaac00cfb27b85748d2cc403cd2194434b163896d375126ea </code></pre><ul><li>注：token过期之后可重新创建，命令为：kubeadm token create –print-join-command</li></ul><h1 id="7-部署集群网络插件calico"><a href="#7-部署集群网络插件calico" class="headerlink" title="7.部署集群网络插件calico"></a>7.部署集群网络插件calico</h1><h2 id="7-1-下载calico资源文件"><a href="#7-1-下载calico资源文件" class="headerlink" title="7.1 下载calico资源文件"></a>7.1 下载calico资源文件</h2><pre><code class="hljs">curl https://docs.projectcalico.org/v3.20/manifests/calico.yaml -O</code></pre><h2 id="7-2-配置pod网络IP段"><a href="#7-2-配置pod网络IP段" class="headerlink" title="7.2 配置pod网络IP段"></a>7.2 配置pod网络IP段</h2><pre><code class="hljs">vi calico.yaml- name: CALICO_IPV4POOL_CIDR  value: &quot;172.30.0.0/16&quot;</code></pre><ul><li>注：CALICO_IPV4POOL_CIDR的值与初始化Master时–pod-network-cidr指定的值保持一致</li></ul><h2 id="7-3-部署集群网络插件calico"><a href="#7-3-部署集群网络插件calico" class="headerlink" title="7.3 部署集群网络插件calico"></a>7.3 部署集群网络插件calico</h2><pre><code class="hljs">kubectl apply -f calico.yaml</code></pre><h1 id="8-查看集群状态"><a href="#8-查看集群状态" class="headerlink" title="8.查看集群状态"></a>8.查看集群状态</h1><h2 id="8-1-查看集群组件状态"><a href="#8-1-查看集群组件状态" class="headerlink" title="8.1 查看集群组件状态"></a>8.1 查看集群组件状态</h2><pre><code class="hljs">kubectl get cs</code></pre><h2 id="8-2-查看集群网络组件"><a href="#8-2-查看集群网络组件" class="headerlink" title="8.2 查看集群网络组件"></a>8.2 查看集群网络组件</h2><pre><code class="hljs">kubectl -n kube-system get pods -o wide</code></pre><h2 id="8-3-查看集群节点状态"><a href="#8-3-查看集群节点状态" class="headerlink" title="8.3 查看集群节点状态"></a>8.3 查看集群节点状态</h2><pre><code class="hljs">kubectl get nodes -o wide</code></pre><h2 id="8-4-配置kube-proxy代理模式为ipvs"><a href="#8-4-配置kube-proxy代理模式为ipvs" class="headerlink" title="8.4 配置kube-proxy代理模式为ipvs"></a>8.4 配置kube-proxy代理模式为ipvs</h2><h3 id="8-4-1-修改configmap"><a href="#8-4-1-修改configmap" class="headerlink" title="8.4.1 修改configmap"></a>8.4.1 修改configmap</h3><pre><code class="hljs">kubectl edit cm kube-proxy -n kube-systemmode: &quot;ipvs&quot;</code></pre><h3 id="8-4-2-重启kube-proxy"><a href="#8-4-2-重启kube-proxy" class="headerlink" title="8.4.2 重启kube-proxy"></a>8.4.2 重启kube-proxy</h3><pre><code class="hljs">kubectl -n kube-system get pod | grep kube-proxy | awk &#39;&#123;print $1&#125;&#39; | xargs kubectl -n kube-system delete pod</code></pre><h1 id="9-部署可视化管理UI"><a href="#9-部署可视化管理UI" class="headerlink" title="9.部署可视化管理UI"></a>9.部署可视化管理UI</h1><h2 id="9-1-部署kuboard"><a href="#9-1-部署kuboard" class="headerlink" title="9.1 部署kuboard"></a>9.1 部署kuboard</h2><pre><code class="hljs">kubectl apply -f https://kuboard.cn/install-script/kuboard.yaml</code></pre><h2 id="9-2-获取登陆token"><a href="#9-2-获取登陆token" class="headerlink" title="9.2 获取登陆token"></a>9.2 获取登陆token</h2><pre><code class="hljs">echo $(kubectl -n kube-system get secret $(kubectl -n kube-system get secret | grep kuboard-user | awk &#39;&#123;print $1&#125;&#39;) -o go-template=&#39;&#123;&#123;.data.token&#125;&#125;&#39; | base64 -d)</code></pre><h2 id="9-3-访问Kuboard"><a href="#9-3-访问Kuboard" class="headerlink" title="9.3 访问Kuboard"></a>9.3 访问Kuboard</h2><p><a href="http://worker节点ip:32567/">http://worker节点IP:32567</a></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.jianshu.com/p/64bb556a2006">https://www.jianshu.com/p/64bb556a2006</a></li><li><a href="https://www.cnblogs.com/yx-book/p/14855450.html">https://www.cnblogs.com/yx-book/p/14855450.html</a></li><li><a href="https://blog.csdn.net/weixin_44379843/article/details/125400358">https://blog.csdn.net/weixin_44379843/article/details/125400358</a></li><li><a href="https://blog.csdn.net/mengshicheng1992/article/details/115659507">https://blog.csdn.net/mengshicheng1992/article/details/115659507</a></li><li><a href="https://blog.csdn.net/u014320421/article/details/118325544">https://blog.csdn.net/u014320421/article/details/118325544</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>kubernetes存储持久化详解</title>
    <link href="/linux/KubernetesStorage/"/>
    <url>/linux/KubernetesStorage/</url>
    
    <content type="html"><![CDATA[<p>存储持久化，是容器化技术中绕不开的话题。因为容器中的文件在磁盘上是临时存储的，也即是说容器中的文件随着容器的重启或销毁也会随之丢失，这就给应用程序的容器化带来了一项巨大的挑战。Kubernetes集群提供了多种不同的方案来解决这些问题，管理员可以根据需求来进行选择</p><hr><h1 id="1-Volume"><a href="#1-Volume" class="headerlink" title="1.Volume"></a>1.Volume</h1><p>volume，即卷，核心是一个目录，其中可能存有数据，Pod中的容器可以访问该目录中的数据，所采用的特定的卷类型将决定该目录如何形成的、使用何种介质保存数据以及目录中存放的内容</p><p>pod资源定义文件声明及引用卷时，.spec.volumes表示Pod所使用的卷，.spec.containers[*].volumeMounts字段表示卷在容器中的挂载位置。容器中的进程看到的是由它们的Docker镜像和卷组成的文件系统视图，镜像位于文件系统层次结构的根部，各个卷则挂载在镜像内的指定路径上。卷不能挂载到其他卷之上，也不能与其他卷有硬链接，Pod配置中的每个容器必须独立指定各个卷的挂载位置</p><p>卷的缺点很明显，就是无论使用哪种类型的存储都需要手动定义，指明存储类型以及相关配置，也即是说卷的使用者还需要理解存储的底层实现，这就给卷的使用带来了极大的不便</p><p>kubernetes支持多种类型的卷，如下：</p><ul><li>emptyDir</li><li>hostPath</li><li>gcePersistentDisk</li><li>awsElasticBlockStore</li><li>nfs</li><li>iscsi</li><li>fc (fibre channel)</li><li>flocker</li><li>glusterfs</li><li>rbd</li><li>cephfs</li><li>gitRepo</li><li>secret</li><li>persistentVolumeClaim</li><li>downwardAPI</li><li>projected</li><li>azureFileVolume</li><li>azureDisk</li><li>vsphereVolume</li><li>Quobyte</li><li>PortworxVolume</li><li>ScaleIO</li><li>StorageOS</li><li>local</li></ul><hr><h2 id="1-1-本地存储类卷，emptyDir、hostPath、local"><a href="#1-1-本地存储类卷，emptyDir、hostPath、local" class="headerlink" title="1.1 本地存储类卷，emptyDir、hostPath、local"></a>1.1 本地存储类卷，emptyDir、hostPath、local</h2><h3 id="1-1-1-EmptyDir"><a href="#1-1-1-EmptyDir" class="headerlink" title="1.1.1 EmptyDir"></a>1.1.1 EmptyDir</h3><p>EmptyDir，一个空目录，生命周期和所属Pod完全一致，即存储的数据随之pod的销毁而被删除，主要用于存储同一Pod内不同容器间共享工作过程中产生的文件，如缓存数据、检查点数据等</p><pre><code class="hljs">vi nginx.yamlapiVersion: v1kind: Podmetadata:  name: nginxspec:  containers:  - image: sword618/nginx    name: nginx    # 设置容器所要挂载的卷    volumeMounts:    # 设置卷挂载到容器的路径    - mountPath: /tmp/nginx      # 设置所要挂载的卷的名称      name: nginx-cache  # 设置卷的信息  volumes:  # 设置卷名称  - name: nginx-cache    # 设置卷的类型    emptyDir: &#123;&#125;</code></pre><h3 id="1-1-2-Hostpath"><a href="#1-1-2-Hostpath" class="headerlink" title="1.1.2 Hostpath"></a>1.1.2 Hostpath</h3><p>Hostpath，将node上文件系统挂载到pod，若Pod跨主机重建，其内容则无法保证，所以可搭配DaemonSet使用，可用于存储日志、cADvisor监控数据等</p><pre><code class="hljs">vi nginx.yamlapiVersion: v1kind: Podmetadata:  name: nginx  namespace: swordspec:  containers:  - image: sword618/nginx    name: nginx    volumeMounts:    - mountPath: /var/log/nginx      name: nginx-logs  volumes:  - name: nginx-logs    hostPath:      # 设置node节点目录      path: /var/log/nginx</code></pre><h2 id="1-2-分布式存储类卷，如NFS、GlusterFS、CephFS等"><a href="#1-2-分布式存储类卷，如NFS、GlusterFS、CephFS等" class="headerlink" title="1.2 分布式存储类卷，如NFS、GlusterFS、CephFS等"></a>1.2 分布式存储类卷，如NFS、GlusterFS、CephFS等</h2><p>适用于不同node上的pod数据共享，如静态html、图片资源、web资源等</p><pre><code class="hljs">vi nginx-deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata:  name: nginx-deployment  namespace: swordspec:  selector:    matchLabels:      app: nginx-servers  replicas: 2  template:    metadata:      labels:        app: nginx-servers    spec:      containers:        - name: nginx          image: sword618/nginx          imagePullPolicy: IfNotPresent          ports:            - containerPort: 80          name: port          protocol: TCP      volumeMounts:        - mountPath: /var/nginx/html        name: web      volumes:      - name: web        nfs:        # 设置nfs服务器        server: 172.16.100.200        # 设置nfs服务器目录        path: /data/web/html              </code></pre><h2 id="1-3-公有云存储，如AWS、GCE、Auzre等"><a href="#1-3-公有云存储，如AWS、GCE、Auzre等" class="headerlink" title="1.3 公有云存储，如AWS、GCE、Auzre等"></a>1.3 公有云存储，如AWS、GCE、Auzre等</h2><h1 id="2-PV-x2F-PVC"><a href="#2-PV-x2F-PVC" class="headerlink" title="2.PV&#x2F;PVC"></a>2.PV&#x2F;PVC</h1><p>PV，PersistentVolume，即持久卷，由管理员预先创建或由StorageClass动态供应，是一段网络存储。PV与普通Volume一样使用卷插件实现，所不同的是拥有独立于任何使用PV的Pod的生命周期。PV属于集群资源，类似于node，不属于任何pod。PV将存储实现的底层逻辑抽象出来，kubernetes用户无需关注存储的实现细节，无论其背后是NFS、iSCSI还是特定于云平台的存储系统，直接申领使用即可</p><p>PVC，PersistentVolumeClaim，即持久卷申领，是用户对存储的引用，属于命名空间资源，消耗PV，类似于pod消耗node资源。PVC根据accessModes和capacity字段，自动搜索到符合这两个字段配置的PV并进行绑定，或者也可以通过标签选择器来定向进行匹配，一个PV只能被一个PVC绑定</p><h2 id="2-1-PV支持的类型"><a href="#2-1-PV支持的类型" class="headerlink" title="2.1 PV支持的类型"></a>2.1 PV支持的类型</h2><p>PersistentVolume是由存储插件来进行实现，目前支持的插件如下：</p><ul><li>awsElasticBlockStore：AWS弹性块存储（EBS）</li><li>azureDisk：Azure Disk</li><li>azureFile：Azure File</li><li>cephfs：CephFS volume</li><li>csi：容器存储接口 (CSI)</li><li>fc：Fibre Channel (FC) 存储</li><li>flexVolume：FlexVolume</li><li>gcePersistentDisk：GCE持久化盘</li><li>glusterfs：Glusterfs卷</li><li>hostPath：HostPath卷，仅供单节点测试环境而不适用于多节点集群，建议使用local卷作为替代</li><li>iscsi，iSCSI (SCSI over IP) 存储</li><li>local，节点上挂载的本地存储设备</li><li>nfs，网络文件系统 (NFS) 存储</li><li>portworxVolume，Portworx卷</li><li>rbd，Rados块设备(RBD)卷</li><li>vsphereVolume，vSphere VMDK卷</li><li>cinder，Cinder OpenStack 块存储，v1.18弃用</li><li>flocker，Flocker存储，v1.22弃用</li><li>quobyte，Quobyte卷，v1.22弃用</li><li>storageos，StorageOS卷，v1.22弃用</li></ul><hr><h2 id="2-2-PV生命周期"><a href="#2-2-PV生命周期" class="headerlink" title="2.2 PV生命周期"></a>2.2 PV生命周期</h2><h3 id="2-2-1-资源供应，Provisioning"><a href="#2-2-1-资源供应，Provisioning" class="headerlink" title="2.2.1 资源供应，Provisioning"></a>2.2.1 资源供应，Provisioning</h3><p>Kubernetes支持两种资源的供应模式，即静态模式(Staic)和动态模式(Dynamic)，其结果就是创建完成的PV</p><ul><li><p>静态模式，集群管理员手工创建多个PV，在定义PV时需要将后端存储的特性进行设置</p></li><li><p>动态模式，集群管理员无须手工创建PV，而是通过StorageClass的设置对后端存储进行描述，标记为某种类型，即Class。PVC对存储类型进行声明，集群将自动完成PV的创建及PVC的绑定</p></li></ul><hr><h3 id="2-2-2-资源绑定，Binding"><a href="#2-2-2-资源绑定，Binding" class="headerlink" title="2.2.2 资源绑定，Binding"></a>2.2.2 资源绑定，Binding</h3><p>PVC定义完成后，Kubernetes集群根据PVC对存储资源的请求，如存储空间和访问模式，在已存在的PV中选择一个满足PVC要求的PV，若能匹配就将该PV与用户定义的PVC进行绑定，从而实现pod存储卷的挂载</p><p>若没有满足PVC要求的PV，PVC则会无限期处于Pending状态，直到系统管理员创建了一个符合要求的PV。也可将两者定向绑定，如使用label等</p><p>PV一旦绑定在某个PVC上就会被其独占，不能再与其他PVC进行绑定。当PVC申请的存储空间比PV的少时，整个PV的空间都能够为PVC所用，但会造成资源的浪费。若资源供应使用的是动态模式，则集群在PVC找到合适的StorageClass后，将会自动创建PV并完成PVC的绑定</p><h3 id="2-2-3-资源使用，Using"><a href="#2-2-3-资源使用，Using" class="headerlink" title="2.2.3 资源使用，Using"></a>2.2.3 资源使用，Using</h3><p>pod读取定义的卷，将PVC挂载到容器内的某个路径以实现存储的持久化。卷的类型为persistentVoulumeClaim，在容器挂载了PVC后就会被持续独占使用。多个Pod可以挂载同一个PVC，应用程序需考虑多个实例共同访问一块存储空间的问题</p><h3 id="2-2-4-资源释放，Releasing"><a href="#2-2-4-资源释放，Releasing" class="headerlink" title="2.2.4 资源释放，Releasing"></a>2.2.4 资源释放，Releasing</h3><p>当用户对存储资源使用完毕后可以删除PVC，与该PVC绑定的PV将会被标记为已释放，此时该PV还不能绑定其他PVC，因为之前绑定的PVC写入的数据可能还留在存储设备上，只有在清除之后该PV才能继续使用</p><h3 id="2-2-5-资源回收，Reclaiming"><a href="#2-2-5-资源回收，Reclaiming" class="headerlink" title="2.2.5 资源回收，Reclaiming"></a>2.2.5 资源回收，Reclaiming</h3><p>PV回收策略，即Reclaim Policy，由集群管理员设置，用于定义与之绑定的PVC释放资源之后对于遗留数据的处理方式。只有PV的存储空间完成回收，才能用于新的PVC的绑定和使用</p><p> 2.3 PV状态</p><ul><li>Available，空闲状态，未被PVC绑定</li><li>Bound，绑定状态，已被PVC绑定</li><li>Released，释放状态，已被PVC解绑，但还未被集群重新声明</li><li>Failed，自动回收失败</li></ul><hr><h2 id="2-4-PV配置参数"><a href="#2-4-PV配置参数" class="headerlink" title="2.4 PV配置参数"></a>2.4 PV配置参数</h2><h3 id="2-4-1-Capacity，存储容量"><a href="#2-4-1-Capacity，存储容量" class="headerlink" title="2.4.1 Capacity，存储容量"></a>2.4.1 Capacity，存储容量</h3><p>描述存储设备具备的能力，定义存储空间，描述字段storage，如storage&#x3D;10G</p><h3 id="2-4-2-Volume-Mode，存储卷模式"><a href="#2-4-2-Volume-Mode，存储卷模式" class="headerlink" title="2.4.2 Volume Mode，存储卷模式"></a>2.4.2 Volume Mode，存储卷模式</h3><p>描述字段volumeMode，可选项Filesystem（文件系统）和Block（块设备），默认值FileSystem</p><h3 id="2-4-3-Access-Modes，访问模式"><a href="#2-4-3-Access-Modes，访问模式" class="headerlink" title="2.4.3 Access Modes，访问模式"></a>2.4.3 Access Modes，访问模式</h3><p>描述应用对存储资源的访问权限，一个PV支持多种访问模式，但挂载时只支持一种模式，可选项：</p><ul><li>ReadWriteOnce（RWO），读写权限，且只能被单个Node挂载</li><li>ReadOnlyMany（ROX），只读权限，允许被多个Node挂载</li><li>ReadWriteMany（RWX），读写权限，允许被多个Node挂载</li></ul><hr><h3 id="2-4-4-Class，存储类别"><a href="#2-4-4-Class，存储类别" class="headerlink" title="2.4.4 Class，存储类别"></a>2.4.4 Class，存储类别</h3><p>设定存储的类别，通过storageClassName参数指定给一个StorageClass资源对象的名称，具有特定类别的PV只能与请求了该类别的PVC进行绑定，未绑定类别的PV则只能与不请求任何类别的PVC进行绑定</p><h3 id="2-4-5-Reclaim-Policy，回收策略"><a href="#2-4-5-Reclaim-Policy，回收策略" class="headerlink" title="2.4.5 Reclaim Policy，回收策略"></a>2.4.5 Reclaim Policy，回收策略</h3><p>描述字段persistentVolumeReclaimPolicy</p><ul><li>Retain，保留，保留数据，需手工处理</li><li>Recycle，回收空间，简单清除文件的操作，如执行rm -rf &#x2F;thevolume&#x2F;*命令</li><li>Delete，删除，与PV相连的后端存储完成Volume的删除操作。目前只有NFS和HostPath支持回收，其余只支持删除</li></ul><hr><h3 id="2-4-6-Mount-Options，挂载选项"><a href="#2-4-6-Mount-Options，挂载选项" class="headerlink" title="2.4.6 Mount Options，挂载选项"></a>2.4.6 Mount Options，挂载选项</h3><p>描述字段mountOptions，在将PV挂载到Node上时，根据后端存储的特点，可能需要设置额外的挂载参数，可根据PV定义中的mountOptions字段进行设置</p><h3 id="2-4-7-Node-Affinity，节点亲和性，仅用于Local存储卷"><a href="#2-4-7-Node-Affinity，节点亲和性，仅用于Local存储卷" class="headerlink" title="2.4.7 Node Affinity，节点亲和性，仅用于Local存储卷"></a>2.4.7 Node Affinity，节点亲和性，仅用于Local存储卷</h3><p>限制只能通过某些Node来访问Volume，描述字段nodeAffinity，使用这些Volume的Pod将被调度到满足条件的Node上</p><pre><code class="hljs">apiVersion: v1kind: PersistentVolumemetadata:  name: pv001spec:  accessModes:    - ReadOnlyMany  capacity:    storage: 100Mi  nfs:    path: /data/web/html    server: 192.168.100.100---apiVersion: v1kind: PersistentVolumemetadata:  name: pv002spec:  accessModes:    - ReadWriteMany  capacity:    storage: 1Gi  persistentVolumeReclaimPolicy: Delete  nfs:    path: /data/log    server: 192.168.100.100---apiVersion: v1kind: PersistentVolumemetadata:  name: pv003spec:  accessModes:    - ReadWriteOnce  capacity:    storage: 100Mi  nfs:    path: /data/log    server: 192.168.100.100---apiVersion: v1kind: PersistentVolumemetadata:  name: pv004  labels:    app: nginxspec:  accessModes:    - ReadWriteMany  capacity:    storage: 1GMi  persistentVolumeReclaimPolicy: Recycle  nfs:    path: /data/log/nginx    server: 192.168.100.100---apiVersion: v1kind: PersistentVolumemetadata:  name: pv005spec:  storageClassName: nfs  accessModes:    - ReadWriteMany  capacity:    storage: 500Mi  nfs:    path: /data/web    server: 192.168.100.100---apiVersion: v1kind: PersistentVolumemetadata:  # 设置PV名称  name: pv006  # 设置PV标签，用于PVC的定向绑定  labels:    app: tomcatspec:  # 设置存储类别  storageClassName: nfs  # 设置访问模式  accessModes:    - ReadWriteMany  # 设置PV的存储空间  capacity:    storage: 500Mi  # 设置PV的回收策略  persistentVolumeReclaimPolicy: Retain  nfs:    path: /data/web    server: 192.168.100.100</code></pre><h2 id="2-5-PVC配置参数"><a href="#2-5-PVC配置参数" class="headerlink" title="2.5 PVC配置参数"></a>2.5 PVC配置参数</h2><h3 id="2-5-1-Resources，资源请求"><a href="#2-5-1-Resources，资源请求" class="headerlink" title="2.5.1 Resources，资源请求"></a>2.5.1 Resources，资源请求</h3><p>描述对存储资源的请求，目前仅支持request.storage的设置，即是存储空间的大小，PV的存储容量高于此值才能进行绑定，可用于PV的筛选</p><h3 id="2-5-2-访问模式，AccessModes"><a href="#2-5-2-访问模式，AccessModes" class="headerlink" title="2.5.2 访问模式，AccessModes"></a>2.5.2 访问模式，AccessModes</h3><p>用于描述对存储资源的访问权限，与绑定的PV一致，可用于PV的筛选</p><h3 id="2-5-3-存储卷模式，Volume-Modes"><a href="#2-5-3-存储卷模式，Volume-Modes" class="headerlink" title="2.5.3 存储卷模式，Volume Modes"></a>2.5.3 存储卷模式，Volume Modes</h3><p>用于描述希望使用的PV存储卷模式，支持两种模式，即Filesystem（文件系统，默认值）模式和Block（块）模式</p><h3 id="2-5-4-存储类，Class"><a href="#2-5-4-存储类，Class" class="headerlink" title="2.5.4 存储类，Class"></a>2.5.4 存储类，Class</h3><p>PVC定义时可指定后端存储的类别，即storageClassName字段，从而减少对后端存储特性的详细信息的依赖。此时，只有设置了该Class的PV才能被系统选出，并与该PVC进行绑定</p><p>PVC也可不设置Class，若storageClassName字段的值被设置为空，即storageClassName&#x3D;””，则表示该PVC不要求特定的存储，kubernetes集群将只选择未设定Class的PV与之匹配和绑定。若完全不设置storageClassName字段，将根据kubernetes集群是否启用了名为DefaultStorageClass的admission controller进行相应的操作</p><h3 id="2-5-5-选择条件，Selector"><a href="#2-5-5-选择条件，Selector" class="headerlink" title="2.5.5 选择条件，Selector"></a>2.5.5 选择条件，Selector</h3><p>用对PVC对PV的定向绑定，由matchLabels和matchExpressions字段这两个进行设置，若设置两个字段，则Selector的逻辑是所有条件同时满足才能完成匹配，包括Class</p><hr><ul><li><p>matchLabels，标签匹配</p></li><li><p>matchExpressions，表达式匹配，通过键值对和运算符组合的表达式进行匹配，支持的运算符包括In、NotIn、Exists、DoesNotExist</p></li></ul><hr><pre><code class="hljs">apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: nginx-web  namespace: defaultspec:  accessModes:  - ReadOnlyMany  resources:    requests:      storage: 50Mi---apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: nginx-log  namespace: defaultspec:  accessModes:  - ReadWriteMany  resources:    requests:      storage: 1Gi  selector:     matchLabels:      app: nginx---apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: tomcat-log  namespace: defaultspec:  accessModes:  - ReadWriteMany  resources:    requests:      storage: 500Mi---apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: tomcat-web  namespace: defaultspec:  # 设置PVC存储类别，用于指定存储类型  storageClassName: nfs  # 设置访问模式，匹配相同模式的PV  accessModes:  - ReadWriteMany  # 设置PVC所申请存储空间的大小  resources:    requests:      storage: 500Mi</code></pre><h1 id="3-StorageClass"><a href="#3-StorageClass" class="headerlink" title="3.StorageClass"></a>3.StorageClass</h1><p>StorageClass，即存储类，用于实现集群存储资源的动态供应，允许按需创建PV，在用户请求时自动供应，由集群自动完成PV的创建和绑定</p><p>集群管理员根据存储底层机制不同的特性和性能定义各种不同StorageClass对象，每个对象指定一个相应的供应器，即provisioner，供应器向卷供应商提供在创建卷时需要的数据卷信息及相关参数，并映射到不同的服务质量等级或备份策略，或是由集群管理员制定的任意策略。定义了storageClassName的PVC被创建申请资源时，相应的StorageClass对象被调用，根据定义好的传递给该驱动的参数将驱动启动，StorageClass的后端存储从而制备一个PV并进行绑定，最终实现了存储资源的动态创建</p><p>若PVC没有指定storageClassName，则会被绑定到由集群管理员设置的默认StorageClass。若集群准入控制器插件，即DefaultStorageClass未启用或未设置默认StorageClass，等效于PVC设置storageClassName的值为空，则只能绑定未设置Class的PV</p><h2 id="3-1-配置参数"><a href="#3-1-配置参数" class="headerlink" title="3.1 配置参数"></a>3.1 配置参数</h2><h3 id="3-1-1-Provisioner，供应器"><a href="#3-1-1-Provisioner，供应器" class="headerlink" title="3.1.1 Provisioner，供应器"></a>3.1.1 Provisioner，供应器</h3><p>描述存储资源的提供者，可视为后端存储驱动，决定了动态创建PV时所用的卷插件，必选参数</p><h3 id="3-1-2-Parameters，参数"><a href="#3-1-2-Parameters，参数" class="headerlink" title="3.1.2 Parameters，参数"></a>3.1.2 Parameters，参数</h3><p>描述存储类的卷，取决于后端存储供应器</p><h3 id="3-1-3-Reclaim-Policy，回收策略"><a href="#3-1-3-Reclaim-Policy，回收策略" class="headerlink" title="3.1.3 Reclaim Policy，回收策略"></a>3.1.3 Reclaim Policy，回收策略</h3><p>由StorageClass动态供应的PV将继承该值，默认为Delete，删除PVC与之绑定的PV也将根据其默认的回收策略被删除，若需保留PV数据，则在动态绑定成功后手动将自动生成PV的回收策略改为Retain。通过StorageClass手动创建并管理的PV将会继承创建时指定的回收策略</p><h3 id="3-1-4-Mount-Options，挂载选项"><a href="#3-1-4-Mount-Options，挂载选项" class="headerlink" title="3.1.4 Mount Options，挂载选项"></a>3.1.4 Mount Options，挂载选项</h3><p>由StorageClass动态供应的PV将继承该值，若卷插件不支持挂载选项却指定了挂载选项，则PV的制备操作就会失败</p><h3 id="3-1-5-VolumeBindingMode，卷绑定模式"><a href="#3-1-5-VolumeBindingMode，卷绑定模式" class="headerlink" title="3.1.5 VolumeBindingMode，卷绑定模式"></a>3.1.5 VolumeBindingMode，卷绑定模式</h3><p>描述存储类制备PV及与PVC绑定的时间点，默认为Immediate，即创建PVC成功立即制备PV并完成两者的绑定。将该字段的值设置为WaitForFirstConsumer表示延迟PV的制备与绑定直到申请PVC的pod被成功创建，此时会根据pod调度指定的约束条件选择制备PV的时机，选择依据包括资源需求、节点筛选器、pod亲和性和互斥性以及污点和容忍度，适用于跨集群的业务场景</p><p>目前支持动态供应延迟绑定的插件有AWSElasticBlockStore、GCEPersistentDisk和AzureDisk，其余插件只支持预创建绑定PV的延迟绑定</p><h2 id="3-2-设置默认StorageClass"><a href="#3-2-设置默认StorageClass" class="headerlink" title="3.2 设置默认StorageClass"></a>3.2 设置默认StorageClass</h2><h3 id="3-2-1-启用DefaultStorageClass"><a href="#3-2-1-启用DefaultStorageClass" class="headerlink" title="3.2.1 启用DefaultStorageClass"></a>3.2.1 启用DefaultStorageClass</h3><pre><code class="hljs">sudo vi /opt/kubernetes/cfg/kube-apiserver.conf--enable-admission-plugins=DefaultStorageClass</code></pre><h3 id="3-2-2-创建provisioner"><a href="#3-2-2-创建provisioner" class="headerlink" title="3.2.2 创建provisioner"></a>3.2.2 创建provisioner</h3><pre><code class="hljs">apiVersion: v1kind: ServiceAccountmetadata:  name: nfs-client-provisioner  namespace: default---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata:  name: nfs-client-provisioner-runnerrules:  - apiGroups: [&quot;&quot;]    resources: [&quot;persistentvolumes&quot;]    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;]  - apiGroups: [&quot;&quot;]    resources: [&quot;persistentvolumeclaims&quot;]    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;]  - apiGroups: [&quot;storage.k8s.io&quot;]    resources: [&quot;storageclasses&quot;]    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]  - apiGroups: [&quot;&quot;]    resources: [&quot;events&quot;]    verbs: [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata:  name: run-nfs-client-provisionersubjects:  - kind: ServiceAccount    name: nfs-client-provisioner    namespace: defaultroleRef:  kind: ClusterRole  name: nfs-client-provisioner-runner  apiGroup: rbac.authorization.k8s.io---kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata:  name: leader-locking-nfs-client-provisioner  namespace: defaultrules:  - apiGroups: [&quot;&quot;]    resources: [&quot;endpoints&quot;]    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]---kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata:  name: leader-locking-nfs-client-provisioner  namespace: defaultsubjects:  - kind: ServiceAccount    name: nfs-client-provisioner    namespace: defaultroleRef:  kind: Role  name: leader-locking-nfs-client-provisioner  apiGroup: rbac.authorization.k8s.io---apiVersion: apps/v1kind: Deploymentmetadata:  name: nfs-client-provisioner  labels:    app: nfs-client-provisioner  namespace: defaultspec:  replicas: 1  strategy:    type: Recreate  selector:    matchLabels:      app: nfs-client-provisioner  template:    metadata:      labels:        app: nfs-client-provisioner    spec:      serviceAccountName: nfs-client-provisioner      containers:        - name: nfs-client-provisioner          image: quay.io/external_storage/nfs-client-provisioner:latest          volumeMounts:            - name: nfs-client-root              mountPath: /persistentvolumes          env:            - name: PROVISIONER_NAME              # 设置nfs provisioner名称，storageclass需保持一致              value: nfs-client            - name: NFS_SERVER              # 设置nfs服务器IP              value: 192.168.100.100            - name: NFS_PATH              # 设置nfs路径              value: /data      volumes:        - name: nfs-client-root          nfs:            server: 192.168.100.100            path: /data</code></pre><h3 id="3-2-3-创建默认StorageClass"><a href="#3-2-3-创建默认StorageClass" class="headerlink" title="3.2.3 创建默认StorageClass"></a>3.2.3 创建默认StorageClass</h3><pre><code class="hljs">vi sc.yamlapiVersion: storage.k8s.io/v1  kind: StorageClass  metadata:  name: sc-nfs  annotations:    # 设置为集群默认storageclass    storageclass.kubernetes.io/is-default-class: &quot;true&quot;# 设置动态卷供应器名称，对应供应器PROVISIONER_NAMEprovisioner: nfs-clientparameters:  # 设置PVC删除时保留PV数据  archiveOnDelete: &quot;true&quot; </code></pre><ul><li>注：集群中只能有一个默认StorageClass，否则将无法为PVC创建相应的PV</li></ul><hr><pre><code class="hljs">vi nginx-deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata:  name: nginx-deployment  spec:  selector:    matchLabels:      app: nginx-servers  replicas: 2  template:    metadata:      labels:        app: nginx-servers    spec:      containers:        - name: nginx          image: sword618/nginx          imagePullPolicy: IfNotPresent          ports:            - containerPort: 80              name: http-nginx              protocol: TCP          ports:          volumeMounts:            - name: web              mountPath: /usr/share/nginx/html            - name: logs              mountPath: /var/log/nginx      volumes:        - name: web          persistentVolumeClaim:            claimName: nginx-web        - name: logs          persistentVolumeClaim:            claimName: nginx-log---apiVersion: v1kind: Servicemetadata:  name: nginx-servicespec:  type: NodePort  sessionAffinity: ClientIP  selector:    app: nginx-servers  ports:  - port: 80    targetPort: 80    nodePort: 32000</code></pre><p>vi tomcat-deployment.yaml</p><pre><code class="hljs">apiVersion: apps/v1kind: Deploymentmetadata:  name: tomcat-deployment  spec:  selector:    matchLabels:      app: tomcat-servers  replicas: 2  template:    metadata:      labels:        app: tomcat-servers    spec:      containers:        - name: tomcat          image: sword618/tomcat          imagePullPolicy: IfNotPresent          ports:            - containerPort: 8080              name: http-tomcat              protocol: TCP          ports:          volumeMounts:            - name: web              mountPath: /usr/local/tomcat/webapps            - name: logs              mountPath: /usr/local/tomcat/logs      volumes:        - name: web          persistentVolumeClaim:            claimName: tomcat-web        - name: logs          persistentVolumeClaim:            claimName: tomcat-log---apiVersion: v1kind: Servicemetadata:  name: tomcat-servicespec:  type: NodePort  selector:    app: tomcat-servers  ports:  - port: 8080    targetPort: 8080    nodePort: 32080</code></pre><h1 id="4-ConfigMap"><a href="#4-ConfigMap" class="headerlink" title="4.ConfigMap"></a>4.ConfigMap</h1><p>ConfigMap，即配置项，用于将非机密性的配置数据存储于键值对,实现了环境配置信息和容器镜像的解耦，将容器化的应用与其配置信息分离，避免因应用的配置修改而重新构建镜像的操作。ConfigMap文件大小被限制为1M，且只能被相同namespace的pod引用，若pod引用了不存在的configmap则pod的创建将会失败</p><h2 id="4-1-创建configmap"><a href="#4-1-创建configmap" class="headerlink" title="4.1 创建configmap"></a>4.1 创建configmap</h2><h3 id="4-1-1-键值对创建方式"><a href="#4-1-1-键值对创建方式" class="headerlink" title="4.1.1 键值对创建方式"></a>4.1.1 键值对创建方式</h3><pre><code class="hljs">kubectl create configmap redis-conf-001 --from-literal=maxclients=1024 --from-literal=requirepass=redis</code></pre><h3 id="4-1-2-yaml资源文件创建方式"><a href="#4-1-2-yaml资源文件创建方式" class="headerlink" title="4.1.2 yaml资源文件创建方式"></a>4.1.2 yaml资源文件创建方式</h3><pre><code class="hljs"># 类属性键值对vi configmap-redis002.yamlapiVersion: v1kind: ConfigMapmetadata:  name: redis-conf-002data:  maxclients: &quot;1024&quot;  requirepass: &quot;redis&quot;</code></pre><hr><pre><code class="hljs"># 类文件键值对vi configmap-redis003.yamlapiVersion: v1kind: ConfigMapmetadata:  name: redis-conf-003data:  redis.conf: |    maxclients  1024    requirepass redis</code></pre><h3 id="4-1-3-配置文件创建方式"><a href="#4-1-3-配置文件创建方式" class="headerlink" title="4.1.3 配置文件创建方式"></a>4.1.3 配置文件创建方式</h3><pre><code class="hljs"># 单个文件方式kubectl create configmap nginx-conf-001 --from-file=nginx.conf# 整个目录方式，但不含子目录kubectl create configmap nginx-conf-002 --from-file=/etc/nginx/conf.d</code></pre><h2 id="4-2-pod引用configmap"><a href="#4-2-pod引用configmap" class="headerlink" title="4.2 pod引用configmap"></a>4.2 pod引用configmap</h2><p>configmap有三种引用方式，即环境变量、容器启动命令行参数和数据卷文件</p><h3 id="4-2-1-环境变量方式"><a href="#4-2-1-环境变量方式" class="headerlink" title="4.2.1 环境变量方式"></a>4.2.1 环境变量方式</h3><pre><code class="hljs"># 引用单个configmapvi redis001.yamlapiVersion: v1kind: Podmetadata:  name: redis001spec:  containers:    - name: redis-server      image: sword618/redis      imagePullPolicy: IfNotPresent      env:        - name: passwd          valueFrom:            configMapKeyRef:              name: redis-conf-001              key: requirepass# 引用多个configmapvi redis002.yamlapiVersion: v1kind: Podmetadata:  name: redis002spec:  containers:    - name: redis-server      image: sword618/redis      imagePullPolicy: IfNotPresent       env:        - name: passwd          valueFrom:            configMapKeyRef:              name: redis-conf-001              key: requirepass        - name: maxclients          valueFrom:            configMapKeyRef:              name: redis-conf-002              key: maxclients# 引用所有键值对vi redis003.yamlapiVersion: v1kind: Podmetadata:  name: redis003spec:  containers:    - name: redis-server      image: sword618/redis      imagePullPolicy: IfNotPresent      envFrom:      - configMapRef:          name: redis-conf-002</code></pre><h3 id="4-2-2-容器启动命令参数方式"><a href="#4-2-2-容器启动命令参数方式" class="headerlink" title="4.2.2 容器启动命令参数方式"></a>4.2.2 容器启动命令参数方式</h3><pre><code class="hljs">vi redis004.yamlapiVersion: v1kind: Podmetadata:  name: redis004spec:  containers:    - name: redis-server      image: sword618/redis      imagePullPolicy: IfNotPresent      command: [ &quot;/usr/bin/redis-server&quot; ]      args: [&quot;--requirepass&quot;,&quot;$(requirepass)&quot;,&quot;--maxclients&quot;,&quot;$(maxclients)&quot;]      envFrom:      - configMapRef:          name: redis-conf-001</code></pre><h3 id="4-2-3-数据卷文件方式"><a href="#4-2-3-数据卷文件方式" class="headerlink" title="4.2.3 数据卷文件方式"></a>4.2.3 数据卷文件方式</h3><pre><code class="hljs">vi redis005.yamlapiVersion: v1kind: Podmetadata:  name: redis005spec:  containers:  - name: redis-server    image: sword618/redis    volumeMounts:    - name: redis-conf      mountPath: /etc/redis      readOnly: true  volumes:                                         - name: redis-conf    configMap:      name: redis-conf-003</code></pre><hr><pre><code class="hljs">vi nginx001.yamlapiVersion: v1kind: Podmetadata:  name: nginx001spec:  containers:  - name: nginx-server    image: sword618/nginx    volumeMounts:    - name: nginx-conf      mountPath: /etc/nginx      readOnly: true  volumes:                                         - name: nginx-conf    configMap:      name: nginx-conf-001</code></pre><hr><pre><code class="hljs">vi nginx002.yamlapiVersion: v1kind: Podmetadata:  name: nginx002spec:  containers:  - name: nginx-server    image: sword618/nginx    volumeMounts:    - name: nginx-conf      mountPath: /etc/nginx      readOnly: true  volumes:                                         - name: nginx-conf    configMap:      name: nginx-conf-001</code></pre><h1 id="5-Secret"><a href="#5-Secret" class="headerlink" title="5.Secret"></a>5.Secret</h1><p>Secret，包含少量敏感信息如密码、令牌或密钥的对象。这类信息当然可被放在Pod规约或者镜像中，但存储于Secret由于有加密措施就大大减少了机密暴露的风险</p><p>Secret类型由type字段设置，默认为Opaque</p><ul><li>Opaque，集群用户创建的base64编码格式的数据，用于存储密码、密钥等，但数据可通过base64–decode解码得到原始数据，加密性不强</li><li>Service Account，集群内置的服务账户，用于访问API对象的权限分配，由Kubernetes集群自动创建和挂载</li><li>kubernetes.io&#x2F;dockerconfigjson，用于存储私有镜像库的认证信息</li><li>kubernetes.io&#x2F;basic-auth，基本身份认证数据，用于存储基本身份认证所需的凭据信息</li><li>kubernetes.io&#x2F;ssh-auth，SSH身份认证数据，用于存储SSH身份认证所需的凭据</li><li>kubernetes.io&#x2F;tls，SSL证书数据，用于存储证书及相关密钥，提供给Ingress用以终结TLS链接，也可以用于其他资源或者负载</li></ul><hr><h2 id="5-1-启用静态Secret数据加密"><a href="#5-1-启用静态Secret数据加密" class="headerlink" title="5.1 启用静态Secret数据加密"></a>5.1 启用静态Secret数据加密</h2><pre><code class="hljs">sudo vi /opt/kubernetes/cfg/kube-apiserver.conf--experimental-encryption-provider-config</code></pre><h2 id="5-2-创建secret"><a href="#5-2-创建secret" class="headerlink" title="5.2 创建secret"></a>5.2 创建secret</h2><h3 id="5-2-1-键值对方式"><a href="#5-2-1-键值对方式" class="headerlink" title="5.2.1 键值对方式"></a>5.2.1 键值对方式</h3><pre><code class="hljs">kubectl create secret generic redis-auth --from-literal=requirepass=&#39;redis&#39;</code></pre><h2 id="5-2-2-文件方式"><a href="#5-2-2-文件方式" class="headerlink" title="5.2.2 文件方式"></a>5.2.2 文件方式</h2><pre><code class="hljs">kubectl create secret tls nginx-ssl --key=nginx.key --cert=nginx.crtkubectl create secret generic ssh-key --from-file=ssh-privatekey=id_rsa --from-file=ssh-publickey=id_rsa.pubkubectl create secret generic regcred --from-file=.dockerconfigjson=config.json --type=kubernetes.io/dockerconfigjson</code></pre><h2 id="5-2-3-yaml资源文件方式"><a href="#5-2-3-yaml资源文件方式" class="headerlink" title="5.2.3 yaml资源文件方式"></a>5.2.3 yaml资源文件方式</h2><pre><code class="hljs"># 将认证信息转换为base64编码格式，以密文存储echo -n &#39;admin&#39; | base64YWRtaW4=echo -n &#39;nginx&#39; | base64bmdpbng=vi nginx-auth.yamlapiVersion: v1kind: Secretmetadata:  name: nginx-authtype: kubernetes.io/basic-authstringData:  username: YWRtaW4=  password: bmdpbng=</code></pre><h2 id="5-3-pod引用Secret"><a href="#5-3-pod引用Secret" class="headerlink" title="5.3 pod引用Secret"></a>5.3 pod引用Secret</h2><h3 id="5-3-1-环境变量方式"><a href="#5-3-1-环境变量方式" class="headerlink" title="5.3.1 环境变量方式"></a>5.3.1 环境变量方式</h3><pre><code class="hljs">vi redis.yamlapiVersion: v1kind: Podmetadata:  name: redisspec:  containers:  - name: redis-server    image: sword618/redis    command: [ &quot;/usr/bin/redis-server&quot; ]    args: [&quot;--requirepass&quot;,&quot;$(requirepass)&quot;]    env:      - name: requirepass        valueFrom:          secretKeyRef:            name: redis-auth            key: requirepass</code></pre><h3 id="5-3-1-数据卷文件方式"><a href="#5-3-1-数据卷文件方式" class="headerlink" title="5.3.1 数据卷文件方式"></a>5.3.1 数据卷文件方式</h3><pre><code class="hljs">vi nginx.yamlapiVersion: v1kind: Podmetadata:   name: nginxspec:  containers:  - name: nginx-server    image: sword618/nginx    imagePullPolicy: IfNotPresent    volumeMounts:    - name: nginx-cert      mountPath: /etc/nginx/ssl      readOnly: true    - name: nginx-auth      mountPath: /etc/nginx/auth      readOnly: true  volumes:  - name: nginx-cert    secret:      secretName: nginx-ssl  - name: nginx-auth    secret:      secretName: nginx-auth</code></pre><hr><h3 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h3><ul><li><a href="https://blog.51cto.com/ylw6006/2068953">https://blog.51cto.com/ylw6006/2068953</a></li><li><a href="https://zhuanlan.zhihu.com/p/347526819">https://zhuanlan.zhihu.com/p/347526819</a></li><li><a href="https://www.cnblogs.com/xull0651/p/15457326.html">https://www.cnblogs.com/xull0651/p/15457326.html</a></li><li><a href="https://blog.csdn.net/ljx1528/article/details/119382467">https://blog.csdn.net/ljx1528/article/details/119382467</a></li><li><a href="https://blog.csdn.net/Micky_Yang/article/details/108308772">https://blog.csdn.net/Micky_Yang/article/details/108308772</a></li><li><a href="https://blog.csdn.net/star1210644725/article/details/112416760">https://blog.csdn.net/star1210644725/article/details/112416760</a></li><li><a href="https://blog.csdn.net/weixin_43114954/article/details/120296-687">https://blog.csdn.net/weixin_43114954/article/details/120296-687</a></li><li><a href="https://kubernetes.io/zh/docs/concepts/storage/persistent-volumes">https://kubernetes.io/zh/docs/concepts/storage/persistent-volumes</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>存储</tag>
      
      <tag>分布式存储</tag>
      
      <tag>云存储</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes集群标签与注解</title>
    <link href="/linux/KubernetesLabelAnnotations/"/>
    <url>/linux/KubernetesLabelAnnotations/</url>
    
    <content type="html"><![CDATA[<p>标签和注解，Kubernetes集群基本概念，即是以键值对形式附加到集群资源对象（Node、Pod、Service）的元数据，应急集群应用的组织、标记及分组，以便于灵活、方便地进行资源分配、调度、配置、部署等管理工作</p><h1 id="1-标签"><a href="#1-标签" class="headerlink" title="1.标签"></a>1.标签</h1><p>Labels，标签，配合标签选择器用于查询和选择特定的资源对象，可在创建时附加，也可以通过更新操作进行修改，应用场景如下：</p><ul><li>环境区分，关联不同的环境或特征，如开发、测试和生产环境或服务层次（前端、后端、数据库），以便针对不同环境进行部署和管理</li><li>版本管理，标记不同版本的应用，方便进行版本控制和回滚操作</li><li>负载均衡，选择特定的资源对象集合，选择性地将应用暴露给特定的服务或进行负载均衡</li><li>故障排查，筛选特定的应用实例，方便进行故障排查和日志分析</li><li>策略管理，定义发布策略，如灰度发布、AB测试等</li></ul><h2 id="1-1-创建标签"><a href="#1-1-创建标签" class="headerlink" title="1.1 创建标签"></a>1.1 创建标签</h2><pre><code class="hljs">kubectl label service nginx version = 1.0</code></pre><h2 id="1-2-查询标签"><a href="#1-2-查询标签" class="headerlink" title="1.2 查询标签"></a>1.2 查询标签</h2><pre><code class="hljs">kubectl get service nginx --show-labels</code></pre><h2 id="1-3-修改标签"><a href="#1-3-修改标签" class="headerlink" title="1.3 修改标签"></a>1.3 修改标签</h2><pre><code class="hljs">kubectl label service nginx &quot;version = 2.0&quot;</code></pre><h2 id="1-4-删除标签"><a href="#1-4-删除标签" class="headerlink" title="1.4 删除标签"></a>1.4 删除标签</h2><pre><code class="hljs">kubectl label service nginx version-</code></pre><ul><li>注：-，连字符，表示删除标签，若需删除多个标签则只需在每个之后附加连字符即可</li></ul><h1 id="2-注解"><a href="#2-注解" class="headerlink" title="2.注解"></a>2.注解</h1><p>Annotations，注解，用于存储集群资源对象额外的非标识性信息，通常作为上下文信息被集成工具、监控系统和自动化流程调用，以更加灵活的方式传递配置信息，应用场景如下：</p><ul><li>文档配置，将特定资源的目的、使用方法或历史见解以文档的形式作详细记录与描述，如资源对象更新的原因及滚动发布的副本集</li><li>工具集成，Kubernetes生态系统各种工具和控制器的提示或指令，如Prometheus监控集成配置等</li><li>参数配置，定义应用程序的配置参数，如数据库连接字符串、环境变量及Git哈希、时间戳、pull请求单号等</li></ul><h2 id="2-1-文档配置"><a href="#2-1-文档配置" class="headerlink" title="2.1 文档配置"></a>2.1 文档配置</h2><pre><code class="hljs">apiVersion: v1kind: Podmetadata:  name: data-processing-pod  annotations:    purpose: &quot;This pod is responsible for processing large datasets and generating reports.&quot;spec:  containers:    - name: data-processor      image: data-processor-image</code></pre><h2 id="2-2-工具集成"><a href="#2-2-工具集成" class="headerlink" title="2.2 工具集成"></a>2.2 工具集成</h2><pre><code class="hljs">apiVersion: v1kind: Podmetadata:  name: monitored-pod  annotations:    prometheus.io/scrape: &quot;true&quot;    prometheus.io/port: &quot;8080&quot;spec:  containers:    - name: web-app      image: web-app-image      ports:        - containerPort: 8080</code></pre><h2 id="2-3-参数配置"><a href="#2-3-参数配置" class="headerlink" title="2.3 参数配置"></a>2.3 参数配置</h2><pre><code class="hljs">apiVersion: apps/v1kind: Deploymentmetadata:  name: app-deploymentspec:  replicas: 3  selector:    matchLabels:      app: my-app  template:    metadata:      labels:        app: my-app      annotations:        app-config/database-url: &quot;jdbc:mysql://db-server:3306/mydatabase&quot;        app-config/max-connections: &quot;100&quot;    spec:      containers:        - name: my-app-container          image: my-app-image</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.jianshu.com/p/1815a4281948">https://www.jianshu.com/p/1815a4281948</a></li><li><a href="https://blog.csdn.net/zhaopeng_yu/article/details/134920491">https://blog.csdn.net/zhaopeng_yu/article/details/134920491</a></li><li><a href="https://blog.csdn.net/weixin_45310323/article/details/130718456">https://blog.csdn.net/weixin_45310323/article/details/130718456</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes集群控制器详解</title>
    <link href="/linux/KubernetesController/"/>
    <url>/linux/KubernetesController/</url>
    
    <content type="html"><![CDATA[<p>Controller，即控制器，用于驱动Kubernetes集群资源对象当前状态（status）逼近用户提交的期望状态，通过API Server提供的（List &amp; Watch）接口实时地循环监控资源对象的状态，以便于发现故障并执行自动化修复流程。Controller由主节点的Controller Manager组件负责管理，不同的资源对象由各自的控制器进行管理，如Deployment Controller、Node Controller、Namespace Controller、Service Controller等</p><h1 id="1-Node控制器"><a href="#1-Node控制器" class="headerlink" title="1.Node控制器"></a>1.Node控制器</h1><p>Node Controller，即节点控制器，用于管理和监控集群所有的节点资源，由kubelet组件通过API Server组件注册自身节点并周期性地将汇报节点的状态信息，最后将之更行到etcd数据库。节点信息包括节点健康状况、节点资源、节点名称、节点地址信息、操作系统版本、Dooker版本、kubelet版本等，节点健康状况包含就绪（True）、未就绪（False）、未知（Unknown）三种</p><h1 id="2-ReplicaSet控制器"><a href="#2-ReplicaSet控制器" class="headerlink" title="2.ReplicaSet控制器"></a>2.ReplicaSet控制器</h1><p>ReplicaSet，即副本集控制器，用于保障集群内od副本数量，目前已被Deployment代替，无需直接操作ReplicaSet</p><h1 id="3-Deployment控制器"><a href="#3-Deployment控制器" class="headerlink" title="3.Deployment控制器"></a>3.Deployment控制器</h1><p>Deployment，即部署集控制器，集群内管理应用副本的API对象。Deployment并不直接控制Pod，而是通过ReplicaSet实现对Pod的管理，即是依据所定义的策略将ReplicaSet与Pod更新到预期状态，提供了运行Pod的能力，且为Pod提供滚动升级、伸缩、副本等功能，一般用于运行无状态的应用</p><h1 id="4-StatefulSet控制器"><a href="#4-StatefulSet控制器" class="headerlink" title="4.StatefulSet控制器"></a>4.StatefulSet控制器</h1><p>StatefulSet，即有状态集管理器，用于管理一组具有唯一固定ID的Pod集的部署和扩缩容，并提供持久化存储和持久化标识符，如Kafka集群等</p><h1 id="5-DaemonSet控制器"><a href="#5-DaemonSet控制器" class="headerlink" title="5.DaemonSet控制器"></a>5.DaemonSet控制器</h1><p>DaemonSet，即守护进程集控制器，用于确保所有节点运行一个Pod的副本，集群新增节点将会自动为其创建一个Pod，节点移除时相应的Pod也将被回收。DaemonSet的删除将会删除对应创建的所有Pod</p><h1 id="6-Job控制器"><a href="#6-Job控制器" class="headerlink" title="6.Job控制器"></a>6.Job控制器</h1><p>Job，即任务管理器，用于批量处理短暂的一次性任务，也就是仅执行一次的任务，保证批处理任务的一个或多个Pod成功结束</p><h1 id="7-CronJob控制器"><a href="#7-CronJob控制器" class="headerlink" title="7.CronJob控制器"></a>7.CronJob控制器</h1><p>CronJob，即定时任务处理器，用于周期性的处理任务，根据Cron表达式定时执行Job</p><h1 id="8-ResourceQuota控制器"><a href="#8-ResourceQuota控制器" class="headerlink" title="8.ResourceQuota控制器"></a>8.ResourceQuota控制器</h1><p>ResourceQuota，即资源配额控制器，用于限制集群资源对象所占用系统的物理资源，避免由于某些业务在设计或实现上的缺陷所导致整个系统运行紊乱甚至宕机，如容器CPU和Memory资源限制、Namespace所属Pod数量、Service数量、PV数量等的限制</p><h1 id="9-Service-x2F-Endpoints控制器"><a href="#9-Service-x2F-Endpoints控制器" class="headerlink" title="9.Service&#x2F;Endpoints控制器"></a>9.Service&#x2F;Endpoints控制器</h1><p>Service&#x2F;Endpoints Controller，即Service&#x2F;控制器，用于监听Service及其服务端点的变化，并根据监测到的事件做出相应的动作，如Service的Endpoints对象的增加、删除或Endpoint列表的修改，以实现网络流量的路由转发，动态更新负载均衡的后端服务器</p>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Tomcat性能优化方案</title>
    <link href="/linux/TomcatOptimization/"/>
    <url>/linux/TomcatOptimization/</url>
    
    <content type="html"><![CDATA[<h1 id="1-内存"><a href="#1-内存" class="headerlink" title="1.内存"></a>1.内存</h1><p>默认为128M，内存溢出报错 OUT of memory</p><h1 id="2-maxpermsize"><a href="#2-maxpermsize" class="headerlink" title="2.maxpermsize"></a>2.maxpermsize</h1><p>默认64M，决定了保持对象的大小，溢出报错 outofmemoryerror:permgen</p><h1 id="3-maxthreads"><a href="#3-maxthreads" class="headerlink" title="3.maxthreads"></a>3.maxthreads</h1><p>最大并发线程数，默认为200，请求量大于该值时，并发请求将进入等待队列。增加该值能提供并发处理能力，但会消耗系统资源，一般设为最大请求数即可。此外，该值还受限于操作系统的内核参数，即Linux的open files</p><h1 id="4-connection-timeout"><a href="#4-connection-timeout" class="headerlink" title="4.connection timeout"></a>4.connection timeout</h1><p>连接超时时长，默认为60秒，表示从开始接受请求到开始处理请求的等待时间，故障表现为502异常</p><h1 id="5-acceptcount"><a href="#5-acceptcount" class="headerlink" title="5.acceptcount"></a>5.acceptcount</h1><p>最大队列数，默认为100，表示所有线程都在处理请求时，新请求进入等待队列的最大个数。超出该值时，所有新请求都将被拒绝，故障表现为 connection refused</p>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Java</tag>
      
      <tag>Tomcat</tag>
      
      <tag>性能优化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes集群Pod详解</title>
    <link href="/linux/KubernetesPod/"/>
    <url>/linux/KubernetesPod/</url>
    
    <content type="html"><![CDATA[<p>Pod，Kubernetes集群所管理的最小单元，由一个或者多个紧密相连的容器组成，作为容器环境下的逻辑主机被部署于同一节点，用于承载具体的应用实例。Pod内的容器总是并置的且共同调度，共享一组上下文配置，如PID命名空间（同一个Pod中应用可以看到其它进程）、网络命名空间（同一个Pod的中的应用对相同的IP地址和端口有权限）、IPC命名空间（同一个Pod中的应用可以通过VPC或者POSIX进行通信）、UTS命名空间（同一个Pod中的应用共享一个主机名称），还共享存储、网络、以及容器运行规约等资源</p><hr><h1 id="1-设计理念"><a href="#1-设计理念" class="headerlink" title="1.设计理念"></a>1.设计理念</h1><p>Pod为什么要被引入，直接用容器管理应用程序不行吗？容器管理单个进程的应用程序时是有效的，而对于多个进程组合而成的应用程序则难于管理，如以下两种处理方式</p><h2 id="1-1-应用程序进程封装到一个容器"><a href="#1-1-应用程序进程封装到一个容器" class="headerlink" title="1.1 应用程序进程封装到一个容器"></a>1.1 应用程序进程封装到一个容器</h2><p>Docker管理的进程是pid为1的主进程，由于应用进程代替init直接占用了1的进程号，将导致孤儿进程和僵尸进程不能被正常回收。最终，容器的资源将被完全占用。所以，容器的运行方式才会推荐一个容器只干一件事情</p><h2 id="1-2-应用程序进程拆分到单个容器"><a href="#1-2-应用程序进程拆分到单个容器" class="headerlink" title="1.2 应用程序进程拆分到单个容器"></a>1.2 应用程序进程拆分到单个容器</h2><p>此时，应用程序的进程将会被调度到各个不同的节点上，这样往往会造成进程间不能正常通信的问题，从而使应用程序运行异常</p><hr><p>综上，kubernetes需要一个将容器绑定在一起作为一个基本的调度单元进行管理的结构，保证这些容器始终能被调度到同一个节点上，即为Pod设计的初衷</p><h1 id="2-Pod优点"><a href="#2-Pod优点" class="headerlink" title="2.Pod优点"></a>2.Pod优点</h1><h2 id="2-1-透明"><a href="#2-1-透明" class="headerlink" title="2.1 透明"></a>2.1 透明</h2><p>pod中的容器对基础设施可见，使得基础设施可以给容器提供服务，例如线程管理和资源监控，这为用户提供很多便利</p><h2 id="2-2-解耦"><a href="#2-2-解耦" class="headerlink" title="2.2 解耦"></a>2.2 解耦</h2><p>解耦软件依赖关系,独立的容器可以独立的进行重建和重新发布，Kubernetes甚至会在将来支持独立容器的实时更新</p><h2 id="2-3-易用"><a href="#2-3-易用" class="headerlink" title="2.3 易用"></a>2.3 易用</h2><p>用户不需要运行自己的线程管理器，也不需要关心程序的信号以及异常结束码等，只需直接操作pod即可</p><h2 id="2-4-共享"><a href="#2-4-共享" class="headerlink" title="2.4 共享"></a>2.4 共享</h2><p>Pod的中的应用均使用相同的网络命名空间及端口，并且可以通过localhost发现并沟通其他应用，每个Pod都有一个扁平化的网络命名空间下IP地址，使其可以和其他的物理机及其他的容器进行无障碍通信</p><p>Pod还定义了一系列的共享磁盘，让数据在容器重启的时候不会丢失，且可以将这些数据在Pod中的应用进行共享</p><h1 id="3-创建流程"><a href="#3-创建流程" class="headerlink" title="3.创建流程"></a>3.创建流程</h1><h2 id="3-1-集群用户提交请求"><a href="#3-1-集群用户提交请求" class="headerlink" title="3.1 集群用户提交请求"></a>3.1 集群用户提交请求</h2><p>集群用户通过客户端管理命令kubectl或UI向API Server提交pod创建请求</p><h2 id="3-2-master处理请求"><a href="#3-2-master处理请求" class="headerlink" title="3.2 master处理请求"></a>3.2 master处理请求</h2><p>主节点的API Server组件处理用户请求，生成一个包含创建信息的yaml，并将pod信息写入到etcd数据库</p><h2 id="3-3-master协调Pod数"><a href="#3-3-master协调Pod数" class="headerlink" title="3.3 master协调Pod数"></a>3.3 master协调Pod数</h2><p>kube-controller-manager的通过Watch监听机制感知到新建Pod请求事件，创建ReplicaSet控制器，进入一致性协调逻辑，确保实际运行Pod数与期望一致</p><h2 id="3-4-master调度资源"><a href="#3-4-master调度资源" class="headerlink" title="3.4 master调度资源"></a>3.4 master调度资源</h2><p>kube-scheduler通过API Server的watch监听机制感知到了etcd数据库存储的新写入的Pod信息，尝试为Pod分配Node</p><h3 id="3-4-1-即调度预选"><a href="#3-4-1-即调度预选" class="headerlink" title="3.4.1 即调度预选"></a>3.4.1 即调度预选</h3><p>kube-scheduler用一组规则过滤掉不符合要求的主机，如资源不足、标签选择不符、亲和性不符等</p><h3 id="3-4-2-即调度优选"><a href="#3-4-2-即调度优选" class="headerlink" title="3.4.2 即调度优选"></a>3.4.2 即调度优选</h3><p>kube-scheduler为主机打分，对上一步筛选出的符合要求的主机进行打分，制定出一些整体优化策略，如把同一个Replication Controller的副本分布到不同的主机上以平衡主机负载，使用最低负载的主机等</p><h3 id="3-4-3-选择主机"><a href="#3-4-3-选择主机" class="headerlink" title="3.4.3 选择主机"></a>3.4.3 选择主机</h3><p>kube-scheduler选择出打分最高的主机，根据一组相关规则进行binding操作，并将结果存储到etcd中以记录pod分配情况</p><h2 id="3-5-node执行创建操作"><a href="#3-5-node执行创建操作" class="headerlink" title="3.5 node执行创建操作"></a>3.5 node执行创建操作</h2><p>被选中的node上的kubelet定期通过watch监听到etcd数据库的boundpod对象（由master的scheduler组件调用APIServer的API在etcd中创建，描述node上绑定运行的所有pod信息），发现分配给该工作节点上运行的boundpod对象没有更新，则调用Docker API创建并启动pod内的容器</p><h1 id="4-运行状态"><a href="#4-运行状态" class="headerlink" title="4.运行状态"></a>4.运行状态</h1><p>Pod在整个生命周期过程中被系统定义为各种状态，该状态值标识了pod的运行情况</p><h2 id="4-1-重启策略"><a href="#4-1-重启策略" class="headerlink" title="4.1 重启策略"></a>4.1 重启策略</h2><p>Pod中所有容器的重启策略由restartPolicy字段来设置 ，其可能值为Always，OnFailure 和 Never，默认值为Always。restartPolicy仅指通过kubelet在同一节点上重新启动容器，通过kubelet重新启动的退出容器将以指数增加延迟（10s，20s，40s…）重新启动，上限为 5 分钟，并在成功执行 10 分钟后重置</p><p>不同类型的的控制器可以控制 Pod的重启策略</p><ul><li><p>Job，适用于一次性任务如批量计算，任务结束后Pod会被此类控制器清除，重启策略只能是OnFailure或Never</p></li><li><p>Replication Controller,ReplicaSet或Deployment，希望Pod一直运行，重启策略是Always</p></li><li><p>DaemonSet，每个节点上启动一个Pod，重启策略应该是Always</p></li></ul><h2 id="4-2-状态解析"><a href="#4-2-状态解析" class="headerlink" title="4.2 状态解析"></a>4.2 状态解析</h2><h3 id="4-2-1-Pending，挂起状态"><a href="#4-2-1-Pending，挂起状态" class="headerlink" title="4.2.1 Pending，挂起状态"></a>4.2.1 Pending，挂起状态</h3><p>API Server已经创建该Pod但还没有被调度器调度到合适的节点，或者Pod内还有一个或多个容器没有创建成功，包括正在下载的镜像的过程</p><h3 id="4-2-2-Running，运行中状态"><a href="#4-2-2-Running，运行中状态" class="headerlink" title="4.2.2 Running，运行中状态"></a>4.2.2 Running，运行中状态</h3><p>Pod已经绑定到一个节点上，所有容器均已创建，且至少有一个容器处于运行状态、正在启动或正在重启状态</p><h3 id="4-2-3-Succeeded，成功状态"><a href="#4-2-3-Succeeded，成功状态" class="headerlink" title="4.2.3 Succeeded，成功状态"></a>4.2.3 Succeeded，成功状态</h3><p>Pod内所有容器均已成功执行退出，且不会再重启</p><h3 id="4-2-4-Failed，失败状态"><a href="#4-2-4-Failed，失败状态" class="headerlink" title="4.2.4 Failed，失败状态"></a>4.2.4 Failed，失败状态</h3><p>Pod内所有容器均已退出，但至少有一个容器退出为失败状态，即容器以非0状态退出或者被系统终止</p><h3 id="4-2-5-Unknown，未知状态"><a href="#4-2-5-Unknown，未知状态" class="headerlink" title="4.2.5 Unknown，未知状态"></a>4.2.5 Unknown，未知状态</h3><p>由于某种原因无法获取该Pod的状态，可能由于网络通信不畅导致</p><h1 id="5-生命周期"><a href="#5-生命周期" class="headerlink" title="5.生命周期"></a>5.生命周期</h1><p>Pod完整的生命周期由三部分组成，即Init Container、Pod Hook、健康检查</p><h2 id="5-1-Init-Container"><a href="#5-1-Init-Container" class="headerlink" title="5.1 Init Container"></a>5.1 Init Container</h2><p>Init Container，即初始化容器，用于做初始化工作的容器，可以是一个或者多个，若有多个则按定义的顺序依次执行。由于Pod所有容器共享数据卷和Network Namespace，所以Init Container里面产生的数据可以被主容器使用到初始化容器独立于主容器之外，只有所有的初始化容器执行成功后主容器才会被启动，应用场景如下：</p><ul><li><p>服务依赖，等待其他模块Ready，如一个Web服务依赖于另外一个数据库服务，但启动这个Web服务的时候并不能保证依赖的这个数据库服务就已经成功启动，所以可能会出现一段时间内Web服务连接数据库异常问题。此时，可在Web服务Pod中定义一个InitContainer，在这个初始化容器中去检查数据库是否已经准备好了，准备好了过后初始化容器就结束退出，然后再将主容器Web服务启动起来，再去连接数据库就不会有问题</p></li><li><p>初始化配置，如集群里检测所有已经存在的成员节点，为主容器准备好集群的配置信息，这样主容器起来后就能用这个配置信息加入集群</p></li><li><p>其它场景，如将Pod注册到一个中央数据库、配置中心等</p></li></ul><h2 id="5-2-Pod-Hook"><a href="#5-2-Pod-Hook" class="headerlink" title="5.2 Pod Hook"></a>5.2 Pod Hook</h2><p>Pod Hook，即容器生命周期钩子，监听容器生命周期的特定事件，并在事件发生时执行已注册的回调函数，由kubelet发起，运行于容器中的进程启动前或者容器中的进程终止前，包含在容器的生命周期之中</p><h3 id="5-2-1-钩子分类"><a href="#5-2-1-钩子分类" class="headerlink" title="5.2.1 钩子分类"></a>5.2.1 钩子分类</h3><ul><li><p>postStart，容器启动后立即执行，由于是异步执行而没有参数传递给处理程序，无法保证一定在ENTRYPOINT之后运行。若失败则容器会被杀死，并根据RestartPolicy决定是否重启。主要用于资源部署、环境准备等，不过需要注意的是如果钩子花费太长时间以至于不能运行或挂起，容器将不能达到running状态</p></li><li><p>preStop，容器停止前执行，常用于资源清理、优雅关闭应用程序、通知其他系统等，若失败则容器被杀死。若在执行期间挂起，则Pod将停留在running状态并且永远不会达到failed状态</p></li></ul><h3 id="5-2-2-回调函数"><a href="#5-2-2-回调函数" class="headerlink" title="5.2.2 回调函数"></a>5.2.2 回调函数</h3><ul><li><p>exec，在容器内执行特定的命令，将消耗容器的计算资源</p></li><li><p>httpGet，向指定URL发起GET请求用户请求删除含有Pod的资源对象时（如Deployment等），为了让应用程序优雅关闭（完成正在处理的请求再关闭），提供了两种信息通知：默认，通知node执行docker stop命令，docker先向容器中PID为1的进程发送系统信号SIGTERM，然后等待容器中的应用程序终止执行，若等待时间达到设定的超时时间，或者默认超时时间（30s），则继续发送SIGKILL的系统信号强行kill 掉进程；使用Pod生命周期（利用PreStop回调函数），在发送终止信号之前执行默认所有的优雅退出时间都在30秒内，kubectl delete 命令支持–grace-period&#x3D;<seconds>选项，允许用户自定义以覆盖默认值，为0代表强制删除 pod。强制删除pod是从集群状态还有etcd里立刻删除这个pod，APIServer不会等待来自Pod所在节点上kubelet的确认信息：pod已经被终止。在API里pod会被立刻删除，在节点上pods被设置成立刻终止后，在强行杀掉前还会有一个很小的宽限期</p></li></ul><h2 id="5-3-健康检查"><a href="#5-3-健康检查" class="headerlink" title="5.3 健康检查"></a>5.3 健康检查</h2><p>Kubernetes提供了两种探针（Probe，支持exec、tcp和httpGet方式）来探测容器的状态，以保障容器在部署后确实处在正常运行状态</p><ul><li><p>LivenessProbe，即存活探针，探测应用是否处于存活状态，否则kubelet将杀掉该容器，并根据容器的重启策略做相应的处理。若容器不包含LivenessProbe探针，则kubelet认为该容器的LivenessProbe探针返回的值永远都是success</p></li><li><p>ReadinessProbe，即可读性探针，探测容器是否处于可以接收请求的完成(Ready状态)，否则Pod的状态将被修改，Endpoint Controller将从Service的Endpoint中删除包含该容器所在的Pod的Endpoint</p></li></ul><hr><h1 id="1-创建pod"><a href="#1-创建pod" class="headerlink" title="1.创建pod"></a>1.创建pod</h1><h1 id="1-1-创建资源文件"><a href="#1-1-创建资源文件" class="headerlink" title="1.1 创建资源文件"></a>1.1 创建资源文件</h1><pre><code class="hljs">vi nginx.yamlapiVersion: v1kind: Podmetadata:  name: nginx  labels:    app: nginxspec:  containers:  - name: nginx    image: nginx</code></pre><h2 id="1-2-部署Pod"><a href="#1-2-部署Pod" class="headerlink" title="1.2 部署Pod"></a>1.2 部署Pod</h2><pre><code class="hljs">kubectl create -f nginx.yaml</code></pre><h1 id="3-查看Pod"><a href="#3-查看Pod" class="headerlink" title="3.查看Pod"></a>3.查看Pod</h1><pre><code class="hljs">kubectl get pods -o wide</code></pre><h1 id="4-查看pod详情"><a href="#4-查看pod详情" class="headerlink" title="4.查看pod详情"></a>4.查看pod详情</h1><pre><code class="hljs">kubectl describe pod nginx</code></pre><h1 id="5-查看pod日志"><a href="#5-查看pod日志" class="headerlink" title="5.查看pod日志"></a>5.查看pod日志</h1><pre><code class="hljs">kubectl logs nginx</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><p>- </p>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes资源清单详解</title>
    <link href="/linux/KubernetesYaml/"/>
    <url>/linux/KubernetesYaml/</url>
    
    <content type="html"><![CDATA[<p>Kubernetes对象通常通过定义资源清单进行创建，资源清单描述了该对象的属性，决定了用户对其在生命内的期望状态。资源清单有许多可供选择的表示对象属性的字段，必须进行配置的字段的三个，即apiVersion，创建对象所用Kubernetes API版本；kind，创建对象所属类别；metadata，唯一标识对象的元数据，包括一个name、UID和namespace</p><hr><h1 id="1-属性字段"><a href="#1-属性字段" class="headerlink" title="1.属性字段"></a>1.属性字段</h1><pre><code class="hljs">apiVersion，字符串型字段，Kubernetes API版本，基本为V1，查询命令为：       kubectl api-versionskind，定义对象的资源类型与角色，比如Podmetadata，对象型字段，元数据对象，唯一标识对象的元数据  name，字符串型字段，元数据对象的名字，如定义Pod的名字  namespace，字符串型字段，元数据对象的命名空间，默认为spec，对象型字段，对象的详细描述  containers，列表型字段，定义对象的容器列表  name，字符串型字段，定义容器的名字  image，字符串型字段，定义容器所属镜像的名称  imagePullPolicy，字符串型字段，定义镜像拉取策略，可选值有三个：Always，每次都尝试重新拉取镜像；Never；仅使用本地镜像；IfNotPresent，如果本地有镜像就使用本地镜像，没有则拉取镜像，默认为Always    command，列表型字段，指定容器启动命令，可指定多个，若不指定则使用镜像打包时的启动命令    args，列表型字段，指定容器启动命令参数，可指定多个    workingDir，字符串型字段，指定容器的工作目录    volumeMounts，列表型字段，指定容器内部的存储卷配置      name，字符串型字段，指定被容器挂载的存储卷的名称      mountPath，字符串型字段，指定被容器挂载的存储卷的路径      readOnly，布尔型字段，指定存储卷路径的读写模式，true或false，默认为读写模式    ports，列表型字段，指定容器需要用到的端口列表      name，字符串型字段，指定端口名称      containerPort，字符串型字段，指定容器需要监听的端口号      hostPort，字符串型字段，指定容器所在主机需要监听的端口号，默认同containerPort，若定义了该字段，则同台主机无法启动该容器的相同副本，会造成端口号冲突      protocol，字符串型字段，指定端口协议，支持TCP和UDP,默认值为TCP    env，列表型字段，指定容器运行前需要设置的环境变量列表      name，字符串型字段，指定环境变量名称      value，字符串型字段，指定环境变量值    resources，对象型字段，指定资源限制和资源请求      limits，对象型字段，指定设置容器运行时资源的运行上限        cpu，字符串型字段，指定cpu的限制，单位为core数，用于docker run --cpu-shares参数        memory，字符串型字段，指定mem内存的限制      requests，对象型字段，指定容器启动和调度时的限制设置        cpu，字符串型字段，cpu请求，单位为core数，容器启动时初始化可用数量        memory，字符串型字段，内存请求，容器启动的初始化可用数量  restartPolicy，字符串型字段，定义Pod的重启策略，可选值有三个：Always，Pod一旦终止运行则无论任何原因kubelet服务都将尝试重启；OnFailure，Pod以非零码（异常）终止时kubelet才会尝试重启，若是正常结束（退出码为0）则kubelet将不会重启；Never，kubelet只将退出码报告给master，不会进行重启  nodeSelector，对象型字段，定义Node的Label过滤标签，以key:value格式指定  imagePullSecrets，对象型字段，定义拉取镜像时使用的secret，以name:secretkey格式指定  hostNetwork，布尔型字段，定义是否使用主机网络模式，默认值为false，设   置true表示使用宿主机网络， 不使用docker网桥，将无法在同一台宿主机上启动第二个副本</code></pre><h1 id="2-yaml基本语法"><a href="#2-yaml基本语法" class="headerlink" title="2.yaml基本语法"></a>2.yaml基本语法</h1><ul><li>缩进时不允许使用tab，只能用空格</li><li>缩进的数目不固定，可任意选择，但全局缩进数目要保持统一，即相同层级的元素左对齐</li><li>注释使用 #</li></ul><hr><h1 id="3-资源清单基本格式"><a href="#3-资源清单基本格式" class="headerlink" title="3.资源清单基本格式"></a>3.资源清单基本格式</h1><pre><code class="hljs">apiVersion: apps/v1# 声明一个Deployment资源对象kind: Deploymentmetadata:  name: nginx-deployment  spec:  # 通过标签选择被控制的pod  selector:    matchLabels:      app: nginx-servers  # 声明pod副本数  replicas: 2  # 定义pod  template:metadata:  # 给pod打上标签  labels:    app: nginx-serversspec:  containers:    # 声明容器名称、镜像及镜像拉取策略    - name: nginx      image: sword618/nginx      imagePullPolicy: IfNotPresent      ports:        - containerPort: 80          name: http-nginx          protocol: TCP      # 声明容器资源限制      resources:          limits:            cpu: 500m            memory: 200Mi          requests:            cpu: 200m            memory: 100Mi# 声明资源对象分割符          ---apiVersion: v1# 声明一个Service资源对象kind: Servicemetadata:  name: nginx-servicespec:  # 声明service的类型为NodePort  type: NodePort  # 声明service的会话保持机制，None为默认值，表示随机调度；ClientIP表示来自于同一个客户端的请求调度到同一pod  sessionAffinity: ClientIP  # 通过标签选择被控制的pod  selector:    app: nginx-servers  ports:  # 声明service集群内部访问端口  - port: 80    # 声明pod容器端口    targetPort: 80    # 声明绑定到node的端口，如不指定则随机分配，范围为30000-32767    nodePort: 32000</code></pre>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes核心概念详解</title>
    <link href="/linux/KubernetesResource/"/>
    <url>/linux/KubernetesResource/</url>
    
    <content type="html"><![CDATA[<p>Kubernetes集群资源分为三个级别，即命名空间级别、集群级别和元数据级别</p><h1 id="资源分类"><a href="#资源分类" class="headerlink" title="资源分类"></a>资源分类</h1><h2 id="1-命名空间级别"><a href="#1-命名空间级别" class="headerlink" title="1.命名空间级别"></a>1.命名空间级别</h2><p>该类资源从属于某个命名空间，创建、查询、修改及删除时需进行命名空间的指定，若不指定则从属于默认的命名空间为default</p><ul><li><p>WorkLoad，工作负载型资源，如Pod、ReplicaSet、Deployment、StatefulSet、DaemonSet、Job、CronJob</p></li><li><p>ServiceDiscovery LoadBalance，服务发现及负载均衡型资，如Service、Ingress</p></li><li><p>配置与存储型资源，如Volume、CSI</p></li><li><p>特殊类型的存储卷，如ConfigMap、Secret、DownwardAPI</p></li></ul><h2 id="2-集群级别"><a href="#2-集群级别" class="headerlink" title="2.集群级别"></a>2.集群级别</h2><p>该类资源不属于任何命名空间，所有命名空间下都可进行操作，无需进行指定，如Namespace、Node、Role、ClusterRole、RoleBinding、ClusterRoleBinding</p><h2 id="3-元数据级别"><a href="#3-元数据级别" class="headerlink" title="3.元数据级别"></a>3.元数据级别</h2><p>介于上述两者之间，通过指标进行相应操作，如HPA（通过cpu利用率进行平滑扩展）、PodTemplate（pod模板）、LimitRange（资源限制）</p><h1 id="1-Cluster"><a href="#1-Cluster" class="headerlink" title="1.Cluster"></a>1.Cluster</h1><p>cluster，即集群，计算、存储和网络资源的集合，由一系列物理机、虚拟机和其他基础资源组成，由这些资源运行各种基于容器的应用</p><h1 id="2-Master"><a href="#2-Master" class="headerlink" title="2.Master"></a>2.Master</h1><p>master，即主节点，负责整个集群的管理、控制与调度，执行所有的控制命令，为实现高可用可运行多个master</p><h1 id="3-Node"><a href="#3-Node" class="headerlink" title="3.Node"></a>3.Node</h1><p>node，即工作节点，负责运行容器应用，由master管理，监控并汇报容器的状态，同时根据master的要求管理容器的生命周期，是真正用来承载业务的资源</p><h1 id="4-Resource"><a href="#4-Resource" class="headerlink" title="4.Resource"></a>4.Resource</h1><p>resource，即资源，是Kubernetes API中的一个端点，其中存储的是某个类别的API对象的一个集合。实际上，Kubernetes中的所有内容都被抽象为资源，如Pod、Service、Node等都是资源。万物皆对象是Kubernetes的理念，资源即对象，对象就是资源的实例化，如名为nginx的pod、名为node03的node等</p><h1 id="5-Objecte"><a href="#5-Objecte" class="headerlink" title="5.Objecte"></a>5.Objecte</h1><p>objecte，即对象，又称为API对象，是Kubernetes集群中的持久化的实体和管理操作单元，用于表示整个集群的状态，这些状态包括节点上运行了什么样的应用（容器化的）、应用可以使用的资源、应用的管理策略。objecte由API服务器进行操作，如创建、修改、查询、删除等，是一种意图的记录，一旦被成功创建，就意味着将持续工作以确保对象存在。Pod、Deployment、RC、RS、Service、Volume、PV等，都属于kubernetes的对象</p><p>对象创建的过程也即是资源实例化的过程，实质上就是kubectl调用Master组件api-server的API接口对定义了对象属性的资源清单文件进行解析、执行的过程。资源清单文件通常为.yaml格式，kubectl发起API请求时，将文件包含的信息转换成JSON格式进行执行</p><p>API对象有三类属性：元数据metadata、规范spec和状态status</p><ul><li>metadata，元数据，用于标识API对象，每个对象都至少有3个元数据：namespace、name和uid，此外还有标签（label）用于标识和匹配不同的对象</li><li>spec，规范，用于描述对象的期望状态，由用户在创建对象时指定，如用户RC用于设置Pod的期望副本数</li><li>status，状态，用于标识对象当前的状态，由Master时刻监控与管理，以保障与期望状态相匹配</li></ul><h1 id="6-NameSpace"><a href="#6-NameSpace" class="headerlink" title="6.NameSpace"></a>6.NameSpace</h1><p>namespace，即命名空间，是对一组资源和对象的抽象集合，用于实现多租户的资源隔离。常见的pods，services, replication controllers和deployments等资源都属于某一个namespace，而node、persistentVolumes等则不属于任何namespace。namespace通过将集群内部的资源对象聚合到不同的Namespace中，形成逻辑上分组的不同项目、小组或者用户组，便于不同的分组在共享整个集群的资源的同时还能被分别管理</p><p>Kubernetes集群启动后，会创建一个名为default Namespace，若资源对象被创建时不特别指明，则都将被系统创建到名为default</p><p>namespace有两种状态，即Active和Terminating，后者表示正在被删除的过程中，删除完成后其所有的资源都将被删除，default和kube-system命名空间不可删除</p><h1 id="7-Pod"><a href="#7-Pod" class="headerlink" title="7.Pod"></a>7.Pod</h1><p>pod，最小调度及资源单元，包含一个或者多个容器以及存储、网络等各个容器共享的资源，作为一个整体被master调度到一个node上运行，逻辑上表示某种应用的一个实例。每个Pod都有一个特殊的被称为根容器的Pause容器，其对应的镜像属于Kubernetes平台的一部分。根容器的状态代表整个容器组的状态，整个pod内部的多个容器共享pod IP及其挂载的Volume。pod可以被看作是一个豌豆荚，容器则是这个豆荚里的豆子</p><h1 id="8-Label"><a href="#8-Label" class="headerlink" title="8.Label"></a>8.Label</h1><p>label，即标签，是一个由用户自定义的key&#x3D;value的键值对，被绑定到各种资源对象上，例如Node、Pod、Service、RC等，用于资源对象的分组标识与管理。一个资源对象可以定义任意数量的Label，同一个Label也可以被添加到任意数量的资源对象上去。通过这种方式，Controller与Pod之间就建立起一种对应关系，便于Controller对pod下达指令。Label通常在资源对象定义时确认，也可以在对象创建后动态添加或者删除label只对用户而言是有意义的，对内核系统没有直接意义</p><p>label Selector，即标签选择器，用于查询和筛选有某些Label的资源对象，类似于SQL的对象查询机制。label选择器可以由多个必须条件组成，由逗号分隔。在多个必须条件指定的情况下，所有的条件都必须满足，因而逗号起着AND逻辑运算符的作用。label表达式分为两类：</p><ul><li>Equality-based，基于等式</li></ul><p>基于相等性或者不相等性的条件，允许用label的键或者值进行过滤。匹配的对象必须满足所有指定的label约束，尽管他们可能也有额外的label。基于等式的表达式有三种运算符，“&#x3D;”，“&#x3D;&#x3D;”和“!&#x3D;”。前两种代表相等性，是同义运算符，后一种代表非相等性name &#x3D; redis-slave:，匹配所有具有标签name&#x3D;redis-slave的资源对象env !&#x3D; production，匹配所有不具有标签env&#x3D;production的资源对象</p><ul><li>Set-based，基于集合</li></ul><p>基于集合的label条件允许用一组值来过滤键，有三种操作符:in、notin和 exists(仅针对于key符号)，name in (redis-master,redis-slave)，匹配所有具有name&#x3D;redis-master或者name&#x3D;redis-slave的资源对象；name not in (php-fronted)，匹配所有不具有标签name&#x3D;php-fronted的资源对象</p><h1 id="9-Controller"><a href="#9-Controller" class="headerlink" title="9.Controller"></a>9.Controller</h1><p>controller，即控制器，用于创建、管理、扩容、复制、自愈、调度pod的资源。控制器定义了pod的部署特性，如副本数，可供调度的node等。controller通过API Server提供的接口实时监控整个集群每个资源对象的状态，当发生各种故障导致系统状态发生变化时，会尝试将系统状态修复到正常状态</p><h2 id="9-1-Replication-Controller"><a href="#9-1-Replication-Controller" class="headerlink" title="9.1 Replication Controller"></a>9.1 Replication Controller</h2><p>Replication Controller，即副本控制器，简称为RC，用于实现pod的扩容缩容，解决分布式应用的负载均衡及高可用问题，根据整体负载情况进行动态伸缩</p><h2 id="9-2-Replica-Set"><a href="#9-2-Replica-Set" class="headerlink" title="9.2 Replica Set"></a>9.2 Replica Set</h2><p>Replica Set，即副本集，简称为RS，是RC的下一代，两者都能确保集群在任何时间运行指定数量的Pod副本，只在标签选择支持上有所不同，RS支持集合方式的选择，RC仅支持相等方式的选择</p><p>Kubernetes通常不直接使用副本控制器，而是由更高级的deployment来管理，包括协调pod创建、删除和更新，从而维持pod副本数</p><h2 id="9-3-Deployment"><a href="#9-3-Deployment" class="headerlink" title="9.3 Deployment"></a>9.3 Deployment</h2><p>deployment，即部署，用于管理pod和rc，最常用的controller。其主要应用场景为：创建、监测pod和rs、滚动升级和回滚应用、扩容与缩容以及暂停、恢复、更新Deployment</p><p>deployment实际上是对rs和pod的管理，先创建rs，由RS创建pod，即rs控制pod的数量，deployment管理、创建rs，同时控制pod应用的升级、回滚</p><h2 id="9-4-StatefulSet"><a href="#9-4-StatefulSet" class="headerlink" title="9.4 StatefulSet"></a>9.4 StatefulSet</h2><p>statefulset，即有状态服务集，用于解决有状态服务的问题，如复杂的中间件集群MySQL集群、MongoDB集群、Kafka集群、ZooKeeper集群等。statefuleset确保pod的每个副本在整个生命周期中名称保持不变，其他控制器都是无状态的，不提供这种功能</p><h3 id="9-4-1-组成部分"><a href="#9-4-1-组成部分" class="headerlink" title="9.4.1 组成部分"></a>9.4.1 组成部分</h3><p>Headless Service，用于定义网络标志（DNS domain），为pod进行编号volumeClaimTemplates，用于创建PersistentVolumes，挂载到Headless Service StatefulSet，用于定义具体的应用</p><p>StatefulSet每个Pod的DNS格式为statefulSetName-{0..N-1}.serviceName.namespace.svc.cluster.local，其<br>中，serviceName为Headless Service的名字，0..N-1为Pod所在的序号，从0开始到N-1，statefulSetName为StatefulSet<br>的名字，namespace为服务所在的namespace，Headless Servic和StatefulSet必须在相同的namespace，.cluster.local<br>为Cluster Domain</p><h3 id="9-4-2-应用场景"><a href="#9-4-2-应用场景" class="headerlink" title="9.4.2 应用场景"></a>9.4.2 应用场景</h3><ul><li>稳定的持久化存储，即Pod重新调度后还是能访问到相同的持久化数据，基于PVC来实现</li><li>稳定的网络标志，即Pod重新调度后其PodName和HostName不变，基于Headless Service（没有Cluster IP的Service）来实现</li><li>有序部署，有序扩展，即Pod是有顺序的，在部署或者扩展的时候要依据定义的顺序依次依次进行，即从0到N-1，在下一个Pod运行之前所有之前的Pod必须都是Running和Ready状态），基于init containers来实现</li><li>有序收缩，有序删除（从N-1到0）</li><li>当某个pod发生故障需要删除并重新启动时，pod的名称会发生变化，同时statefulset会保证副本按照固定的顺序启动、更新或者删除</li></ul><h2 id="9-5-DaemonSet"><a href="#9-5-DaemonSet" class="headerlink" title="9.5 DaemonSet"></a>9.5 DaemonSet</h2><p>daemonset，即后台守护服务集，能够让每个Node节点运行同一个pod，当节点从集群中被移除后，该节点上的Pod也会被移除，用于部署一些集群的日志采集、监控或者其他系统管理应用</p><h3 id="9-5-1-调度策略"><a href="#9-5-1-调度策略" class="headerlink" title="9.5.1 调度策略"></a>9.5.1 调度策略</h3><p>默认情况下，Pod被分配到具体哪一台Node节点运行是由Scheduler监听ApiServer，查询出还未分配的Node的Pod，再根据调度策略为这些Pod进行调度。但daemonset创建的pod却有些不同，其会忽略Node的unschedulable状态，且即使Scheduler还未启动，DaemonSet Controller仍然能够创建并运行Pod。为daemonset指定Node节点有三种方式；nodeSelector，只调度到匹配指定label的Node；nodeAffinity，功能更丰富的Node选择器，支持集合操作；podAffinity，调度到满足条件的Pod所在的Node</p><h3 id="9-5-2-应用场景"><a href="#9-5-2-应用场景" class="headerlink" title="9.5.2 应用场景"></a>9.5.2 应用场景</h3><ul><li>日志收集守护程序，如fluentd或logstash，在每个节点运行容器</li><li>节点监视守护进程，如prometheus监控集群，可以在每个节点上运行一个node-exporter进程来收集监控节点的信息</li><li>系统程序与集群存储守护程序，如glusterd、ceph要部署在每个节点上提供持久性存储，集群程序kube-proxy, kube-dns等</li></ul><h2 id="9-6-Job"><a href="#9-6-Job" class="headerlink" title="9.6 Job"></a>9.6 Job</h2><p>job，即任务，用于批量处理短暂的一次性任务，保障批处理任务的一个或多个Pod成功结束，由Job Controller负责根据Job Spec进行创建，并持续监控其状态，直至成功结束。其创建的pod任务执行完后就将自动退出，集群也不会再重新将其唤醒。只有job执行完毕后，STATUS状态才为Completed,没有执行完毕的状态为Running，其RestartPolicy (pod重启策略)仅支持Never和OnFailure两种，不支持Always</p><h3 id="9-6-1-job分类"><a href="#9-6-1-job分类" class="headerlink" title="9.6.1 job分类"></a>9.6.1 job分类</h3><ul><li>一次性job，通常创建一个Pod直至其成功结束，适用于数据库迁移场景，此时Spec设为：.spec.completions&#x3D;1，.spec.Parallelism&#x3D;1</li><li>固定结束次数的Job，依次创建一个Pod运行直至completions个成功结束，适用于处理工作队列的场景，此时Spec设为：.spec.completions&#x3D;2+，.spec.Parallelism&#x3D;1</li><li>并行Job，创建一个或多个Pod直至有一个成功结束，适用于多个Pod同时处理工作队列的场景，此时Spec设为：.spec.completions&#x3D;1</li><li>固定结束次数的并行Job：依次创建多个Pod运行直至completions个成功结束，适用于多个Pod同时处理工作队列的场景，此时Spec设为：.spec.completions&#x3D;2+，.spec.Parallelism&#x3D;2+</li></ul><h2 id="9-7-CronJob"><a href="#9-7-CronJob" class="headerlink" title="9.7 CronJob"></a>9.7 CronJob</h2><p>cronjob，即定时任务，类似于Linux系统的crontab，在指定的时间周期运行指定的任务</p><p>CronJob Spec格式：</p><p>.spec.schedule，指定任务运行周期，格式同Cron<br>.spec.jobTemplate，指定需要运行的任务，格式同Job<br>.spec.startingDeadlineSeconds，指定任务开始的截止期限<br>.spec.concurrencyPolicy，指定任务的并发策略，支持Allow、Forbid和Replace</p><h1 id="10-Volume"><a href="#10-Volume" class="headerlink" title="10.Volume"></a>10.Volume</h1><p>volume，即数据卷，用于pod数据存储与共享存储，被挂载到Pod中一个或者多个容器的指定路径下，支持多种后端存储方式，如本地存储，分布式存储，云存储等，解决了容器文件系统临时性的问题</p><h1 id="11-Persistent-Volume"><a href="#11-Persistent-Volume" class="headerlink" title="11.Persistent Volume"></a>11.Persistent Volume</h1><p>Persistent Volume，即持久化数据卷，简称PV，是对底层的共享存储的一种抽象，由管理员进行创建和配置，有多种类型，如Ceph、GlusterFS、NFS等，都是通过插件机制完成与共享存储的对接。PV是集群的存储资源，是一种静态的存在，其生命周期独立于使用PV的任何单个pod</p><p>Persisten Volume ClaimPVC，即持久化卷声明，简称PVC，是用户存储的一种声明，也就是对PV的引用。类似于pod，前者消耗节点资源，后者消耗PV资源。Pod请求CPU、内存（计算资源），PVC请求特定的存储空间和访问模式（存储资源）</p><p>PV是群集中的资源，PVC是对这些资源的请求，此外还充当对资源的检查。PV和PVC之间的相互作用遵循以下生命周期：Provisioning ——-&gt; Binding ——–&gt;Using——&gt;Releasing——&gt;Recycling</p><p>1.Provisioning，即供应准备，通过集群外的存储系统或者云平台来提供存储持久化支持，分为两类：</p><ul><li>Static，静态提供，集群管理员创建多个PV，且携带可供集群用户使用的真实详细的存储信息，存在于Kubernetes API中，可用于消费</li><li>Dynamic，动态提供，当管理员创建的静态PV都不匹配用户的PVC时，集群会尝试为PVC动态配置卷。此配置基于StorageClasses：PVC必须请求一个类，并且管理员必须已创建并配置该类才能进行动态配置。此外，还要求该类的声明有效地为自己禁用动态配置</li></ul><p>2.Binding，即绑定，用户创建pvc并指定需要的资源和访问模式，在此之前pvc会保持未绑定状态</p><p>3.Using，即使用，用户可在pod中像volume一样使用pvc</p><p>4.Releasing，即释放，用户删除pvc来回收存储资源，pv将变成released状态。由于还保留着之前的数据，这些数据需要根据不同的策略来处理，否则这些存储资源无法被其他pvc使用。</p><p>5.Recycling，即回收，pv可以设置三类回收策略：</p><ul><li>Retain，保留策略，允许人工处理保留的数据</li><li>Delete，删除策略，将删除pv和外部关联的存储资源，需要插件支持</li><li>Recycle，回收策略，将执行清除操作，之后可以被新的pvc使用，需要插件支持</li></ul><h1 id="12-ConfigMap"><a href="#12-ConfigMap" class="headerlink" title="12.ConfigMap"></a>12.ConfigMap</h1><p>configMap，即配置项，用于存储非机密性的配置数据，如注册中心地址、数据库地址、nginx地址等，解决了应用程序配置信息和容器镜像之间的依赖关系，分离了配置数据和应用程序代码，方便了应用程序配置信息的更新<br>configMap存储的是一个键值对，可以被pod引用，或者用于为contaroller一样的系统组件存储配置数据，类似于Linux系统中的&#x2F;etc目录，用来存储配置文件的目录。具体作用有三种：设置环境变量；设置容器命令行参数；创建数据卷config文件</p><h1 id="13-Secret"><a href="#13-Secret" class="headerlink" title="13.Secret"></a>13.Secret</h1><p>secret，即机密存储，用于存储敏感信息，如密码、OAuth令牌、SSH密钥等，可以被pod引用，方式有三种：作为挂载到一个或多个容器上的卷中的文件；作为容器的环境变量；kubelet在为Pod拉取镜像时使用</p><h1 id="14-Service"><a href="#14-Service" class="headerlink" title="14.Service"></a>14.Service</h1><p>service，即服务，定义了一组Pod访问策略的抽象。service创建成功后会分配到一个访问入口地址，前端pod通过入口地址将请求发给service，service通过Label Selector关联到后端pod，然后kube-proxy进程负责将请求转发到后端pod群，并在内部实现服务的负载均衡与会话保持机制。此过程中，RC用于保障service的能力与质量。Service不是共用一个负载均衡器的IP地址，而是每个Service分配了一个全局唯一的虚拟IP地址，这样service就成为了一个通信节点，避免了因pod的销毁或重建引发的IP地址变更的问题</p><h1 id="15-Ingress"><a href="#15-Ingress" class="headerlink" title="15.Ingress"></a>15.Ingress</h1><p>ingress，即外部入口，定义了一组外部请求进入集群的路由规则的集合，用于给service提供集群外部访问的URL、负载均衡、SSL终止、HTTP路由ingress由两部分组成，即Ingress controller和Ingress服务。Ingress controller监听Ingress和service的变化，根据规则配置负载均衡并提供访问入口与其他作为kube-controller-manager二进制文件的一部分随集群启动而启动运行的控制器不同，Ingress controller需要用户自定义创建，业内多种反向代理项目都有支持，如Nginx、HAProxy、Envoy、Traefik等。ingress反向代理service，作为service的负载均衡器，集群外部请求再由servi转发到后端的pod集群进行处理</p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://kubernetes.io/zh">https://kubernetes.io/zh</a></li><li><a href="https://www.kubernetes.org.cn/">https://www.kubernetes.org.cn</a></li><li><a href="https://www.cnblogs.com/fanqisoft/p/11533843.html">https://www.cnblogs.com/fanqisoft/p/11533843.html</a></li><li><a href="https://www.cnblogs.com/life-of-coding/p/12156685.html">https://www.cnblogs.com/life-of-coding/p/12156685.html</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes集群部署</title>
    <link href="/linux/Kubernetes/"/>
    <url>/linux/Kubernetes/</url>
    
    <content type="html"><![CDATA[<p>Kubernetes，意为舵手、飞行员，来源于希腊字母，是Google开源的工业级容器集群管理、编排平台，负责基于容器的应用部署、应用伸缩及应用管理，将后面8个字母缩写为K8s</p><h1 id="逻辑架构"><a href="#逻辑架构" class="headerlink" title="逻辑架构"></a>逻辑架构</h1><p>Kubernetes设计理念类似于Linux的分层架构，逻辑架构由内而外分为核心层、应用层、管理层、接口层及生态系统</p><ul><li>核心层，Kubernetes最核心的功能，对外提供API构建高层的应用，对内提供插件式应用执行环境</li><li>应用层，部署（无状态应用、有状态应用、批处理任务、集群应用等）和路由（服务发现、DNS解析等）</li><li>管理层，系统度量（如基础设施、容器和网络的度量），自动化（如自动扩展、动态Provision等）以及策略管理（RBAC、Quota、PSP、NetworkPolicy等）</li><li>接口层，kubectl命令行工具、客户端SDK及集群组件</li><li>生态系统，在接口层之上的庞大容器集群管理调度的生态系统，可以划分为两类，即集群外部系统和集群内部系统，前者包含日志、监控、配置管理、CI&#x2F;CD、Workflow、FaaS、OTS应用、ChatOps等，后者包含CRI、CNI、CVI、镜像仓库、Cloud Provider、集群自身的配置和管理等</li></ul><h1 id="功能组件"><a href="#功能组件" class="headerlink" title="功能组件"></a>功能组件</h1><p>Kubernetes集群是典型的C&#x2F;S二层架构，Master（主节点）作为中央控制节点，通过API从CLI（命令行界面）或UI（用户界面）接收输入操作，然后下发给相应的Node（工作节点）进行最终的执行</p><h2 id="1-Master"><a href="#1-Master" class="headerlink" title="1.Master"></a>1.Master</h2><p>主节点，控制集群工作，由四个核心组件组成，即API Server、Controller Manage、Scheduler及ETCD</p><h3 id="1-1-API-Server"><a href="#1-1-API-Server" class="headerlink" title="1.1 API Server"></a>1.1 API Server</h3><p>集群内所有API资源对象的操作接口，用户与集群进行交互的入口，负责集群各个功能组件之间的通信，整个系统的数据总线和数据中心，具体实现方式是其余组件通过定时调用API Server的REST接口（get、list和watch）写入或读取集群所有对象所要维持的状态。此外，还提供认证、授权、访问控制、API注册和发现等功能</p><h3 id="1-2-Controller-Manage"><a href="#1-2-Controller-Manage" class="headerlink" title="1.2 Controller Manage"></a>1.2 Controller Manage</h3><p>控制管理器，是集群状态自动化维护的核心，管理集群各种控制器的运行，确保集群按预期目标工作，如故障检测、自动扩展、滚动更新等。集群的各类Controller通过API Server提供的接口实时监控集群中特定资源的状态变化，当发生故障导致某资源对象的状态发生变化时，将会尝试将其状态调整为期望的状态</p><h3 id="1-3-Scheduler"><a href="#1-3-Scheduler" class="headerlink" title="1.3 Scheduler"></a>1.3 Scheduler</h3><p>调度器，用于资源的调度，接收来自API Server的请求，并将其分配给运行状况良好的节点。scheduler会对节点的质量进行排名，将Pod部署到最适合的节点。若没有合适的节点，则将Pod置于挂起状态，直到出现合适的节点</p><h3 id="1-4-ETCD"><a href="#1-4-ETCD" class="headerlink" title="1.4 ETCD"></a>1.4 ETCD</h3><p>键值存储系统，用于保存整个集群的状态，分布式存储所有集群数据的高可用数据库。存储整个集群的配置和状态，主节点从其读取节点和容器的状态参数，维护节点间的服务发现和配置共享</p><h3 id="1-5-kubectl"><a href="#1-5-kubectl" class="headerlink" title="1.5  kubectl"></a>1.5  kubectl</h3><p>客户端命令行工具，将接受的命令格式化后发送给API Server，作为整个系统的操作入口</p><h2 id="2-Node"><a href="#2-Node" class="headerlink" title="2.Node"></a>2.Node</h2><p>工作节点，运行业务负载，由三个核心组件组成，即kubelet、kube-proxy、container runtime</p><h3 id="2-1-kubelet"><a href="#2-1-kubelet" class="headerlink" title="2.1 kubelet"></a>2.1 kubelet</h3><p>内部代理，用于维护容器的生命周期，接收API Server发送来的任务并执行，如Pod、容器的创建与销毁，运行在每个工作节点，同时监视Pod状态，若不能正常运行，则向Master反馈，Master再基于该信息决定如何分配任务和资源以达到所需状态。此外，还负责Volume（CVI）和网络（CNI）的管理</p><h3 id="2-2-kube-proxy"><a href="#2-2-kube-proxy" class="headerlink" title="2.2 kube-proxy"></a>2.2 kube-proxy</h3><p>网络代理，用于为Service提供集群内部的服务发现和负载均衡，即为Service提供代理以供外部访问，运行在每个工作节点。Pod之间的访问请求会经过本机Proxy做转发，从而实现本地iptables和规则，以处理路由和流量负载均衡</p><h3 id="2-3-Container-runtime"><a href="#2-3-Container-runtime" class="headerlink" title="2.3 Container runtime"></a>2.3 Container runtime</h3><p>即容器，用于镜像管理及Pod、容器的真正运行（CRI），如docker等，运行在每个工作节点</p><h2 id="3-非核心组件"><a href="#3-非核心组件" class="headerlink" title="3.非核心组件"></a>3.非核心组件</h2><ul><li>kube-dns，用于整个集群的DNS服务</li><li>Ingress Controller，用于为服务提供外网入口</li><li>Heapster，用于提供资源监控</li><li>Dashboard，用于提供GUI</li><li>Federation，用于提供跨可用区的集群</li><li>Fluentd-elasticsearch，用于提供集群日志采集、存储与查询</li></ul><h1 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h1><ul><li>172.16.100.100  master</li><li>172.16.100.180  node01</li><li>172.16.100.200  node02</li></ul><hr><h1 id="1-系统环境配置"><a href="#1-系统环境配置" class="headerlink" title="1.系统环境配置"></a>1.系统环境配置</h1><h2 id="1-1-配置hosts"><a href="#1-1-配置hosts" class="headerlink" title="1.1 配置hosts"></a>1.1 配置hosts</h2><pre><code class="hljs">vi /etc/hosts172.16.100.100  master172.16.100.180  node01172.16.100.200  node02</code></pre><h2 id="1-2-关闭防火墙"><a href="#1-2-关闭防火墙" class="headerlink" title="1.2 关闭防火墙"></a>1.2 关闭防火墙</h2><h2 id="1-3-禁用selinux"><a href="#1-3-禁用selinux" class="headerlink" title="1.3 禁用selinux"></a>1.3 禁用selinux</h2><h2 id="1-4-关闭swap"><a href="#1-4-关闭swap" class="headerlink" title="1.4 关闭swap"></a>1.4 关闭swap</h2><pre><code class="hljs">sudo swapoff -a &amp;&amp; sudo sed -ri &#39;s/.*swap.*/#&amp;/&#39; /etc/fstab</code></pre><h2 id="1-5-配置系统内核参数"><a href="#1-5-配置系统内核参数" class="headerlink" title="1.5 配置系统内核参数"></a>1.5 配置系统内核参数</h2><h3 id="1-5-1-开启路由转发"><a href="#1-5-1-开启路由转发" class="headerlink" title="1.5.1 开启路由转发"></a>1.5.1 开启路由转发</h3><pre><code class="hljs">sudo vi /etc/sysctl.d/k8s.confnet.ipv4.ip_forward=1net.bridge.bridge-nf-call-iptables=1net.bridge.bridge-nf-call-ip6tables=1</code></pre><h3 id="1-5-2-加载内核参数配置"><a href="#1-5-2-加载内核参数配置" class="headerlink" title="1.5.2 加载内核参数配置"></a>1.5.2 加载内核参数配置</h3><pre><code class="hljs">sudo sysctl -p</code></pre><h2 id="1-6-配置集群免密登录"><a href="#1-6-配置集群免密登录" class="headerlink" title="1.6 配置集群免密登录"></a>1.6 配置集群免密登录</h2><h1 id="2-Node节点部署docker"><a href="#2-Node节点部署docker" class="headerlink" title="2.Node节点部署docker"></a>2.Node节点部署docker</h1><h2 id="2-1-部署docker"><a href="#2-1-部署docker" class="headerlink" title="2.1 部署docker"></a>2.1 部署docker</h2><h2 id="2-2-部署镜像仓库"><a href="#2-2-部署镜像仓库" class="headerlink" title="2.2 部署镜像仓库"></a>2.2 部署镜像仓库</h2><h1 id="3-部署etcd集群"><a href="#3-部署etcd集群" class="headerlink" title="3.部署etcd集群"></a>3.部署etcd集群</h1><pre><code class="hljs">sudo mkdir -p /opt/etcd/&#123;bin,cfg,ssl&#125;tar -xzvf etcd-v3.2.32-linux-amd64.tar.gzsudo cp etcd-v3.2.32-linux-amd64/etcd* /opt/etcd/bin</code></pre><h2 id="3-1-创建etcd集群认证证书"><a href="#3-1-创建etcd集群认证证书" class="headerlink" title="3.1 创建etcd集群认证证书"></a>3.1 创建etcd集群认证证书</h2><h3 id="3-1-1-安装SSL证书签发工具cfssl"><a href="#3-1-1-安装SSL证书签发工具cfssl" class="headerlink" title="3.1.1 安装SSL证书签发工具cfssl"></a>3.1.1 安装SSL证书签发工具cfssl</h3><pre><code class="hljs">sudo tar -xzvf cfssl.tar.gz -C /usr/local/binmkdir -p ssl/etcd &amp;&amp; cd ssl/etcdsudo chmod +x /usr/local/bin/cfssl*</code></pre><h3 id="3-1-2-创建CA配置文件，即证书生成策略，规定CA可以颁发哪种类型的证书"><a href="#3-1-2-创建CA配置文件，即证书生成策略，规定CA可以颁发哪种类型的证书" class="headerlink" title="3.1.2 创建CA配置文件，即证书生成策略，规定CA可以颁发哪种类型的证书"></a>3.1.2 创建CA配置文件，即证书生成策略，规定CA可以颁发哪种类型的证书</h3><pre><code class="hljs">vi ca-config.json&#123;  &quot;signing&quot;: &#123;    &quot;default&quot;: &#123;      &quot;expiry&quot;: &quot;87600h&quot;    &#125;,    &quot;profiles&quot;: &#123;      &quot;www&quot;: &#123;         &quot;expiry&quot;: &quot;87600h&quot;,         &quot;usages&quot;: [            &quot;signing&quot;,            &quot;key encipherment&quot;,            &quot;server auth&quot;,            &quot;client auth&quot;        ]      &#125;    &#125;  &#125;&#125;</code></pre><h3 id="3-1-3-创建CA证书签名请求配置文件"><a href="#3-1-3-创建CA证书签名请求配置文件" class="headerlink" title="3.1.3 创建CA证书签名请求配置文件"></a>3.1.3 创建CA证书签名请求配置文件</h3><pre><code class="hljs">vi ca-csr.json&#123;    &quot;CN&quot;: &quot;etcd CA&quot;,    &quot;key&quot;: &#123;        &quot;algo&quot;: &quot;rsa&quot;,        &quot;size&quot;: 2048    &#125;,    &quot;names&quot;: [        &#123;            &quot;C&quot;: &quot;CN&quot;,            &quot;L&quot;: &quot;Beijing&quot;,            &quot;ST&quot;: &quot;Beijing&quot;        &#125;    ]&#125;</code></pre><h3 id="3-1-4-创建CA的私钥（ca-key-pem）、证书（ca-pem）及证书签名请求文件（ca-csr），用于交叉签名或重新签名"><a href="#3-1-4-创建CA的私钥（ca-key-pem）、证书（ca-pem）及证书签名请求文件（ca-csr），用于交叉签名或重新签名" class="headerlink" title="3.1.4 创建CA的私钥（ca-key.pem）、证书（ca.pem）及证书签名请求文件（ca.csr），用于交叉签名或重新签名"></a>3.1.4 创建CA的私钥（ca-key.pem）、证书（ca.pem）及证书签名请求文件（ca.csr），用于交叉签名或重新签名</h3><pre><code class="hljs">cfssl gencert -initca ca-csr.json | cfssljson -bare ca</code></pre><h3 id="3-1-5-创建etcd集群证书签名请求配置文件"><a href="#3-1-5-创建etcd集群证书签名请求配置文件" class="headerlink" title="3.1.5 创建etcd集群证书签名请求配置文件"></a>3.1.5 创建etcd集群证书签名请求配置文件</h3><pre><code class="hljs">vi server-csr.json&#123;    &quot;CN&quot;: &quot;etcd&quot;,    &quot;hosts&quot;: [    &quot;127.0.0.1&quot;,    &quot;172.16.100.100&quot;,    &quot;172.16.100.108&quot;,    &quot;172.16.100.120&quot;,    &quot;172.16.100.128&quot;,    &quot;172.16.100.150&quot;,    &quot;172.16.100.160&quot;,    &quot;172.16.100.180&quot;,    &quot;172.16.100.188&quot;,    &quot;172.16.100.200&quot;    ],    &quot;key&quot;: &#123;        &quot;algo&quot;: &quot;rsa&quot;,        &quot;size&quot;: 2048    &#125;,    &quot;names&quot;: [      &#123;        &quot;C&quot;: &quot;CN&quot;,        &quot;L&quot;: &quot;BeiJing&quot;,        &quot;ST&quot;: &quot;BeiJing&quot;      &#125;    ]&#125;</code></pre><h3 id="3-1-6-创建etcd集群证书和私钥"><a href="#3-1-6-创建etcd集群证书和私钥" class="headerlink" title="3.1.6 创建etcd集群证书和私钥"></a>3.1.6 创建etcd集群证书和私钥</h3><pre><code class="hljs">cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=www server-csr.json | cfssljson -bare serversudo cp *pem /opt/etcd/ssl</code></pre><h2 id="3-2-创建etcd配置文件"><a href="#3-2-创建etcd配置文件" class="headerlink" title="3.2 创建etcd配置文件"></a>3.2 创建etcd配置文件</h2><pre><code class="hljs">sudo vi /opt/etcd/cfg/etcd.conf# 节点配置# 设置节点名称，集群中的唯一标识，不可重复ETCD_NAME=&quot;etcd01&quot;# 设置数据存储目录ETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot;# 设置集群通信监听地址ETCD_LISTEN_PEER_URLS=&quot;https://172.16.100.100:2380&quot;# 设置客户端访问监听地址ETCD_LISTEN_CLIENT_URLS=&quot;https://172.16.100.100:2379&quot;# 集群配置# 设置集群通告地址ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://172.16.100.100:2380&quot;# 设置客户端通告地址ETCD_ADVERTISE_CLIENT_URLS=&quot;https://172.16.100.100:2379&quot;# 设置集群节点地址ETCD_INITIAL_CLUSTER=&quot;etcd01=https://172.16.100.100:2380,etcd02=https://172.16.100.180:2380,etcd03=https://172.16.100.200:2380&quot;# 设置集群TokenETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;# 设置加入集群的当前状态，new为新集群，existing表示加入已有集群ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;</code></pre><hr><ul><li>注：除集群节点地址外，其余节点的配置文件需修改成各自对应的信息</li></ul><h2 id="3-3-创建启动脚本"><a href="#3-3-创建启动脚本" class="headerlink" title="3.3 创建启动脚本"></a>3.3 创建启动脚本</h2><pre><code class="hljs">sudo vi /lib/systemd/system/etcd.service[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyEnvironmentFile=/opt/etcd/cfg/etcd.confExecStart=/opt/etcd/bin/etcd \--cert-file=/opt/etcd/ssl/server.pem \--key-file=/opt/etcd/ssl/server-key.pem \--peer-cert-file=/opt/etcd/ssl/server.pem \--peer-key-file=/opt/etcd/ssl/server-key.pem \--trusted-ca-file=/opt/etcd/ssl/ca.pem \--peer-trusted-ca-file=/opt/etcd/ssl/ca.pem Restart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre><h2 id="3-4-分发etcd的可执行文件、配置文件、认证证书和启动脚本到其余节点"><a href="#3-4-分发etcd的可执行文件、配置文件、认证证书和启动脚本到其余节点" class="headerlink" title="3.4 分发etcd的可执行文件、配置文件、认证证书和启动脚本到其余节点"></a>3.4 分发etcd的可执行文件、配置文件、认证证书和启动脚本到其余节点</h2><pre><code class="hljs">sudo scp -r /opt/etcd node01:/optsudo scp -r /opt/etcd node02:/optsudo scp /lib/systemd/system/etcd.service node01:/lib/systemd/systemsudo scp /lib/systemd/system/etcd.service node02:/lib/systemd/system</code></pre><h2 id="3-5-启动etcd集群"><a href="#3-5-启动etcd集群" class="headerlink" title="3.5 启动etcd集群"></a>3.5 启动etcd集群</h2><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start etcd.servicesudo systemctl enable etcd.service</code></pre><h2 id="3-6-查看etcd集群状态"><a href="#3-6-查看etcd集群状态" class="headerlink" title="3.6 查看etcd集群状态"></a>3.6 查看etcd集群状态</h2><pre><code class="hljs">sudo /opt/etcd/bin/etcdctl --ca-file=/opt/etcd/ssl/ca.pem --cert-file=/opt/etcd/ssl/server.pem --key-file=/opt/etcd/ssl/server-key.pem \--endpoints=&quot;https://172.16.100.100:2379,https://172.16.100.180:2379,https://172.16.100.200:2379&quot; cluster-health</code></pre><h1 id="4-部署CNI网络插件Flannel"><a href="#4-部署CNI网络插件Flannel" class="headerlink" title="4.部署CNI网络插件Flannel"></a>4.部署CNI网络插件Flannel</h1><pre><code class="hljs">sudo mkdir -p /opt/flanneld/&#123;bin,ssl&#125;tar -xzvf flannel-v0.12.0-linux-amd64.tar.gz -C /opt/flanneld/bincp /opt/etcd/ssl/*pem /opt/flanneld/ssl</code></pre><h2 id="4-1-etcd集群写入pod网段"><a href="#4-1-etcd集群写入pod网段" class="headerlink" title="4.1 etcd集群写入pod网段"></a>4.1 etcd集群写入pod网段</h2><pre><code class="hljs">/opt/etcd/bin/etcdctl \--endpoints=&quot;https://172.16.100.100:2379,https://172.16.100.180:2379,https://172.16.100.200:2379&quot; \--ca-file=/opt/flanneld/ssl/ca.pem --cert-file=/opt/flanneld/ssl/server.pem --key-file=/opt/flanneld/ssl/server-key.pem \set /kubernetes/network/config &#39;&#123;&quot;Network&quot;:&quot;172.30.0.0/16&quot;,&quot;SubnetLen&quot;: 24,&quot;Backend&quot;: &#123;&quot;Type&quot;: &quot;vxlan&quot;&#125;&#125;&#39;</code></pre><hr><ul><li>注：该步骤只需执行一次</li></ul><h2 id="4-2-创建flanneld启动脚本"><a href="#4-2-创建flanneld启动脚本" class="headerlink" title="4.2 创建flanneld启动脚本"></a>4.2 创建flanneld启动脚本</h2><pre><code class="hljs">sudo vi /lib/systemd/system/flanneld.service[Unit]Description=Flanneld overlay address etcd agentAfter=network.targetAfter=network-online.targetWants=network-online.targetAfter=etcd.serviceBefore=docker.service[Service]Type=notifyExecStart=/opt/flanneld/bin/flanneld \  -etcd-cafile=/opt/flanneld/ssl/ca.pem \  -etcd-certfile=/opt/flanneld/ssl/server.pem \  -etcd-keyfile=/opt/flanneld/ssl/server-key.pem \  -etcd-endpoints=https://172.16.100.100:2379,https://172.16.100.180:2379,https://172.16.100.200:2379 \  -etcd-prefix=/kubernetes/networkExecStartPost=/opt/flanneld/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/dockerRestart=on-failure[Install]WantedBy=multi-user.targetRequiredBy=docker.service</code></pre><h2 id="4-3-启动flanneld"><a href="#4-3-启动flanneld" class="headerlink" title="4.3 启动flanneld"></a>4.3 启动flanneld</h2><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start flanneld.servicesudo systemctl enable flanneld.service</code></pre><h2 id="4-4-检查分配给各flanneld的Pod网段信息"><a href="#4-4-检查分配给各flanneld的Pod网段信息" class="headerlink" title="4.4 检查分配给各flanneld的Pod网段信息"></a>4.4 检查分配给各flanneld的Pod网段信息</h2><pre><code class="hljs">/opt/etcd/bin/etcdctl \--endpoints=&quot;https://172.16.100.100:2379,https://172.16.100.180:2379,https://172.16.100.200:2379&quot; \--ca-file=/opt/flanneld/ssl/ca.pem --cert-file=/opt/flanneld/ssl/server.pem \--key-file=/opt/flanneld/ssl/server-key.pem ls /kubernetes/network/subnets/kubernetes/network/subnets/172.30.73.0-24/opt/etcd/bin/etcdctl \--endpoints=&quot;https://172.16.100.100:2379,https://172.16.100.180:2379,https://172.16.100.200:2379&quot; \--ca-file=/opt/flanneld/ssl/ca.pem --cert-file=/opt/flanneld/ssl/server.pem \--key-file=/opt/flanneld/ssl/server-key.pem get /kubernetes/network/subnets/172.30.73.0-24&#123;&quot;PublicIP&quot;:&quot;172.16.100.100&quot;,&quot;BackendType&quot;:&quot;vxlan&quot;,&quot;BackendData&quot;:&#123;&quot;VtepMAC&quot;:&quot;d6:b6:62:38:84:52&quot;&#125;&#125;</code></pre><h2 id="4-5-配置docker启动，指定网络参数为flannel"><a href="#4-5-配置docker启动，指定网络参数为flannel" class="headerlink" title="4.5 配置docker启动，指定网络参数为flannel"></a>4.5 配置docker启动，指定网络参数为flannel</h2><pre><code class="hljs">sed -i &#39;/ExecStart/i EnvironmentFile=/run/flannel/docker&#39; /lib/systemd/system/docker.servicesed -i &#39;s/dockerd/dockerd $DOCKER_NETWORK_OPTIONS/g&#39; /lib/systemd/system/docker.servicesystemctl daemon-reload &amp;&amp; systemctl restart docker.service </code></pre><h2 id="4-6-验证flanneld配置是否生效"><a href="#4-6-验证flanneld配置是否生效" class="headerlink" title="4.6 验证flanneld配置是否生效"></a>4.6 验证flanneld配置是否生效</h2><pre><code class="hljs">ip addr | grep flannel | grep inetinet 172.30.71.0/32 scope global flannel.1</code></pre><h2 id="4-7-分发flanneld的可执行文件、认证证书、启动脚本到其余节点"><a href="#4-7-分发flanneld的可执行文件、认证证书、启动脚本到其余节点" class="headerlink" title="4.7 分发flanneld的可执行文件、认证证书、启动脚本到其余节点"></a>4.7 分发flanneld的可执行文件、认证证书、启动脚本到其余节点</h2><pre><code class="hljs">ssh node01 mkdir -p /opt/flanneld/&#123;bin,ssl&#125;ssh node02 mkdir -p /opt/flanneld/&#123;bin,ssl&#125;scp /opt/flanneld/bin/* node01:/opt/flanneld/binscp /opt/flanneld/bin/* node02:/opt/flanneld/binscp /opt/flanneld/ssl/* node01:/opt/flanneld/sslscp /opt/flanneld/ssl/* node02:/opt/flanneld/sslscp /lib/systemd/system/flanneld.service node01:/lib/systemd/systemscp /lib/systemd/system/flanneld.service node02:/lib/systemd/system</code></pre><h1 id="5-部署Master"><a href="#5-部署Master" class="headerlink" title="5.部署Master"></a>5.部署Master</h1><h2 id="5-1-部署kube-apiserver"><a href="#5-1-部署kube-apiserver" class="headerlink" title="5.1 部署kube-apiserver"></a>5.1 部署kube-apiserver</h2><pre><code class="hljs">wget https://dl.k8s.io/v1.20.15/kubernetes-server-linux-amd64.tar.gztar -xzvf kubernetes-server-linux-amd64.tar.gzsudo mkdir -p /opt/kubernetes/&#123;bin,cfg,ssl,logs&#125; sudo cp kubernetes/server/bin/&#123;kube-apiserver,kube-controller-manager,kube-scheduler&#125; /opt/kubernetes/binsudo cp kubernetes/server/bin/kubectl /usr/bin</code></pre><h3 id="5-1-1-创建apiserver证书"><a href="#5-1-1-创建apiserver证书" class="headerlink" title="5.1.1 创建apiserver证书"></a>5.1.1 创建apiserver证书</h3><pre><code class="hljs">mkdir -p ssl/k8s &amp;&amp; cd ssl/k8s</code></pre><h4 id="5-1-1-1-创建CA配置文件"><a href="#5-1-1-1-创建CA配置文件" class="headerlink" title="5.1.1.1 创建CA配置文件"></a>5.1.1.1 创建CA配置文件</h4><pre><code class="hljs">vi ca-config.json&#123;  &quot;signing&quot;: &#123;    &quot;default&quot;: &#123;      &quot;expiry&quot;: &quot;87600h&quot;    &#125;,    &quot;profiles&quot;: &#123;      &quot;kubernetes&quot;: &#123;         &quot;expiry&quot;: &quot;87600h&quot;,         &quot;usages&quot;: [            &quot;signing&quot;,            &quot;key encipherment&quot;,            &quot;server auth&quot;,            &quot;client auth&quot;        ]      &#125;    &#125;  &#125;&#125;</code></pre><h4 id="5-1-1-2-创建CA证书签名请求配置文件"><a href="#5-1-1-2-创建CA证书签名请求配置文件" class="headerlink" title="5.1.1.2 创建CA证书签名请求配置文件"></a>5.1.1.2 创建CA证书签名请求配置文件</h4><pre><code class="hljs">vi ca-csr.json&#123;    &quot;CN&quot;: &quot;kubernetes&quot;,    &quot;key&quot;: &#123;        &quot;algo&quot;: &quot;rsa&quot;,        &quot;size&quot;: 2048    &#125;,    &quot;names&quot;: [        &#123;          &quot;C&quot;: &quot;CN&quot;,          &quot;L&quot;: &quot;Beijing&quot;,          &quot;ST&quot;: &quot;Beijing&quot;,          &quot;O&quot;: &quot;k8s&quot;,          &quot;OU&quot;: &quot;System&quot;        &#125;    ]&#125;</code></pre><h4 id="5-1-1-3-创建CA的私钥（ca-key-pem）、证书（ca-pem）及证书签名请求（ca-csr）"><a href="#5-1-1-3-创建CA的私钥（ca-key-pem）、证书（ca-pem）及证书签名请求（ca-csr）" class="headerlink" title="5.1.1.3 创建CA的私钥（ca-key.pem）、证书（ca.pem）及证书签名请求（ca.csr）"></a>5.1.1.3 创建CA的私钥（ca-key.pem）、证书（ca.pem）及证书签名请求（ca.csr）</h4><pre><code class="hljs">cfssl gencert -initca ca-csr.json | cfssljson -bare ca -</code></pre><h4 id="5-1-1-4-创建apiserver证书签名请求配置文件"><a href="#5-1-1-4-创建apiserver证书签名请求配置文件" class="headerlink" title="5.1.1.4 创建apiserver证书签名请求配置文件"></a>5.1.1.4 创建apiserver证书签名请求配置文件</h4><pre><code class="hljs">vi server-csr.json&#123;  &quot;CN&quot;: &quot;kubernetes&quot;,  &quot;hosts&quot;: [  &quot;127.0.0.1&quot;,  &quot;10.254.0.1&quot;,  &quot;172.16.100.100&quot;,  &quot;172.16.100.108&quot;,  &quot;172.16.100.120&quot;,  &quot;172.16.100.128&quot;,  &quot;172.16.100.150&quot;,  &quot;172.16.100.160&quot;,  &quot;172.16.100.180&quot;,  &quot;172.16.100.188&quot;,  &quot;172.16.100.200&quot;,  &quot;kubernetes&quot;,  &quot;kubernetes.default&quot;,  &quot;kubernetes.default.svc&quot;,  &quot;kubernetes.default.svc.cluster&quot;,  &quot;kubernetes.default.svc.cluster.local&quot;],  &quot;key&quot;: &#123;      &quot;algo&quot;: &quot;rsa&quot;,      &quot;size&quot;: 2048    &#125;,  &quot;names&quot;: [    &#123;      &quot;C&quot;: &quot;CN&quot;,      &quot;L&quot;: &quot;BeiJing&quot;,      &quot;ST&quot;: &quot;BeiJing&quot;,      &quot;O&quot;: &quot;k8s&quot;,      &quot;OU&quot;: &quot;System&quot;    &#125;  ]&#125;</code></pre><hr><ul><li>注：hosts字段IP为集群所有Master&#x2F;Node&#x2F;LB&#x2F;VIP的IP，为方便后期扩容可将所有预留IP都包含进去</li></ul><h4 id="5-1-1-5-创建apiserver认证证书"><a href="#5-1-1-5-创建apiserver认证证书" class="headerlink" title="5.1.1.5 创建apiserver认证证书"></a>5.1.1.5 创建apiserver认证证书</h4><pre><code class="hljs">cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes server-csr.json | cfssljson -bare serversudo cp *.pem /opt/kubernetes/ssl</code></pre><h3 id="5-1-2-创建kube-apiserver配置文件"><a href="#5-1-2-创建kube-apiserver配置文件" class="headerlink" title="5.1.2 创建kube-apiserver配置文件"></a>5.1.2 创建kube-apiserver配置文件</h3><pre><code class="hljs">sudo vi /opt/kubernetes/cfg/kube-apiserver.confKUBE_APISERVER_OPTS=&quot;--logtostderr=false \--v=2 \--log-dir=/opt/kubernetes/logs \--etcd-servers=https://172.16.100.100:2379,https://172.16.100.180:2379,https://172.16.100.200:2379 \--bind-address=172.16.100.100 \--secure-port=6443 \--advertise-address=172.16.100.100 \--allow-privileged=true \--service-cluster-ip-range=10.254.0.0/24 \--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,NodeRestriction,DefaultStorageClass \--authorization-mode=RBAC,Node \--enable-bootstrap-token-auth=true \--token-auth-file=/opt/kubernetes/cfg/token.csv \--service-node-port-range=30000-32767 \--kubelet-client-certificate=/opt/kubernetes/ssl/server.pem \--kubelet-client-key=/opt/kubernetes/ssl/server-key.pem \--tls-cert-file=/opt/kubernetes/ssl/server.pem  \--tls-private-key-file=/opt/kubernetes/ssl/server-key.pem \--client-ca-file=/opt/kubernetes/ssl/ca.pem \--service-account-key-file=/opt/kubernetes/ssl/ca-key.pem \--service-account-signing-key-file=/opt/kubernetes/ssl/server-key.pem \--service-account-issuer=https://kubernetes.default.svc.cluster.local \--etcd-cafile=/opt/etcd/ssl/ca.pem \--etcd-certfile=/opt/etcd/ssl/server.pem \--etcd-keyfile=/opt/etcd/ssl/server-key.pem \--requestheader-client-ca-file=/opt/kubernetes/ssl/ca.pem \--proxy-client-cert-file=/opt/kubernetes/ssl/server.pem \--proxy-client-key-file=/opt/kubernetes/ssl/server-key.pem \--requestheader-allowed-names=kubernetes \--requestheader-extra-headers-prefix=X-Remote-Extra- \--requestheader-group-headers=X-Remote-Group \--requestheader-username-headers=X-Remote-User \--enable-aggregator-routing=true \--audit-log-maxage=30 \--audit-log-maxbackup=3 \--audit-log-maxsize=100 \--audit-log-path=/opt/kubernetes/logs/k8s-audit.log&quot;</code></pre><hr><ul><li>advertise-address，集群Apiserver通告地址，若未指定或为空则为bind-address地址</li><li>service-cluster-ip-range，集群内部虚拟网络service的IP地址池，也即为pod的统一访问IP</li></ul><h3 id="5-1-3-创建token文件"><a href="#5-1-3-创建token文件" class="headerlink" title="5.1.3 创建token文件"></a>5.1.3 创建token文件</h3><pre><code class="hljs">sudo vi /opt/kubernetes/cfg/token.csv5aad5e038ca05f6bcb256c3118e8366c,kubelet-bootstrap,10001,&quot;system:node-bootstrapper&quot;</code></pre><hr><ul><li>注：token文件用于启用TLS Bootstrapping机制，简化了Node节点kubelet和kube-proxy与kube-apiserver的通信认证流程，其格式为“token，用户名，UID，用户组”。token可用如下命令随机生成，head -c 16 &#x2F;dev&#x2F;urandom | od -An -t x | tr -d ‘ ‘</li></ul><h3 id="5-1-4-创建启动脚本"><a href="#5-1-4-创建启动脚本" class="headerlink" title="5.1.4 创建启动脚本"></a>5.1.4 创建启动脚本</h3><pre><code class="hljs">sudo vi /lib/systemd/system/kube-apiserver.service[Unit]Description=Kubernetes API ServerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=/opt/kubernetes/cfg/kube-apiserver.confExecStart=/opt/kubernetes/bin/kube-apiserver $KUBE_APISERVER_OPTSRestart=on-failure[Install]WantedBy=multi-user.target</code></pre><h3 id="5-1-5-启动kube-apiserver"><a href="#5-1-5-启动kube-apiserver" class="headerlink" title="5.1.5 启动kube-apiserver"></a>5.1.5 启动kube-apiserver</h3><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start kube-apiserversudo systemctl enable kube-apiserver</code></pre><h2 id="5-2-部署kube-controller-manager"><a href="#5-2-部署kube-controller-manager" class="headerlink" title="5.2 部署kube-controller-manager"></a>5.2 部署kube-controller-manager</h2><h3 id="5-2-1-创建配置文件"><a href="#5-2-1-创建配置文件" class="headerlink" title="5.2.1 创建配置文件"></a>5.2.1 创建配置文件</h3><pre><code class="hljs">sudo vi /opt/kubernetes/cfg/kube-controller-manager.confKUBE_CONTROLLER_MANAGER_OPTS=&quot;--logtostderr=false \--v=2 \--log-dir=/opt/kubernetes/logs \--leader-elect=true \--bind-address=127.0.0.1 \--allocate-node-cidrs=true \--cluster-cidr=172.30.0.0/16 \--service-cluster-ip-range=10.254.0.0/24 \--kubeconfig=/opt/kubernetes/cfg/kube-controller-manager.kubeconfig \--cluster-signing-cert-file=/opt/kubernetes/ssl/ca.pem \--cluster-signing-key-file=/opt/kubernetes/ssl/ca-key.pem  \--root-ca-file=/opt/kubernetes/ssl/ca.pem \--service-account-private-key-file=/opt/kubernetes/ssl/ca-key.pem \--cluster-signing-duration=87600h0m0s&quot;</code></pre><hr><ul><li>cluster-cidr，pod IP地址池</li><li>service-cluster-ip-range，与Apiserver保持一致</li><li>experimental-cluster-signing-duratio，签名证书有效期，1.19版本之后用–cluster-signing-duration&#x3D;87600h0m0s</li></ul><h3 id="5-2-2-创建kube-controller-manager集群认证令牌"><a href="#5-2-2-创建kube-controller-manager集群认证令牌" class="headerlink" title="5.2.2 创建kube-controller-manager集群认证令牌"></a>5.2.2 创建kube-controller-manager集群认证令牌</h3><h4 id="5-2-2-1-创建kube-controller-manager证书签名请求配置文件"><a href="#5-2-2-1-创建kube-controller-manager证书签名请求配置文件" class="headerlink" title="5.2.2.1 创建kube-controller-manager证书签名请求配置文件"></a>5.2.2.1 创建kube-controller-manager证书签名请求配置文件</h4><pre><code class="hljs">vi kube-controller-manager-csr.json&#123;  &quot;CN&quot;: &quot;system:kube-controller-manager&quot;,  &quot;hosts&quot;: [],  &quot;key&quot;: &#123;    &quot;algo&quot;: &quot;rsa&quot;,    &quot;size&quot;: 2048  &#125;,  &quot;names&quot;: [    &#123;      &quot;C&quot;: &quot;CN&quot;,      &quot;L&quot;: &quot;BeiJing&quot;,       &quot;ST&quot;: &quot;BeiJing&quot;,      &quot;O&quot;: &quot;k8s&quot;,      &quot;OU&quot;: &quot;System&quot;    &#125;  ]&#125;</code></pre><h4 id="5-2-2-2-创建kube-controller-manager证书"><a href="#5-2-2-2-创建kube-controller-manager证书" class="headerlink" title="5.2.2.2 创建kube-controller-manager证书"></a>5.2.2.2 创建kube-controller-manager证书</h4><pre><code class="hljs">cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager</code></pre><h4 id="5-2-2-3-设置环境变量"><a href="#5-2-2-3-设置环境变量" class="headerlink" title="5.2.2.3 设置环境变量"></a>5.2.2.3 设置环境变量</h4><pre><code class="hljs">KUBE_APISERVER=&quot;https://172.16.100.100:6443&quot;</code></pre><h4 id="5-2-2-4-设置集群参数"><a href="#5-2-2-4-设置集群参数" class="headerlink" title="5.2.2.4 设置集群参数"></a>5.2.2.4 设置集群参数</h4><pre><code class="hljs">kubectl config set-cluster kubernetes \  --certificate-authority=/opt/kubernetes/ssl/ca.pem \  --embed-certs=true \  --server=$&#123;KUBE_APISERVER&#125; \  --kubeconfig=/opt/kubernetes/cfg/kube-controller-manager.kubeconfig</code></pre><h4 id="5-2-2-5-设置客户端认证参数"><a href="#5-2-2-5-设置客户端认证参数" class="headerlink" title="5.2.2.5 设置客户端认证参数"></a>5.2.2.5 设置客户端认证参数</h4><pre><code class="hljs">kubectl config set-credentials kube-controller-manager \  --client-certificate=kube-controller-manager.pem \  --client-key=kube-controller-manager-key.pem \  --embed-certs=true \  --kubeconfig=/opt/kubernetes/cfg/kube-controller-manager.kubeconfig</code></pre><h4 id="5-2-2-6-设置上下文参数"><a href="#5-2-2-6-设置上下文参数" class="headerlink" title="5.2.2.6 设置上下文参数"></a>5.2.2.6 设置上下文参数</h4><pre><code class="hljs">kubectl config set-context default \  --cluster=kubernetes \  --user=kube-controller-manager \  --kubeconfig=/opt/kubernetes/cfg/kube-controller-manager.kubeconfig</code></pre><h4 id="5-2-2-7-切换上下文"><a href="#5-2-2-7-切换上下文" class="headerlink" title="5.2.2.7 切换上下文"></a>5.2.2.7 切换上下文</h4><pre><code class="hljs">kubectl config use-context default --kubeconfig=/opt/kubernetes/cfg/kube-controller-manager.kubeconfig</code></pre><h3 id="5-2-3-创建启动脚本"><a href="#5-2-3-创建启动脚本" class="headerlink" title="5.2.3 创建启动脚本"></a>5.2.3 创建启动脚本</h3><pre><code class="hljs">sudo vi /usr/lib/systemd/system/kube-controller-manager.service[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=/opt/kubernetes/cfg/kube-controller-manager.confExecStart=/opt/kubernetes/bin/kube-controller-manager $KUBE_CONTROLLER_MANAGER_OPTSRestart=on-failure[Install]WantedBy=multi-user.target</code></pre><h3 id="5-2-4-启动kube-controller-manager"><a href="#5-2-4-启动kube-controller-manager" class="headerlink" title="5.2.4 启动kube-controller-manager"></a>5.2.4 启动kube-controller-manager</h3><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start kube-controller-managersudo systemctl enable kube-controller-manager</code></pre><h2 id="5-3-部署kube-scheduler"><a href="#5-3-部署kube-scheduler" class="headerlink" title="5.3 部署kube-scheduler"></a>5.3 部署kube-scheduler</h2><h3 id="5-3-1-创建配置文件"><a href="#5-3-1-创建配置文件" class="headerlink" title="5.3.1 创建配置文件"></a>5.3.1 创建配置文件</h3><pre><code class="hljs">sudo vi /opt/kubernetes/cfg/kube-scheduler.confKUBE_SCHEDULER_OPTS=&quot;--logtostderr=false \--v=2 \--log-dir=/opt/kubernetes/logs \--leader-elect \--kubeconfig=/opt/kubernetes/cfg/kube-scheduler.kubeconfig \--bind-address=127.0.0.1&quot;</code></pre><h3 id="5-3-2-创建kube-scheduler集群认证令牌"><a href="#5-3-2-创建kube-scheduler集群认证令牌" class="headerlink" title="5.3.2 创建kube-scheduler集群认证令牌"></a>5.3.2 创建kube-scheduler集群认证令牌</h3><h4 id="5-3-2-1-创建kube-scheduler证书签名请求配置文件"><a href="#5-3-2-1-创建kube-scheduler证书签名请求配置文件" class="headerlink" title="5.3.2.1 创建kube-scheduler证书签名请求配置文件"></a>5.3.2.1 创建kube-scheduler证书签名请求配置文件</h4><pre><code class="hljs">vi kube-scheduler-csr.json&#123;  &quot;CN&quot;: &quot;system:kube-scheduler&quot;,  &quot;hosts&quot;: [],  &quot;key&quot;: &#123;    &quot;algo&quot;: &quot;rsa&quot;,    &quot;size&quot;: 2048  &#125;,  &quot;names&quot;: [    &#123;      &quot;C&quot;: &quot;CN&quot;,      &quot;L&quot;: &quot;BeiJing&quot;,       &quot;ST&quot;: &quot;BeiJing&quot;,      &quot;O&quot;: &quot;system:masters&quot;,      &quot;OU&quot;: &quot;System&quot;    &#125;  ] &#125;</code></pre><h4 id="5-3-2-2-创建kube-scheduler证书"><a href="#5-3-2-2-创建kube-scheduler证书" class="headerlink" title="5.3.2.2 创建kube-scheduler证书"></a>5.3.2.2 创建kube-scheduler证书</h4><pre><code class="hljs">cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-scheduler-csr.json | cfssljson -bare kube-scheduler</code></pre><h4 id="5-3-2-3-设置环境变量"><a href="#5-3-2-3-设置环境变量" class="headerlink" title="5.3.2.3 设置环境变量"></a>5.3.2.3 设置环境变量</h4><pre><code class="hljs">KUBE_APISERVER=&quot;https://172.16.100.100:6443&quot;</code></pre><h4 id="5-3-2-4-设置集群参数"><a href="#5-3-2-4-设置集群参数" class="headerlink" title="5.3.2.4 设置集群参数"></a>5.3.2.4 设置集群参数</h4><pre><code class="hljs">kubectl config set-cluster kubernetes \  --certificate-authority=/opt/kubernetes/ssl/ca.pem \  --embed-certs=true \  --server=$&#123;KUBE_APISERVER&#125; \  --kubeconfig=/opt/kubernetes/cfg/kube-scheduler.kubeconfig</code></pre><h4 id="5-3-2-5-设置客户端认证参数"><a href="#5-3-2-5-设置客户端认证参数" class="headerlink" title="5.3.2.5 设置客户端认证参数"></a>5.3.2.5 设置客户端认证参数</h4><pre><code class="hljs">kubectl config set-credentials kube-scheduler \  --client-certificate=kube-scheduler.pem \  --client-key=kube-scheduler-key.pem \  --embed-certs=true \  --kubeconfig=/opt/kubernetes/cfg/kube-scheduler.kubeconfig</code></pre><h4 id="5-3-2-6-设置上下文参数"><a href="#5-3-2-6-设置上下文参数" class="headerlink" title="5.3.2.6 设置上下文参数"></a>5.3.2.6 设置上下文参数</h4><pre><code class="hljs">kubectl config set-context default \  --cluster=kubernetes \  --user=kube-scheduler \  --kubeconfig=/opt/kubernetes/cfg/kube-scheduler.kubeconfig</code></pre><h4 id="5-3-2-7-切换上下文"><a href="#5-3-2-7-切换上下文" class="headerlink" title="5.3.2.7 切换上下文"></a>5.3.2.7 切换上下文</h4><pre><code class="hljs">kubectl config use-context default --kubeconfig=/opt/kubernetes/cfg/kube-scheduler.kubeconfig</code></pre><h3 id="5-3-3-创建启动脚本"><a href="#5-3-3-创建启动脚本" class="headerlink" title="5.3.3 创建启动脚本"></a>5.3.3 创建启动脚本</h3><pre><code class="hljs">sudo vi /usr/lib/systemd/system/kube-scheduler.service[Unit]Description=Kubernetes SchedulerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=/opt/kubernetes/cfg/kube-scheduler.confExecStart=/opt/kubernetes/bin/kube-scheduler $KUBE_SCHEDULER_OPTSRestart=on-failure[Install]WantedBy=multi-user.target</code></pre><h3 id="5-3-4-启动kube-scheduler"><a href="#5-3-4-启动kube-scheduler" class="headerlink" title="5.3.4 启动kube-scheduler"></a>5.3.4 启动kube-scheduler</h3><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start kube-schedulersudo systemctl enable kube-scheduler</code></pre><h2 id="5-4-部署kubectl"><a href="#5-4-部署kubectl" class="headerlink" title="5.4 部署kubectl"></a>5.4 部署kubectl</h2><h3 id="5-4-1-创建kubectl集群认证证书"><a href="#5-4-1-创建kubectl集群认证证书" class="headerlink" title="5.4.1 创建kubectl集群认证证书"></a>5.4.1 创建kubectl集群认证证书</h3><h4 id="5-4-1-1-创建证书签名请求配置文件"><a href="#5-4-1-1-创建证书签名请求配置文件" class="headerlink" title="5.4.1.1 创建证书签名请求配置文件"></a>5.4.1.1 创建证书签名请求配置文件</h4><pre><code class="hljs">vi admin-csr.json&#123;  &quot;CN&quot;: &quot;admin&quot;,  &quot;hosts&quot;: [],  &quot;key&quot;: &#123;    &quot;algo&quot;: &quot;rsa&quot;,    &quot;size&quot;: 2048  &#125;,  &quot;names&quot;: [    &#123;      &quot;C&quot;: &quot;CN&quot;,      &quot;L&quot;: &quot;BeiJing&quot;,      &quot;ST&quot;: &quot;BeiJing&quot;,      &quot;O&quot;: &quot;system:masters&quot;,      &quot;OU&quot;: &quot;System&quot;    &#125;  ]&#125;</code></pre><h4 id="5-4-1-2-创建kubectl证书签名"><a href="#5-4-1-2-创建kubectl证书签名" class="headerlink" title="5.4.1.2 创建kubectl证书签名"></a>5.4.1.2 创建kubectl证书签名</h4><pre><code class="hljs">cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare adminsudo cp admin*pem /opt/kubernetes/ssl</code></pre><h4 id="5-4-1-3-设置环境变量"><a href="#5-4-1-3-设置环境变量" class="headerlink" title="5.4.1.3 设置环境变量"></a>5.4.1.3 设置环境变量</h4><pre><code class="hljs">sudo mkdir -p /root/.kubeKUBE_CONFIG=&quot;/root/.kube/config&quot;KUBE_APISERVER=&quot;https://172.16.100.100:6443&quot;</code></pre><h4 id="5-4-1-4-设置集群参数"><a href="#5-4-1-4-设置集群参数" class="headerlink" title="5.4.1.4 设置集群参数"></a>5.4.1.4 设置集群参数</h4><pre><code class="hljs">sudo kubectl config set-cluster kubernetes \  --certificate-authority=/opt/kubernetes/ssl/ca.pem \  --embed-certs=true \  --server=$&#123;KUBE_APISERVER&#125; \  --kubeconfig=$&#123;KUBE_CONFIG&#125;</code></pre><h4 id="5-4-1-5-设置客户端认证参数"><a href="#5-4-1-5-设置客户端认证参数" class="headerlink" title="5.4.1.5 设置客户端认证参数"></a>5.4.1.5 设置客户端认证参数</h4><pre><code class="hljs">sudo kubectl config set-credentials cluster-admin \  --client-certificate=/opt/kubernetes/ssl/admin.pem \  --client-key=/opt/kubernetes/ssl/admin-key.pem \  --embed-certs=true \  --kubeconfig=$&#123;KUBE_CONFIG&#125;</code></pre><h4 id="5-4-1-6-设置上下文参数"><a href="#5-4-1-6-设置上下文参数" class="headerlink" title="5.4.1.6 设置上下文参数"></a>5.4.1.6 设置上下文参数</h4><pre><code class="hljs">sudo kubectl config set-context default \  --cluster=kubernetes \  --user=cluster-admin \  --kubeconfig=$&#123;KUBE_CONFIG&#125;</code></pre><h4 id="5-4-1-7-设置默认上下文"><a href="#5-4-1-7-设置默认上下文" class="headerlink" title="5.4.1.7 设置默认上下文"></a>5.4.1.7 设置默认上下文</h4><pre><code class="hljs">sudo kubectl config use-context default --kubeconfig=$&#123;KUBE_CONFIG&#125;</code></pre><h3 id="5-4-2-配置kubectl命令补全功能"><a href="#5-4-2-配置kubectl命令补全功能" class="headerlink" title="5.4.2 配置kubectl命令补全功能"></a>5.4.2 配置kubectl命令补全功能</h3><pre><code class="hljs">echo &quot;source &lt;(kubectl completion bash)&quot; &gt;&gt; /root/.bash_profilesource /root/.bash_profile </code></pre><h3 id="5-4-3-授权kubelet-bootstrap用户允许请求证书"><a href="#5-4-3-授权kubelet-bootstrap用户允许请求证书" class="headerlink" title="5.4.3 授权kubelet-bootstrap用户允许请求证书"></a>5.4.3 授权kubelet-bootstrap用户允许请求证书</h3><pre><code class="hljs">kubectl create clusterrolebinding kubelet-bootstrap \--clusterrole=system:node-bootstrapper \--user=kubelet-bootstrap</code></pre><h2 id="5-5-查看集群状态"><a href="#5-5-查看集群状态" class="headerlink" title="5.5 查看集群状态"></a>5.5 查看集群状态</h2><pre><code class="hljs">kubectl get cs</code></pre><h1 id="6-部署Node"><a href="#6-部署Node" class="headerlink" title="6.部署Node"></a>6.部署Node</h1><pre><code class="hljs">ssh node01 mkdir -p /opt/kubernetes/&#123;bin,cfg,ssl,logs&#125;ssh node02 mkdir -p /opt/kubernetes/&#123;bin,cfg,ssl,logs&#125;sudo scp kubernetes/server/bin/&#123;kubelet,kube-proxy&#125; node01:/opt/kubernetes/binsudo scp kubernetes/server/bin/&#123;kubelet,kube-proxy&#125; node02:/opt/kubernetes/binsudo scp /opt/kubernetes/ssl/* node01:/opt/kubernetes/sslsudo scp /opt/kubernetes/ssl/* node02:/opt/kubernetes/sslsudo yum install -y ipvsadm ipsetsudo apt install -y ipvsadm ipset</code></pre><h2 id="6-1-部署kubelet"><a href="#6-1-部署kubelet" class="headerlink" title="6.1 部署kubelet"></a>6.1 部署kubelet</h2><h3 id="6-1-1-创建配置文件"><a href="#6-1-1-创建配置文件" class="headerlink" title="6.1.1 创建配置文件"></a>6.1.1 创建配置文件</h3><pre><code class="hljs">sudo vi /opt/kubernetes/cfg/kubelet.confKUBELET_OPTS=&quot;--logtostderr=false \--v=2 \--log-dir=/opt/kubernetes/logs \--hostname-override=node01 \--kubeconfig=/opt/kubernetes/cfg/kubelet.kubeconfig \--bootstrap-kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig \--config=/opt/kubernetes/cfg/kubelet-config.yml \--cert-dir=/opt/kubernetes/ssl \--pod-infra-container-image=hub.sword.com/library/pause&quot;</code></pre><hr><ul><li>hostname-override，node节点主机名或IP</li><li>pod-infra-container-image，集群默认镜像拉取仓库</li></ul><h3 id="6-1-2-创建参数文件"><a href="#6-1-2-创建参数文件" class="headerlink" title="6.1.2 创建参数文件"></a>6.1.2 创建参数文件</h3><pre><code class="hljs">sudo vi /opt/kubernetes/cfg/kubelet-config.ymlkind: KubeletConfigurationapiVersion: kubelet.config.k8s.io/v1beta1address: 0.0.0.0port: 10250readOnlyPort: 10255cgroupDriver: cgroupfsclusterDNS:- 10.254.0.2clusterDomain: cluster.local failSwapOn: falseauthentication:  anonymous:    enabled: false  webhook:    cacheTTL: 2m0s    enabled: true  x509:    clientCAFile: /opt/kubernetes/ssl/ca.pem authorization:  mode: Webhook  webhook:    cacheAuthorizedTTL: 5m0s    cacheUnauthorizedTTL: 30sevictionHard:  imagefs.available: 15%  memory.available: 100Mi  nodefs.available: 10%  nodefs.inodesFree: 5%maxOpenFiles: 1000000maxPods: 110</code></pre><h3 id="6-1-3-创建kubelet集群认证令牌"><a href="#6-1-3-创建kubelet集群认证令牌" class="headerlink" title="6.1.3 创建kubelet集群认证令牌"></a>6.1.3 创建kubelet集群认证令牌</h3><h4 id="6-1-3-1-设置环境变量"><a href="#6-1-3-1-设置环境变量" class="headerlink" title="6.1.3.1 设置环境变量"></a>6.1.3.1 设置环境变量</h4><pre><code class="hljs">KUBE_APISERVER=&quot;https://172.16.100.100:6443&quot;TOKEN=&quot;5aad5e038ca05f6bcb256c3118e8366c&quot;</code></pre><h4 id="6-1-3-2-设置集群参数"><a href="#6-1-3-2-设置集群参数" class="headerlink" title="6.1.3.2 设置集群参数"></a>6.1.3.2 设置集群参数</h4><pre><code class="hljs">kubectl config set-cluster kubernetes \  --certificate-authority=/opt/kubernetes/ssl/ca.pem \  --embed-certs=true \  --server=$&#123;KUBE_APISERVER&#125; \  --kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig</code></pre><h4 id="6-1-3-3-设置客户端认证参数"><a href="#6-1-3-3-设置客户端认证参数" class="headerlink" title="6.1.3.3 设置客户端认证参数"></a>6.1.3.3 设置客户端认证参数</h4><pre><code class="hljs">kubectl config set-credentials kubelet-bootstrap \  --token=$&#123;TOKEN&#125; \  --kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig</code></pre><h4 id="6-1-3-4-设置上下文参数"><a href="#6-1-3-4-设置上下文参数" class="headerlink" title="6.1.3.4 设置上下文参数"></a>6.1.3.4 设置上下文参数</h4><pre><code class="hljs">kubectl config set-context default \  --cluster=kubernetes \  --user=&quot;kubelet-bootstrap&quot; \  --kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig</code></pre><h4 id="6-1-3-5-切换上下文"><a href="#6-1-3-5-切换上下文" class="headerlink" title="6.1.3.5 切换上下文"></a>6.1.3.5 切换上下文</h4><pre><code class="hljs">kubectl config use-context default --kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfigsudo scp /opt/kubernetes/cfg/bootstrap.kubeconfig node01:/opt/kubernetes/cfgsudo scp /opt/kubernetes/cfg/bootstrap.kubeconfig node02:/opt/kubernetes/cfg</code></pre><h3 id="6-1-4-创建启动脚本"><a href="#6-1-4-创建启动脚本" class="headerlink" title="6.1.4 创建启动脚本"></a>6.1.4 创建启动脚本</h3><pre><code class="hljs">sudo vi /lib/systemd/system/kubelet.service[Unit]Description=Kubernetes KubeletAfter=docker.service[Service]EnvironmentFile=/opt/kubernetes/cfg/kubelet.confExecStart=/opt/kubernetes/bin/kubelet $KUBELET_OPTSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre><h3 id="6-1-5-启动kubelet"><a href="#6-1-5-启动kubelet" class="headerlink" title="6.1.5 启动kubelet"></a>6.1.5 启动kubelet</h3><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start kubelet.servicesudo systemctl enable kubelet</code></pre><h2 id="6-2-审批请求节点加入集群"><a href="#6-2-审批请求节点加入集群" class="headerlink" title="6.2 审批请求节点加入集群"></a>6.2 审批请求节点加入集群</h2><h3 id="6-2-1-查看请求签名的Node"><a href="#6-2-1-查看请求签名的Node" class="headerlink" title="6.2.1 查看请求签名的Node"></a>6.2.1 查看请求签名的Node</h3><pre><code class="hljs">kubectl get csrNAME                                                   AGE   SIGNERNAME                                    REQUESTOR           CONDITIONnode-csr-iWWLUr2rvtR2-8JoxHNUubpq0ui4v5aDl5hidlYbejc   9s    kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Pending</code></pre><h3 id="6-2-2-批准kubelet证书申请"><a href="#6-2-2-批准kubelet证书申请" class="headerlink" title="6.2.2 批准kubelet证书申请"></a>6.2.2 批准kubelet证书申请</h3><pre><code class="hljs">kubectl certificate approve node-csr-iWWLUr2rvtR2-8JoxHNUubpq0ui4v5aDl5hidlYbejccertificatesigningrequest.certificates.k8s.io/node-csr-iWWLUr2rvtR2-8JoxHNUubpq0ui4v5aDl5hidlYbejc approved</code></pre><h2 id="6-3-查看集群中的Node状态"><a href="#6-3-查看集群中的Node状态" class="headerlink" title="6.3 查看集群中的Node状态"></a>6.3 查看集群中的Node状态</h2><pre><code class="hljs">kubectl get nodeNAME     STATUS   ROLES    AGE   VERSIONnode01   Ready    &lt;none&gt;   8s    v1.19.3</code></pre><h3 id="6-3-2-Node节点设置标签"><a href="#6-3-2-Node节点设置标签" class="headerlink" title="6.3.2 Node节点设置标签"></a>6.3.2 Node节点设置标签</h3><pre><code class="hljs">kubectl label nodes node01 node-role.kubernetes.io/worker01=kubectl label nodes node01 node-role.kubernetes.io/etcd-1=# 删除标签etcd-1kubectl label nodes node01 node-role.kubernetes.io/etcd-1-</code></pre><h3 id="6-3-3-分发kubelet的配置文件、参数文件、认证证书、认证令牌、启动脚本到其余节点，并加入集群"><a href="#6-3-3-分发kubelet的配置文件、参数文件、认证证书、认证令牌、启动脚本到其余节点，并加入集群" class="headerlink" title="6.3.3 分发kubelet的配置文件、参数文件、认证证书、认证令牌、启动脚本到其余节点，并加入集群"></a>6.3.3 分发kubelet的配置文件、参数文件、认证证书、认证令牌、启动脚本到其余节点，并加入集群</h3><h2 id="6-4-部署kube-proxy"><a href="#6-4-部署kube-proxy" class="headerlink" title="6.4 部署kube-proxy"></a>6.4 部署kube-proxy</h2><h3 id="6-4-1-创建配置文件"><a href="#6-4-1-创建配置文件" class="headerlink" title="6.4.1 创建配置文件"></a>6.4.1 创建配置文件</h3><pre><code class="hljs">sudo vi /opt/kubernetes/cfg/kube-proxy.confKUBE_PROXY_OPTS=&quot;--logtostderr=false \--v=2 \--log-dir=/opt/kubernetes/logs \--hostname-override=node01 \--proxy-mode=ipvs \--cluster-cidr=172.30.0.0/16 \--kubeconfig=/opt/kubernetes/cfg/kube-proxy.kubeconfig&quot;</code></pre><hr><ul><li>hostname-override，node主机名或IP</li><li>proxy-mode，代理模式，默认为iptables</li><li>cluster-cidr，pod地址池，与controller-manager保持一致</li></ul><h3 id="6-4-2-创建kube-proxy集群认证证书"><a href="#6-4-2-创建kube-proxy集群认证证书" class="headerlink" title="6.4.2 创建kube-proxy集群认证证书"></a>6.4.2 创建kube-proxy集群认证证书</h3><h4 id="6-4-2-1-创建kube-proxy证书签名请求配置文件"><a href="#6-4-2-1-创建kube-proxy证书签名请求配置文件" class="headerlink" title="6.4.2.1 创建kube-proxy证书签名请求配置文件"></a>6.4.2.1 创建kube-proxy证书签名请求配置文件</h4><pre><code class="hljs">vi kube-proxy-csr.json&#123;  &quot;CN&quot;: &quot;system:kube-proxy&quot;,  &quot;hosts&quot;: [],  &quot;key&quot;: &#123;    &quot;algo&quot;: &quot;rsa&quot;,    &quot;size&quot;: 2048  &#125;,  &quot;names&quot;: [    &#123;      &quot;C&quot;: &quot;CN&quot;,      &quot;L&quot;: &quot;BeiJing&quot;,      &quot;ST&quot;: &quot;BeiJing&quot;,      &quot;O&quot;: &quot;k8s&quot;,      &quot;OU&quot;: &quot;System&quot;    &#125;  ]&#125;</code></pre><h4 id="6-4-2-2-创建kube-proxy证书"><a href="#6-4-2-2-创建kube-proxy证书" class="headerlink" title="6.4.2.2 创建kube-proxy证书"></a>6.4.2.2 创建kube-proxy证书</h4><pre><code class="hljs">cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxycp kube-proxy*pem /opt/kubernetes/ssl</code></pre><h4 id="6-4-2-3-设置环境变量"><a href="#6-4-2-3-设置环境变量" class="headerlink" title="6.4.2.3 设置环境变量"></a>6.4.2.3 设置环境变量</h4><pre><code class="hljs">KUBE_APISERVER=&quot;https://172.16.100.100:6443&quot;</code></pre><h4 id="6-4-2-4-设置集群参数"><a href="#6-4-2-4-设置集群参数" class="headerlink" title="6.4.2.4 设置集群参数"></a>6.4.2.4 设置集群参数</h4><pre><code class="hljs">kubectl config set-cluster kubernetes \  --certificate-authority=/opt/kubernetes/ssl/ca.pem \  --embed-certs=true \  --server=$&#123;KUBE_APISERVER&#125; \  --kubeconfig=/opt/kubernetes/cfg/kube-proxy.kubeconfig</code></pre><h4 id="6-4-2-5-设置客户端认证参数"><a href="#6-4-2-5-设置客户端认证参数" class="headerlink" title="6.4.2.5 设置客户端认证参数"></a>6.4.2.5 设置客户端认证参数</h4><pre><code class="hljs">kubectl config set-credentials kube-proxy \  --client-certificate=/opt/kubernetes/ssl/kube-proxy.pem \  --client-key=/opt/kubernetes/ssl/kube-proxy-key.pem \  --embed-certs=true \  --kubeconfig=/opt/kubernetes/cfg/kube-proxy.kubeconfig</code></pre><h4 id="6-4-2-6-设置上下文参数"><a href="#6-4-2-6-设置上下文参数" class="headerlink" title="6.4.2.6 设置上下文参数"></a>6.4.2.6 设置上下文参数</h4><pre><code class="hljs">kubectl config set-context default \  --cluster=kubernetes \  --user=kube-proxy \  --kubeconfig=/opt/kubernetes/cfg/kube-proxy.kubeconfig</code></pre><h4 id="6-4-2-7-切换上下文"><a href="#6-4-2-7-切换上下文" class="headerlink" title="6.4.2.7 切换上下文"></a>6.4.2.7 切换上下文</h4><pre><code class="hljs">kubectl config use-context default --kubeconfig=/opt/kubernetes/cfg/kube-proxy.kubeconfig</code></pre><h3 id="6-4-3-创建启动脚本"><a href="#6-4-3-创建启动脚本" class="headerlink" title="6.4.3 创建启动脚本"></a>6.4.3 创建启动脚本</h3><pre><code class="hljs">sudo vi /lib/systemd/system/kube-proxy.service[Unit]Description=Kubernetes ProxyAfter=network.target[Service]EnvironmentFile=/opt/kubernetes/cfg/kube-proxy.confExecStart=/opt/kubernetes/bin/kube-proxy $KUBE_PROXY_OPTSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre><h3 id="6-4-4-启动kube-proxy"><a href="#6-4-4-启动kube-proxy" class="headerlink" title="6.4.4 启动kube-proxy"></a>6.4.4 启动kube-proxy</h3><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start kube-proxy.servicesudo systemctl enable kube-proxy.service</code></pre><h3 id="6-4-5-分发kube-proxy的配置文件、认证证书、认证令牌、启动脚本到其余节点"><a href="#6-4-5-分发kube-proxy的配置文件、认证证书、认证令牌、启动脚本到其余节点" class="headerlink" title="6.4.5 分发kube-proxy的配置文件、认证证书、认证令牌、启动脚本到其余节点"></a>6.4.5 分发kube-proxy的配置文件、认证证书、认证令牌、启动脚本到其余节点</h3><h1 id="7-部署测试pod"><a href="#7-部署测试pod" class="headerlink" title="7.部署测试pod"></a>7.部署测试pod</h1><h2 id="7-1-创建资源配置清单"><a href="#7-1-创建资源配置清单" class="headerlink" title="7.1 创建资源配置清单"></a>7.1 创建资源配置清单</h2><pre><code class="hljs">vi test.yamlapiVersion: v1kind: Podmetadata:  name: nginx  labels:    app: nginxspec:  containers:  - name: nginx    image: sword618/nginx    imagePullPolicy: IfNotPresent  restartPolicy: Always</code></pre><h1 id="7-2-部署测试pod"><a href="#7-2-部署测试pod" class="headerlink" title="7.2 部署测试pod"></a>7.2 部署测试pod</h1><pre><code class="hljs"> kubectl apply -f test.yaml</code></pre><h2 id="7-3-检查pod状态"><a href="#7-3-检查pod状态" class="headerlink" title="7.3 检查pod状态"></a>7.3 检查pod状态</h2><pre><code class="hljs">kubectl get pods -o wide</code></pre><h1 id="8-部署服务发现组件coredns"><a href="#8-部署服务发现组件coredns" class="headerlink" title="8.部署服务发现组件coredns"></a>8.部署服务发现组件coredns</h1><h2 id="8-1-下载配置文件"><a href="#8-1-下载配置文件" class="headerlink" title="8.1 下载配置文件"></a>8.1 下载配置文件</h2><pre><code class="hljs">wget https://github.com/coredns/deployment/blob/master/kubernetes/coredns.yaml.sed -O coredns.yaml</code></pre><h2 id="8-2-修改配置文件DNS服务器地址"><a href="#8-2-修改配置文件DNS服务器地址" class="headerlink" title="8.2 修改配置文件DNS服务器地址"></a>8.2 修改配置文件DNS服务器地址</h2><ul><li>clusterIP，对应kubelet的clusterDNS，这里改为10.254.0.2</li></ul><h2 id="8-3-部署coredns"><a href="#8-3-部署coredns" class="headerlink" title="8.3 部署coredns"></a>8.3 部署coredns</h2><pre><code class="hljs">kubectl apply -f coredns.yaml</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/ljx1528/article/details/108465272">https://blog.csdn.net/ljx1528/article/details/108465272</a></li><li><a href="https://blog.csdn.net/weixin_47748185/article/details/113651747">https://blog.csdn.net/weixin_47748185/article/details/113651747</a></li><li><a href="https://blog.csdn.net/wtl1992/article/details/121187506">https://blog.csdn.net/wtl1992/article/details/121187506</a></li><li><a href="https://www.cnblogs.com/bixiaoyu/p/11720864.html">https://www.cnblogs.com/bixiaoyu/p/11720864.html</a></li><li><a href="https://blog.csdn.net/jato333/article/details/123956783">https://blog.csdn.net/jato333/article/details/123956783</a></li><li><a href="https://blog.csdn.net/guijianchouxyz/article/details/115585456">https://blog.csdn.net/guijianchouxyz/article/details/115585456</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>容器云</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Frp内网穿透工具的安装与配置</title>
    <link href="/geek/Frp/"/>
    <url>/geek/Frp/</url>
    
    <content type="html"><![CDATA[<p>Frp，是一款跨平台的开源免费的内网穿透工具，实质上就是一个反向代理服务器，将访问公网IP的请求通过端口转发到内网，再将内网服务器的响应信息通过公网IP发送给客户端，从而使处于内网或防火墙后的设备具备了对外提供服务的功能。frp支持TCP、UDP、HTTP及HTTPS等众多协议，速度只受公网IP的带宽限制，功能强大而易用</p><hr><h1 id="1-下载frp软件包"><a href="#1-下载frp软件包" class="headerlink" title="1.下载frp软件包"></a>1.下载frp软件包</h1><pre><code class="hljs"># https://github.com/fatedier/frp/releases  wget https://github.com/fatedier/frp/releases/download/v0.36.2/frp_0.36.2_linux_amd64.tar.gz</code></pre><h1 id="2-配置公网服务器端"><a href="#2-配置公网服务器端" class="headerlink" title="2.配置公网服务器端"></a>2.配置公网服务器端</h1><pre><code class="hljs">tar -xzvf frp_0.36.2_linux_amd64.tar.gzsudo mv frp_0.36.2_linux_amd64 /usr/local/frp  </code></pre><h2 id="2-1-创建配置文件"><a href="#2-1-创建配置文件" class="headerlink" title="2.1 创建配置文件"></a>2.1 创建配置文件</h2><pre><code class="hljs">sudo vi /usr/local/frp/frps.ini [common]# 设置服务端监听端口，用于和内网设备通信bind_port = 8000# 设置服务端可视化仪表板访问端口# dashboard_port = 8001# 设置服务端可视化仪表板用户名# dashboard_user = admin# 设置服务端可视化仪表板登录密码# dashboard_pwd = admin@2020# 设置服务端连接的身份认证令牌密码token = FrpServer@2020</code></pre><h2 id="2-2-创建启动脚本"><a href="#2-2-创建启动脚本" class="headerlink" title="2.2 创建启动脚本"></a>2.2 创建启动脚本</h2><pre><code class="hljs">sudo vi /lib/systemd/system/frps.service [Unit]Description=Frp Server ServiceAfter=network.target[Service]Type=simpleUser=nobodyRestart=on-failureRestartSec=5sExecStart=/usr/local/frp/frps -c /usr/local/frp/frps.ini[Install]WantedBy=multi-user.target</code></pre><h2 id="2-3-启动frp服务"><a href="#2-3-启动frp服务" class="headerlink" title="2.3 启动frp服务"></a>2.3 启动frp服务</h2><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl enable frpssudo systemctl start frps</code></pre><h1 id="3-配置内网客户端"><a href="#3-配置内网客户端" class="headerlink" title="3.配置内网客户端"></a>3.配置内网客户端</h1><pre><code class="hljs">tar -xzvf frp_0.36.2_linux_amd64.tar.gzsudo mv frp_0.36.2_linux_amd64 /usr/local/frp</code></pre><h2 id="3-1-创建配置文件"><a href="#3-1-创建配置文件" class="headerlink" title="3.1 创建配置文件"></a>3.1 创建配置文件</h2><pre><code class="hljs">sudo vi /usr/local/frp/frpc.ini [common]# 设置服务端公网IPserver_addr = 42.192.96.124# 设置服务端通信端口server_port = 8000# 设置服务端身份认证的令牌密码token = FrpServer@2020 [ssh]# 设置转发协议，tcp、udp、httptype = tcp# 设置内网IPlocal_ip = 127.0.0.1# 设置内网端口local_port = 22# 设置服务端转发端口，用于内网流量的转发及外部访remote_port = 221[http]# 设置转发协议，tcp、udp、httptype = tcp# 设置内网IPlocal_ip = 127.0.0.1# 设置内网端口local_port = 80# 设置服务端转发端口，用于内网流量的转发及外部访问remote_port = 880</code></pre><h2 id="3-2-创建启动脚本"><a href="#3-2-创建启动脚本" class="headerlink" title="3.2 创建启动脚本"></a>3.2 创建启动脚本</h2><pre><code class="hljs">sudo vi /lib/systemd/system/frpc.service [Unit]Description=Frp Server ServiceAfter=network.target[Service]Type=simpleUser=nobodyRestart=on-failureRestartSec=5sExecStart=/usr/local/frp/frpc -c /usr/local/frp/frpc.ini[Install]WantedBy=multi-user.target</code></pre><h2 id="3-3-启动frp服务"><a href="#3-3-启动frp服务" class="headerlink" title="3.3 启动frp服务"></a>3.3 启动frp服务</h2><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl enable frpcsudo systemctl start frpc</code></pre><h1 id="4-测试连接"><a href="#4-测试连接" class="headerlink" title="4.测试连接"></a>4.测试连接</h1><pre><code class="hljs">ssh -p221 42.192.96.124</code></pre>]]></content>
    
    
    <categories>
      
      <category>极客</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>极客</tag>
      
      <tag>Frp</tag>
      
      <tag>内网穿透</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CentOS配置Nginx反向代理内网Yum源服务器</title>
    <link href="/linux/CentOSYumLAN/"/>
    <url>/linux/CentOSYumLAN/</url>
    
    <content type="html"><![CDATA[<h1 id="1-yum源服务器配置本地源"><a href="#1-yum源服务器配置本地源" class="headerlink" title="1.yum源服务器配置本地源"></a>1.yum源服务器配置本地源</h1><h2 id="1-1-挂载ISO镜像文件"><a href="#1-1-挂载ISO镜像文件" class="headerlink" title="1.1 挂载ISO镜像文件"></a>1.1 挂载ISO镜像文件</h2><pre><code class="hljs">sudo mkdir -p /mnt/cdromsudo mount -o loop -t iso9660 /home/kvm/images/CentOS-7-x86_64-Minimal-2009.iso /mnt/cdrom</code></pre><h2 id="1-2-创建yum源配置文件"><a href="#1-2-创建yum源配置文件" class="headerlink" title="1.2 创建yum源配置文件"></a>1.2 创建yum源配置文件</h2><pre><code class="hljs">sudo mkdir -p /etc/yum.repos.d/baksudo mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/baksudo vi /etc/yum.repos.d/CentOS-Media.repo[c7-media]name=CentOS-$releasever - Mediabaseurl=file:///mnt/cdromgpgcheck=1enabled=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7</code></pre><h2 id="1-3-清除缓存及旧包"><a href="#1-3-清除缓存及旧包" class="headerlink" title="1.3 清除缓存及旧包"></a>1.3 清除缓存及旧包</h2><pre><code class="hljs">sudo yum clean all &amp;&amp; sudo yum makecache</code></pre><h2 id="1-4-测试本地yum源"><a href="#1-4-测试本地yum源" class="headerlink" title="1.4 测试本地yum源"></a>1.4 测试本地yum源</h2><pre><code class="hljs">sudo yum list</code></pre><h1 id="2-yum源服务器安装nginx"><a href="#2-yum源服务器安装nginx" class="headerlink" title="2.yum源服务器安装nginx"></a>2.yum源服务器安装nginx</h1><pre><code class="hljs">sudo vi /etc/nginx/conf.d/yum.confserver &#123;  listen       80;  server_name  172.16.100.100;    access_log  /var/log/nginx/yum_access.log  main;  error_log  /var/log/nginx/yum_error.log;  location / &#123;    root/mnt/cdrom;    autoindexon;    indexindex.html index.htm;  &#125;</code></pre><p>}</p><h1 id="3-集群其余服务器配置yum源"><a href="#3-集群其余服务器配置yum源" class="headerlink" title="3.集群其余服务器配置yum源"></a>3.集群其余服务器配置yum源</h1><h2 id="3-1-创建yum源配置文件"><a href="#3-1-创建yum源配置文件" class="headerlink" title="3.1 创建yum源配置文件"></a>3.1 创建yum源配置文件</h2><pre><code class="hljs">sudo mkdir /etc/yum.repos.d/bak &amp;&amp; sudo mv *.repo baksudo vi /etc/yum.repos.d/CentOS-Nginx.repo[c7-media]name=CentOS-$releasever - Mediabaseurl=http://172.16.100.100gpgcheck=1enabled=1gpgkey=fhttp://172.16.100.100/RPM-GPG-KEY-CentOS-7</code></pre><h2 id="3-2-清除缓存及旧包"><a href="#3-2-清除缓存及旧包" class="headerlink" title="3.2 清除缓存及旧包"></a>3.2 清除缓存及旧包</h2><pre><code class="hljs">sudo yum clean allsudo yum makecache</code></pre><h2 id="3-3-测试yum源服务器"><a href="#3-3-测试yum源服务器" class="headerlink" title="3.3 测试yum源服务器"></a>3.3 测试yum源服务器</h2><pre><code class="hljs">sudo yum list</code></pre>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>CentOS</tag>
      
      <tag>Yum</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Cloudreve搭建私有云盘</title>
    <link href="/geek/Cloudreve/"/>
    <url>/geek/Cloudreve/</url>
    
    <content type="html"><![CDATA[<p>Cloudreve，由go语言开发的开源免费的网盘系统，用于快速搭建公私兼备的网盘，支持本地服务器、远程服务器、OneDrive、七牛云存储、阿里云OSS、又拍云、Amazon S3等作为存储后端，所存储的图片、视频、音频、Office文档支持在线预览，对于文本文件、Markdown文件支持在线编辑</p><p>Cloudreve是NAS方案的替代，本地服务器接上机械盘可用作家庭私有云存储，搭配aria2离线下载服务器直接可转换为在线视频网站。Cloudreve相比于Nextcloud的优点在于速度快，较为轻量，不像后者那么臃肿，但缺点是没有APP，没有文件自动同步的功能，上传大量文件不是很友好，因为设备锁屏后网络断掉会导致上传失败</p><hr><h1 id="1-下载程序包"><a href="#1-下载程序包" class="headerlink" title="1.下载程序包"></a>1.下载程序包</h1><pre><code class="hljs">mkdir /web/cloudreve &amp;&amp; cd /web/cloudrevewget https://github.com/cloudreve/Cloudreve/releases/download/3.2.1/cloudreve_3.2.1_linux_amd64.tar.gz .</code></pre><h1 id="2-创建配置文件"><a href="#2-创建配置文件" class="headerlink" title="2.创建配置文件"></a>2.创建配置文件</h1><pre><code class="hljs">vi conf.ini[System]Mode = masterListen = :5212ProxyHeader = X-Forwarded-For[Database]Type = mysqlPort = 3306User = cloudrevePassword = cloudreveHost = 127.0.0.1Name = cloudreveCharset = utf8mb4TablePrefix = sword_[Redis]Server = 127.0.0.1:6379Password = redisDB = 1</code></pre><h1 id="3-创建数据库"><a href="#3-创建数据库" class="headerlink" title="3.创建数据库"></a>3.创建数据库</h1><pre><code class="hljs">MariaDB [(none)]&gt; create database cloudreve character set utf8mb4;  Query OK, 1 row affected (0.016 sec)MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON cloudreve.* TO &#39;cloudreve&#39;@&#39;127.0.0.1&#39; IDENTIFIED BY &#39;cloudreve&#39;; Query OK, 0 rows affected (0.023 sec)MariaDB [(none)]&gt; flush privileges;Query OK, 0 rows affected (0.010 sec)</code></pre><h1 id="4-安装redis"><a href="#4-安装redis" class="headerlink" title="4.安装redis"></a>4.安装redis</h1><h1 id="5-启动cloudreve"><a href="#5-启动cloudreve" class="headerlink" title="5.启动cloudreve"></a>5.启动cloudreve</h1><pre><code class="hljs">tar -xzvf cloudreve_3.2.1_linux_amd64.tar.gzchmod +x cloudreve/web/cloudreve/cloudreve -c /web/cloudreve/conf.ini\_ \_ \_ / \\ | \_ \_ \_ | |\_ \_ \_  / / | |/ \_ | | | |/ \_ | ‘/ \_ \\ \\ / / \_ \\  / /| | () | || | (| | | | /\\ V / /  _/||\_\_\_/ \_\_,|_,|| **\_| \_/ \_**|V3.3.1 Commit #a1252c8 Pro=false[Info] 2021-02-20 15:09:46 初始化数据库连接[Info] 2021-02-20 15:09:46 开始进行数据库初始化…[Info] 2021-02-20 15:09:47 初始管理员账号：admin@cloudreve.org[Info] 2021-02-20 15:09:47 初始管理员密码：yXoWFk7p[Info] 2021-02-20 15:09:50 数据库初始化结束[Info] 2021-02-20 15:09:50 初始化任务队列，WorkerNum = 10[Info] 2021-02-20 15:09:50 初始化定时任务…[Info] 2021-02-20 15:09:50 当前运行模式：Master[Info] 2021-02-20 15:09:50 开始监听 :5212</code></pre><hr><ul><li>注：首次启动cloudreve会随机生成管理密码，登录系统后再行修改</li></ul><h1 id="6-创建启动脚本"><a href="#6-创建启动脚本" class="headerlink" title="6.创建启动脚本"></a>6.创建启动脚本</h1><pre><code class="hljs">sudo vi /lib/systemd/system/cloudreve.service[Unit]Description=CloudreveDocumentation=https://docs.cloudreve.orgAfter=network.targetWants=network.targetAfter=mysqld.service[Service]User=swordWorkingDirectory=/web/cloudreveExecStart=/web/cloudreve/cloudreve -c /web/cloudreve/conf.iniRestart=on-abnormalRestartSec=5sKillMode=mixedStandardOutput=nullStandardError=syslog[Install]WantedBy=multi-user.target</code></pre><h1 id="7-启动cloudreve"><a href="#7-启动cloudreve" class="headerlink" title="7.启动cloudreve"></a>7.启动cloudreve</h1><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl enable cloudreve.servicesudo systemctl start cloudreve.service</code></pre><h1 id="8-配置nginx反向代理"><a href="#8-配置nginx反向代理" class="headerlink" title="8.配置nginx反向代理"></a>8.配置nginx反向代理</h1><pre><code class="hljs">sudo vi /etc/nginx/conf.d/cloudreve.confserver &#123;  listen       80;  server_name  localhost;  location / &#123;  access_log  /var/log/nginx/cloudreve_access.log  main;  error_log  /var/log/nginx/cloudreve_error.log;  &#125;&#125;</code></pre><h1 id="9-登录cloudreve，重置管理员密码"><a href="#9-登录cloudreve，重置管理员密码" class="headerlink" title="9.登录cloudreve，重置管理员密码"></a>9.登录cloudreve，重置管理员密码</h1><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://docs.cloudreve.org/">https://docs.cloudreve.org</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>极客</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云存储</tag>
      
      <tag>Cloudreve</tag>
      
      <tag>极客</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Leanote搭建私有云笔记服务器</title>
    <link href="/geek/Leanote/"/>
    <url>/geek/Leanote/</url>
    
    <content type="html"><![CDATA[<p>Leanote，即蚂蚁笔记，由Go语言的web框架revel开发的开源云笔记软件，集知识管理、笔记、分享、博客功能于一身，支持多笔记本、标签分类、笔记共享、添加与保存附件、代码高亮等功能。此外，还支持平台自建和办公协作，适用于规模较小的团队，团队成员共用一个用户组，将笔记共享到这个组里，所有组员都可以浏览、编辑笔记，可以非常方便地进行协作与知识共享</p><hr><h1 id="1-安装mongodb"><a href="#1-安装mongodb" class="headerlink" title="1.安装mongodb"></a>1.安装mongodb</h1><h2 id="1-1-下载mongodb"><a href="#1-1-下载mongodb" class="headerlink" title="1.1 下载mongodb"></a>1.1 下载mongodb</h2><h2 id="1-2-安装mongodb"><a href="#1-2-安装mongodb" class="headerlink" title="1.2 安装mongodb"></a>1.2 安装mongodb</h2><pre><code class="hljs">tar -xzvf mongodb-*.tgzsudo mv mongodb-* /usr/local/mongodbsudo mkdir -p /usr/local/mongodb /var/log/mongodb </code></pre><h2 id="1-3-创建启动脚本"><a href="#1-3-创建启动脚本" class="headerlink" title="1.3 创建启动脚本"></a>1.3 创建启动脚本</h2><pre><code class="hljs">sudo vi /lib/systemd/systemd/mongodb.service[Unit] Description=mongodbAfter=network.target remote-fs.target nss-lookup.target[Service]Type=forkingExecStart=/usr/local/mongodb/bin/mongod --dbpath /usr/local/mongodb/data \--logpath /var/log/mongodb/mongod.log --fork ExecReload=/bin/kill -s HUP $MAINPIDExecStop=/bin/kill -s QUIT $MAINPID PrivateTmp=true[Install] WantedBy=multi-user.target</code></pre><h2 id="1-4-启动mongodb"><a href="#1-4-启动mongodb" class="headerlink" title="1.4 启动mongodb"></a>1.4 启动mongodb</h2><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start mongodb.servicesudo systemctl enable mongodb.service</code></pre><h1 id="2-安装leanote"><a href="#2-安装leanote" class="headerlink" title="2.安装leanote"></a>2.安装leanote</h1><h2 id="2-1-下载leanote二进制安装包"><a href="#2-1-下载leanote二进制安装包" class="headerlink" title="2.1 下载leanote二进制安装包"></a>2.1 下载leanote二进制安装包</h2><p><a href="http://leanote.org/#download">http://leanote.org/#download</a></p><h2 id="2-2-安装leanote"><a href="#2-2-安装leanote" class="headerlink" title="2.2 安装leanote"></a>2.2 安装leanote</h2><pre><code class="hljs">tar -xzvf leanote-linux-amd64-v2.6.1.bin.tar.gzsudo mv leanote-linux-amd64-v2.6.1 /web/leanote</code></pre><h2 id="2-3-修改配置文件"><a href="#2-3-修改配置文件" class="headerlink" title="2.3 修改配置文件"></a>2.3 修改配置文件</h2><pre><code class="hljs">sudo vi /web/leanote/conf/app.confhttp.addr=0.0.0.0http.port=8888site.url=http://localhost:8888adminUsername=admindb.host=127.0.0.1 db.port=27017app.secret=V85ZzBfTnzpsHyjQX4zukbQ8qBtju9y2aDM55VWxAH8Qop21poekx3xkcDVvrD0Y</code></pre><h2 id="2-4-导入leanote初始数据"><a href="#2-4-导入leanote初始数据" class="headerlink" title="2.4 导入leanote初始数据"></a>2.4 导入leanote初始数据</h2><pre><code class="hljs">sudo /usr/local/mongodb/bin/mongorestore -h 127.0.0.1 \-d leanote --dir /web/leanote/mongodb_backup/leanote_install_data</code></pre><h2 id="2-5-创建启动脚本"><a href="#2-5-创建启动脚本" class="headerlink" title="2.5 创建启动脚本"></a>2.5 创建启动脚本</h2><pre><code class="hljs">sudo vi /lib/systemd/system/leanote.service[Unit]Description=Leanote ServiceAfter=syslog.target network.target[Service]User=swordExecStart=/web/leanote/bin/run.shExecReload=/bin/kill -s HUP $MAINPIDExecStop=/bin/kill -s QUIT $MAINPID[Install]WantedBy=multi-user.target</code></pre><h2 id="2-5-启动leanoe"><a href="#2-5-启动leanoe" class="headerlink" title="2.5 启动leanoe"></a>2.5 启动leanoe</h2><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start leanote.servicesudo systemctl enable leanote.service </code></pre><h1 id="3-登录leanote"><a href="#3-登录leanote" class="headerlink" title="3.登录leanote"></a>3.登录leanote</h1><p><a href="http://ip:8888/">http://ip:8888</a></p><p>管理员账号：<a href="mailto:&#x61;&#100;&#x6d;&#x69;&#110;&#64;&#x6c;&#x65;&#97;&#x6e;&#111;&#116;&#101;&#46;&#x63;&#x6f;&#x6d;">&#x61;&#100;&#x6d;&#x69;&#110;&#64;&#x6c;&#x65;&#97;&#x6e;&#111;&#116;&#101;&#46;&#x63;&#x6f;&#x6d;</a><br>初始密码：abc123</p>]]></content>
    
    
    <categories>
      
      <category>极客</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>极客</tag>
      
      <tag>Leanote</tag>
      
      <tag>Mongodb</tag>
      
      <tag>私有云</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ubuntu桌面配置及优化</title>
    <link href="/geek/UbuntuDesktop/"/>
    <url>/geek/UbuntuDesktop/</url>
    
    <content type="html"><![CDATA[<h1 id="1-设置系统禁用UTC"><a href="#1-设置系统禁用UTC" class="headerlink" title="1.设置系统禁用UTC"></a>1.设置系统禁用UTC</h1><pre><code class="hljs">sudo timedatectl set-local-rtc 1 --adjust-system-clock</code></pre><h1 id="2-配置中文环境"><a href="#2-配置中文环境" class="headerlink" title="2.配置中文环境"></a>2.配置中文环境</h1><h2 id="2-1-安装中文字体"><a href="#2-1-安装中文字体" class="headerlink" title="2.1 安装中文字体"></a>2.1 安装中文字体</h2><pre><code class="hljs">sudo apt -y install xfonts-intl-chinese xfonts-wqy fonts-wqy-zenhei fonts-wqy-microhei </code></pre><h2 id="2-2-设置中文字符"><a href="#2-2-设置中文字符" class="headerlink" title="2.2 设置中文字符"></a>2.2 设置中文字符</h2><pre><code class="hljs">sudo dpkg-reconfigure locales</code></pre><h2 id="2-3-安装中文输入法"><a href="#2-3-安装中文输入法" class="headerlink" title="2.3 安装中文输入法"></a>2.3 安装中文输入法</h2><pre><code class="hljs">sudo apt -y install ibus-clutter ibus-libpinyin</code></pre><h1 id="3-安装美化工具"><a href="#3-安装美化工具" class="headerlink" title="3.安装美化工具"></a>3.安装美化工具</h1><pre><code class="hljs">sudo apt -y install gnome-tweak-tool # gnome-shell-extensions chrome-gnome-shell gnome-shell-extentions-dashtodock</code></pre><h1 id="4-下载安装主题及图标"><a href="#4-下载安装主题及图标" class="headerlink" title="4.下载安装主题及图标"></a>4.下载安装主题及图标</h1><h2 id="4-1-安装主题"><a href="#4-1-安装主题" class="headerlink" title="4.1 安装主题"></a>4.1 安装主题</h2><pre><code class="hljs">tar -xzvf WhiteSur.tar.gz &amp;&amp; sudo mv WhiteSur /usr/share/themes</code></pre><h2 id="4-2-安装图标"><a href="#4-2-安装图标" class="headerlink" title="4.2 安装图标"></a>4.2 安装图标</h2><pre><code class="hljs">tar -xzvf Bigsur.tar.gz &amp;&amp; sudo mv Bigsur /usr/share/icons</code></pre><ul><li>注：主题与图标下载网站为 <a href="https://www.gnome-look.org/">https://www.gnome-look.org</a></li></ul><h1 id="5-安装常用插件"><a href="#5-安装常用插件" class="headerlink" title="5.安装常用插件"></a>5.安装常用插件</h1><ul><li><p>dock插件<br>dash to dock</p></li><li><p>托盘插件<br>TopIcons Plus</p></li><li><p>顶部状态栏天气插件<br>OpenWeather</p></li><li><p>注：插件下载网站为 <a href="https://extensions.gnome.org/">https://extensions.gnome.org</a></p></li></ul><h1 id="6-安装微信、QQ"><a href="#6-安装微信、QQ" class="headerlink" title="6.安装微信、QQ"></a>6.安装微信、QQ</h1><pre><code class="hljs">sudo apt install gitgit clone https://gitee.com/wszqkzqk/deepin-wine-for-ubuntu.gitcd deepin-wine-for-ubuntu &amp;&amp; ./install_2.8.22.shwget https://packages.deepin.com/deepin/pool/non-  free/d/deepin.com.wechat/deepin.com.wechat_2.6.2.31deepin0_i386.debwget https://mirrors.aliyun.com/deepin/pool/non- free/d/deepin.com.wechat/deepin.com.qq.im_9.1.8deepin0_i386.debsudo dpkg -i deepin.com*.deb</code></pre><h1 id="7-配置系统开机禁用桌面环境"><a href="#7-配置系统开机禁用桌面环境" class="headerlink" title="7.配置系统开机禁用桌面环境"></a>7.配置系统开机禁用桌面环境</h1><pre><code class="hljs">sudo systemctl set-default multi-user.target</code></pre><h1 id="8-安装软件管理包"><a href="#8-安装软件管理包" class="headerlink" title="8.安装软件管理包"></a>8.安装软件管理包</h1><pre><code class="hljs">sudo apt -y install gnome-software </code></pre><h1 id="9-禁用包管理服务packageKit"><a href="#9-禁用包管理服务packageKit" class="headerlink" title="9.禁用包管理服务packageKit"></a>9.禁用包管理服务packageKit</h1><pre><code class="hljs">sudo systemctl stop packagekit.servicesudo systemctl disable packagekit.servicesudo systemctl mask packagekit.servicesudo mv -v /etc/apt/apt.conf.d/20packagekit&#123;,.disabled&#125;</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://zhuanlan.zhihu.com/p/139305626">https://zhuanlan.zhihu.com/p/139305626</a></li><li><a href="https://www.cnblogs.com/feipeng8848/p/12808128.html">https://www.cnblogs.com/feipeng8848/p/12808128.html</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>极客</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>极客</tag>
      
      <tag>Ubuntu</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ubuntu配置国内Apt源</title>
    <link href="/geek/UbuntuApt/"/>
    <url>/geek/UbuntuApt/</url>
    
    <content type="html"><![CDATA[<h3 id="1-备份apt源配置文件"><a href="#1-备份apt源配置文件" class="headerlink" title="1.备份apt源配置文件"></a>1.备份apt源配置文件</h3><pre><code class="hljs">sudo mv /etc/apt/sources.list /etc/apt/sources.list.bak</code></pre><h3 id="2-创建apt源配置文件"><a href="#2-创建apt源配置文件" class="headerlink" title="2.创建apt源配置文件"></a>2.创建apt源配置文件</h3><pre><code class="hljs">sudo vi /etc/apt/sources.list# ustc mirrorsdeb https://mirrors.ustc.edu.cn/ubuntu/ bionic main restricted universe multiversedeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic main restricted universe multiversedeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-updates main restricted universe multiversedeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-updates main restricted universe multiversedeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-backports main restricted universe multiversedeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-backports main restricted universe multiversedeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-security main restricted universe multiversedeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-security main restricted universe multiversedeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-proposed main restricted universe multiversedeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse</code></pre><h3 id="3-更新软件包"><a href="#3-更新软件包" class="headerlink" title="3.更新软件包"></a>3.更新软件包</h3><pre><code class="hljs"># 更新存储库索引sudo apt update# 升级所有可更新的包sudo apt upgrade -y# 删除不需要的依赖包sudo apt autoremove</code></pre>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
      <category>极客</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>极客</tag>
      
      <tag>Apt</tag>
      
      <tag>Ubuntu</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ubuntu配置无线网络</title>
    <link href="/geek/UbuntuWireless/"/>
    <url>/geek/UbuntuWireless/</url>
    
    <content type="html"><![CDATA[<hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://zhuanlan.zhihu.com/p/587860602">https://zhuanlan.zhihu.com/p/587860602</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
      <category>极客</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>极客</tag>
      
      <tag>Ubuntu</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ubuntu配置静态IP</title>
    <link href="/geek/UbuntuIP/"/>
    <url>/geek/UbuntuIP/</url>
    
    <content type="html"><![CDATA[<h3 id="1-查看网卡名称"><a href="#1-查看网卡名称" class="headerlink" title="1.查看网卡名称"></a>1.查看网卡名称</h3><pre><code class="hljs">ip a</code></pre><h3 id="2-创建网卡配置文件"><a href="#2-创建网卡配置文件" class="headerlink" title="2.创建网卡配置文件"></a>2.创建网卡配置文件</h3><pre><code class="hljs">sudo mv /etc/netplan/01-network-manager-all.yaml /etc/netplan/01-network-manager-all.yaml.baksudo vi /etc/netplan/01-network-manager-all.yaml# This file describes the network interfaces available on your system# For more information, see netplan(5).network:  version: 2  renderer: networkd  ethernets:    enp3s0:      addresses:        - 172.16.100.120/24      gateway4: 172.16.100.1      nameservers:          addresses: [172.16.100.1, 8.8.8.8]        </code></pre><h3 id="3-应用网卡配置"><a href="#3-应用网卡配置" class="headerlink" title="3.应用网卡配置"></a>3.应用网卡配置</h3><pre><code class="hljs">sudo netplan apply</code></pre><h3 id="4-配置vi环境"><a href="#4-配置vi环境" class="headerlink" title="4.配置vi环境"></a>4.配置vi环境</h3><pre><code class="hljs">sudo vi /etc/vim/vimrc.tinyset nocompatibleset backspace=2</code></pre>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
      <category>极客</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>网络</tag>
      
      <tag>极客</tag>
      
      <tag>Ubuntu</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>KVM虚拟机安装与配置</title>
    <link href="/linux/KVM/"/>
    <url>/linux/KVM/</url>
    
    <content type="html"><![CDATA[<p>KVM，Kernel-based Virtual Machine，即基于内核的虚拟机，开源的操作系统虚拟化模块，目前已集成到Linux的各个发行版，用于Linux实现Hypervisor，是基于虚拟化扩展（Intel VT或AMD-V）的X86硬件的原生全虚拟化解决方案</p><p>KVM最初由Qumranet公司的Avi Kivity开发，作为VDI产品的后台虚拟化解决方案。为了简化开发，Avi Kivity并没有选择从底层开始新写一个Hypervisor，通过加载模块的方式使Linux变成一个Hypervisor，硬件管理等还是通过Linux kernel来完成。2006年10月，在先后完成了基本功能、动态迁移以及主要的性能优化之后，Qumranet正式对外宣布了KVM的诞生。同月，KVM模块的源代码被正式纳入Linux kernel，成为内核源代码的一部分。KVM支持多种处理器平台，如最常见的以Intel和AMD为代表的x86和x86_64平台，其余如PowerPC、S&#x2F;390、ARM等非x86架构的平台</p><h1 id="虚拟化"><a href="#虚拟化" class="headerlink" title="虚拟化"></a>虚拟化</h1><p>虚拟化，计算机领域资源管理技术，通过将计算机的各种实体资源(CPU、内 存、存储、网络等)予以抽象和转化，并进行分割、组合，最终实现最大化利用物理资源的解决方案。实现原理是通过引入Virtual Machine Monitor(VMM，虚拟机监控器，也称为Hypervisor) ，将物理主机抽象、分割成多个虚拟的逻辑意义上的主机，向下掌控实际的物理资源，向上支撑多个操作系统及其之上的运行环境和应用程序</p><h2 id="软件虚拟化和硬件虚拟化"><a href="#软件虚拟化和硬件虚拟化" class="headerlink" title="软件虚拟化和硬件虚拟化"></a>软件虚拟化和硬件虚拟化</h2><h3 id="1-软件虚拟化技术"><a href="#1-软件虚拟化技术" class="headerlink" title="1.软件虚拟化技术"></a>1.软件虚拟化技术</h3><p>软件虚拟化，通过纯软件的环境来模拟执行虚拟机操作系统的指令，如QEMU，原理是通过软件的二进制翻译仿真出目标平台呈现给虚拟机，虚拟机的每一条目标平台指令都会被QEMU截取并翻译成宿主机平台的指令，然后交给实际的物理平台执行。显然，由于新增了模拟翻译工作量，其性能是比较差的，软件复杂度也大大增加。但优点是可以呈现各种平台给虚拟机，只需其二进制翻译支持</p><h3 id="2-硬件虚拟化技术"><a href="#2-硬件虚拟化技术" class="headerlink" title="2.硬件虚拟化技术"></a>2.硬件虚拟化技术</h3><p>硬件虚拟化，计算机硬件本身提供了让客户机指令独立执行的能力，不再完全依赖于VMM。以x86架构为例，其提供一个略微受限制的硬件运行环境供客户机运行（non-root mode），绝大多数情况下客户机在此受限环境中运行与原生系统在非虚拟化环境中运行没有什么区别，不需要像软件虚拟化那样每条指令都先翻译再执行。而VMM运行在root mode，拥有完整的硬件访问控制权限。只在少数必要时，某些客户机指令的运行才需要被VMM截获并做相应处理，之后客户机返回并继续运行于non-root mode。其性能接近于原生系统，且极大地简化了VMM的软件设计架构</p><h2 id="半虚拟化和全虚拟化"><a href="#半虚拟化和全虚拟化" class="headerlink" title="半虚拟化和全虚拟化"></a>半虚拟化和全虚拟化</h2><h3 id="1-半虚拟化"><a href="#1-半虚拟化" class="headerlink" title="1.半虚拟化"></a>1.半虚拟化</h3><p>Para-Virtualization，即半虚拟化，是基于软件虚拟化的配合VMM，并通过修改虚拟机操作系统代码，将原来在物理机上执行的一些特权指令修改成可以和VMM直接交互的方式，从而实现操作系统的定制化。这样，就不会再有捕获异常、翻译和模拟的过程，性能损耗较少</p><h3 id="2-全虚拟化"><a href="#2-全虚拟化" class="headerlink" title="2.全虚拟化"></a>2.全虚拟化</h3><p>Full Virtualization，即全虚拟化，客户机的操作系统完全不需改动，所有软件都能在虚拟机中运行。由于全虚拟化需要模拟出完整的、和 物理平台一模一样的平台给客户机，将会增加虚拟化层的复杂度</p><hr><p>2005年硬件虚拟化兴起之前，软件实现的全虚拟化完败于VMM和客户机操作系统协同运作的半虚拟化。2006年之后以Intel VT-x、VT-d为代表的硬件虚拟化技术的兴起，让由硬件虚拟化辅助的全虚拟化全面超过了半虚拟化。但是，以virtio为代表的半虚拟化技术也一直在演进发展，性能上只是略逊于全虚拟化，加之其较少的平台依赖性，依然受到广泛的欢迎</p><h1 id="1-体系架构"><a href="#1-体系架构" class="headerlink" title="1.体系架构"></a>1.体系架构</h1><p>KVM虚拟化的核心主要由两个模块构成，即KVM内核模块和QEMU</p><h2 id="1-1-KVM内核模块"><a href="#1-1-KVM内核模块" class="headerlink" title="1.1 KVM内核模块"></a>1.1 KVM内核模块</h2><p>KVM模块是KVM虚拟化技术的核心部分，目前已集成于Linux内核，是标准的Linux字符集设备（&#x2F;dev&#x2F;kvm），负责宿主机物理CPU和内存的虚拟化，如初始化CPU硬件并打开虚拟化模式、创建虚拟机的内核数据结构、CPU执行模式的切换、vCPU的执行、管理虚拟机的虚拟内存、地址与宿主机物理内存、地址之间的的映射关系等。KVM模块将Linux主机变成为一个虚拟机监视器（VMM），并在原有的Linux两种执行模式基础上新增用于虚拟机运行的客户模式，该客户模式拥有自己独立的内核模式和用户模式</p><h2 id="1-2-QEMU"><a href="#1-2-QEMU" class="headerlink" title="1.2 QEMU"></a>1.2 QEMU</h2><p>QEMU，即Quick Emulator，由法布里斯·贝拉(Fabrice Bellard)以C语言编写的开源的处理器模拟软件，纯软件的实现虚拟化技术，可独立运行，但性能较低。QEMU有两种工作模式：系统模式，可模拟整个计算机系统；用户模式，可运行不同于当前硬件平台的其他平台上的程序，如x86平台运行ARM平台的程序。开源的VirtualBox、Xen虚拟化产品，其核心底层的虚拟化部分就有集成和使用QEMU</p><hr><p>KVM为适配QEMU，将其代码进行部分修改，即为QEMU-KVM，再与KVM组合即成为KVM虚拟化平台，二者相互配合完成虚拟化工作</p><h1 id="2-工作原理"><a href="#2-工作原理" class="headerlink" title="2.工作原理"></a>2.工作原理</h1><h2 id="2-1-KVM内核"><a href="#2-1-KVM内核" class="headerlink" title="2.1 KVM内核"></a>2.1 KVM内核</h2><p>KVM内核运行于内核模式，负责硬件的虚拟化、客户模式的切换及处理因I&#x2F;O或者其他指令引起的客户模式退出，即异常处理。由于计算机用户无法直接跟内核模块交互，因此借助运行于用户模式的QEMU模拟的设备来实现和内核模式的KVM的交互。KVM模块提供&#x2F;dev&#x2F;kvm接口，需要用户空间程序通过借口设置一个客户机虚拟服务器的地址空间，向他提供模拟的I&#x2F;O，并将它的视频显示映射回宿主的显示屏</p><h2 id="2-2-QEMU-KVM"><a href="#2-2-QEMU-KVM" class="headerlink" title="2.2 QEMU-KVM"></a>2.2 QEMU-KVM</h2><p>QEMU-KVM运行于用户模式，将虚拟机以常规Linux进程的方式创建并运行，并模拟虚拟机的硬件设备，如磁盘，网卡，显卡等。QEMU-KVM通过KVM模块提供的系统接口调用进入内核空间，由KVM模块将虚拟机置于CPU的内核模式运行，IO操作则由KVM模块进行模式切换，将会从上次系统调用的接口返回给QEMU-KVM，最后再由QEMU-KVM负责解析和处理。QEMU-KVM依赖于KVM内核的配合，达到了硬件虚拟化的速度，大大弥补了软件虚拟化性能不足的弱点。此外，由于QEMU模拟IO设备效率不高，目前通常采用半虚拟化的virtio方式虚拟IO设备</p><h2 id="2-3-KVM虚拟机"><a href="#2-3-KVM虚拟机" class="headerlink" title="2.3 KVM虚拟机"></a>2.3 KVM虚拟机</h2><p>KVM虚拟机运行于客户模式，是一个标准的Linux进程，其虚拟CPU对应QEMU进程中的一个执行线程，内存空间被映射到QEMU的进程地址空间，在启动时分配</p><h1 id="3-工作流程"><a href="#3-工作流程" class="headerlink" title="3.工作流程"></a>3.工作流程</h1><h2 id="3-1-创建虚拟机"><a href="#3-1-创建虚拟机" class="headerlink" title="3.1 创建虚拟机"></a>3.1 创建虚拟机</h2><p>运行于用户模式的Qemu-kvm通过ioctl系统调用操作&#x2F;dev&#x2F;kvm字符设备，即kvm模块，创建VM和VCPU</p><h2 id="3-2-数据结构初始化"><a href="#3-2-数据结构初始化" class="headerlink" title="3.2 数据结构初始化"></a>3.2 数据结构初始化</h2><p>KVM内核模块负责数据结构的创建即初始化，然后进行CPU模式切换，返回到用户模式</p><h2 id="3-3-虚拟机调度"><a href="#3-3-虚拟机调度" class="headerlink" title="3.3 虚拟机调度"></a>3.3 虚拟机调度</h2><p>Qemu-kvm通过ioctl调用运行VCPU，即调度相应的虚拟机运行</p><h2 id="3-4-运行虚拟机"><a href="#3-4-运行虚拟机" class="headerlink" title="3.4 运行虚拟机"></a>3.4 运行虚拟机</h2><p>Linux内核进行相关处理后，执行VMLAUNCH指令，通过VM-Entry进入虚拟机并运行于非根模式下</p><h2 id="3-5-虚拟机执行指令"><a href="#3-5-虚拟机执行指令" class="headerlink" title="3.5 虚拟机执行指令"></a>3.5 虚拟机执行指令</h2><p>虚拟机执行非特权指令可直接在宿主机物理CPU上运行，特权指令、外部中断、或虚拟机内部异常时将产生VM-Exit，并将相关信息记录到VMCS（virtual-machine control data structures，虚拟机控制数据结构）</p><h1 id="4-管理工具"><a href="#4-管理工具" class="headerlink" title="4.管理工具"></a>4.管理工具</h1><p>虚拟化解决方案离不开良好的管理和运维工具，部署、运维、管理的复杂度与灵活性是企业实施虚拟化时重点考虑的问题。KVM目前已经有从libvirt API、virsh命令行到OpenStack云管理平台等一整套管理工具，尽管与老牌虚拟化巨头VMware提供的商业化虚拟化管理工具相比在功能和易用性上有所差距，但KVM这一整套管理工具都是API化的、开源的，在使用的灵活性以及对其做二次开发的定制化方面仍有一定优势</p><h2 id="4-1-libvirt"><a href="#4-1-libvirt" class="headerlink" title="4.1 libvirt"></a>4.1 libvirt</h2><p>libvirt，最广为流行的对KVM虚拟化进行管理的工具和应用程序接口，已经是事实上的虚拟化接口标准。作为通用的虚拟化API，不但能管理KVM，还能管理VMware、Hyper-V、Xen、VirtualBox等其他虚拟化方案</p><h2 id="4-2-virsh"><a href="#4-2-virsh" class="headerlink" title="4.2 virsh"></a>4.2 virsh</h2><p>virsh，由C语言编写的使用libvirt API的虚拟化管理工具，源代码也在libvirt这个开源项目中，常用的管理KVM虚拟化的命令行工具，对于系统管理员在单个宿主机上进行运维操作可能是最佳选择</p><h2 id="4-3-virt-manager"><a href="#4-3-virt-manager" class="headerlink" title="4.3 virt-manager"></a>4.3 virt-manager</h2><p>virt-manager，虚拟机图形化管理软件，底层与虚拟化交互的部分仍然是调用libvirt API来操作。virt-manager除了提供虚拟机生命周期（包括：创建、启动、停 止、打快照、动态迁移等）管理的基本功能，还提供性能和资源使用率的监控，同时内置了VNC和SPICE客户端，方便图形化连接到虚拟客户机中。virt-manager在RHEL、 CentOS、Fedora等操作系统上是非常流行的虚拟化管理软件，在管理的机器数量规模较小时，virt-manager是很好的选择。因其图形化操作的易用性，成为新手入门学习虚拟化操作的首选管理软件</p><h2 id="4-4-OpenStack"><a href="#4-4-OpenStack" class="headerlink" title="4.4 OpenStack"></a>4.4 OpenStack</h2><p>OpenStack，开源的基础架构即服务（IaaS）云计算管理平台，可用于构建共有云和私有云服务的基础设施，是目前业界使用最广泛功能最强大的云管理平台，不仅提供了管理虚拟机的丰富功能，还有非常多其他重要管理功能，如对象存储、块存储、网络、镜像、身份验证、编排服务、控制面板等，同样是基于libvirt API来完成对底层虚拟化的管理</p><hr><h1 id="1-查看宿主机是否开启虚拟化功能"><a href="#1-查看宿主机是否开启虚拟化功能" class="headerlink" title="1.查看宿主机是否开启虚拟化功能"></a>1.查看宿主机是否开启虚拟化功能</h1><pre><code class="hljs"># 若为0表示未开启，重启进入BIOS开启egrep &#39;(vmx|svm)&#39; /proc/cpuinfo|wc -l</code></pre><h1 id="2-安装kvm"><a href="#2-安装kvm" class="headerlink" title="2.安装kvm"></a>2.安装kvm</h1><pre><code class="hljs">sudo yum -y install qemu-kvm libvirt virt-installsudo apt -y install qemu qemu-kvm libvirt-bin virtinst libosinfo-binsudo apt -y install qemu qemu-kvm libvirt-daemon-system libvirt-clients virtinst libosinfo-bin</code></pre><h1 id="3-将当前用户添加到libvirt组用于管理虚拟机"><a href="#3-将当前用户添加到libvirt组用于管理虚拟机" class="headerlink" title="3.将当前用户添加到libvirt组用于管理虚拟机"></a>3.将当前用户添加到libvirt组用于管理虚拟机</h1><pre><code class="hljs">sudo usermod -a -G libvirt $USER</code></pre><h1 id="4-启动kvm管理工具进程"><a href="#4-启动kvm管理工具进程" class="headerlink" title="4.启动kvm管理工具进程"></a>4.启动kvm管理工具进程</h1><pre><code class="hljs">sudo systemctl start libvirtdsudo systemctl enable libvirtd</code></pre><h1 id="5-安装虚拟机"><a href="#5-安装虚拟机" class="headerlink" title="5.安装虚拟机"></a>5.安装虚拟机</h1><h2 id="5-1-查看可用操作系统类型"><a href="#5-1-查看可用操作系统类型" class="headerlink" title="5.1 查看可用操作系统类型"></a>5.1 查看可用操作系统类型</h2><pre><code class="hljs">osinfo-query os</code></pre><h2 id="5-2-安装centos7虚拟机"><a href="#5-2-安装centos7虚拟机" class="headerlink" title="5.2 安装centos7虚拟机"></a>5.2 安装centos7虚拟机</h2><pre><code class="hljs">sudo virt-install \--name=centos7 --memory=1024,maxmemory=2048 --vcpus=1,maxvcpus=2 --os-variant=centos7.0 \--location=/home/kvm/images/CentOS-7-x86_64-Minimal-2009.iso --disk /home/kvm/templates/centos7.qcow2,size=30 --network network=default \--graphics=none --console=pty,target_type=serial --extra-args=&#39;console=ttyS0&#39;</code></pre><h2 id="5-3-安装debian10虚拟机"><a href="#5-3-安装debian10虚拟机" class="headerlink" title="5.3 安装debian10虚拟机"></a>5.3 安装debian10虚拟机</h2><pre><code class="hljs">sudo virt-install \--name=debian10 --memory=1024,maxmemory=2048 --vcpus=1,maxvcpus=2 --os-variant=debian10 \--location=/home/kvm/images/debian-10.9.0-amd64-netinst.iso --disk /home/kvm/templates/debian10.qcow2,size=30 --network network=default \--graphics=none --console=pty,target_type=serial --extra-args=&#39;console=ttyS0&#39;</code></pre><h2 id="5-4-安装ubuntu18虚拟机"><a href="#5-4-安装ubuntu18虚拟机" class="headerlink" title="5.4 安装ubuntu18虚拟机"></a>5.4 安装ubuntu18虚拟机</h2><pre><code class="hljs">sudo virt-install \--name=ubuntu18 --memory=1024,maxmemory=2048 --vcpus=1,maxvcpus=2 --os-variant=ubuntu18.04 \--location=/home/kvm/images/ubuntu-18.04.5-server-amd64.iso --disk /home/kvm/templates/ubuntu18.qcow2,size=30 --network network=default \--graphics=none --console=pty,target_type=serial --extra-args=&#39;console=ttyS0&#39;</code></pre><h1 id="6-常用虚拟机管理命令"><a href="#6-常用虚拟机管理命令" class="headerlink" title="6.常用虚拟机管理命令"></a>6.常用虚拟机管理命令</h1><pre><code class="hljs"># 查看所有虚拟机状态sudo virsh list –all# 启动虚拟机centos7sudo virsh start centos7# 设置虚拟机开机启动sudo virsh autostart centos7# 解除虚拟机自动启动sudo virsh autostart --disable centos7# 进入虚拟机sudo virsh console centos7# 挂起虚拟机sudo virsh suspend centos7# 恢复挂起的虚拟机sudo virsh resume centos7# 关闭虚拟机sudo virsh shutdown centos7# 强制关闭虚拟机sudo virsh destroy centos7# 删除虚拟机，只删除配置文件，保留虚拟机磁盘sudo virsh undefine centos7</code></pre><h1 id="7-克隆虚拟机"><a href="#7-克隆虚拟机" class="headerlink" title="7.克隆虚拟机"></a>7.克隆虚拟机</h1><pre><code class="hljs"># 克隆之前先关闭虚拟机sudo virsh shutdown centos7sudo virt-clone -o centos7 -n master -f /home/kvm/servers/master.qcow2</code></pre><h1 id="8-kvm开启虚拟机嵌套虚拟化"><a href="#8-kvm开启虚拟机嵌套虚拟化" class="headerlink" title="8.kvm开启虚拟机嵌套虚拟化"></a>8.kvm开启虚拟机嵌套虚拟化</h1><h2 id="8-1-查看宿主机是否已开启嵌套虚拟化功能"><a href="#8-1-查看宿主机是否已开启嵌套虚拟化功能" class="headerlink" title="8.1 查看宿主机是否已开启嵌套虚拟化功能"></a>8.1 查看宿主机是否已开启嵌套虚拟化功能</h2><pre><code class="hljs">cat /sys/module/kvm_intel/parameters/nested</code></pre><h2 id="8-2-创建配置文件"><a href="#8-2-创建配置文件" class="headerlink" title="8.2 创建配置文件"></a>8.2 创建配置文件</h2><pre><code class="hljs">sudo vi /etc/modprobe.d/kvm-nested.confoptions kvm-intel nested=1options kvm-intel enable_shadow_vmcs=1options kvm-intel enable_apicv=1options kvm-intel ept=1</code></pre><h2 id="8-3-关闭所有虚拟机，重新启用kvm-intel模块"><a href="#8-3-关闭所有虚拟机，重新启用kvm-intel模块" class="headerlink" title="8.3 关闭所有虚拟机，重新启用kvm_intel模块"></a>8.3 关闭所有虚拟机，重新启用kvm_intel模块</h2><pre><code class="hljs">modprobe -r kvm_intelmodprobe -a kvm_intel</code></pre><h2 id="8-4-验证嵌套虚拟化功能"><a href="#8-4-验证嵌套虚拟化功能" class="headerlink" title="8.4 验证嵌套虚拟化功能"></a>8.4 验证嵌套虚拟化功能</h2><pre><code class="hljs">cat /sys/module/kvm_intel/parameters/nested</code></pre><h2 id="8-5-设置虚拟机配置文件支持嵌套虚拟化"><a href="#8-5-设置虚拟机配置文件支持嵌套虚拟化" class="headerlink" title="8.5 设置虚拟机配置文件支持嵌套虚拟化"></a>8.5 设置虚拟机配置文件支持嵌套虚拟化</h2><pre><code class="hljs">virsh edit node01&lt;cpu mode=&#39;custom&#39; match=&#39;exact&#39;&gt;  # 开启虚拟机嵌套功能  &lt;feature policy=&#39;require&#39; name=&#39;vmx&#39;/&gt;&lt;/cpu&gt;</code></pre><h2 id="8-6-开启虚拟机，验证嵌套虚拟化功能"><a href="#8-6-开启虚拟机，验证嵌套虚拟化功能" class="headerlink" title="8.6 开启虚拟机，验证嵌套虚拟化功能"></a>8.6 开启虚拟机，验证嵌套虚拟化功能</h2><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://cloud.tencent.com/developer/article/1079148">https://cloud.tencent.com/developer/article/1079148</a></li><li><a href="https://blog.csdn.net/yulsh/article/details/91790804">https://blog.csdn.net/yulsh/article/details/91790804</a></li><li><a href="https://blog.csdn.net/weixin_30875157/article/details/97096593">https://blog.csdn.net/weixin_30875157/article/details/97096593</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>KVM</tag>
      
      <tag>虚拟化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Debian配置国内Apt源</title>
    <link href="/linux/DebianApt/"/>
    <url>/linux/DebianApt/</url>
    
    <content type="html"><![CDATA[<h1 id="1-备份apt源配置文件"><a href="#1-备份apt源配置文件" class="headerlink" title="1.备份apt源配置文件"></a>1.备份apt源配置文件</h1><pre><code class="hljs">sudo mv /etc/apt/sources.list /etc/apt/sources.list.bak</code></pre><h1 id="2-创建apt源配置文件"><a href="#2-创建apt源配置文件" class="headerlink" title="2.创建apt源配置文件"></a>2.创建apt源配置文件</h1><pre><code class="hljs">sudo vi /etc/apt/sources.list# ustc mirrorsdeb http://mirrors.ustc.edu.cn/debian/ buster maindeb http://mirrors.ustc.edu.cn/debian/ buster-updates maindeb-src http://mirrors.ustc.edu.cn/debian/ buster maindeb-src http://mirrors.ustc.edu.cn/debian/ buster-updates maindeb-src http://mirrors.ustc.edu.cn/debian/ buster-backports main non-free contribdeb https://mirrors.ustc.edu.cn/debian/ buster-backports main contrib non-freedeb https://mirrors.ustc.edu.cn/debian-security/ buster/updates main contrib non-freedeb-src https://mirrors.ustc.edu.cn/debian-security/ buster/updates main contrib non-free</code></pre><h1 id="3-更新软件包"><a href="#3-更新软件包" class="headerlink" title="3.更新软件包"></a>3.更新软件包</h1><pre><code class="hljs"># 刷新存储库索引sudo apt update -y# 升级所有可更新的包sudo apt upgrade -y</code></pre><h1 id="4-设置时区及时间格式"><a href="#4-设置时区及时间格式" class="headerlink" title="4.设置时区及时间格式"></a>4.设置时区及时间格式</h1><pre><code class="hljs">ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtimelocalectl set-locale LC_TIME=en_GB.UTF-8</code></pre><h1 id="5-安装常用工具软件"><a href="#5-安装常用工具软件" class="headerlink" title="5.安装常用工具软件"></a>5.安装常用工具软件</h1><pre><code class="hljs">sudo apt install -y sudo bash-completion</code></pre>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Debian</tag>
      
      <tag>Apt</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Debian配置静态IP</title>
    <link href="/linux/DebianIP/"/>
    <url>/linux/DebianIP/</url>
    
    <content type="html"><![CDATA[<h1 id="1-查看系统网卡名称"><a href="#1-查看系统网卡名称" class="headerlink" title="1.查看系统网卡名称"></a>1.查看系统网卡名称</h1><pre><code class="hljs">ip addr</code></pre><h1 id="2-修改网卡配置"><a href="#2-修改网卡配置" class="headerlink" title="2.修改网卡配置"></a>2.修改网卡配置</h1><pre><code class="hljs">sudo cp /etc/network/interfaces /etc/network/interfaces.baksudo vi /etc/network/interfaces # This file describes the network interfaces available on your system# and how to activate them. For more information, see interfaces(5).source /etc/network/interfaces.d/*# The loopback network interfaceauto loiface lo inet loopback# The primary network interfaceallow-hotplug enp3s0auto enp3s0iface enp3s0 inet static  address 192.168.100.100/24  broadcast 192.168.100.255  network 192.168.100.1  gateway 192.168.100.1</code></pre><h1 id="3-重启网络服务"><a href="#3-重启网络服务" class="headerlink" title="3.重启网络服务"></a>3.重启网络服务</h1><pre><code class="hljs">sudo systemctl restart networking.service</code></pre><h1 id="4-配置vim环境"><a href="#4-配置vim环境" class="headerlink" title="4.配置vim环境"></a>4.配置vim环境</h1><pre><code class="hljs">sudo vi /etc/vim/vimrc.tinyset nocompatibleset backspace=2 </code></pre>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>网络</tag>
      
      <tag>Debian</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PostgreSQL数据库编译安装</title>
    <link href="/linux/PostgreSQL/"/>
    <url>/linux/PostgreSQL/</url>
    
    <content type="html"><![CDATA[<p>PostgreSQL，基于C语言实现的先进的开源企业级关系型数据库管理系统，以其强大的扩展性、可靠性、稳定性、数据一致性和对SQL标准的严格遵循享誉业内。作为开源数据库，PostgreSQL拥有媲美商业数据库的功能，且资源消耗更少，还支持互联网特征的功能，如数据类型支持网络地址类型、XML类型、JSON类型、UUID类型及数组类型。PostgreSQL是完全的事务安全性数据库，是唯一能做到零数据丢失的开源数据库。此外，PostgreSQL遵循BDS开源协议，同Linux内核版本一样，任何组织和个人都可对其进行封装而转化为商业化的可盈利的产品，如阿里云的PolarDB‌、华为云的OpenGauss‌和人大金仓的KingBase等</p><h1 id="发展历史"><a href="#发展历史" class="headerlink" title="发展历史"></a>发展历史</h1><p>PostgreSQL起源于1977年加州大学伯克利分校Michael Stonebraker教授开发的用于教学演示的数据库管理系统Ingres</p><h2 id="1-Ingres"><a href="#1-Ingres" class="headerlink" title="1.Ingres"></a>1.Ingres</h2><p>1977年，加州大学伯克利分校著名数据库科学家Michael Stonebraker发起了Ingres项目，并于1982年离开伯克利时商业化，使之成为Relational Technologies公司的产品，后来Relational Tecchnologies被Computer Associates（CA）收购</p><h2 id="2-Postgres"><a href="#2-Postgres" class="headerlink" title="2.Postgres"></a>2.Postgres</h2><p>1985年，Michael Stonebraker回到伯克利，为了解决Ingres中的数据关系维护问题，启动了“后Ingres”（post-Ingres）项目，即为Postgres的源头。这个项目由美国国防高级研究计划局（DARPA）、陆军研究办公室（ARO）、国家科学基金会（NSF）以及ESL公司共同赞助。次年始，Michael Stonebraker教授发表了一系列论文，探讨了新的数据库的结构设计和扩展设计，并于1987年发布了第一个演示性系统，且在1988年的数据管理国际会议（ACM-SIGMOD）上作为展示。后来由于源代码维护的时间日益增加，占用了太多本应该用于数据库研究的时间，为减少支持的负担，伯克利的Postgres项目在发布版本4.2后正式终止</p><h2 id="3-Postgres95"><a href="#3-Postgres95" class="headerlink" title="3.Postgres95"></a>3.Postgres95</h2><p>1994年，来自中国香港的两名伯克利研究生Andrew Yu和Jolly Chen向Postgres中增加了SQL语言的解释器，并将Postgres改名为Postgres95，随后将其源代码共享到互联网。于是，Postgres95成为一个开放源码的原伯克利Postgres代码的继承者</p><h2 id="4-PostgreSQL"><a href="#4-PostgreSQL" class="headerlink" title="4.PostgreSQL"></a>4.PostgreSQL</h2><p>1996年，Postgres95更名为PostgresSQL，意为融合SQL，版本号也重新从6.0开始，即重新使用伯克利Postgres项目的版本顺序。此后，PostgreSQL进入黄金发展阶段，目前Slogan为“世界上最先进的开源关系型数据库” ，号称开源界的Oracle</p><h1 id="1-安装依赖包"><a href="#1-安装依赖包" class="headerlink" title="1.安装依赖包"></a>1.安装依赖包</h1><pre><code class="hljs">sudo yum install -y gcc openssl-devel perl-devel perl-ExtUtils-Embed zlib-devel libxml2-devel readline-devel python3-devel sudo apt install -y gcc libssl-dev libpcre3-dev zlib1g-dev libperl-dev libreadline-dev libxml2-dev python3-dev</code></pre><h1 id="2-创建postgres用户"><a href="#2-创建postgres用户" class="headerlink" title="2.创建postgres用户"></a>2.创建postgres用户</h1><pre><code class="hljs">sudo groupadd postgres &amp;&amp; sudo useradd postgres -m -g postgres</code></pre><h1 id="3-编译安装PostgreSQL"><a href="#3-编译安装PostgreSQL" class="headerlink" title="3.编译安装PostgreSQL"></a>3.编译安装PostgreSQL</h1><pre><code class="hljs">tar -xzvf postgresql-10.0.tar.gz &amp;&amp; cd postgresql-10.0sudo ./configure --prefix=/usr/local/pgsql --with-zlib --with-perl --with-python --with-openssl --with-libxmlsudo make &amp;&amp; sudo make install</code></pre><h1 id="4-编译安装安装第三方工具"><a href="#4-编译安装安装第三方工具" class="headerlink" title="4.编译安装安装第三方工具"></a>4.编译安装安装第三方工具</h1><pre><code class="hljs">cd contribsudo make &amp;&amp; sudo make install</code></pre><h1 id="5-初始化PostgreSQL数据库"><a href="#5-初始化PostgreSQL数据库" class="headerlink" title="5.初始化PostgreSQL数据库"></a>5.初始化PostgreSQL数据库</h1><pre><code class="hljs">sudo mkdir /usr/local/pgsql/datasudo chown -R postgres.postgres /usr/local/pgsqlsu - postgres -c &quot;/usr/local/pgsql/bin/initdb -D /usr/local/pgsql/data&quot;</code></pre><h1 id="6-配置启动脚本"><a href="#6-配置启动脚本" class="headerlink" title="6.配置启动脚本"></a>6.配置启动脚本</h1><pre><code class="hljs">sudo cp contrib/start-scripts/linux /etc/init.d/pgsqldsudo chmod +x /etc/init.d/pgsqld</code></pre><h1 id="7-启动PostgreSQL"><a href="#7-启动PostgreSQL" class="headerlink" title="7.启动PostgreSQL"></a>7.启动PostgreSQL</h1><pre><code class="hljs">sudo /etc/init.d/pgsqld start</code></pre><h1 id="8-验证数据库登录"><a href="#8-验证数据库登录" class="headerlink" title="8.验证数据库登录"></a>8.验证数据库登录</h1><pre><code class="hljs">sudo ln -s /usr/local/pgsql/bin/psql /usr/binpsql -h 127.0.0.1 -U postgres -d postgres</code></pre><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://yunche.pro/blog/?id=303">https://yunche.pro/blog/?id=303</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>SQL</tag>
      
      <tag>数据库</tag>
      
      <tag>PostgreSQL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux系统日志轮转工具Logrotate</title>
    <link href="/linux/Logrotate/"/>
    <url>/linux/Logrotate/</url>
    
    <content type="html"><![CDATA[<p>Logrotate，Linux系统及应用程序日志的自动化管理工具，现行的LInux发行版基本上默认集成，只需指定配置文件作为管理策略即可自动化地按时完成对日志文件的分割与清理。Logrotate工作机制是基于定时任务Crond按照时间或大小自动对日志文件进行切割、压缩及清理，执行周期通常是每天</p><h1 id="1-配置解析"><a href="#1-配置解析" class="headerlink" title="1.配置解析"></a>1.配置解析</h1><pre><code class="hljs">vi /etc/logrotate.d/nginx# 设置日志组存储目录/var/log/nginx/*.log &#123;    # 设置日志周期，daily表示每天，weekly表示每周，monthly表示每月    daily    # 设置日志轮转方式，create，默认方式，适用于可重新打开的日志文件，即新建文件并重命名原文件，同时通知程序重新打开日志文件，如syslog、nginx和MySQL日志等，还可设置轮转后的日志权限，不设置则保持原日志权限；nocreate，不建立新的日志文件；copytruncate，将当前日志先拷贝一份再清空原文件；nocopytruncate，转储日志文件但不清空原文件    create 0640 nginx root    # 设置日志文件保留数    rotate 10    # 设置忽略掉文件找不到的错误    missingok    # 设置空文档不做处理    notifempty    # 设置日志文件超过指定的大小再进行轮转    size 100M    # 设置通过gzip压缩轮转后的日志，默认为nocompress，即不压缩    compress    # 设置是否延迟压缩，即本次轮转的日志下一次轮转时再压缩，nodelaycompress表示本次轮转即压缩    delaycompress    # 设置日志组共享脚本，即所有日志文件都轮转完毕后统一执行一次脚本    sharedscripts    postrotate      /bin/kill -USR1 `cat /run/nginx.pid 2&gt;/dev/null` 2&gt;/dev/null || true    endscript&#125;</code></pre><h1 id="2-工作流程"><a href="#2-工作流程" class="headerlink" title="2.工作流程"></a>2.工作流程</h1><ul><li><p>1.Crond服务每小时读取配置文件&#x2F;etc&#x2F;cron.d&#x2F;0hourly，执行&#x2F;etc&#x2F;cron.hourly&#x2F;0anacron脚本中的anacron程序，以检查是否有因停机而未执行的定时任务</p></li><li><p>2.读取anacron程序的配置文件&#x2F;etc&#x2F;anacrontab，执行&#x2F;etc&#x2F;cron.daily、&#x2F;etc&#x2F;cron.weekly&#x2F;etc&#x2F;cron.monthly配置的脚本</p></li><li><p>3.执行脚本&#x2F;etc&#x2F;cron.daily&#x2F;logrotate，调用logrotate日志切割程序，读取主配置文件及子配置文件的的任务规则以完成日志的切割</p></li></ul><h1 id="3-运行命令"><a href="#3-运行命令" class="headerlink" title="3.运行命令"></a>3.运行命令</h1><ul><li>-d, –debug ：debug 模式，测试配置文件是否有错误</li><li>-f, –force ：强制轮转文件</li><li>-m, –mail&#x3D;command ：压缩日志后，发送日志到指定邮箱</li><li>-s, –state&#x3D;statefile ：使用指定的状态文件</li><li>-v, –verbose ：显示轮转过程</li></ul><h2 id="3-1-调试模式"><a href="#3-1-调试模式" class="headerlink" title="3.1 调试模式"></a>3.1 调试模式</h2><pre><code class="hljs"># 不会真正进行日志切割，而是打印出整个执行流程和调用的脚本等详细信息，用于调试配置logrotate -d -v /etc/logrotate.d/nginx</code></pre><h2 id="3-2-手动模式"><a href="#3-2-手动模式" class="headerlink" title="3.2 手动模式"></a>3.2 手动模式</h2><pre><code class="hljs"># 不到轮转周期或日志大小的条件时手工执行轮转logrotate -f /etc/logrotate.d/nginx</code></pre><h1 id="4-配置示例"><a href="#4-配置示例" class="headerlink" title="4.配置示例"></a>4.配置示例</h1><h2 id="4-1-Nginx"><a href="#4-1-Nginx" class="headerlink" title="4.1 Nginx"></a>4.1 Nginx</h2><pre><code class="hljs">/var/logs/nginx/*access.log &#123;  monthly  missingok  rotate 12  # compress  # delaycompress  copytruncate  notifempty  create 644 root root  sharedscripts  prerotate    if [ -d /etc/logrotate.d/httpd-prerotate ]; then \        run-parts /etc/logrotate.d/httpd-prerotate; \    fi \  endscript  postrotate    [ -s /var/run/nginx.pid ] &amp;&amp; kill -USR1 `cat /var/run/nginx.pid`  endscript&#125;</code></pre><h2 id="4-2-MySQL"><a href="#4-2-MySQL" class="headerlink" title="4.2 MySQL"></a>4.2 MySQL</h2><pre><code class="hljs">/usr/log/mysql/data/mysql-server.log &#123;  weekly  rotate 12  missingok  copytruncate  notifempty  create 644 root root  sharedscripts  postrotate    # just if mysqld is really running    if test -x /usr/local/mysql/bin/mysqladmin &amp;&amp; \       /usr/local/mysql/bin/mysqladmin -h127.0.0.1 -uchecker -pchecking ping &amp;&gt;/dev/null    then       /usr/local/mysql/bin/mysqladmin -h127.0.0.1 -uchecker -pchecking flush-logs    fi  endscript&#125;</code></pre><h2 id="4-3-Zabbix"><a href="#4-3-Zabbix" class="headerlink" title="4.3 Zabbix"></a>4.3 Zabbix</h2><pre><code class="hljs">/var/log/zabbix/*.log &#123;  daily  rotate 7  compress  delaycompress  missingok  notifempty  create 640 zabbix zabbix&#125;</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/m0_68099711/article/details/132587464">https://blog.csdn.net/m0_68099711/article/details/132587464</a></li><li><a href="https://blog.csdn.net/weixin_45565886/article/details/139454804">https://blog.csdn.net/weixin_45565886/article/details/139454804</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>日志分析</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux系统配置Cron定时任务</title>
    <link href="/linux/Crontab/"/>
    <url>/linux/Crontab/</url>
    
    <content type="html"><![CDATA[<p>Cron，Linux系统周期性执行程序的任务调度工具，守护进程为crond，默认安装且开机自启，管理命令为crontab。crond服务定期检查系统中是否有要执行的任务工作，默认每分钟检查一次，若检查到定时任务便会根据预先设定的规则自动执行该任务，类似于闹钟，从而实现计划任务的自动化执行，如日志轮询、数据备份、缓存清理、时钟同步及监控告警等</p><h1 id="1-配置文件"><a href="#1-配置文件" class="headerlink" title="1.配置文件"></a>1.配置文件</h1><ul><li>&#x2F;etc&#x2F;crontab，系统级任务调度列表，该文件所定义的调度任务都以用户名为文件名存储于&#x2F;var&#x2F;spool&#x2F;cron&#x2F;</li><li>&#x2F;etc&#x2F;cron.d&#x2F;，用于存储要执行的crontab文件或脚本的目录，便于以文件粒度对不同用户不同类别的任务进行管理，且所定义的任务调度文件需遵循Cron的命名规范才能被扫描到。系统预设四个目录但并未完全启用，即&#x2F;etc&#x2F;cron.hourly、&#x2F;etc&#x2F;cron.daily、&#x2F;etc&#x2F;cron.weekly、&#x2F;etc&#x2F;cron.monthly，用于定义每小时&#x2F;天&#x2F;周&#x2F;月要执行的任务</li><li>&#x2F;etc&#x2F;cron.allow、etc&#x2F;cron.deny，默认情况下只有root用户可创建定时任务，建议两个文件保留一个，即白名单与黑名单，用于指定允许进行定时任务的普通用户</li></ul><h1 id="2-任务格式"><a href="#2-任务格式" class="headerlink" title="2.任务格式"></a>2.任务格式</h1><pre><code class="hljs">cat /etc/crontab# 设置命令解释器SHELL=/bin/bash# 设置环境变量，即定时任务所调用命令的目录，建议使用命令或脚本的绝对路径，避免环境变量引发的异常问题PATH=/sbin:/bin:/usr/sbin:/usr/bin# 设置任务调用输出信息的电子邮件接收用户MAILTO=root# For details see man 4 crontabs# Example of job definition:# .---------------- minute (0 - 59)# |  .------------- hour (0 - 23)# |  |  .---------- day of month (1 - 31)# |  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...# |  |  |  |  .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat# |  |  |  |  |# *  *  *  *  * user-name  command# 分 时 日  月 周 执行用户    要执行的程序或命令</code></pre><h2 id="2-1-基本格式"><a href="#2-1-基本格式" class="headerlink" title="2.1 基本格式"></a>2.1 基本格式</h2><p>crontab文件包含多个任务，每个任务都被描述为一行，且遵循特定的时间格式，具体如下：</p><pre><code class="hljs">*    *    *    *    *    user    command分钟 小时 日期  月份  星期   用户  要执行的程序或命令</code></pre><ul><li>前5个配置项用于表示任务执行的时间</li><li>user，用于指定执行任务的用户</li><li>command，要执行的程序或命令，需保证有执行权限</li></ul><h2 id="2-2-时间格式"><a href="#2-2-时间格式" class="headerlink" title="2.2 时间格式"></a>2.2 时间格式</h2><p>Cron的任务由时间+动作构成，时间分为分、时、日、月、周五种，操作符如下：</p><ul><li>*，表示每，即取值范围内的所有数字都要执行调度，如30表示每小时的30分</li><li>&#x2F;，表示每过多少个数字，用于指定时间间隔与频率，如*&#x2F;5表示每5分钟，</li><li>-，表示时间区间，如10-15表示每小时的10-15分钟，0-29&#x2F;2表示每个小时前半个小时每2分钟</li><li>,，表示散列数字，即不同时间区间的间隔，如7-11,13-15表示每天7-11点和13-15点</li></ul><h2 id="2-3-命令格式"><a href="#2-3-命令格式" class="headerlink" title="2.3 命令格式"></a>2.3 命令格式</h2><ul><li>单条命令，如* * * * * echo -e $(date ‘+%Y%m%d’) &gt;&gt; &#x2F;root&#x2F;tmp.log</li><li>脚本绝对路径，如* * * * * &#x2F;root&#x2F;scripts&#x2F;test.sh</li><li>多条命令，以;分隔，如* * * * * . &#x2F;etc&#x2F;profile;&#x2F;bin&#x2F;sh &#x2F;root&#x2F;scripts&#x2F;test.sh</li></ul><h1 id="3-执行日志"><a href="#3-执行日志" class="headerlink" title="3.执行日志"></a>3.执行日志</h1><ul><li>&#x2F;var&#x2F;log&#x2F;cron，定时任务日志</li><li>&#x2F;var&#x2F;spool&#x2F;mail&#x2F;root，root用户邮件信息，每次任务都将写入，由此将产生大量的临时小文件，建议将定时任务的结果重定向为NULL或文件，且避免不必要的命令输出，以免小文件过多导致inode不足，影响磁盘写入</li></ul><h1 id="4-任务配置"><a href="#4-任务配置" class="headerlink" title="4.任务配置"></a>4.任务配置</h1><p>Cron定时任务有两种配置方式，即系统定时任务配置文件crontab和crontab命令配置，两种方式并行，建议统一选择一种方式</p><h2 id="4-1-查看任务列表"><a href="#4-1-查看任务列表" class="headerlink" title="4.1 查看任务列表"></a>4.1 查看任务列表</h2><pre><code class="hljs"># 查看当前用户的定时任务列表crontab -l# 查看root的定时任务列表sudo crontab -l</code></pre><h2 id="4-2-新增定时任务"><a href="#4-2-新增定时任务" class="headerlink" title="4.2 新增定时任务"></a>4.2 新增定时任务</h2><pre><code class="hljs">crontab -e# 测试定时任务，建议每条任务都做好注释10 * * * * echo &#39;cron test&#39; &gt; /home/sword/cron.log</code></pre><h2 id="4-3-验证任务列表"><a href="#4-3-验证任务列表" class="headerlink" title="4.3 验证任务列表"></a>4.3 验证任务列表</h2><pre><code class="hljs">crontab -l# 测试定时任务，建议每条任务都做好注释10 * * * * echo &#39;cron test&#39; &gt; /home/sword/cron.logsudo cat /var/spool/cron/sword# 测试定时任务，建议每条任务都做好注释10 * * * * echo &#39;cron test&#39; &gt; /home/sword/cron.log</code></pre><h2 id="4-4-删除定时任务"><a href="#4-4-删除定时任务" class="headerlink" title="4.4 删除定时任务"></a>4.4 删除定时任务</h2><pre><code class="hljs">crontab -r</code></pre><ul><li>注：该命令将会清空用户下所有定时任务，且不可恢复</li></ul><h2 id="4-5-备份定时任务"><a href="#4-5-备份定时任务" class="headerlink" title="4.5 备份定时任务"></a>4.5 备份定时任务</h2><p>由于crontab -r命令将直接清空任务列表，所以不建议使用crontab -e直接编辑，而是将任务列表写入备份文件，再通过crontab file命令更新任务表，以防误操作而无法恢复</p><h3 id="4-5-1-创建任务表文件"><a href="#4-5-1-创建任务表文件" class="headerlink" title="4.5.1 创建任务表文件"></a>4.5.1 创建任务表文件</h3><pre><code class="hljs">vi cron_task.bak# 测试定时任务，建议每条任务都做好注释10 * * * * echo &#39;cron test&#39; &gt; /home/sword/cron.log# 测试定时任务备份，建议每条任务都做好注释45 * * * * echo &#39;cron bak&#39; &gt; /home/sword/cron.bak</code></pre><h3 id="4-5-2-创建定时任务表"><a href="#4-5-2-创建定时任务表" class="headerlink" title="4.5.2 创建定时任务表"></a>4.5.2 创建定时任务表</h3><pre><code class="hljs">crontab cron_task.bak</code></pre><h3 id="4-5-3-验证定时任务"><a href="#4-5-3-验证定时任务" class="headerlink" title="4.5.3 验证定时任务"></a>4.5.3 验证定时任务</h3><pre><code class="hljs">sudo cat /var/spool/cron/sword</code></pre><h1 id="5-配置案例"><a href="#5-配置案例" class="headerlink" title="5.配置案例"></a>5.配置案例</h1><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://juejin.cn/post/7065972818965430286">https://juejin.cn/post/7065972818965430286</a></li><li><a href="https://www.runoob.com/w3cnote/linux-crontab-tasks.html">https://www.runoob.com/w3cnote/linux-crontab-tasks.html</a></li><li><a href="https://blog.csdn.net/qq_37510195/article/details/129530014">https://blog.csdn.net/qq_37510195/article/details/129530014</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Crontab</tag>
      
      <tag>定时任务</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Docker镜像仓库管理</title>
    <link href="/linux/DockerRepository/"/>
    <url>/linux/DockerRepository/</url>
    
    <content type="html"><![CDATA[<p>Repository，即镜像仓库，用于镜像的集中存放与管理。一般的，仓库和仓库注册服务器（Registry）不做区分。实际上，仓库注册服务器是管理仓库的具体服务器，其中往往存放着多个仓库，每个仓库中又包含了多个镜像，每个镜像有不同的标签（tag）仓库可以被认为是一个具体的项目或目录，如仓库地址hub.docker&#x2F;nginx，hub.docker是注册服务器地址，nginx是仓库名</p><hr><h1 id="仓库分类"><a href="#仓库分类" class="headerlink" title="仓库分类"></a>仓库分类</h1><ul><li><p>公共仓库，即Public Registry，最大的公共仓库是Docker Hub，由docker官方维护，存放了数量庞大的镜像供用户下载，通过docker login命令，输入用户名、密码和邮箱来完成注册和登录，本地用户目录的.dockercfg中将保存用户的认证信息</p></li><li><p>私有仓库，即Private Registry，是本地服务器所建的镜像仓库。私有仓库的创建有两种方式，即docker-registry和harbor，前者是官方提供的工具，安装了docker之后直接pull即可；后者是开源工具，Github可下载</p></li></ul><h1 id="registry搭建私有仓库"><a href="#registry搭建私有仓库" class="headerlink" title="registry搭建私有仓库"></a>registry搭建私有仓库</h1><h2 id="1-拉取官方镜像"><a href="#1-拉取官方镜像" class="headerlink" title="1.拉取官方镜像"></a>1.拉取官方镜像</h2><pre><code class="hljs">docker pull registry</code></pre><h2 id="2-创建并启动registry容器"><a href="#2-创建并启动registry容器" class="headerlink" title="2.创建并启动registry容器"></a>2.创建并启动registry容器</h2><pre><code class="hljs">docker run -p 5000:5000 -d --restart=always \-v /var/lib/docker/images:/var/lib/registry \--name sword-registry registry</code></pre><h2 id="3-http方式测试镜像仓库"><a href="#3-http方式测试镜像仓库" class="headerlink" title="3.http方式测试镜像仓库"></a>3.http方式测试镜像仓库</h2><h3 id="3-1-配置docker镜像仓库拉取策略"><a href="#3-1-配置docker镜像仓库拉取策略" class="headerlink" title="3.1 配置docker镜像仓库拉取策略"></a>3.1 配置docker镜像仓库拉取策略</h3><pre><code class="hljs">sudo vi /etc/docker/daemon.json&#123; &quot;insecure-registries&quot;:[&quot;172.16.100.100:5000&quot;]&#125;# 重启docker程序systemctl restart docker</code></pre><h3 id="3-2-镜像推送私有仓库"><a href="#3-2-镜像推送私有仓库" class="headerlink" title="3.2 镜像推送私有仓库"></a>3.2 镜像推送私有仓库</h3><pre><code class="hljs"># 更改nginx镜像标签docker tag nginx:latest 172.16.100.100:5000/nginx:v1.0.0# 推送到私有仓库docker push 172.16.100.100:5000/nginx:v1.0.0</code></pre><h3 id="3-3-查看私有仓库存放的镜像"><a href="#3-3-查看私有仓库存放的镜像" class="headerlink" title="3.3 查看私有仓库存放的镜像"></a>3.3 查看私有仓库存放的镜像</h3><pre><code class="hljs"># 显示所有镜像curl http://172.16.100.100:5000/v2/_catalog# 显示nginx镜像的标签curl http://172.16.100.100:5000/v2/nginx/tags/list</code></pre><h3 id="3-4-从私有仓库拉取镜像"><a href="#3-4-从私有仓库拉取镜像" class="headerlink" title="3.4 从私有仓库拉取镜像"></a>3.4 从私有仓库拉取镜像</h3><pre><code class="hljs"># 删除本地nginx镜像docker rmi 172.16.100.100:5000/nginx:v1.0.0# 从私有仓库拉取nginx镜像docker pull 172.16.100.100:5000/nginx</code></pre><hr><ul><li>注：docker连接仓库默认通过安全的https协议连接，http连接用于测试场景</li></ul><h2 id="4-创建SSL自签名证书"><a href="#4-创建SSL自签名证书" class="headerlink" title="4.创建SSL自签名证书"></a>4.创建SSL自签名证书</h2><h3 id="4-1-创建RSA密钥"><a href="#4-1-创建RSA密钥" class="headerlink" title="4.1 创建RSA密钥"></a>4.1 创建RSA密钥</h3><pre><code class="hljs">openssl genrsa -des3 -out registry.key 2048</code></pre><h3 id="4-2-创建CSR，即证书签名请求文件"><a href="#4-2-创建CSR，即证书签名请求文件" class="headerlink" title="4.2 创建CSR，即证书签名请求文件"></a>4.2 创建CSR，即证书签名请求文件</h3><pre><code class="hljs">openssl req -new -key registry.key \-subj &quot;/C=CN/ST=HeNan/L=ShangQiu/O=Sword/OU=Opt/CN=registry.sword.org&quot; \-out registry.csr</code></pre><h3 id="4-3-生成自签名证书"><a href="#4-3-生成自签名证书" class="headerlink" title="4.3 生成自签名证书"></a>4.3 生成自签名证书</h3><pre><code class="hljs">openssl x509 -req -days 3650 -in registry.csr -signkey registry.key -out registry.crt# 退掉密码mv registry.key registry.bak.keyopenssl rsa -in registry.bak.key -out registry.key</code></pre><h2 id="5-配置nginx反向代理"><a href="#5-配置nginx反向代理" class="headerlink" title="5.配置nginx反向代理"></a>5.配置nginx反向代理</h2><pre><code class="hljs">sudo vi /etc/nginx/conf.d/registry.confserver &#123;    listen       443 ssl;    server_name  localhost;    charset utf-8;    ssl_certificate      /etc/nginx/ssl/registry.crt;    ssl_certificate_key  /etc/nginx/ssl/registry.key;    ssl_session_cache    shared:SSL:1m;    ssl_session_timeout  5m;    ssl_ciphers  ECDHE-RSA-AES128-GCM-SHA256:HIGH:!aNULL:!MD5:!RC4:!DHE;    ssl_prefer_server_ciphers  on;    ssl_protocols TLSv1 TLSv1.1 TLSv1.2;    location / &#123;        # auth_basic &quot;Nginx Server Auth&quot;;        # auth_basic_user_file /etc/nginx/conf.d/.auth_list;        # limit_rate   1024k;        access_log  /var/log/nginx/registry_access.log  main;        error_log  /var/log/nginx/registry_error.log;        proxy_pass http://127.0.0.1:5000;        &#125;&#125;</code></pre><h2 id="6-操作系统信任SSL证书"><a href="#6-操作系统信任SSL证书" class="headerlink" title="6.操作系统信任SSL证书"></a>6.操作系统信任SSL证书</h2><pre><code class="hljs"># Debian/Ubuntusudo cp /etc/nginx/ssl/registry.crt /etc/ssl/certs# CentOSsudo cp /etc/nginx/ssl/registry.crt /etc/pki/tls/certs</code></pre><h2 id="7-启动nginx，重启docker"><a href="#7-启动nginx，重启docker" class="headerlink" title="7.启动nginx，重启docker"></a>7.启动nginx，重启docker</h2><h2 id="8-验证镜像仓库"><a href="#8-验证镜像仓库" class="headerlink" title="8.验证镜像仓库"></a>8.验证镜像仓库</h2><pre><code class="hljs">sudo docker tag nginx registry.sword.org/nginxsudo docker push registry.sword.org/nginx</code></pre><h2 id="9-nginx配置auth-basic，实现镜像仓库访问认证"><a href="#9-nginx配置auth-basic，实现镜像仓库访问认证" class="headerlink" title="9.nginx配置auth_basic，实现镜像仓库访问认证"></a>9.nginx配置auth_basic，实现镜像仓库访问认证</h2><hr><h1 id="harbor搭建私有仓库"><a href="#harbor搭建私有仓库" class="headerlink" title="harbor搭建私有仓库"></a>harbor搭建私有仓库</h1><h2 id="1-配置仓库域名解析"><a href="#1-配置仓库域名解析" class="headerlink" title="1.配置仓库域名解析"></a>1.配置仓库域名解析</h2><pre><code class="hljs">vi /etc/hosts172.16.100.100hub.sword.com</code></pre><h2 id="2-部署docker-compose"><a href="#2-部署docker-compose" class="headerlink" title="2.部署docker-compose"></a>2.部署docker-compose</h2><pre><code class="hljs"># 下载docker-composecurl -L https://github.com/docker/compose/releases/download/1.25.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-composechmod +x /usr/local/bin/docker-composeln -s /usr/local/bin/docker-compose /usr/bin/docker-compose</code></pre><h2 id="3-创建https证书"><a href="#3-创建https证书" class="headerlink" title="3.创建https证书"></a>3.创建https证书</h2><pre><code class="hljs">mkdir -p /var/lib/docker/images/certscd /var/lib/docker/images/certs</code></pre><h3 id="3-1-生成私钥，需设置密码"><a href="#3-1-生成私钥，需设置密码" class="headerlink" title="3.1 生成私钥，需设置密码"></a>3.1 生成私钥，需设置密码</h3><pre><code class="hljs">openssl genrsa -des3 -out harbor.key 4096</code></pre><h3 id="3-2-生成CA证书，需输入密码"><a href="#3-2-生成CA证书，需输入密码" class="headerlink" title="3.2 生成CA证书，需输入密码"></a>3.2 生成CA证书，需输入密码</h3><pre><code class="hljs">openssl req -sha512 -new \-subj &quot;/C=CN/ST=HeNan/L=ShangQiu/O=sword/OU=opt/CN=hub.sword.com&quot; \-key harbor.key \-out harbor.csr</code></pre><h3 id="3-3-备份证书"><a href="#3-3-备份证书" class="headerlink" title="3.3 备份证书"></a>3.3 备份证书</h3><pre><code class="hljs">cp harbor.key harbor.key.org</code></pre><h3 id="3-4-退掉私钥密码以便docker访问，也可参考官方的双向认证"><a href="#3-4-退掉私钥密码以便docker访问，也可参考官方的双向认证" class="headerlink" title="3.4 退掉私钥密码以便docker访问，也可参考官方的双向认证"></a>3.4 退掉私钥密码以便docker访问，也可参考官方的双向认证</h3><pre><code class="hljs">openssl rsa -in harbor.key.org -out harbor.key</code></pre><h3 id="3-5-使用证书进行签名"><a href="#3-5-使用证书进行签名" class="headerlink" title="3.5 使用证书进行签名"></a>3.5 使用证书进行签名</h3><pre><code class="hljs">openssl x509 -req -days 365 -in harbor.csr -signkey harbor.key -out harbor.crt</code></pre><h2 id="4-将https证书发送给连接仓库的docker"><a href="#4-将https证书发送给连接仓库的docker" class="headerlink" title="4.将https证书发送给连接仓库的docker"></a>4.将https证书发送给连接仓库的docker</h2><pre><code class="hljs">mkdir -p /etc/docker/certs.d/hub.sword.comcp /var/lib/docker/images/certs/harbor.crt /etc/docker/certs.d/hub.sword.com</code></pre><h2 id="5-安装harbor"><a href="#5-安装harbor" class="headerlink" title="5.安装harbor"></a>5.安装harbor</h2><pre><code class="hljs">tar -xzvf harbor-offline-installer-v2.0.0.tgzcd harbor &amp;&amp; cp harbor.yml.tmpl harbor.yml</code></pre><h3 id="5-1-创建配置文件"><a href="#5-1-创建配置文件" class="headerlink" title="5.1 创建配置文件"></a>5.1 创建配置文件</h3><pre><code class="hljs">vi harbor.yml# 设置仓库域名，与证书域名一致hostname: hub.sword.com# 设置证书路径certificate: /var/lib/docker/images/certs/harbor.crtprivate_key: /var/lib/docker/images/certs/harbor.key# 设置仓库登录密码，用户名默认为adminharbor_admin_password: Harbor12345# 设置镜像挂载目录data_volume: /var/lib/docker/images# 设置harbor仓库日志目录location: /var/log/harbor</code></pre><h3 id="5-2-安装harbor"><a href="#5-2-安装harbor" class="headerlink" title="5.2 安装harbor"></a>5.2 安装harbor</h3><pre><code class="hljs">./install.sh</code></pre><h2 id="6-镜像的推送与拉取"><a href="#6-镜像的推送与拉取" class="headerlink" title="6.镜像的推送与拉取"></a>6.镜像的推送与拉取</h2><pre><code class="hljs"># 登录仓库docker login hub.sword.com# 设置镜像标签，带上项目名称docker tag nginx hub.sword.com/library/nginxdocker push hub.sword.com/library/nginxdocker pull hub.sword.com/library/nginx</code></pre><h2 id="7-harbor的启动与停止"><a href="#7-harbor的启动与停止" class="headerlink" title="7.harbor的启动与停止"></a>7.harbor的启动与停止</h2><pre><code class="hljs"># 启动harbordocker-compose up -d# 停止harbordocker-compose down</code></pre><h2 id="8-访问镜像仓库"><a href="#8-访问镜像仓库" class="headerlink" title="8.访问镜像仓库"></a>8.访问镜像仓库</h2><p><a href="https://hub.sword.com/">https://hub.sword.com</a></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.cnblogs.com/zhuzi91/p/12364200.html">https://www.cnblogs.com/zhuzi91/p/12364200.html</a></li><li><a href="https://blog.csdn.net/zfw_666666/article/details/128918101">https://blog.csdn.net/zfw_666666/article/details/128918101</a></li><li><a href="https://blog.csdn.net/u013276277/article/details/102994771">https://blog.csdn.net/u013276277/article/details/102994771</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Docker</tag>
      
      <tag>容器</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Docker镜像管理</title>
    <link href="/linux/DockerImage/"/>
    <url>/linux/DockerImage/</url>
    
    <content type="html"><![CDATA[<p>docker镜像，可理解为Linux的文件系统（Root FileSystem），这个文件系统包含可以运行在Linux内核的程序及相应的数据。镜像类似于虚拟机的模版，容器则类似于由镜像创建的虚拟机</p><hr><h1 id="镜像分层"><a href="#镜像分层" class="headerlink" title="镜像分层"></a>镜像分层</h1><ul><li><p>Layer，即分层，镜像由一系列层（layers）组成，这些层都有独立的文件系统（包括文件和文件夹，称之为分支）。docker的UnionFS，即联合文件系统将这些层联合，从而形成一个单独连贯的文件系统，即为镜像。正是由于这个分层的特性，docker才会如此轻量。升级应用程序不用像虚拟机那样重新构建发布整个镜像，只需新增一层即可，这样就使得镜像的构建十分简单与快速。例如，centos镜像中安装nginx就构建了nginx镜像。此时，nginx镜像的底层是centos操作系统镜像，其上叠加一个ngnx层镜像，底层的centos镜像就被称为nginx镜像层的父镜像</p></li><li><p>read-only，即只读，镜像在构建完成之后便不可以再修改，而上面所说的添加一层构建新的镜像，实际是通过创建一个临时的容器，并在容器上增加或删除一些文件而形成的新镜像，所以说容器支持动态改变。当运行容器时，若使用的镜像在本地中不存在，docker就会自动从docker镜像仓库中下载，默认是从Docker Hub公共镜像源下载镜像。当公共仓库的镜像不能满足需求时，可以本地自定义构建镜像</p></li></ul><hr><h1 id="1-镜像操作"><a href="#1-镜像操作" class="headerlink" title="1.镜像操作"></a>1.镜像操作</h1><h2 id="1-1-查看本地镜像"><a href="#1-1-查看本地镜像" class="headerlink" title="1.1 查看本地镜像"></a>1.1 查看本地镜像</h2><pre><code class="hljs">docker images</code></pre><h2 id="1-2-官方公共镜像仓库中搜索nginx镜像"><a href="#1-2-官方公共镜像仓库中搜索nginx镜像" class="headerlink" title="1.2 官方公共镜像仓库中搜索nginx镜像"></a>1.2 官方公共镜像仓库中搜索nginx镜像</h2><pre><code class="hljs">docker search nginx</code></pre><h2 id="1-3-官方公共镜像仓库拉取nginx镜像到本地"><a href="#1-3-官方公共镜像仓库拉取nginx镜像到本地" class="headerlink" title="1.3 官方公共镜像仓库拉取nginx镜像到本地"></a>1.3 官方公共镜像仓库拉取nginx镜像到本地</h2><pre><code class="hljs">docker pull nginx</code></pre><h2 id="1-4-更改镜像的tag，即标签"><a href="#1-4-更改镜像的tag，即标签" class="headerlink" title="1.4 更改镜像的tag，即标签"></a>1.4 更改镜像的tag，即标签</h2><pre><code class="hljs">docker tag nginx:latest nginx:v1.0.0</code></pre><h2 id="1-5-查看镜像元数据"><a href="#1-5-查看镜像元数据" class="headerlink" title="1.5 查看镜像元数据"></a>1.5 查看镜像元数据</h2><pre><code class="hljs">docker inspect nginx:v1.0.0</code></pre><h2 id="1-6-删除镜像"><a href="#1-6-删除镜像" class="headerlink" title="1.6 删除镜像"></a>1.6 删除镜像</h2><pre><code class="hljs">docker rmi nginx</code></pre><h1 id="2-镜像构建"><a href="#2-镜像构建" class="headerlink" title="2.镜像构建"></a>2.镜像构建</h1><h2 id="2-1-修改本地镜像构建新镜像"><a href="#2-1-修改本地镜像构建新镜像" class="headerlink" title="2.1 修改本地镜像构建新镜像"></a>2.1 修改本地镜像构建新镜像</h2><h3 id="2-1-1-创建容器"><a href="#2-1-1-创建容器" class="headerlink" title="2.1.1 创建容器"></a>2.1.1 创建容器</h3><pre><code class="hljs">docker run --name nginx -p 80:80 -d nginx</code></pre><h3 id="2-1-2-容器配置Java环境"><a href="#2-1-2-容器配置Java环境" class="headerlink" title="2.1.2 容器配置Java环境"></a>2.1.2 容器配置Java环境</h3><pre><code class="hljs">docker exec -it nginx /bin/shyum -y install java-1.8.0-openjdk java-1.8.0-openjdk-develexit</code></pre><h3 id="2-1-3-提交容器的修改操作，将之打包为镜像，-m，描述信息；-a，镜像维护者"><a href="#2-1-3-提交容器的修改操作，将之打包为镜像，-m，描述信息；-a，镜像维护者" class="headerlink" title="2.1.3 提交容器的修改操作，将之打包为镜像，-m，描述信息；-a，镜像维护者"></a>2.1.3 提交容器的修改操作，将之打包为镜像，-m，描述信息；-a，镜像维护者</h3><pre><code class="hljs">docker commit -m=&quot;jdk1.8&quot; -a=&quot;admin@sword.com&quot; 2b4e18c57e01 nginx:java</code></pre><h3 id="2-1-4-查看镜像构建历史"><a href="#2-1-4-查看镜像构建历史" class="headerlink" title="2.1.4 查看镜像构建历史"></a>2.1.4 查看镜像构建历史</h3><pre><code class="hljs">docker history nginx:java</code></pre><h3 id="2-1-5-导出镜像"><a href="#2-1-5-导出镜像" class="headerlink" title="2.1.5 导出镜像"></a>2.1.5 导出镜像</h3><pre><code class="hljs">docker save -o docker_nginx.tar.gz nginx:java</code></pre><h3 id="2-1-6-镜像包发送到其他服务器导入镜像"><a href="#2-1-6-镜像包发送到其他服务器导入镜像" class="headerlink" title="2.1.6 镜像包发送到其他服务器导入镜像"></a>2.1.6 镜像包发送到其他服务器导入镜像</h3><pre><code class="hljs">docker load -i docker_nginx.tar.gz</code></pre><h1 id="2-2-dockfile构建新镜像"><a href="#2-2-dockfile构建新镜像" class="headerlink" title="2.2 dockfile构建新镜像"></a>2.2 dockfile构建新镜像</h1><p>Dockfile，由Docker程序解释的包含一系列指令和说明的脚本，其每一条指令都会在镜像上创建一个新层，逐层累积即构建起来完整的镜像</p><p>Dockerfile的指令忽略大小写，一般是大写，每行只支持一条指令，每条指令可以携带多个参数。指令分为两种，即构建指令和设置指令，前者用于构建镜像而不会在容器上执行，后者用于设置镜像的属性，将在容器中进行执行</p><p>Dockerfile分为四部分，即基础镜像信息、维护者信息、镜像操作指令和容器启动指令</p><h3 id="2-2-1-创建Dockerfile文件"><a href="#2-2-1-创建Dockerfile文件" class="headerlink" title="2.2.1 创建Dockerfile文件"></a>2.2.1 创建Dockerfile文件</h3><pre><code class="hljs">vi Dockerfile# 底层镜像FROM alpine# 镜像维护者LABEL MAINTAINER admin@sword.comRUN sed -i &#39;s/dl-cdn.alpinelinux.org/mirrors.tuna.tsinghua.edu.cn/g&#39; /etc/apk/repositories \&amp;&amp; apk update &amp;&amp; apk upgrade &amp;&amp; apk add -U --no-cache ca-certificates curl tzdata bash busybox-extras nginx \&amp;&amp; cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; echo &quot;Asia/shanghai&quot; &gt;&gt; /etc/timezone \&amp;&amp; rm -rf /tmp/* /var/cache/apk/* &amp;&amp; apk del tzdata# 镜像操作指令ADD nginx.conf /etc/nginx# 容器对外暴露的端口EXPOSE 80 443# 环境变量ENV LANG C.UTF-8# 容器启动指令CMD [&quot;/usr/sbin/nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot;]</code></pre><h3 id="2-2-2-构建镜像；-t-指定镜像标签"><a href="#2-2-2-构建镜像；-t-指定镜像标签" class="headerlink" title="2.2.2 构建镜像；-t,指定镜像标签"></a>2.2.2 构建镜像；-t,指定镜像标签</h3><pre><code class="hljs">docker build -t nginx:v1.0.0 .</code></pre><h3 id="2-2-3-查看镜像"><a href="#2-2-3-查看镜像" class="headerlink" title="2.2.3 查看镜像"></a>2.2.3 查看镜像</h3><pre><code class="hljs">docker images</code></pre><hr><ul><li><p>构建tomcat镜像</p><pre><code class="hljs">FROM alpineLABEL MAINTAINER admin@sword.comADD apache-tomcat-8.0.1.tar.gz /optRUN sed -i &#39;s/dl-cdn.alpinelinux.org/mirrors.tuna.tsinghua.edu.cn/g&#39; /etc/apk/repositories \&amp;&amp; apk update &amp;&amp; apk add -U --no-cache tzdata openjdk8 &amp;&amp; mv /opt/apache-tomcat-8.0.1 /opt/tomcat \&amp;&amp; cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; echo &quot;Asia/shanghai&quot; &gt;&gt; /etc/timezone \&amp;&amp; rm -rf /tmp/* /var/cache/apk/* &amp;&amp; apk del tzdataEXPOSE 8080ENV LANG C.UTF-8CMD [&quot;/usr/local/tomcat/bin/catalina.sh&quot;,&quot;run&quot;]</code></pre></li><li><p>构建redis镜像</p><pre><code class="hljs">FROM alpineLABEL MAINTAINER admin@sword.comRUN sed -i &#39;s/dl-cdn.alpinelinux.org/mirrors.tuna.tsinghua.edu.cn/g&#39; /etc/apk/repositories \&amp;&amp; apk update &amp;&amp; apk add -U --no-cache tzdata redis \&amp;&amp; cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; mkdir /etc/redis &amp;&amp; mv /etc/redis.conf /etc/redis &amp;&amp; echo &quot;Asia/shanghai&quot; &gt;&gt; /etc/timezone \&amp;&amp; rm -rf /tmp/* /var/cache/apk/* &amp;&amp; apk del tzdataEXPOSE 6379ENV LANG C.UTF-8ENTRYPOINT /usr/bin/redis-server /etc/redis/redis.conf</code></pre></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Docker</tag>
      
      <tag>容器</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Docker容器管理</title>
    <link href="/linux/DockerContainer/"/>
    <url>/linux/DockerContainer/</url>
    
    <content type="html"><![CDATA[<p>docker容器，是独立运行的一个或一组应用及其环境，是docker镜像的一个实例，核心就是该容器所执行的应用程序。可将容器当作是简易版的Linux环境，这个环境只涵盖了应用程序及其基础运行环境，如root用户权限、进程空间、用户空间和网络空间等<br>容器只用来运行单进程，而不是像虚拟机那样模拟一个完整的操作系统环境。实质上，容器的是设计初衷就是运行一个应用而非一台机器</p><p>docker容器运行方式分为两类，即交互式和守护式，一般都为守护式</p><hr><h1 id="生命周期"><a href="#生命周期" class="headerlink" title="生命周期"></a>生命周期</h1><ul><li>检查本地是否存在指定镜像，若不存在就从公有仓库下载到本地</li><li>加载镜像以创建并启动容器</li><li>分配文件系统给容器，并在只读的镜像层外面挂载一层可读写层</li><li>从宿主机配置的网桥接口中桥接一个虚拟接口到容器，并从地址池分配一个地址</li><li>执行用户指定的应用程序</li><li>应用程序执行完毕后终止容器</li></ul><hr><h1 id="1-查看本地所有容器"><a href="#1-查看本地所有容器" class="headerlink" title="1.查看本地所有容器"></a>1.查看本地所有容器</h1><pre><code class="hljs">docker ps -a</code></pre><h1 id="2-创建容器"><a href="#2-创建容器" class="headerlink" title="2.创建容器"></a>2.创建容器</h1><pre><code class="hljs"># 创建交互式容器，执行完毕即被终止docker run -it ubuntu /bin/bash# 创建守护式容器，后台运行，持续提供服务docker run -p 80:80 -d --name nginx nginx</code></pre><h1 id="3-启停容器"><a href="#3-启停容器" class="headerlink" title="3.启停容器"></a>3.启停容器</h1><pre><code class="hljs"># 启动容器docker start nginx# 查看容器的端口映射docker port nginx# 重启容器docker restart nginx## 停止容器docker stop nginx</code></pre><h1 id="4-进入容器后台并分配一个伪终端"><a href="#4-进入容器后台并分配一个伪终端" class="headerlink" title="4.进入容器后台并分配一个伪终端"></a>4.进入容器后台并分配一个伪终端</h1><pre><code class="hljs">docker exec -it nginx /bin/bash</code></pre><h1 id="5-容器与宿主机之间的文件传输"><a href="#5-容器与宿主机之间的文件传输" class="headerlink" title="5.容器与宿主机之间的文件传输"></a>5.容器与宿主机之间的文件传输</h1><h2 id="5-1-将容器内文件复制到宿主机指定目录"><a href="#5-1-将容器内文件复制到宿主机指定目录" class="headerlink" title="5.1 将容器内文件复制到宿主机指定目录"></a>5.1 将容器内文件复制到宿主机指定目录</h2><pre><code class="hljs">docker cp nginx:/etc/nginx/nginx.conf /root</code></pre><h2 id="5-2-将宿主机文件复制到容器指定目录"><a href="#5-2-将宿主机文件复制到容器指定目录" class="headerlink" title="5.2 将宿主机文件复制到容器指定目录"></a>5.2 将宿主机文件复制到容器指定目录</h2><pre><code class="hljs">docker cp /root/nginx.conf nginx:/etc/nginx</code></pre><h1 id="6-检查容器文件结构的更改"><a href="#6-检查容器文件结构的更改" class="headerlink" title="6.检查容器文件结构的更改"></a>6.检查容器文件结构的更改</h1><pre><code class="hljs">docker diff nginx</code></pre><h1 id="7-查看容器中的所有进程信息，支持管道"><a href="#7-查看容器中的所有进程信息，支持管道" class="headerlink" title="7.查看容器中的所有进程信息，支持管道"></a>7.查看容器中的所有进程信息，支持管道</h1><pre><code class="hljs">docker top nginx</code></pre><h1 id="8-暂停容器"><a href="#8-暂停容器" class="headerlink" title="8.暂停容器"></a>8.暂停容器</h1><h2 id="8-1-暂停nginx容器中所有进程"><a href="#8-1-暂停nginx容器中所有进程" class="headerlink" title="8.1 暂停nginx容器中所有进程"></a>8.1 暂停nginx容器中所有进程</h2><pre><code class="hljs">docker pause nginx</code></pre><h2 id="8-2-恢复nginx容器中所有进程"><a href="#8-2-恢复nginx容器中所有进程" class="headerlink" title="8.2 恢复nginx容器中所有进程"></a>8.2 恢复nginx容器中所有进程</h2><pre><code class="hljs">docker upause nginx</code></pre><h1 id="9-查看docker容器事件"><a href="#9-查看docker容器事件" class="headerlink" title="9.查看docker容器事件"></a>9.查看docker容器事件</h1><h2 id="9-1-查看2020年8月1日后的所有事件"><a href="#9-1-查看2020年8月1日后的所有事件" class="headerlink" title="9.1 查看2020年8月1日后的所有事件"></a>9.1 查看2020年8月1日后的所有事件</h2><pre><code class="hljs">docker events  --since=&quot;2020-08-01&quot;</code></pre><h2 id="9-2-持续输出docker镜像为nginx-latest的相关事件，直到2020年9月1日"><a href="#9-2-持续输出docker镜像为nginx-latest的相关事件，直到2020年9月1日" class="headerlink" title="9.2 持续输出docker镜像为nginx:latest的相关事件，直到2020年9月1日"></a>9.2 持续输出docker镜像为nginx:latest的相关事件，直到2020年9月1日</h2><pre><code class="hljs">docker events -f &quot;image&quot;=&quot;nginx:latest&quot; --until=&quot;2020-09-01&quot;</code></pre><h1 id="10-查看容器元数据"><a href="#10-查看容器元数据" class="headerlink" title="10.查看容器元数据"></a>10.查看容器元数据</h1><pre><code class="hljs">docker inspect 92e5e5f86102</code></pre><h1 id="11-导出导入容器"><a href="#11-导出导入容器" class="headerlink" title="11.导出导入容器"></a>11.导出导入容器</h1><h2 id="11-1-将nginx容器的文件系统导出到tar归档文件"><a href="#11-1-将nginx容器的文件系统导出到tar归档文件" class="headerlink" title="11.1 将nginx容器的文件系统导出到tar归档文件"></a>11.1 将nginx容器的文件系统导出到tar归档文件</h2><pre><code class="hljs">docker export -o nginx-`date +%Y%m%d`.tar nginx</code></pre><h2 id="11-2-将nginx容器的文件系统的归档文件导入为镜像"><a href="#11-2-将nginx容器的文件系统的归档文件导入为镜像" class="headerlink" title="11.2 将nginx容器的文件系统的归档文件导入为镜像"></a>11.2 将nginx容器的文件系统的归档文件导入为镜像</h2><pre><code class="hljs">cat nginx-20200801.tar | docker import - nginx:v1.0.0</code></pre><h1 id="12-删除容器"><a href="#12-删除容器" class="headerlink" title="12.删除容器"></a>12.删除容器</h1><pre><code class="hljs">docker rm nginx</code></pre>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Docker</tag>
      
      <tag>容器</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Docker的安装与配置</title>
    <link href="/linux/Docker/"/>
    <url>/linux/Docker/</url>
    
    <content type="html"><![CDATA[<p>Docker，由go语言基于是Linux容器（LXC）等技术开发的开源应用容器引擎，致力于实现轻量级的操作系统虚拟化解决方案，是当前最热门的容器化技术之一。相比于传统的虚拟化技术，容器是在操作系统层面上实现的虚拟化，直接复用本地主机的操作系统，而传统的虚拟化方式则是在硬件层面实现</p><hr><h3 id="docker与虚拟机"><a href="#docker与虚拟机" class="headerlink" title="docker与虚拟机"></a>docker与虚拟机</h3><ul><li><p>两者都是虚拟化资源管理技术，是将计算机的各种实体资源，如服务器、网络、内存等抽象转化后呈现出来，使用户以更好的方式来应用这些资源。虚拟化目标往往是为了在同一个主机上运行多个系统或者应用，从而提高资源的利用率，降低成本，方便管理及容错容</p></li><li><p>虚拟机是在硬件层面实现虚拟化，需要有额外的虚拟机管理应用和虚拟机操作系统，运行的是完整的操作系统。虚拟机擅长于彻底隔离整个运行环境，如云服务提供商通常采用虚拟机技术隔离不同的用户</p></li><li><p>docker容器技术是在操作系统层面上实现虚拟化，运行的是不完整的操作系统当然也能运行完整的操作系统，通常用于隔离不同的应用，如前端、后端及数据库</p></li></ul><hr><p>简单来说，虚拟机是虚拟出一台服务器来运行应用程序，docker只是虚拟出了保障应用程序的运行所需的环境</p><h3 id="体系架构"><a href="#体系架构" class="headerlink" title="体系架构"></a>体系架构</h3><p>docker是典型的C&#x2F;S架构，其中docker daemon作为server端接受client的请求，然后进行处理，如容器的创建、运行、分发。两者可以运行在一个机器上，也可通过socket或者RESTful API通信</p><ul><li><p>docker daemon，即docker守护进程，一般在宿主主机后台运行，用户并不直接和守护进程进行交互，而是通过 Docker client间接和其通信</p></li><li><p>Docker client，即docker客户端，实际是docker的二进制程序，是用户与 Docker 交互方式，接收用户指令并且与后台的docker守护进程通信</p></li></ul><hr><h3 id="底层实现"><a href="#底层实现" class="headerlink" title="底层实现"></a>底层实现</h3><p>docker底层的两个核心技术是Namespaces和Control groups</p><h5 id="1-Namespaces，即命名空间，用于容器资源的隔离"><a href="#1-Namespaces，即命名空间，用于容器资源的隔离" class="headerlink" title="1.Namespaces，即命名空间，用于容器资源的隔离"></a>1.Namespaces，即命名空间，用于容器资源的隔离</h5><ul><li><p>pid namespace，进程隔离，不同namespace中可以有相同pid。所有LXC进程在docker中的父进程为docker进程，每个lxc进程具有不同的namespace</p></li><li><p>net namespace，网络隔离，虽然pid namespace实现了进程的隔离，但网络端口还是共享宿主机端口，每个net namespace有独立的network、devices、IP addresses、IP routing tables、&#x2F;proc&#x2F;net等目录，从而实现了容器的网络隔离。docker默认采用veth的方式将容器中的虚拟网卡同宿主机上的docker bridge:docker0相连接</p></li><li><p>ipc namespace，进程通信隔离，容器间进程交互还是采用linux常见的进程间交互方法 (interprocess communication，即IPC)，包括常见的信号量、消息队列和共享内存，容器进程间的交互实际上还是宿主机上具有相同pid namespace中的进程间交互</p></li><li><p>mnt namespace，文件系统隔离，类似chroot，将一个进程放到一个特定的目录执行。mnt namespace允许不同namespace的进程看到不同的文件结构，这样每个namespace中的进程所看到的文件目录就被隔离开了。在容器里看到的文件系统就是一个完整的linux系统，有&#x2F;etc、&#x2F;lib等，通过chroot来实现</p></li><li><p>uts namespace，UNIX分时隔离，即UNIX Time-sharing System，允许每个容器拥有独立的hostname和domain name，使其在网络上可以被视作一个独立的节点而非宿主机上的一个进程</p></li><li><p>user namespace，用户隔离，允许每个容器可以有不同的user和group id，容器内部的用户是独立的存在，可以用于在容器内部执行程序而不是一定要用宿主机上的用户</p></li></ul><hr><h4 id="2-cgroups，Control-groups，即控制组，用于资源的配额与度量"><a href="#2-cgroups，Control-groups，即控制组，用于资源的配额与度量" class="headerlink" title="2.cgroups，Control groups，即控制组，用于资源的配额与度量"></a>2.cgroups，Control groups，即控制组，用于资源的配额与度量</h4><p>Namespaces实现了容器操作系统层的隔离，对外展现出一种独立计算机的能力，但不同的namespace之间资源还是会相互竞争，这就需要类似ulimit来管理每个容器所能使用的资源，控制组就是用于实现这个功能  </p><h3 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h3><ul><li>镜像，images，只读的模版，类似于虚拟机的镜像，用于创建容器  </li><li>容器，container，由镜像创建的可运行实例，类似于linux系统环境，用于运行和隔离应用程序  </li><li>仓库，repository，镜像的集中存放地，分为公共仓库和私有仓库，多个仓库组成仓库注册服务器</li></ul><hr><h1 id="1-下载安装包"><a href="#1-下载安装包" class="headerlink" title="1.下载安装包"></a>1.下载安装包</h1><pre><code class="hljs">wget https://download.docker.com/linux/static/stable/x86_64/docker-19.03.12.tgz</code></pre><h1 id="2-安装docker"><a href="#2-安装docker" class="headerlink" title="2.安装docker"></a>2.安装docker</h1><pre><code class="hljs"># sudo apt-get update &amp;&amp; sudo apt-get install -y docker.io  # curl -fsSL https://get.docker.com -o get-docker.sh &amp;&amp; sh get-docker.sh  tar -xvf docker-19.03.12.tgz  sudo cp docker/* /usr/bin </code></pre><h1 id="3-配置仓库加速"><a href="#3-配置仓库加速" class="headerlink" title="3.配置仓库加速"></a>3.配置仓库加速</h1><pre><code class="hljs">sudo mkdir -p /etc/dockersudo vi /etc/docker/daemon.json  &#123;    &quot;registry-mirrors&quot;: [    &quot;https://registry.docker-cn.com&quot;,    &quot;https://docker.mirrors.ustc.edu.cn&quot;    ]  &#125;  </code></pre><h1 id="4-创建启动脚本"><a href="#4-创建启动脚本" class="headerlink" title="4.创建启动脚本"></a>4.创建启动脚本</h1><pre><code class="hljs">sudo vi /lib/systemd/system/docker.service  [Unit]Description=Docker Application Container Engine  Documentation=https://docs.docker.com  After=network-online.target firewalld.service  Wants=network-online.target  [Service]  Type=notify  ExecStart=/usr/bin/dockerd  ExecReload=/bin/kill -s HUP $MAINPID  LimitNOFILE=infinity  LimitNPROC=infinity  LimitCORE=infinity  TimeoutStartSec=0  Delegate=yes  KillMode=process  Restart=on-failure  StartLimitBurst=3  StartLimitInterval=60s  [Install]  WantedBy=multi-user.target  </code></pre><h1 id="5-启动Docker"><a href="#5-启动Docker" class="headerlink" title="5.启动Docker"></a>5.启动Docker</h1><pre><code class="hljs">sudo systemctl daemon-reload sudo systemctl start docker.servicesudo systemctl enable docker.service </code></pre><h1 id="6-拉取镜像"><a href="#6-拉取镜像" class="headerlink" title="6.拉取镜像"></a>6.拉取镜像</h1><pre><code class="hljs"># 官方公共镜像仓库中搜索nginx镜像  sudo docker search nginx  # 拉取nginx镜像到本地  sudo docker pull nginx  # 查看本地镜像  sudo docker images</code></pre><h1 id="7-创建容器"><a href="#7-创建容器" class="headerlink" title="7.创建容器"></a>7.创建容器</h1><pre><code class="hljs"># 创建nginx容器实例，使之后台运行，暴露80端口  sudo docker run --name nginx -p 80:80 -d nginx  # 查看所有容器实例sudo docker ps -a  # 查看nginx实例日志sudo docker logs nginx</code></pre>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Docker</tag>
      
      <tag>容器</tag>
      
      <tag>云原生</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ELK实时分析Nginx日志</title>
    <link href="/linux/ELK-Nginx/"/>
    <url>/linux/ELK-Nginx/</url>
    
    <content type="html"><![CDATA[<p>Nginx是云计算最基础最常用的组件，许多业务架构的入口流量都是由其进行转发，其日志对业务的动作分析有着重大意义</p><p>Logstash，日志收集工具，可以从本地磁盘、网络服务、消息队列中收集各种日志，自带的grok具有强大的日志解析、切割功能。其工作流程大致分为三个阶段:输入input –&gt; 处理filter（非必须） –&gt; 输出output</p><p>Logstash配置文件由三部分组成</p><ul><li>input{}：此模块负责收集日志，可从文件、消息队列（redis、kafka、MQ等）、tcp端口等产生日志的业务直接写入logstash</li><li>filter{}：此模块负责过滤收集到的日志，并根据过滤后对日志定义显示字段</li><li>output{}：此模块负责将过滤后的日志输出到elasticsearch或者文件、redis等</li></ul><hr><h1 id="1-部署ELK"><a href="#1-部署ELK" class="headerlink" title="1.部署ELK"></a>1.部署ELK</h1><h1 id="2-配置nginx日志格式"><a href="#2-配置nginx日志格式" class="headerlink" title="2.配置nginx日志格式"></a>2.配置nginx日志格式</h1><pre><code class="hljs">sudo vi /etc/nginx/nginx.conflog_format  main  &#39;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#39;              &#39;$status $body_bytes_sent &quot;$http_referer&quot; &#39;              &#39;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot; &#39;               &#39;&quot;$request_time&quot; &quot;$upstream_response_time&quot; &quot;$upstream_addr&quot; &#39;;</code></pre><h1 id="3-配置logstash过滤nginx日志"><a href="#3-配置logstash过滤nginx日志" class="headerlink" title="3.配置logstash过滤nginx日志"></a>3.配置logstash过滤nginx日志</h1><pre><code class="hljs">sudo vi /home/sword/logstash/config/nginx.ymlinput&#123;  file &#123;  path =&gt; [&quot;/var/log/nginx/*access.log&quot;]  type =&gt; &quot;nginx_access&quot;  start_position =&gt; &quot;beginning&quot;  &#125;  file &#123;    path =&gt; [&quot;/var/log/nginx/*error.log&quot;]    type =&gt; &quot;nginx_error&quot;    start_position =&gt; &quot;beginning&quot;    &#125;&#125;# 配置过滤器filter &#123;if [type] == &quot;nginx_access&quot; &#123;  # 配置正则表达式结构化nginx日志  grok &#123;match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;IPORHOST:remote_addr&#125; - %&#123;DATA:remote_user&#125; \[%&#123;HTTPDATE:access_time&#125;\] \&quot;%&#123;WORD:http_method&#125; %&#123;DATA:url&#125; HTTP/%&#123;NUMBER:http_version&#125;\&quot; %&#123;NUMBER:response_code&#125; %&#123;NUMBER:bytes_sent&#125; %&#123;NUMBER:body_bytes_sent&#125; \&quot;%&#123;DATA:http_referer&#125;\&quot; \&quot;%&#123;DATA:http_user_agent&#125;\&quot; \&quot;%&#123;DATA:http_x_forwarded_for&#125;\&quot; \&quot;%&#123;DATA:request_time&#125;\&quot; \&quot;%&#123;DATA:upstream_response_time&#125;\&quot; \&quot;%&#123;DATA:upstream_status&#125;\&quot; \&quot;%&#123;HOSTPORT:upstream_addr&#125;\&quot;&quot;&#125;    &#125;  # 配置时间处理规则  date &#123;    match =&gt; [&quot;access_time&quot;, &quot;dd/MMM/yyyy:HH:mm:ss Z&quot;]    remove_field =&gt; [&quot;timestamp&quot;]    &#125;  if [remote_addr] != &quot;-&quot; &#123;   # 配置IP解析规则   geoip &#123;     source =&gt; &quot;remote_addr&quot;     database =&gt; &quot;/home/sword/logstash/tools/geoip/GeoLite2-City.mmdb&quot;     add_field =&gt; [ &quot;[geoip][coordinates]&quot;, &quot;%&#123;[geoip][longitude]&#125;&quot; ]     add_field =&gt; [ &quot;[geoip][coordinates]&quot;, &quot;%&#123;[geoip][latitude]&#125;&quot;  ]     fields =&gt; [&quot;ip&quot;,&quot;city_name&quot;,&quot;region_name&quot;,&quot;country_name&quot;,&quot;continent_code&quot;,&quot;longitude&quot;,&quot;latitude&quot;,&quot;location&quot;]    &#125;  &#125;  # 配置浏览器解析规则  if [http_user_agent] != &quot;-&quot; &#123;    useragent &#123;      target =&gt; &quot;http_user&quot;      source =&gt; &quot;http_user_agent&quot;      &#125;  &#125;# if [url] =~ &quot;jpress/article&quot; or [url] =~ &quot;post&quot; &#123;    #mutate &#123;         #add_field =&gt; &#123;&quot;url_tmp&quot; =&gt; &quot;%&#123;[url]&#125;&quot;&#125;       #&#125; #&#125; #mutate &#123;   #split =&gt; [&quot;url_tmp&quot;,&quot;/&quot;]   #add_field =&gt; &#123;&quot;article&quot; =&gt; &quot;%&#123;[url_tmp][3]&#125;&quot;&#125;       #&#125; #if [article] =~ &quot;url_tmp&quot; &#123;        #drop &#123;&#125; #&#125;  urldecode&#123;    all_fields=&gt;true  &#125;  # 配置类型转换规则  mutate &#123;    convert =&gt; [&quot;request_time&quot;, &quot;float&quot;]    convert =&gt; [&quot;upstream_response_time&quot;, &quot;float&quot;]    convert =&gt; [&quot;body_bytes_sent&quot;, &quot;integer&quot;]    # 剔除掉多余字段    remove_field =&gt; [&quot;_id&quot;,&quot;_score&quot;,&quot;_type&quot;,&quot;message&quot;,&quot;http_user_agent&quot;,&quot;url_tmp&quot;]  &#125;&#125;if [type] == &quot;nginx_error&quot; &#123;  grok &#123;    match =&gt; [ &quot;message&quot; , &quot;(?&lt;timestamp&gt;%&#123;YEAR&#125;[./-]%&#123;MONTHNUM&#125;[./-]%&#123;MONTHDAY&#125;[- ]%&#123;TIME&#125;) \[%&#123;LOGLEVEL:severity&#125;\] %&#123;POSINT:pid&#125;#%&#123;NUMBER&#125;: %&#123;GREEDYDATA:errormessage&#125;(?:, client: (?&lt;clientip&gt;%&#123;IP&#125;|%&#123;HOSTNAME&#125;))(?:, server: %&#123;IPORHOST:server&#125;?)(?:, request: %&#123;QS:request&#125;)?(?:, upstream: (?&lt;upstream&gt;\&quot;%&#123;URI&#125;\&quot;|%&#123;QS&#125;))?(?:, host: %&#123;QS:request_host&#125;)?(?:, referrer: \&quot;%&#123;URI:referrer&#125;\&quot;)?&quot;]  &#125;  geoip &#123;    source =&gt; &quot;clientip&quot;    database =&gt; &quot;/home/sword/logstash/tools/geoip/GeoLite2-City.mmdb&quot;    add_field =&gt; [ &quot;[geoip][coordinates]&quot;, &quot;%&#123;[geoip][longitude]&#125;&quot; ]    add_field =&gt; [ &quot;[geoip][coordinates]&quot;, &quot;%&#123;[geoip][latitude]&#125;&quot;  ]    fields =&gt; [&quot;ip&quot;,&quot;city_name&quot;,&quot;region_name&quot;,&quot;country_name&quot;,&quot;continent_code&quot;,&quot;longitude&quot;,&quot;latitude&quot;,&quot;location&quot;] &#125;    mutate &#123;     convert =&gt; [ &quot;[geoip][coordinates]&quot;, &quot;float&quot;]     remove_field =&gt; [&quot;timestamp&quot;,&quot;message&quot;,&quot;_id&quot;,&quot;_score&quot;,&quot;_type&quot;]    &#125;&#125;  if &quot;_geoip_lookup_failure&quot; in [tags] &#123; drop &#123; &#125; &#125;&#125;# 配置输出规则output&#123;if [type] == &quot;nginx_access&quot; &#123;  elasticsearch&#123;  hosts =&gt; [&quot;127.0.0.1:9200&quot;]  index =&gt; &quot;logstash-nginx-access-%&#123;+YYYY.MM.dd&#125;&quot;  &#125;&#125;if [type] == &quot;nginx_error&quot; &#123;  elasticsearch&#123;  hosts =&gt; [&quot;127.0.0.1:9200&quot;]  index =&gt; &quot;logstash-nginx-error-%&#123;+YYYY.MM.dd&#125;&quot;  &#125;&#125;# 输出到屏幕# stdout &#123; codec =&gt; rubydebug &#125;&#125;</code></pre><h1 id="4-重启logstash"><a href="#4-重启logstash" class="headerlink" title="4.重启logstash"></a>4.重启logstash</h1><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.cnblogs.com/liuning8023/p/5502460.html">https://www.cnblogs.com/liuning8023/p/5502460.html</a></li><li><a href="https://blog.51cto.com/aegis8/1900969?source=dra">https://blog.51cto.com/aegis8/1900969?source=dra</a></li><li><a href="https://www.cnblogs.com/liuning8023/p/5502460.html">https://www.cnblogs.com/liuning8023/p/5502460.html</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>ELK</tag>
      
      <tag>Logstash</tag>
      
      <tag>日志分析</tag>
      
      <tag>大数据</tag>
      
      <tag>Nginx</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Bind配置DNS主从服务器</title>
    <link href="/linux/BindCluster/"/>
    <url>/linux/BindCluster/</url>
    
    <content type="html"><![CDATA[<h1 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h1><ul><li>Master 172.16.100.100</li><li>Slaver 172.16.100.150</li></ul><h1 id="1-主从服务器安装Bind"><a href="#1-主从服务器安装Bind" class="headerlink" title="1.主从服务器安装Bind"></a>1.主从服务器安装Bind</h1><h1 id="2-主服务器配置解析文件"><a href="#2-主服务器配置解析文件" class="headerlink" title="2.主服务器配置解析文件"></a>2.主服务器配置解析文件</h1><h1 id="3-配置主服务器区域文件允许转发到从服务器"><a href="#3-配置主服务器区域文件允许转发到从服务器" class="headerlink" title="3.配置主服务器区域文件允许转发到从服务器"></a>3.配置主服务器区域文件允许转发到从服务器</h1><pre><code class="hljs">vi /etc/named.rfc1912.zones# 配置正向解析区域zone &quot;sxs0618.com.&quot; IN &#123;  type master;  file &quot;sxs0618.com.zone&quot;;  allow-update &#123; none; &#125;;  # 配置允许转发的从服务器  allow-transfer &#123; 172.16.100.150;&#125;;&#125;;zone&quot;100.16.172.in-addr.arpa.&quot; IN &#123;  type master;  file&quot;100.16.172.in-addr.zone&quot;;  allow-update &#123; none; &#125;;      # 配置允许转发的从服务器  allow-transfer&#123; 172.16.100.150; &#125;; &#125;;</code></pre><h1 id="4-主服务器配置解析文件"><a href="#4-主服务器配置解析文件" class="headerlink" title="4.主服务器配置解析文件"></a>4.主服务器配置解析文件</h1><h2 id="4-1-配置正向解析文件"><a href="#4-1-配置正向解析文件" class="headerlink" title="4.1 配置正向解析文件"></a>4.1 配置正向解析文件</h2><pre><code class="hljs">vi /var/named/sxs0618.com.zone$TTL    86400$ORIGIN sxs0618.com.@       IN  SOA     dns1.sxs0618.com. admin.sxs0618.com. (                                    20190518; serial                                    1H      ; refresh                                    5M      ; retry                                    1W      ; expire                                    3H )    ; minimum         IN         NS          dns1         IN         NS          dns2dns1     IN         A           172.16.100.100dns2     IN         A           172.16.100.150         IN         MX          10  mail.sxs0618.com.www      IN         A           172.16.100.100mail     IN         A           172.16.100.200bbs      IN         A           172.16.100.150ftp      IN         A           172.16.100.100</code></pre><h2 id="4-2-配置反向解析文件"><a href="#4-2-配置反向解析文件" class="headerlink" title="4.2 配置反向解析文件"></a>4.2 配置反向解析文件</h2><pre><code class="hljs">vi /var/named/sxs0618.com.local$TTL    604800$ORIGIN 100.16.172.in-addr.arpa.@       IN SOA  dns1.sxs0618.com. admin.sxs0618.com. (                                    20190518; serial                                    1H      ; refresh                                    5M      ; retry                                    1W      ; expire                                    3H )    ; minimum        IN        NS            dns1.sxs0618.com.        IN        NS            dns2.sxs0618.com.100     IN        PTR           www.sxs0618.com.200     IN        PTR           mail.sxs0618.com.150     IN        PTR           bbs.sxs0618.com.100     IN        PTR           ftp.sxs0618.com.</code></pre><h1 id="5-配置从服务器区域文件"><a href="#5-配置从服务器区域文件" class="headerlink" title="5.配置从服务器区域文件"></a>5.配置从服务器区域文件</h1><pre><code class="hljs">vi /etc/named.rfc1912.zones# 配置正向解析区域zone &quot;sxs0618.com.&quot; IN &#123;  type slave;  masters &#123; 172.16.100.100; &#125;;  file &quot;slaves/sxs0618.com.zone&quot;;  allow-transfer &#123; none;&#125;;&#125;;zone&quot;100.16.172.in-addr.arpa.&quot; IN &#123;  type slave;  masters &#123; 172.16.100.100; &#125;;  file&quot;slaves/100.16.172.in-addr.zone&quot;;  allow-transfer&#123; none; &#125;; &#125;;</code></pre><h1 id="6-启动从服务器，开始区域文件同步"><a href="#6-启动从服务器，开始区域文件同步" class="headerlink" title="6.启动从服务器，开始区域文件同步"></a>6.启动从服务器，开始区域文件同步</h1><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.cnblogs.com/fyy-hhzzj/p/9066477.html">https://www.cnblogs.com/fyy-hhzzj/p/9066477.html</a></li><li><a href="https://cloud.tencent.com/developer/article/1363063">https://cloud.tencent.com/developer/article/1363063</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Bind</tag>
      
      <tag>DNS</tag>
      
      <tag>域名解析</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ELK实时分析Linux系统SSH远程登录</title>
    <link href="/linux/ELK-SSH/"/>
    <url>/linux/ELK-SSH/</url>
    
    <content type="html"><![CDATA[<p>Linux系统的安全日志为&#x2F;var&#x2F;log&#x2F;secure，记录了系统验证和授权的信息，只要涉及账号和密码的程序都会被记录，如SSH登录。对安全日志的监控有助于了解服务器的安全漏洞，从而采取相应的措施以提高安全性</p><hr><h1 id="SSH远程登录日志分析"><a href="#SSH远程登录日志分析" class="headerlink" title="SSH远程登录日志分析"></a>SSH远程登录日志分析</h1><h2 id="SSH登录成功的日志"><a href="#SSH登录成功的日志" class="headerlink" title="SSH登录成功的日志"></a>SSH登录成功的日志</h2><pre><code class="hljs">Nov  7 00:57:50 localhost sshd[22514]: Accepted password for root from 192.168.28.1 port 18415 ssh2Nov  7 00:57:50 localhost sshd[22514]: pam_unix(sshd:session): session opened for user root by (uid=0)</code></pre><h2 id="SSH登录失败的日志"><a href="#SSH登录失败的日志" class="headerlink" title="SSH登录失败的日志"></a>SSH登录失败的日志</h2><pre><code class="hljs">Nov  7 00:59:12 localhost sshd[22602]: Failed password for root from 192.168.28.1 port 18443 ssh2Nov  7 00:59:14 localhost sshd[22602]: error: Received disconnect from 192.168.28.1 port 18443:0:  [preauth]</code></pre><hr><ul><li>综上，只需匹配出Accepted、Failed这两个字段即可判定远程登录的性质</li></ul><hr><h1 id="1-部署ELK"><a href="#1-部署ELK" class="headerlink" title="1.部署ELK"></a>1.部署ELK</h1><h1 id="2-创建logstash配置文件"><a href="#2-创建logstash配置文件" class="headerlink" title="2.创建logstash配置文件"></a>2.创建logstash配置文件</h1><pre><code class="hljs">sudo vi /usr/local/logstash/config/sshd.confinput&#123;  file &#123;   path =&gt; [&quot;/var/log/secure&quot;]   type =&gt; &quot;ssh_login&quot;   start_position =&gt; &quot;beginning&quot;   &#125;&#125;filter &#123;  if [type] == &quot;ssh_login&quot; &#123;    grok &#123;       match =&gt; &#123;&quot;message&quot; =&gt; &quot;.*sshd\[\d+\]: %&#123;WORD:status&#125; .* %&#123;USER:username&#125; from.*%&#123;IP:clientip&#125;.*&quot;&#125;    &#125;    if [message] !~ &quot;Accepted | Failed&quot; &#123;       drop &#123;&#125;     &#125;    geoip &#123;       source =&gt; &quot;clientip&quot;       database =&gt; &quot;/home/sword/logstash/tools/geoip/GeoLite2-City.mmdb&quot;       add_field =&gt; [ &quot;[geoip][coordinates]&quot;, &quot;%&#123;[geoip][longitude]&#125;&quot; ]       add_field =&gt; [ &quot;[geoip][coordinates]&quot;, &quot;%&#123;[geoip][latitude]&#125;&quot; ]      fields =&gt; [&quot;ip&quot;,&quot;city_name&quot;,&quot;region_name&quot;,&quot;country_name&quot;,&quot;continent_code&quot;,&quot;longitude&quot;,&quot;latitude&quot;,&quot;location&quot;]    &#125;    mutate &#123;       convert =&gt; [ &quot;[geoip][coordinates]&quot;, &quot;float&quot;]       remove_field =&gt; [&quot;timestamp&quot;,&quot;message&quot;,&quot;_id&quot;,&quot;_score&quot;,&quot;_type&quot;]     &#125;     if &quot;_geoip_lookup_failure&quot; in [tags] &#123; drop &#123; &#125; &#125;    &#125;&#125;output&#123;  if [type] == &quot;ssh_login&quot; &#123;    elasticsearch&#123;       hosts =&gt; [&quot;127.0.0.1:9200&quot;]       index =&gt; &quot;logstash-ssh-login-%&#123;+YYYY.MM.dd&#125;&quot;    &#125;  &#125;    stdout &#123; codec =&gt; rubydebug &#125;&#125;</code></pre><h1 id="3-重启logstash"><a href="#3-重启logstash" class="headerlink" title="3.重启logstash"></a>3.重启logstash</h1>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>ELK</tag>
      
      <tag>SSH</tag>
      
      <tag>Logstash</tag>
      
      <tag>日志分析</tag>
      
      <tag>大数据</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Nextcloud搭建私有云盘</title>
    <link href="/geek/Nextcloud/"/>
    <url>/geek/Nextcloud/</url>
    
    <content type="html"><![CDATA[<h1 id="1-安装Nginx、PHP、MySQL"><a href="#1-安装Nginx、PHP、MySQL" class="headerlink" title="1.安装Nginx、PHP、MySQL"></a>1.安装Nginx、PHP、MySQL</h1><p>CREATE DATABASE nextcloud;<br>GRANT ALL PRIVILEGES ON nextcloud.* TO ‘nextcloud‘@’localhost’ IDENTIFIED BY ‘your-strong_password’;<br>FLUSH PRIVILEGES;</p>]]></content>
    
    
    <categories>
      
      <category>极客</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>存储</tag>
      
      <tag>云存储</tag>
      
      <tag>极客</tag>
      
      <tag>私有云</tag>
      
      <tag>Nextcloud</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Halo搭建博客系统</title>
    <link href="/geek/Halo/"/>
    <url>/geek/Halo/</url>
    
    <content type="html"><![CDATA[<p>Halo，由Java开发的现代的开源免费的轻量级博客系统，界面简洁美观，功能强大易用。Halo基于流行的Java框架Spring Boot进行后台开发，数据存储于关系数据库，如MySQL、PostgreSQL等，具备良好的稳定性和高性能。此外，Halo内置强大的Markdown编辑器让写作更为流畅，精美的主题与插件足以满足个性化需求，健全的权限管理和安全控制机制保障了数据的安全性，是专注于内容创作者的理想工具</p><h1 id="1-配置Java环境"><a href="#1-配置Java环境" class="headerlink" title="1.配置Java环境"></a>1.配置Java环境</h1><h1 id="2-安装MySQL数据库"><a href="#2-安装MySQL数据库" class="headerlink" title="2.安装MySQL数据库"></a>2.安装MySQL数据库</h1><h1 id="3-创建Halo数据库"><a href="#3-创建Halo数据库" class="headerlink" title="3.创建Halo数据库"></a>3.创建Halo数据库</h1><pre><code class="hljs">sudo mysql -uroot -pEnter password: Welcome to the MariaDB monitor.  Commands end with ; or \g.Your MariaDB connection id is 206076Server version: 10.5.6-MariaDB-log MariaDB ServerCopyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.Type &#39;help;&#39; or &#39;\h&#39; for help. Type &#39;\c&#39; to clear the current input statement.MariaDB [(none)]&gt; create database halo character set utf8 collate utf8_bin;Query OK, 1 row affected (0.002 sec)MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON halo.* TO &#39;halo&#39;@&#39;%&#39; IDENTIFIED BY &#39;halo&#39;;Query OK, 0 rows affected (0.156 sec)MariaDB [(none)]&gt; flush privileges;Query OK, 0 rows affected (0.080 sec)</code></pre><h1 id="4-安装Halo"><a href="#4-安装Halo" class="headerlink" title="4.安装Halo"></a>4.安装Halo</h1><pre><code class="hljs">wget https://dl.halo.run/release/halo-1.6.1.jarmv halo-1.6.1.jar /web/halo.jar</code></pre><h1 id="5-创建配置文件"><a href="#5-创建配置文件" class="headerlink" title="5.创建配置文件"></a>5.创建配置文件</h1><pre><code class="hljs">cd ~ &amp;&amp; mkdir .halovi .halo/application.yamlserver:  port: 8090  # Response data gzip.  compression:    enabled: truespring:  datasource:    # MySQL database configuration.    driver-class-name: com.mysql.cj.jdbc.Driver    url: jdbc:mysql://127.0.0.1:3306/halo?characterEncoding=utf8&amp;useSSL=false&amp;serverTimezone=Asia/Shanghai&amp;allowPublicKeyRetrieval=true    username: halo    password: halo  redis:    # Redis cache configuration.    port: 6379    database: 1    host: 127.0.0.1    # password: 123456halo:  # Your admin client path is https://your-domain/&#123;admin-path&#125;  admin-path: admin  # memory or level or redis  cache: redis</code></pre><h1 id="6-创建启动脚本"><a href="#6-创建启动脚本" class="headerlink" title="6.创建启动脚本"></a>6.创建启动脚本</h1><pre><code class="hljs">sudo vi /lib/systemd/system/halo.service[Unit]Description=Halo ServiceDocumentation=https://halo.runAfter=network-online.targetWants=network-online.target[Service]User=swordType=simpleExecStart=/usr/bin/java -server -Xms256m -Xmx256m -jar /web/halo.jar ExecStop=/bin/kill -s QUIT $MAINPIDRestart=alwaysStandOutput=syslogStandError=inherit[Install]WantedBy=multi-user.target</code></pre><h1 id="7-启动Halo"><a href="#7-启动Halo" class="headerlink" title="7.启动Halo"></a>7.启动Halo</h1><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start halo.servicesudo systemctl enable halo.service</code></pre><h1 id="8-配置Nginx反向代理"><a href="#8-配置Nginx反向代理" class="headerlink" title="8.配置Nginx反向代理"></a>8.配置Nginx反向代理</h1><pre><code class="hljs">vi /etc/nginx/conf.d/halo.confserver &#123;    listen       80;    server_name  localhost;    charset utf-8;    location / &#123;        access_log  /var/log/nginx/halo_access.log  main;        error_log  /var/log/nginx/halo_error.log;        proxy_pass http://127.0.0.1:8090;    &#125;&#125;</code></pre><h1 id="9-访问Halo"><a href="#9-访问Halo" class="headerlink" title="9.访问Halo"></a>9.访问Halo</h1><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://docs.halo.run/">https://docs.halo.run</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>极客</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>极客</tag>
      
      <tag>Java</tag>
      
      <tag>MySQL</tag>
      
      <tag>博客</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ELK日志分析平台集成Kafka作为缓存服务</title>
    <link href="/linux/ELK-Kafka/"/>
    <url>/linux/ELK-Kafka/</url>
    
    <content type="html"><![CDATA[<p>Redis的消息推送功能多用于实时性较高而持久性较低的场景，且redis本身并没有消息队列的容错机制。由于Redis的数据存储于内存，生产的消息只能一次性全部处消费掉，且没有留下任何痕迹，若消息在未到达消费者前因网络中断或其它缘由就发生丢失，而接受者并不知道有此消息，最终就会导致分布式事务的不一致，排查问题也无从着手</p><p>kafka的数据是consumer端的，所以对于消费而言consumer被赋予极大的自由度。consumer可以顺序地消费消息，也可以重新消费之前处理过的消息</p><h1 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h1><ul><li>1.Filebeat收集应用服务器的日志数据，发送到Kafka消息系统</li><li>2.Logstash从Kafka消息队列取到日志数据，过滤、结构化后发送到Elasticsearch集群存储</li><li>3.Kibana连接Elasticsearch集群提供数据的可视化分析与展示</li></ul><h1 id="1-安装Elasticsearch、Logstash、Filebeat、Kafka"><a href="#1-安装Elasticsearch、Logstash、Filebeat、Kafka" class="headerlink" title="1.安装Elasticsearch、Logstash、Filebeat、Kafka"></a>1.安装Elasticsearch、Logstash、Filebeat、Kafka</h1><h1 id="2-配置ilebeat"><a href="#2-配置ilebeat" class="headerlink" title="2.配置ilebeat"></a>2.配置ilebeat</h1><pre><code class="hljs">sudo vi /usr/local/filebeat/filebeat.ymlfilebeat.prospectors:input &#123;  kafka &#123;    bootstrap_servers =&gt; [&quot;172.16.100.100:9092,172.16.100.150:9092,172.16.100.200:9092&quot;]topics =&gt; [&quot;nginx_access&quot;]codec =&gt; &quot;json&quot;auto_offset_reset =&gt; &quot;latest&quot;consumer_threads =&gt; 1decorate_events =&gt; truetype =&gt; &quot;nginx_access&quot;  &#125;&#125;output.kafka:hosts: [&quot;172.16.100.100:9092&quot;,&quot;172.16.100.150:9092&quot;,&quot;172.16.100.200:9092&quot;]topic: &#39;nginx_access&#39;partition.round_robin:    reachable_only: falserequired_acks: 1compression: gzipmax_message_bytes: 1000000</code></pre><h1 id="3-配置logstash"><a href="#3-配置logstash" class="headerlink" title="3.配置logstash"></a>3.配置logstash</h1><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.jianshu.com/p/87e737c081d4">https://www.jianshu.com/p/87e737c081d4</a></li><li><a href="https://www.cnblogs.com/wangmo/p/9505883.html">https://www.cnblogs.com/wangmo/p/9505883.html</a></li><li><a href="https://doc.yonyoucloud.com/doc/logstash-best-practice-cn/input/redis.html">https://doc.yonyoucloud.com/doc/logstash-best-practice-cn/input/redis.html</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>ELK</tag>
      
      <tag>日志分析</tag>
      
      <tag>大数据</tag>
      
      <tag>Kafka</tag>
      
      <tag>Filebeat</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ELK集成Redis作为缓存服务器</title>
    <link href="/linux/ELK-Redis/"/>
    <url>/linux/ELK-Redis/</url>
    
    <content type="html"><![CDATA[<p>Filebeat、Logstash、Elasticsearch、Kibana构成了经典的ELK架构，适用于数据量较小的环境。大型生产环境中，业务流量的突发峰值将对Elasticsearch造成巨大的冲击，存在安全隐患。鉴于此，引入消息队列（Redis、Kafka或RabbitMQ）作为缓冲，用以削平峰值及异步容错，缓解了后端Elasticsearch集群的压力</p><hr><h1 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h1><ul><li>Filebeat收集应用服务器的日志数据，发送到Redis消息队列</li><li>Logstash从Redis消息队列取到日志数据，过滤、结构化后发送到Elasticsearch集群存储</li><li>kibana连接Elasticsearch集群提供数据的可视化分析与展示</li></ul><hr><h1 id="1-安装elk、filebeat、redis"><a href="#1-安装elk、filebeat、redis" class="headerlink" title="1.安装elk、filebeat、redis"></a>1.安装elk、filebeat、redis</h1><h1 id="2-配置filebeat"><a href="#2-配置filebeat" class="headerlink" title="2.配置filebeat"></a>2.配置filebeat</h1><pre><code class="hljs">sudo vi /usr/local/filebeat/filebeat.ymlfilebeat.prospectors:- type: log  enabled: true  paths:    - /var/log/nginx/access.log  tags: [&quot;nginx&quot;]# 输出到redisoutput.redis:  # 设置redis地址与端口  hosts: [&quot;172.16.100.200:6379&quot;]  # 设置存储在redis队列的key  key: &quot;nginx_zabbix&quot;  # 设置redis连接密码  password: &quot;redis&quot;  # 设置redis存储数据库  db: 0  # 设置redis连接超时时长  timeout: 5</code></pre><h1 id="3-配置logstash"><a href="#3-配置logstash" class="headerlink" title="3.配置logstash"></a>3.配置logstash</h1><pre><code class="hljs">input&#123;redis &#123;  host =&gt; &quot;172.16.100.200&quot;  port =&gt; &quot;6379&quot;  password =&gt; &quot;redis&quot;  key =&gt; &quot;nginx_zabbix&quot;  type =&gt; &quot;nginx&quot;  # 设置数据类型为队列  data_type =&gt; &quot;list&quot;  db =&gt; 0  timeout =&gt; 5  &#125;&#125;output&#123;  elasticsearch&#123;    hosts =&gt; [&quot;172.16.100.150:9200&quot;,&quot;172.16.100.200:9200&quot;]    index =&gt; &quot;nginx-%&#123;+YYYY.MM.dd&#125;&quot;    document_type =&gt; &quot;nginx&quot;    &#125;stdout &#123; codec =&gt; rubydebug &#125;&#125;</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.cnblogs.com/wangmo/p/9505883.html">https://www.cnblogs.com/wangmo/p/9505883.html</a></li><li><a href="https://www.jianshu.com/p/87e737c081d4">https://www.jianshu.com/p/87e737c081d4</a></li><li><a href="https://doc.yonyoucloud.com/doc/logstash-best-practice-cn/input/redis.html">https://doc.yonyoucloud.com/doc/logstash-best-practice-cn/input/redis.html</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>ELK</tag>
      
      <tag>日志分析</tag>
      
      <tag>大数据</tag>
      
      <tag>Filebeat</tag>
      
      <tag>Redis</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ELK配置Filebeat收集日志</title>
    <link href="/linux/ELK-Filebeat/"/>
    <url>/linux/ELK-Filebeat/</url>
    
    <content type="html"><![CDATA[<p>Filebeat，Elastic公司开发的轻量级日志收集工具，用于高效地收集、转换和发送日志数据。其资源消耗相较于Logstash非常之低，扩展性也更强，适用于多种日志处理场景。经典的日志分析平台通常这样构成，即Filebeat部署于应用服务器负责日志收集，Logstash负责日志解析、过滤及格式化，Elasticsearch集群负责日志存储</p><h1 id="1-部署ELK"><a href="#1-部署ELK" class="headerlink" title="1.部署ELK"></a>1.部署ELK</h1><h1 id="2-配置logstash"><a href="#2-配置logstash" class="headerlink" title="2.配置logstash"></a>2.配置logstash</h1><h2 id="2-1-创建配置文件"><a href="#2-1-创建配置文件" class="headerlink" title="2.1 创建配置文件"></a>2.1 创建配置文件</h2><pre><code class="hljs">sudo vi /usr/local/logstash/config/system.ymlinput&#123;# 定义日志源为filebeatbeats &#123;port =&gt; 5044    &#125;&#125;output&#123;elasticsearch&#123;hosts =&gt; [&quot;172.16.100.200:9200&quot;]index =&gt; &quot;system-%&#123;+YYYY.MM.dd&#125;&quot;document_type =&gt; &quot;system&quot;&#125;stdout &#123; codec =&gt; rubydebug &#125;&#125;</code></pre><h2 id="2-2-启动logstash"><a href="#2-2-启动logstash" class="headerlink" title="2.2 启动logstash"></a>2.2 启动logstash</h2><pre><code class="hljs">nohup /usr/local/logstash/bin/logstash -f /usr/local/logstash/config/system.yml &amp;</code></pre><h1 id="3-部署filebeat"><a href="#3-部署filebeat" class="headerlink" title="3.部署filebeat"></a>3.部署filebeat</h1><pre><code class="hljs">tar -xzvf filebeat-6.6.2-linux-x86_64.tar.gzsudo mv filebeat-6.6.2-linux-x86_64 /usr/local/filebeat</code></pre><h2 id="3-1-配置filebeat"><a href="#3-1-配置filebeat" class="headerlink" title="3.1 配置filebeat"></a>3.1 配置filebeat</h2><pre><code class="hljs">sudo vi /usr/local/filebeat/system.ymlfilebeat.prospectors:# 设置日志类型，log或stdin- input_type: logenabled: true# 设置日志路径paths:- /var/log/messages- /var/log/secure# 设置字符集编码encoding: utf-8# 设置文档类型，即es集群_type的值document_type: system# 设置日志标签，便于logstash的过滤tags: [&quot;httpserver&quot;]# 排除.gz结尾的文件exclude_files: [&quot;.gz$&quot;]# 只发送的日志信息# include_lines:[&quot;^ERR&quot;,&quot;^WARN&quot;]# 不发送的日志信息# exclude_lines:[&quot;^OK&quot;]# 添加自定义字段# fields:# logIndex:httpserver# logsource:http-server# logtype:httpserver_system# projecta:zrlog- input_type: logenabled: truepaths:- /usr/local/php-fpm/var/*.logtags: [&quot;php&quot;]output.logstash:hosts: [&quot;172.16.100.150:5044&quot;]# 设置filebeat的id，默认为beatsclient_id: httpserver</code></pre><h2 id="3-2-启动filebeat"><a href="#3-2-启动filebeat" class="headerlink" title="3.2 启动filebeat"></a>3.2 启动filebeat</h2><pre><code class="hljs">nohup /usr/local/filebeat/filebeat -e -c /usr/local/filebeat/filebeat.yml &amp;</code></pre><h1 id="4-登陆kibana"><a href="#4-登陆kibana" class="headerlink" title="4.登陆kibana"></a>4.登陆kibana</h1><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.dgstack.cn/archives/2858.html">https://www.dgstack.cn/archives/2858.html</a></li><li><a href="https://blog.csdn.net/xuguokun1986/article/details/73560195/">https://blog.csdn.net/xuguokun1986/article/details/73560195/</a></li><li><a href="https://blog.51cto.com/michaelkang/1864225">https://blog.51cto.com/michaelkang/1864225</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>ELK</tag>
      
      <tag>日志分析</tag>
      
      <tag>大数据</tag>
      
      <tag>Filebeat</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ELK日志分析平台的搭建与配置</title>
    <link href="/linux/ELK/"/>
    <url>/linux/ELK/</url>
    
    <content type="html"><![CDATA[<p>ELK，即由Elasticsearch、Logstash、Kibana这三个开源软件作为核心组件构建的分布式实时日志分析平台，用于实时分析系统日志、应用程序日志和安全日志，以便于掌握服务器负荷、性能及安全性，从而及时采取措施纠正错误</p><hr><ul><li>Elasticsearch，开源分布式搜索引擎，提供搜集、分析、存储数据三大功能，且所有节点数据都是均等的</li><li>Logstash，用于日志的收集与过滤，支持大量的数据获取方式，负责从每台服务器抓取日志数据，对数据进行格式转换和处理后，输出到Elasticsearch中存储</li><li>Kibana，为Logstash和ElasticSearch提供日志分析的web界面，用于汇总、分析和搜索重要日志数据</li></ul><hr><h1 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h1><ul><li><p>1.logstash分布于各个应用服务器节点，用于收集日志及数据，经过过滤与处理后发送es到集群</p></li><li><p>2.elasticsearch集群为日志数据创建索引，并将之以分片的形式进行分布式压缩存储</p></li><li><p>3.kibana从ES集群中查询数据生成图表，提供直观的web呈现方式，实现数据的可视化</p></li></ul><hr><h1 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h1><ul><li>172.16.100.100 node01 kibana</li><li>172.16.100.120 node02 es logstash</li><li>172.16.100.200 master es</li></ul><hr><h1 id="1-部署ES集群"><a href="#1-部署ES集群" class="headerlink" title="1.部署ES集群"></a>1.部署ES集群</h1><h2 id="1-1-创建elasticsearch用户"><a href="#1-1-创建elasticsearch用户" class="headerlink" title="1.1 创建elasticsearch用户"></a>1.1 创建elasticsearch用户</h2><pre><code class="hljs">sudo useradd elasticsearch</code></pre><h2 id="1-2-安装JDK，配置JAVA环境"><a href="#1-2-安装JDK，配置JAVA环境" class="headerlink" title="1.2 安装JDK，配置JAVA环境"></a>1.2 安装JDK，配置JAVA环境</h2><pre><code class="hljs">sudo yum install -y java-1.8.0-openjdk java-1.8.0-openjdk-develsudo apt install -y default-jdk</code></pre><h2 id="1-3-安装ES"><a href="#1-3-安装ES" class="headerlink" title="1.3 安装ES"></a>1.3 安装ES</h2><pre><code class="hljs">tar -xzvf elasticsearch-6.6.2.tar.gzsudo mv elasticsearch-6.6.2 /usr/local/elasticsearch</code></pre><h2 id="1-4-创建ES集群配置文件"><a href="#1-4-创建ES集群配置文件" class="headerlink" title="1.4 创建ES集群配置文件"></a>1.4 创建ES集群配置文件</h2><pre><code class="hljs">sudo vi /usr/local/elasticsearch/config/elasticsearch.yml# 设置ES集群名称cluster.name: sword# 设置节点名称node.name: node-master# 设置elasticsearch数据存储路径        path.data: /usr/local/elasticsearch/data# 设置elasticsearch日志存储路径     path.logs: /usr/local/elasticsearch/logs# 设置elasticsearch监听地址，默认为localhost network.host: 172.16.100.200# 设置elasticsearch监听端口http.port:9200# 设置集ES集群主机组，自动发现节点及选定主节点discovery.zen.ping.unicast.hosts: [&quot;172.16.100.120&quot;, &quot;172.16.100.200&quot;]# centos6需要配置bootstrap.memory_lock: falsebootstrap.system_call_filter: false</code></pre><hr><ul><li>注：其余ES节点只需改动节点名称即可</li></ul><h2 id="1-5-配置elasticsearch用户系统资源限制"><a href="#1-5-配置elasticsearch用户系统资源限制" class="headerlink" title="1.5 配置elasticsearch用户系统资源限制"></a>1.5 配置elasticsearch用户系统资源限制</h2><h3 id="1-5-1-配置打开文件最大数"><a href="#1-5-1-配置打开文件最大数" class="headerlink" title="1.5.1 配置打开文件最大数"></a>1.5.1 配置打开文件最大数</h3><pre><code class="hljs">sudo vi /etc/security/limits.conf# elasticsearch    soft    nproc           16384# elasticsearch    hard    nproc           16384# soft表示告警值，hard表示阀值elasticsearch    soft    nofile          65536elasticsearch    hard    nofile          65536</code></pre><h3 id="1-5-2-配置进程最大数，默认为4096，会覆盖掉limits-conf配置"><a href="#1-5-2-配置进程最大数，默认为4096，会覆盖掉limits-conf配置" class="headerlink" title="1.5.2 配置进程最大数，默认为4096，会覆盖掉limits.conf配置"></a>1.5.2 配置进程最大数，默认为4096，会覆盖掉limits.conf配置</h3><pre><code class="hljs">sudo vi /etc/security/limits.d/20-nproc.confelasticsearch    soft    nproc    4096elasticsearch    hard    nproc    4096</code></pre><h2 id="1-6-配置内核参数"><a href="#1-6-配置内核参数" class="headerlink" title="1.6 配置内核参数"></a>1.6 配置内核参数</h2><h3 id="1-6-1-设置进程占用VMA-虚拟内存区域-的数量"><a href="#1-6-1-设置进程占用VMA-虚拟内存区域-的数量" class="headerlink" title="1.6.1 设置进程占用VMA(虚拟内存区域)的数量"></a>1.6.1 设置进程占用VMA(虚拟内存区域)的数量</h3><pre><code class="hljs">sudo vi /etc/sysctl.conf# 默认为65536vm.max_map_count=262144</code></pre><h3 id="1-6-2-加载内核参数配置"><a href="#1-6-2-加载内核参数配置" class="headerlink" title="1.6.2 加载内核参数配置"></a>1.6.2 加载内核参数配置</h3><pre><code class="hljs">sudo sysctl -p</code></pre><h2 id="1-7-启动ES集群"><a href="#1-7-启动ES集群" class="headerlink" title="1.7 启动ES集群"></a>1.7 启动ES集群</h2><pre><code class="hljs">mkdir /usr/local/elasticsearch/datachown -R elasticsearch.elasticsearch /usr/local/elasticsearchsu - elasticsearchcd /usr/local/elasticsearch/binnohup ./elasticsearch &amp;</code></pre><h2 id="1-8-查询ES集群状态"><a href="#1-8-查询ES集群状态" class="headerlink" title="1.8 查询ES集群状态"></a>1.8 查询ES集群状态</h2><pre><code class="hljs">curl -XGET &#39;http://172.16.100.200:9200/_cat/nodes?v&#39;</code></pre><h1 id="2-部署kibana"><a href="#2-部署kibana" class="headerlink" title="2.部署kibana"></a>2.部署kibana</h1><pre><code class="hljs">tar -xzvf kibana-6.6.2-linux-x86_64.tar.gzsudo mv kibana-6.6.2-linux-x86_64 /usr/local/kibana</code></pre><h2 id="2-1-创建kibana配置文件"><a href="#2-1-创建kibana配置文件" class="headerlink" title="2.1 创建kibana配置文件"></a>2.1 创建kibana配置文件</h2><pre><code class="hljs">sudo vi /usr/local/kibana/config/kibana.yml# 设置kibana监听端口server.port: 5601# 设置kibana监听地址server.host: &quot;172.16.100.150&quot;# 设置elasticsearch地址elasticsearch.hosts: [&quot;http://172.16.100.200:9200&quot;]</code></pre><h2 id="2-2-启动kibana"><a href="#2-2-启动kibana" class="headerlink" title="2.2.启动kibana"></a>2.2.启动kibana</h2><pre><code class="hljs">cd /usr/local/kibana/binnohup ./kibana &amp;</code></pre><h1 id="3-部署logstash"><a href="#3-部署logstash" class="headerlink" title="3.部署logstash"></a>3.部署logstash</h1><pre><code class="hljs">tar -xzvf logstash-6.6.2.tar.gzsudo mv logstash-6.6.2 /usr/local/logstash</code></pre><h2 id="3-1-创建logstash解析配置文件"><a href="#3-1-创建logstash解析配置文件" class="headerlink" title="3.1 创建logstash解析配置文件"></a>3.1 创建logstash解析配置文件</h2><pre><code class="hljs">vi /usr/local/logstash/config/syslog.yml# 定义日志源input &#123;  file &#123;    # 设置源日志文件路径    path =&gt; [&quot;/var/log/messages&quot;]    # 设置日志类型，相当于关系数据库中的table    type =&gt; &quot;system&quot;    # 设置从哪里开始收集日志    start_position =&gt; &quot;beginning&quot;    # 定义监听端口    # port =&gt; 10514  &#125;  file &#123;    path =&gt; [&quot;/var/log/secure&quot;]    type =&gt; &quot;secure&quot;    start_position =&gt; &quot;beginning&quot;    &#125;  &#125;  # 定义日志过滤规则  # filter &#123;  #&#125;  # 定义日志输出  output &#123;    if [type] == &quot;system&quot; &#123;      # 输出到es      elasticsearch &#123;      # 设置es地址与端口，多个用逗号隔开      hosts =&gt; [&quot;172.16.100.200:9200&quot;]      # 设置日志的索引名称，相当于关系型数据库中的database      index =&gt; &quot;system-%&#123;+YYYY.MM.dd&#125;&quot;      &#125;    &#125;    if [type] == &quot;secure&quot; &#123;      elasticsearch &#123;        hosts =&gt; [&quot;172.16.100.200:9200&quot;]        index =&gt; &quot;secure-%&#123;+YYYY.MM.dd&#125;&quot;      &#125;    &#125;  # 输出到当前终端  stdout &#123; codec =&gt; rubydebug &#125;&#125;</code></pre><h2 id="3-2-启动logstash"><a href="#3-2-启动logstash" class="headerlink" title="3.2 启动logstash"></a>3.2 启动logstash</h2><pre><code class="hljs">cd /usr/local/logstash/bin# 检测配置文件的语法./logstash -f /etc/logstash/conf.d/syslog.conf -t# 后台启动logstashnohup ./logstash -f /usr/local/logstash/config/syslog.conf &gt; nohup.log &amp;</code></pre><h1 id="4-登陆kibana查看日志分析效果"><a href="#4-登陆kibana查看日志分析效果" class="headerlink" title="4.登陆kibana查看日志分析效果"></a>4.登陆kibana查看日志分析效果</h1><p><a href="http://172.16.100.200:5601/">http://172.16.100.200:5601</a></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="http://www.cnblogs.com/yuhuLin/p/7018858.html">http://www.cnblogs.com/yuhuLin/p/7018858.html</a></li><li><a href="https://blog.51cto.com/xiaorenwutest/2135897">https://blog.51cto.com/xiaorenwutest/2135897</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>ELK</tag>
      
      <tag>日志分析</tag>
      
      <tag>大数据</tag>
      
      <tag>Elasticsearch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ansible自动化运维工具的安装与配置</title>
    <link href="/linux/Ansible/"/>
    <url>/linux/Ansible/</url>
    
    <content type="html"><![CDATA[<p>Ansible，由Python开发的开源的自动化运维配置管理工具，用于实现批量系统配置、批量程序部署、批量运行命令等自动化任务功能。Ansible基于ssh进行主机通信，远程主机无需安装客户端或代理，其本身并没有批量部署的能力，而是依赖所运行的各种模块</p><h1 id="特点优势"><a href="#特点优势" class="headerlink" title="特点优势"></a>特点优势</h1><ul><li>部署简单，只需在控制设备上部署Ansible环境，而不需要在被控制设备上进行任何操作，主从集中化管理</li><li>使用安全协议SSH对设备进行管理</li><li>配置简单、功能强大、可扩展性强</li><li>支持API及自定义模块，可通过Python轻松扩展</li><li>通过playbooks定制强大的配置、状态管理</li><li>对云计算平台、大数据都有很好的支持</li></ul><h1 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h1><p>Ansible主要由5部分组成</p><h2 id="Ansible"><a href="#Ansible" class="headerlink" title="Ansible"></a>Ansible</h2><p>Ansible核心，主要用于处理请求</p><h2 id="Modules"><a href="#Modules" class="headerlink" title="Modules"></a>Modules</h2><p>Ansible模块，由Ansible自带的核心模块和自定义模块两部分组成</p><h2 id="3-Plugins"><a href="#3-Plugins" class="headerlink" title="3.Plugins"></a>3.Plugins</h2><p>Ansible模块功能的补充，如链接插件、邮件插件等等</p><h2 id="Playbooks"><a href="#Playbooks" class="headerlink" title="Playbooks"></a>Playbooks</h2><p>Ansible剧本，定义Ansible多任务配置文件，由Ansible自动执行</p><h2 id="Inventory"><a href="#Inventory" class="headerlink" title="Inventory"></a>Inventory</h2><p>Ansible被控主机清单</p><h1 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h1><ul><li>master 172.16.100.100</li><li>node01 172.16.100.120</li><li>node02 172.16.100.200</li></ul><hr><h1 id="1-安装依赖包"><a href="#1-安装依赖包" class="headerlink" title="1.安装依赖包"></a>1.安装依赖包</h1><pre><code class="hljs">sudo yum -y install gcc gcc-c++ make cmake automake autoconf openssl openssl-devel zlib zlib-devel pcre pcre-devel libffi-devel unzip </code></pre><h1 id="2-配置集群主机ssh免密钥登陆"><a href="#2-配置集群主机ssh免密钥登陆" class="headerlink" title="2.配置集群主机ssh免密钥登陆"></a>2.配置集群主机ssh免密钥登陆</h1><h1 id="3-安装python"><a href="#3-安装python" class="headerlink" title="3.安装python"></a>3.安装python</h1><pre><code class="hljs"># yum install -y pythontar -xzvf Python-2.7.8.tgz &amp;&amp; cd Python-2.7.8sudo ./configure --prefix=/usr/local &amp;&amp; make installsudo cp /usr/local/include/python2.7/* /usr/local/include -rsudo mv /usr/bin/python /usr/bin/python_baksudo ln -s /usr/bin/python2.7 /usr/bin/python</code></pre><h1 id="4-安装Python模块"><a href="#4-安装Python模块" class="headerlink" title="4.安装Python模块"></a>4.安装Python模块</h1><h2 id="4-1-安装setuptools模块"><a href="#4-1-安装setuptools模块" class="headerlink" title="4.1 安装setuptools模块"></a>4.1 安装setuptools模块</h2><pre><code class="hljs">unzip setuptools-40.1.0.zip &amp;&amp; cd setuptools-40.1.0sudo python setup.py install</code></pre><h2 id="4-2-安装pycrypto模块"><a href="#4-2-安装pycrypto模块" class="headerlink" title="4.2 安装pycrypto模块"></a>4.2 安装pycrypto模块</h2><pre><code class="hljs">tar -xzvf pycrypto-2.6.1.tar.gz &amp;&amp; cd pycrypto-2.6.1sudo python setup.py install</code></pre><h2 id="4-3-安装PyYAML模块"><a href="#4-3-安装PyYAML模块" class="headerlink" title="4.3 安装PyYAML模块"></a>4.3 安装PyYAML模块</h2><pre><code class="hljs">tar -xzvf PyYAML-3.11.tar.gz &amp;&amp; cd PyYAML-3.11sudo python setup.py install</code></pre><h2 id="4-4-安装Jinja2模块"><a href="#4-4-安装Jinja2模块" class="headerlink" title="4.4 安装Jinja2模块"></a>4.4 安装Jinja2模块</h2><pre><code class="hljs">tar -xzvf MarkupSafe-0.9.3.tar.gz &amp; cd MarkupSafe-0.9.3sudo python setup.py installtar -xzvf Jinja2-2.7.3.tar.gz &amp;&amp; cd Jinja2-2.7.3sudo python setup.py install</code></pre><h2 id="4-5-安装cryptography模块"><a href="#4-5-安装cryptography模块" class="headerlink" title="4.5 安装cryptography模块"></a>4.5 安装cryptography模块</h2><pre><code class="hljs">tar -xzvf pycparser-2.20.tar.gz &amp;&amp; cd pycparser-2.20sudo python setup.py installtar -xzvf cffi-1.12.3.tar.gz &amp;&amp; cd cffi-1.12.3sudo python setup.py installtar -xzvf ipaddress-1.0.23.tar.gz &amp;&amp; cd ipaddress-1.0.23sudo python setup.py installtar -xzvf enum34-1.1.10.tar.gz &amp;&amp; cd enum34-1.1.10sudo python setup.py installtar -xzvf six-1.14.0.tar.gz &amp;&amp; cd six-1.14.0sudo python setup.py installtar -xzvf cryptography-2.8.tar.gz &amp;&amp; cd cryptography-2.8sudo python setup.py install</code></pre><h2 id="4-6-安装simplejson模块"><a href="#4-6-安装simplejson模块" class="headerlink" title="4.6 安装simplejson模块"></a>4.6 安装simplejson模块</h2><pre><code class="hljs">tar -xzvf simplejson-3.6.5.tar.gz &amp;&amp; cd simplejson-3.6.5sudo python setup.py install</code></pre><h1 id="5-安装ansible"><a href="#5-安装ansible" class="headerlink" title="5.安装ansible"></a>5.安装ansible</h1><pre><code class="hljs">tar -xzvf ansible-2.7.9.tar.gz &amp;&amp; cd ansible-2.7.9sudo python setup.py installsudo mkdir /etc/ansible &amp;&amp; sudo cp examples/ansible.cfg /etc/ansible</code></pre><h1 id="6-配置ansible主机组"><a href="#6-配置ansible主机组" class="headerlink" title="6.配置ansible主机组"></a>6.配置ansible主机组</h1><pre><code class="hljs">sudo vi /etc/ansible/hosts[master]master[worker]worker0[1:3]# 设置主机组cluster，master和worker这两个主机组都属于其内置组[cluster:children]masterworker[data]data01    </code></pre><h1 id="7-ansible批量执行命令"><a href="#7-ansible批量执行命令" class="headerlink" title="7.ansible批量执行命令"></a>7.ansible批量执行命令</h1><pre><code class="hljs">ansible cluster -m shell -a &#39;uptime&#39;</code></pre>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Ansible</tag>
      
      <tag>自动化运维</tag>
      
      <tag>云计算</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux系统安装Supervisor进程管理工具</title>
    <link href="/linux/Supervisor/"/>
    <url>/linux/Supervisor/</url>
    
    <content type="html"><![CDATA[<p>Supervisor，Python开发的通用的C&#x2F;S架构的进程管理系统，用于启动、停止、重启、监听一个或多个进程，异常退出时自动重启，实现集中式精确管理Linux服务器运行的的大量进程</p><h1 id="1-安装python"><a href="#1-安装python" class="headerlink" title="1.安装python"></a>1.安装python</h1><pre><code class="hljs">sudo yum install -y python</code></pre><h1 id="2-安装supervisor"><a href="#2-安装supervisor" class="headerlink" title="2.安装supervisor"></a>2.安装supervisor</h1><pre><code class="hljs"># sudo yum install -y python-setuptools &amp;&amp; easy_install supervisortar -xzvf supervisor-3.3.5.tar.gzcd supervisor-3.3.5sudo python setup.py install</code></pre><h1 id="3-创建supervisor主配置文件"><a href="#3-创建supervisor主配置文件" class="headerlink" title="3.创建supervisor主配置文件"></a>3.创建supervisor主配置文件</h1><pre><code class="hljs">sudo mkdir -p /etc/supervisor/conf.d /var/log/supervisor /var/run/supervisorsudo echo_supervisord_conf &gt; /etc/supervisor/supervisord.conf</code></pre><h1 id="4-supervisor主配置文件的修改"><a href="#4-supervisor主配置文件的修改" class="headerlink" title="4.supervisor主配置文件的修改"></a>4.supervisor主配置文件的修改</h1><pre><code class="hljs">sudo vi /etc/supervisor/supervisord.conf[unix_http_server]file=/var/run/supervisor/supervisor.sock ;[supervisord]logfile=/var/log/supervisor/supervisord.log ;pidfile=/var/run/supervisor/supervisord.pid ;[supervisorctl]serverurl=unix:///var/run/supervisor/supervisor.sock ;[include]files=/etc/supervisor/conf.d/*.conf</code></pre><h1 id="5-创建elasticsearch的supervisor管理配置文件"><a href="#5-创建elasticsearch的supervisor管理配置文件" class="headerlink" title="5.创建elasticsearch的supervisor管理配置文件"></a>5.创建elasticsearch的supervisor管理配置文件</h1><pre><code class="hljs">sudo vi /etc/supervisor/conf.d/nginx.conf[program:elasticsearch]# 设置进程运行用户user=sword# 设置进程启动路径command=/home/sword/elasticsearch/bin/elasticsearch# 设置进程数numprocs=1# 设置进程是否随supervisord的启动而启动autostart=false# 设置进程是否随supervisord的重启而重启autorestart=false# 设置进程启动失败后最大重试次数startretries=3# 设置进程权重，即启动优先级，默认999，数值越小优先级越高priority=1# 设置进程启动后确认其为启动正常的时间startsecs=3# 设置进程启动日志文件路径stdout_logfile = /var/log/supervisor/elasticsearch.log# 设置进程启动错误日志文件路径stderr_logfile = /var/log/supervisor/elasticsearch-error.log</code></pre><h1 id="6-创建supervisor启动脚本"><a href="#6-创建supervisor启动脚本" class="headerlink" title="6.创建supervisor启动脚本"></a>6.创建supervisor启动脚本</h1><pre><code class="hljs">sudo vi /lib/systemd/system/supervisord.service[Unit]Description=Process Monitoring and Control DaemonAfter=rc-local.service nss-user-lookup.target[Service]Type=forkingExecStart=/usr/bin/supervisord -c   /etc/supervisor/supervisord.confExecStop=/usr/bin/supervisord shutdownExecReload=/usr/bin/supervisord reloadkillMode=processRestart=on-failureRestartSec=42s[Install]WantedBy=multi-user.target</code></pre><h1 id="7-启动supervisor，测试supervisor的监控及管理功能"><a href="#7-启动supervisor，测试supervisor的监控及管理功能" class="headerlink" title="7.启动supervisor，测试supervisor的监控及管理功能"></a>7.启动supervisor，测试supervisor的监控及管理功能</h1><pre><code class="hljs">sudo systemctl daemon-reloadsudo systemctl start supervisord.servicsudo systemctl enable supervisord.servic</code></pre><h1 id="8-启动elasticsearch，测试supervisor的监控及管理功能"><a href="#8-启动elasticsearch，测试supervisor的监控及管理功能" class="headerlink" title="8.启动elasticsearch，测试supervisor的监控及管理功能"></a>8.启动elasticsearch，测试supervisor的监控及管理功能</h1><pre><code class="hljs">sudo supervisorctl statussudo supervisorctl start elasticsearch</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.cnblogs.com/binglansky/p/9246780.html">https://www.cnblogs.com/binglansky/p/9246780.html</a></li><li><a href="https://www.cnblogs.com/toutou/p/supervisor.html#_label1">https://www.cnblogs.com/toutou/p/supervisor.html#_label1</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Supervisor</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux系统配置普通用户Sudo权限</title>
    <link href="/linux/Sudo/"/>
    <url>/linux/Sudo/</url>
    
    <content type="html"><![CDATA[<p>Root，Linux系统的超级管理员用户，可访问Linux系统任何文件和运行任何命令，拥有系统完全控制的能力。一个错误的命令可能会破坏系统文件和配置，日常的管理操作应谨慎使用root用户。为安全考虑，普通用户的权限有所限制，这就给日常运维工作带来了极大的不便。基于此，Linux系统引入了sudo的概念，通过配置sudo使得普通用户拥有root命令的某些权限，既保障了运维工作的便捷性，又将风险固定了一定的可控范围内</p><h1 id="1-安装sudo"><a href="#1-安装sudo" class="headerlink" title="1.安装sudo"></a>1.安装sudo</h1><pre><code class="hljs">yum install -y sudoapt install -y sudo</code></pre><h1 id="2-配置sudo审计日志"><a href="#2-配置sudo审计日志" class="headerlink" title="2.配置sudo审计日志"></a>2.配置sudo审计日志</h1><pre><code class="hljs">echo &quot;Defaults logfile=/var/log/sudo.log&quot;  &gt;&gt; /etc/sudoers</code></pre><h1 id="3-配置普通用户sword-sudo权限"><a href="#3-配置普通用户sword-sudo权限" class="headerlink" title="3.配置普通用户sword sudo权限"></a>3.配置普通用户sword sudo权限</h1><pre><code class="hljs">vi /etc/sudoers# 配置sword拥有root用户所有权限，且免密码输入sword    ALL=(ALL)       NOPASSWD:ALL# 配置sword拥有virsh及systemctl命令执行的权限，且免密码输入# sword   ALL=(ALL)       NOPASSWD: /usr/bin/virsh,/bin/systemctl</code></pre><h1 id="4-检查配置文件"><a href="#4-检查配置文件" class="headerlink" title="4.检查配置文件"></a>4.检查配置文件</h1><pre><code class="hljs">visudo -c</code></pre><h1 id="5-测试sudo及其审计功能"><a href="#5-测试sudo及其审计功能" class="headerlink" title="5.测试sudo及其审计功能"></a>5.测试sudo及其审计功能</h1><pre><code class="hljs">su - swordsudo systemctl restart sshd</code></pre>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux系统配置SSH免密登录</title>
    <link href="/linux/SSH/"/>
    <url>/linux/SSH/</url>
    
    <content type="html"><![CDATA[<h1 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h1><ul><li>172.16.100.100  master</li><li>172.16.100.120  node01</li><li>172.16.100.200  node02</li></ul><hr><h1 id="1-安装依赖包"><a href="#1-安装依赖包" class="headerlink" title="1.安装依赖包"></a>1.安装依赖包</h1><pre><code class="hljs">yum install -y openssh-clients</code></pre><h1 id="2-生成公钥私钥对"><a href="#2-生成公钥私钥对" class="headerlink" title="2.生成公钥私钥对"></a>2.生成公钥私钥对</h1><pre><code class="hljs">ssh-keygen -t rsa</code></pre><h1 id="3-分发公钥到集群其余节点"><a href="#3-分发公钥到集群其余节点" class="headerlink" title="3.分发公钥到集群其余节点"></a>3.分发公钥到集群其余节点</h1><pre><code class="hljs">ssh-copy-id -i /root/.ssh/id_rsa.pub 172.16.100.100ssh-copy-id -i /root/.ssh/id_rsa.pub 172.16.100.120ssh-copy-id -i /root/.ssh/id_rsa.pub 172.16.100.200</code></pre><h1 id="4-测试免密登录"><a href="#4-测试免密登录" class="headerlink" title="4.测试免密登录"></a>4.测试免密登录</h1><pre><code class="hljs">ssh node01</code></pre>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>SSH</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux系统磁盘管理机制LVM详解</title>
    <link href="/linux/LVM/"/>
    <url>/linux/LVM/</url>
    
    <content type="html"><![CDATA[<p>LVM，Logical Volume Manager，即逻辑卷管理，Linux系统磁盘分区管理机制，是建立在硬盘和分区之上的一个逻辑层，为文件系统屏蔽了下层磁盘分区布局，从而提高磁盘分区管理的灵活性。其工作机制是将若干磁盘分区连接为一个整块的卷组（Volume Group），从而形成一个存储池，系统管理员可以从存储池随意创建逻辑卷组（Logical Volumes），最终在逻辑卷组上创建文件系统。由此，可以灵活方便地调整存储卷组的大小，并对磁盘存储按照组的方式命名、管理和分配，如按照使用用途进行定义development和sales，而不是使用物理磁盘名sda和sdb。若系统添加了新的磁盘，也不必将磁盘的文件移动到新的磁盘，而是通过LVM直接扩展文件系统，即可实现跨越磁盘的扩容。此外，LVM机制还适用于快照、热备份和数据动态迁移的场景，当某个物理硬盘出现故障时，可以在不停机的情况下进行数据恢复和重建从而提高了数据的可靠性与可用性</p><h1 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h1><h2 id="1-PV"><a href="#1-PV" class="headerlink" title="1.PV"></a>1.PV</h2><p>PV，Physical Volume，即物理卷，LVM的基本存储逻辑块，处于最底层，可以是整个硬盘、硬盘上的分区或从逻辑上与磁盘分区具有同样功能的设备，如hda1、sda1, c0d0p1、raid等。其基本单元为PE，Physical Extent，即物理区域，是具有唯一编号的可以被LVM寻址的最小存储单元，大小可根据实际情况在创建物理卷时指定，默认为4MB，大小确定后将不能改变，且同一个卷组中的所有物理卷的PE的大小需一致</p><h2 id="2-VG"><a href="#2-VG" class="headerlink" title="2.VG"></a>2.VG</h2><p>VG，Volume Group，即逻辑卷组，基于物理卷所建立的操作系统逻辑上的硬盘，相当于非LVM系统的物理硬盘，由一个或多个PV组成且可动态地添加，一个或多个VG即构成一个LVM系统。基于VG即可建立逻辑分区（LV，Logical Volume），相当于非LVM系统中的硬盘分区，其上可创建文件系统，如&#x2F;root、&#x2F;home等，且大小可伸缩</p><h1 id="2-创建流程"><a href="#2-创建流程" class="headerlink" title="2.创建流程"></a>2.创建流程</h1><h2 id="2-1-创建物理卷"><a href="#2-1-创建物理卷" class="headerlink" title="2.1 创建物理卷"></a>2.1 创建物理卷</h2><p>pvcreate命令用于将物理磁盘或分区创建为物理卷</p><h2 id="2-2-创建卷组"><a href="#2-2-创建卷组" class="headerlink" title="2.2 创建卷组"></a>2.2 创建卷组</h2><p>vgcreate命令用于将多个物理卷组成一个卷组</p><h2 id="2-3-创建逻辑卷"><a href="#2-3-创建逻辑卷" class="headerlink" title="2.3 创建逻辑卷"></a>2.3 创建逻辑卷</h2><p>lvcreate命令用于在卷组上创建逻辑卷</p><h2 id="2-4-格式化逻辑卷"><a href="#2-4-格式化逻辑卷" class="headerlink" title="2.4 格式化逻辑卷"></a>2.4 格式化逻辑卷</h2><p>mkfs命令用于格式化逻辑卷，以便挂载使用</p><h2 id="2-5-挂载逻辑卷"><a href="#2-5-挂载逻辑卷" class="headerlink" title="2.5 挂载逻辑卷"></a>2.5 挂载逻辑卷</h2><p>mount命令用于将逻辑卷挂载到指定的目录下</p><hr><h1 id="1-安装LVM"><a href="#1-安装LVM" class="headerlink" title="1.安装LVM"></a>1.安装LVM</h1><pre><code class="hljs">yum install -y lvmapt install -y lvm</code></pre><h1 id="2-添加磁盘"><a href="#2-添加磁盘" class="headerlink" title="2.添加磁盘"></a>2.添加磁盘</h1><h2 id="2-1-查看当前磁盘"><a href="#2-1-查看当前磁盘" class="headerlink" title="2.1 查看当前磁盘"></a>2.1 查看当前磁盘</h2><pre><code class="hljs">lsblkNAME MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT                                                                                  sr0   11:0    1 1024M  0 rom                                                                                              vda   252:0    0  256G  0 disk                                                                                             ├─vda1  252:1    0    1G  0 part /boot                                                                                       └─vda2  252:2    0  255G  0 part                                                                                               ├─centos_centos7-root 253:0 0 50G 0 lvm  /                                                                                             ├─centos_centos7-swap 253:1 0 2G 0 lvm  [SWAP]                                                                                        └─centos_centos7-home 253:2 0 203G 0 lvm  /home                                                                                       vdb   252:16   0  100G  0 disk</code></pre><h2 id="2-2-查看当前系统分区"><a href="#2-2-查看当前系统分区" class="headerlink" title="2.2 查看当前系统分区"></a>2.2 查看当前系统分区</h2><pre><code class="hljs">fdisk -lDisk /dev/vda: 274.9 GB, 274877906944 bytes, 536870912 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x0007a3a7   Device Boot      Start         End      Blocks   Id  System/dev/vda1   *        2048     2099199     1048576   83  Linux/dev/vda2         2099200   536870911   267385856   8e  Linux LVMDisk /dev/mapper/centos_centos7-root: 53.7 GB, 53687091200 bytes, 104857600 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk /dev/mapper/centos_centos7-swap: 2147 MB, 2147483648 bytes, 4194304 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk /dev/mapper/centos_centos7-home: 218.0 GB, 217961201664 bytes, 425705472 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk /dev/vdb: 107.4 GB, 107374182400 bytes, 209715200 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytes</code></pre><h2 id="2-3-查看当前文件系统"><a href="#2-3-查看当前文件系统" class="headerlink" title="2.3 查看当前文件系统"></a>2.3 查看当前文件系统</h2><pre><code class="hljs">df -hFilesystem                       Size  Used Avail Use% Mounted ondevtmpfs                         908M     0  908M   0% /devtmpfs                            919M     0  919M   0% /dev/shmtmpfs                            919M  8.6M  911M   1% /runtmpfs                            919M     0  919M   0% /sys/fs/cgroup/dev/mapper/centos_centos7-root   50G  2.0G   49G   4% //dev/mapper/centos_centos7-home  203G   33M  203G   1% /home/dev/vda1                       1014M  223M  792M  22% /boottmpfs                             82M     0   82M   0% /run/user/1000tmpfs                             82M     0   82M   0% /run/user/0</code></pre><h1 id="3-查看当前逻辑卷组"><a href="#3-查看当前逻辑卷组" class="headerlink" title="3.查看当前逻辑卷组"></a>3.查看当前逻辑卷组</h1><pre><code class="hljs">vgdisplay--- Volume group ---VG Name               centos_centos7System ID             Format                lvm2Metadata Areas        1Metadata Sequence No  4VG Access             read/writeVG Status             resizableMAX LV                0Cur LV                3Open LV               3Max PV                0Cur PV                1Act PV                1VG Size               &lt;255.00 GiBPE Size               4.00 MiBTotal PE              65279Alloc PE / Size       65278 / 254.99 GiBFree  PE / Size       1 / 4.00 MiBVG UUID               lLM9n5-z4TE-QIfp-NzXY-LVrS-gfRt-G4wnW0</code></pre><h1 id="4-基于新磁盘创建物理卷"><a href="#4-基于新磁盘创建物理卷" class="headerlink" title="4.基于新磁盘创建物理卷"></a>4.基于新磁盘创建物理卷</h1><h2 id="4-1-创建物理卷"><a href="#4-1-创建物理卷" class="headerlink" title="4.1 创建物理卷"></a>4.1 创建物理卷</h2><pre><code class="hljs">pvcreate /dev/vdb  Physical volume &quot;/dev/vdb&quot; successfully created.</code></pre><h2 id="4-2-验证物理卷"><a href="#4-2-验证物理卷" class="headerlink" title="4.2 验证物理卷"></a>4.2 验证物理卷</h2><pre><code class="hljs">pvscan   PV /dev/vdb    VG lvm00            lvm2 [&lt;100.00 GiB / 96.00 MiB free]  PV /dev/vda2   VG centos_centos7   lvm2 [&lt;255.00 GiB / 4.00 MiB free]  Total: 2 [354.99 GiB] / in use: 2 [354.99 GiB] / in no VG: 0 [0   ]pvdisplay   --- Physical volume ---  PV Name               /dev/vdb  VG Name               lvm00  PV Size               100.00 GiB / not usable 4.00 MiB  Allocatable           yes   PE Size               4.00 MiB  Total PE              25599  Free PE               24  Allocated PE          25575  PV UUID               oJo0QX-053y-pmGd-1uVQ-9ucZ-I6Rk-7YUDEj</code></pre><h1 id="5-基于物理卷创建逻辑卷组"><a href="#5-基于物理卷创建逻辑卷组" class="headerlink" title="5.基于物理卷创建逻辑卷组"></a>5.基于物理卷创建逻辑卷组</h1><h2 id="5-1-创建逻辑卷组"><a href="#5-1-创建逻辑卷组" class="headerlink" title="5.1 创建逻辑卷组"></a>5.1 创建逻辑卷组</h2><pre><code class="hljs">vgcreate lvm00 /dev/vdb  Volume group &quot;lvm00&quot; successfully created</code></pre><h2 id="5-2-验证逻辑卷组"><a href="#5-2-验证逻辑卷组" class="headerlink" title="5.2 验证逻辑卷组"></a>5.2 验证逻辑卷组</h2><pre><code class="hljs">vgscan   Reading volume groups from cache.  Found volume group &quot;lvm00&quot; using metadata type lvm2  Found volume group &quot;centos_centos7&quot; using metadata type lvm2vgdisplay   --- Volume group ---  VG Name               lvm00  System ID               Format                lvm2  Metadata Areas        1  Metadata Sequence No  2  VG Access             read/write  VG Status             resizable  MAX LV                0  Cur LV                1  Open LV               1  Max PV                0  Cur PV                1  Act PV                1  VG Size               &lt;100.00 GiB  PE Size               4.00 MiB  Total PE              25599  Alloc PE / Size       25575 / 99.90 GiB  Free  PE / Size       24 / 96.00 MiB  VG UUID               Se1Djo-IpJw-vj9b-2eBm-WrH6-mLLb-2rb4w1</code></pre><h1 id="6-基于逻辑卷组创建逻辑卷"><a href="#6-基于逻辑卷组创建逻辑卷" class="headerlink" title="6.基于逻辑卷组创建逻辑卷"></a>6.基于逻辑卷组创建逻辑卷</h1><h2 id="6-1-创建逻辑卷"><a href="#6-1-创建逻辑卷" class="headerlink" title="6.1 创建逻辑卷"></a>6.1 创建逻辑卷</h2><pre><code class="hljs">lvcreate -L 99.9G -n data lvm00</code></pre><h2 id="6-2-验证逻辑卷"><a href="#6-2-验证逻辑卷" class="headerlink" title="6.2 验证逻辑卷"></a>6.2 验证逻辑卷</h2><pre><code class="hljs">lvscan  ACTIVE            &#39;/dev/lvm00/data&#39; [99.90 GiB] inherit  ACTIVE            &#39;/dev/centos_centos7/swap&#39; [2.00 GiB] inherit  ACTIVE            &#39;/dev/centos_centos7/home&#39; [202.99 GiB] inherit  ACTIVE            &#39;/dev/centos_centos7/root&#39; [50.00 GiB] inheritlvdisplay   --- Logical volume ---  LV Path                /dev/lvm00/data  LV Name                data  VG Name                lvm00  LV UUID                59KnzQ-a5rz-NwKP-a3aI-mgB3-dy2B-zdj3wX  LV Write Access        read/write  LV Creation host, time centos7, 2020-02-06 20:22:34 +0800  LV Status              available  # open                 1  LV Size                99.90 GiB  Current LE             25575  Segments               1  Allocation             inherit  Read ahead sectors     auto  - currently set to     8192  Block device           253:3</code></pre><h1 id="7-基于逻辑卷创建磁盘分区"><a href="#7-基于逻辑卷创建磁盘分区" class="headerlink" title="7.基于逻辑卷创建磁盘分区"></a>7.基于逻辑卷创建磁盘分区</h1><h2 id="7-1-格式化磁盘分区"><a href="#7-1-格式化磁盘分区" class="headerlink" title="7.1 格式化磁盘分区"></a>7.1 格式化磁盘分区</h2><pre><code class="hljs">mkfs.ext4 /dev/lvm00/data</code></pre><h2 id="7-2-挂载磁盘分区"><a href="#7-2-挂载磁盘分区" class="headerlink" title="7.2 挂载磁盘分区"></a>7.2 挂载磁盘分区</h2><pre><code class="hljs">mkdir -p /datamount /dev/lvm00/data /data</code></pre><h1 id="8-验证文件系统"><a href="#8-验证文件系统" class="headerlink" title="8.验证文件系统"></a>8.验证文件系统</h1><pre><code class="hljs">df -hFilesystem                       Size  Used Avail Use% Mounted ondevtmpfs                         908M     0  908M   0% /devtmpfs                            919M     0  919M   0% /dev/shmtmpfs                            919M  8.6M  911M   1% /runtmpfs                            919M     0  919M   0% /sys/fs/cgroup/dev/mapper/centos_centos7-root   50G  2.0G   49G   4% //dev/mapper/centos_centos7-home  203G   33M  203G   1% /home/dev/vda1                       1014M  223M  792M  22% /boottmpfs                             82M     0   82M   0% /run/user/1000tmpfs                             82M     0   82M   0% /run/user/0/dev/mapper/lvm00-data            99G   61M   94G   1% /data</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.51cto.com/13911915/2155531">https://blog.51cto.com/13911915/2155531</a></li><li><a href="https://blog.csdn.net/m0_57554344/article/details/130558310">https://blog.csdn.net/m0_57554344/article/details/130558310</a></li><li><a href="https://blog.csdn.net/weixin_48195231/article/details/131010340">https://blog.csdn.net/weixin_48195231/article/details/131010340</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>存储</tag>
      
      <tag>LVM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux系统搭建FTP服务器</title>
    <link href="/linux/FTP/"/>
    <url>/linux/FTP/</url>
    
    <content type="html"><![CDATA[<h1 id="1-安装ftp"><a href="#1-安装ftp" class="headerlink" title="1.安装ftp"></a>1.安装ftp</h1><pre><code class="hljs">yum install -y vsftpdapt install -y vsftpd</code></pre><h1 id="2-创建配置文件"><a href="#2-创建配置文件" class="headerlink" title="2.创建配置文件"></a>2.创建配置文件</h1><pre><code class="hljs">mv /etc/vsftpd/vsftpd.conf /etc/vsftpd/vsftpd.conf.bakvi /etc/vsftpd/vsftpd.conf# 开启监听listen=YES# 验证文件的名字                            pam_service_name=vsftpd# 允许由userlist_file文件限制登陆的用户                         userlist_enable=YES# 支持tcp_wrappers,限制访问(/etc/hosts.allow,/etc/hosts.deny)                          tcp_wrappers=YES# 显示目录档案dirmessage_enable=YES# 自定义ftp上传路径（注意文件夹权限）local_root=/projects# 工作模式及其端口设置# VSFTP有两种工作模式，即PORT模式，主动模式；PASV模式，被动模式# 设置FTP服务器监听端口，默认为21listen_port=21# 设置使用20端口传输数据，默认为YESconnect_from_port_20=YES# PORT模式下FTP数据连接使用的端口，默认值为20。ftp_data_port=20# 设置是否启用PASV模式，默认值为YES# pasv_enable=NO# 设置PASV模式工作端口范围# pasv_min_port=0# pasv_max_port=0# 所有登陆用户都写权限,全局设置，默认为YESwrite_enable=YES# 是否允许本地用户登陆，默认为YESlocal_enable=YES# 本地用户登陆后所在目录，默认为该用户的家目录local_root=/ftp# 本地用户上传文件时的umask值，默认为077local_umask=022# 本地用户上传文件的权限，与chmod 所使用的数值相同，默认为0666# file_open_mode=0755# 设置登陆用户切换目录的权限# 禁用用户列表文件控制目录切换权限，所有用户权限由chroot_local_user控制，默认为NO# chroot_list_enable=NO# 设置控制权限，默认为NO，chroot_local_user=YES# 设置控制目录切换的列表文件的路径# chroot_list_file=/etc/vsftpd.chroot_list# 启用日志文件             xferlog_enable=YES# 使用本地时间而不是GMT                   use_localtime=YES# vsftpd日志存放位置                     vsftpd_log_file=/var/log/vsftpd.log# 用户登陆日志   dual_log_enable=YES# 记录上传下载文件的日志             xferlog_file=/var/log/xferlog# 记录日志使用标准格式          xferlog_std_format=YES# 登陆超时时间              idle_session_timeout=60# 是否允许匿名用户登入，默认为YESanonymous_enable=NO# 匿名登陆是否需要密码，默认为NO# no_anon_password=NO# 匿名登陆所使用的用户名，默认为ftp# ftp_username=ftp# 匿名登陆的家目录，默认为/var/ftp，权限不能为777# anon_root=/var/ftp# 匿名登陆是否有上传文件（非目录）的权限，默认为NO# anon_upload_enable=NO# 匿名登陆是否允许下载可阅读的档案，默认为YES# anon_world_readable_only=YES# 匿名登陆是否有新增目录的权限，默认为NO# anon_mkdir_write_enable=YES/NO（NO）# 匿名登陆是否有删除或者重命名的权限，默认为NO# anon_other_write_enable=NO# 设置是否改变匿名用户上传文件（非目录）的属主，默认为NO# chown_uploads=NO# 设置匿名用户上传文件（非目录）的属主# chown_username=username# 设置匿名登入者新增或上传档案时的umask值，默认为077，则新建档案的对应权限为700anon_umask=022# 匿名登陆是否启用email地址验证功能，默认为NO# deny_email_enable=NO# 匿名登陆email地址验证的验证文件定义，输入该文件内的email address，则允许登陆# banned_email_file=/etc/vsftpd/banner_emails# 设置数据传输模式# 禁用ASCII模式上传数据，即用二进制模式，默认为NO# ascii_upload_enable=NO# 禁用ASCII模式下载数据，即用二进制模式，默认为NO# ascii_download_enable=YES/NO（NO）# 欢迎语设置# 是否启用文件式欢迎语，默认为YESdirmessage_enable=YES# 定义文件式欢迎语的文件路径，默认为.message# message_file=.message# 定义文件式欢迎语的文件路径，默认不启用# banner_file=/etc/vsftpd/banner# 定义字符串式欢迎语，默认为空ftpd_banner=Welcome to FTP server</code></pre><h1 id="3-创建登陆用户"><a href="#3-创建登陆用户" class="headerlink" title="3.创建登陆用户"></a>3.创建登陆用户</h1><pre><code class="hljs">useradd testpasswd test</code></pre><h1 id="4-创建ftp数据目录"><a href="#4-创建ftp数据目录" class="headerlink" title="4.创建ftp数据目录"></a>4.创建ftp数据目录</h1><pre><code class="hljs">mkdir -p /opt/ftp</code></pre><h1 id="5-启动Vsftp"><a href="#5-启动Vsftp" class="headerlink" title="5.启动Vsftp"></a>5.启动Vsftp</h1><pre><code class="hljs">service vsftpd start</code></pre><h1 id="6-验证Ftp服务"><a href="#6-验证Ftp服务" class="headerlink" title="6.验证Ftp服务"></a>6.验证Ftp服务</h1><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="http://blog.51cto.com/yuanbin/108262">http://blog.51cto.com/yuanbin/108262</a></li><li><a href="https://blog.huochengrm.cn/pc/14580.html">https://blog.huochengrm.cn/pc/14580.html</a></li><li><a href="https://blog.csdn.net/hao_wujing/article/details/138346539">https://blog.csdn.net/hao_wujing/article/details/138346539</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>FTP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux系统搭建NTP时间同步服务器</title>
    <link href="/linux/NTP/"/>
    <url>/linux/NTP/</url>
    
    <content type="html"><![CDATA[<p>NTP，Network Time Protocol，即网络时间协议，用于精确同步网络节点的机器时间。NTP提供了高精准度的时间校正，其精度在局域网内可达0.1ms，Internet上精度可达几十ms，且能以加密确认的方式防止网络攻击</p><hr><h1 id="1-安装ntp"><a href="#1-安装ntp" class="headerlink" title="1.安装ntp"></a>1.安装ntp</h1><pre><code class="hljs">yum install -y ntpapt install -y ntp</code></pre><h1 id="2-修改配置文件"><a href="#2-修改配置文件" class="headerlink" title="2.修改配置文件"></a>2.修改配置文件</h1><pre><code class="hljs">vi /etc/ntp.conf# 设置ntp服务器server 172.16.100.100 prefer# server 0.centos.pool.ntp.org iburst# server 1.centos.pool.ntp.org iburst# server 2.centos.pool.ntp.org iburst# server 3.centos.pool.ntp.org iburst</code></pre><h1 id="3-启动ntp服务"><a href="#3-启动ntp服务" class="headerlink" title="3.启动ntp服务"></a>3.启动ntp服务</h1><pre><code class="hljs">systemctl restart ntpd.servicesystemctl enable ntpd.service</code></pre><h1 id="4-配置客户端ntp服务"><a href="#4-配置客户端ntp服务" class="headerlink" title="4.配置客户端ntp服务"></a>4.配置客户端ntp服务</h1><pre><code class="hljs">vi /etc/ntp.conf# 设置ntp服务器server 172.16.100.100</code></pre><h1 id="5-启动ntp服务"><a href="#5-启动ntp服务" class="headerlink" title="5.启动ntp服务"></a>5.启动ntp服务</h1><pre><code class="hljs">systemctl restart ntpd.servicesystemctl enable ntpd.service</code></pre><h1 id="6-验证时间同步"><a href="#6-验证时间同步" class="headerlink" title="6.验证时间同步"></a>6.验证时间同步</h1><pre><code class="hljs"># 查看是否与时间服务器连接成功ntpstat# 查看当前服务器与上层ntp服务器的状态ntpq -p</code></pre>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>NTP</tag>
      
      <tag>时间同步</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux系统配置Iptables防火墙</title>
    <link href="/linux/Iptables/"/>
    <url>/linux/Iptables/</url>
    
    <content type="html"><![CDATA[<p>Iptables，开源的数据包过滤网络防火墙系统，目前已集成于Linux内核，属于Netfilter项目，性能稳定而高效。Iptables工作于网络层，用于实现IP数据包的检测过滤、封包重定向、网络地址转换（NAT）、包速率限制等安全功能，且按照不同的目的被组织成表集合，从而完成对网络数据包进出设备及传输的控制，具备了网络防火墙功能，用以代替昂贵的商业防火墙解决方案</p><h1 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h1><p>Iptables防火墙系统由两部分组成，即netfilter和iptables</p><h2 id="1-netfilter"><a href="#1-netfilter" class="headerlink" title="1.netfilter"></a>1.netfilter</h2><p>netfilter，工作于内核空间，集成于Linux内核的通用网络包过滤模块，用于对进出内核协议栈的数据包进行过滤或修改，从而实现防火墙功能，是Iptables防火墙系统的核心</p><h2 id="2-iptables"><a href="#2-iptables" class="headerlink" title="2.iptables"></a>2.iptables</h2><p>iptables，工作于用户空间，本身并不具备包过滤功能，而是用户制定、管理和存储netfilter操作规则及数据包处理逻辑的接口与命令工具，即数据包触发某条规则就执行相应的处理逻辑</p><h1 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h1><p>Iptables防火墙的核心处理机制是由netfilter对IP数据包进行过滤，所有经过主机的数据包都必然经过netfilter所设的五个控制模块中的某个或某几个，并以用户定义的防火墙规则进行相应的处理动作，最终完成过滤过程。这五个控制模块即为链，每个链对数据包相同的处理动作即为表，由用户通过iptables工具定义的数据包处理逻辑即为规则。表由若干链（处理环节）组成，链由一条或若干条规则（处理逻辑）组成，规则由一些信息包过滤表组成</p><p>形象地讲，数据包类似于原始水流，netfilter所设的控制模块类似于自来水厂所设置的处理环节，iptables定义的过滤数据包的规则就是自来水厂设计的供水方案。自来水厂的原始水流必须经过若干或所有的处理环节，也即是完全符合供水方案之后，才能产生符合标准的自来水，最后经过管道输送给居民</p><h2 id="1-规则"><a href="#1-规则" class="headerlink" title="1.规则"></a>1.规则</h2><p>rules，规则，用户定义的数据包过滤机制，由匹配条件和处理动作两个要素组成，一般定义为：若数据包符合某个条件，就对这个数据包做出这样的处理，也即是符合rule的流量将会去往target。rule的处理对象是IP数据流，通常以五元组标识，即SourceIP、SourcePort、DestIP、DestPort和网络协议。对五元组中的某个或某些要素进行过滤，如放行（accept）、拒绝（reject）和丢弃（drop）等，即构成了rule，防火墙的的配置工作主要就是添加、修改和删除这些rule</p><h3 id="1-1-匹配条件"><a href="#1-1-匹配条件" class="headerlink" title="1.1 匹配条件"></a>1.1 匹配条件</h3><p>匹配条件，由网络五元组标识</p><ul><li>S_IP，源IP</li><li>S_PORT，源端口</li><li>D_IP，目的IP</li><li>D_PORT，目的端口</li><li>TCP&#x2F;UDP，四层协议</li></ul><h3 id="1-2-处理动作"><a href="#1-2-处理动作" class="headerlink" title="1.2 处理动作"></a>1.2 处理动作</h3><p>处理动作，被称为target，即对数据包进行堵、放行或丢弃的过滤方式</p><ul><li>ACCEPT，允许数据包通过</li><li>DROP，直接丢弃数据包，不回应任何信息，客户端只有当该链接超时后才会有反应</li><li>REJECT，拒绝数据包，会给客户端发送一个响应的信息</li><li>SNAT，源NAT，即改写数据包的源IP为指定IP或IP段，可指定端口范围，用于解决私网用户用同一个公网IP上网的问题</li><li>DNAT，目的NAT，即改写数据包的目的IP为指定IP或IP段，可指定端口范围，用于解决私网服务端用公网IP接收请求的问题</li><li>MASQUERADE，SNAT的特殊形式，适用于动态的、临时会变的IP</li><li>REDIRECT，在本机做端口映射</li><li>LOG，日志记录，可用于规则的调试</li></ul><h2 id="2-链"><a href="#2-链" class="headerlink" title="2.链"></a>2.链</h2><p>chains，链，即数据包报文的传输路径，可以理解为规则的检查清单，每条链都存储有一条或数条按照顺序排列的规则待检查。数据包到达某个链，则按顺序从链中第一条规则一个个的进行匹配，若满足就会根据该条规则所定义的方法处理该数据包，否则将继续检查下一条规则，直到匹配到一条规则为止。若该数据包不符合链中任一条规则，就会根据该链预先定义的默认策略进行处理。就这样，每个检查清单上的匹配过程，最终形成了一条有顺序的链。Iptables默认维护五个链，即：</p><ul><li>PREROUTING，数据包到达本机之后进入路由表之前，即路由前数据过滤，用于目标地址转换（DNAT）</li><li>INPUT，数据包通过路由表后目的地为本机，即入站数据过滤</li><li>FORWARDING，数据包通过路由表后目的地不是本机，即转发数据过滤，用于数据转发</li><li>OUTPUT，数据包由本机产生且向外转发，即出站数据过滤</li><li>POSTROUTIONG，数据包通过路由表后发送到网卡接口之前，即路由后过滤，用于源地址转换（SNAT，MASQUERADE）</li></ul><h2 id="3-表"><a href="#3-表" class="headerlink" title="3.表"></a>3.表</h2><p>tables，表，存储于链中相同功能的规则集合，实际使用时规则定义的入口，也即是明确每种表能够应用于哪些链。Iptables默认维护四张表，即：</p><ul><li>filter，过滤规则表，即控制数据包是否允许进出及转发，默认表，对应内核模iptables_filter，可以控制的链：INPUT、FORWARD 和 OUTPUT</li><li>nat，network address translation，网络地址转换规则表，用于修改数据包IP地址、端口等信息，即控制数据包地址转换，对应内核模块iptables_nat，可以控制的链有：PREROUTING、INPUT、OUTPUT和POSTROUTING</li><li>mangle，修改数据标记位规则表，拆解报文做出修改并重新封装，用于修改数据包的TOS(Type Of Service，服务类型)、TTL(Time To Live，生存周期)指以及为数据包设置Mark标记，以实现Qos(Quality Of Service，服务质量)调整以及策略路由等应用，由于需要相应的路由设备支持，因此应用并不广泛，对应内核模块iptables_mangle，即修改数据包的原数据，可以控制的链有：PREROUTING、INPUT、OUTPUT、FORWARD和POSTROUTING</li><li>raw，跟踪数据表规则表，用于决定数据包是否被状态跟踪机制处理，匹配数据包时优于其他表，对应内核模块iptables_raw，即控制nat表连接追踪机制的启用状况，可以控制的链路有：PREROUTING、OUTPUT</li></ul><hr><ul><li>注：数据报文必须按顺序匹配每条链上的一个个规则，所以编写的规则顺序极其关键，每条链上各个表被匹配的优先级为：raw —&gt; mangle —&gt; nat —&gt; filter</li></ul><h2 id="4-链表关系"><a href="#4-链表关系" class="headerlink" title="4.链表关系"></a>4.链表关系</h2><p>链是数据报文流转过程中的处理环节，表是某些相似规则的集合，但由于处理环节的分工不同，每个处理环节可能具有不同的表。实际上，除了Ouptput链能同时有四种表，其他链都只有两种或三种表</p><p><img src="/img/wiki/iptables/iptables001.jpg" alt="iptables001"></p><h1 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h1><p><img src="/img/wiki/iptables/iptables002.jpg" alt="iptables002"></p><ul><li>1.数据包到达网卡，先进入PREROUTING（路由前）链，做路由决策，判断数据包应发往何处，本机或其他机器</li><li>2.若数据包原目标地址为本机，则数据包前往INPUT（入站）链，到达INPUT链后本机任何进程都可进行接收</li><li>3.本机程序发送出数据包，经过OUTPUT链到达POSTROUTING（路由后）链，最后由网卡输出。此时，Iptables相当于主机防火墙的作用，过滤数据包用于服务器本机的安全控制</li><li>4.若数据包原目标地址非本机，即需要转发出去，且内核允许转发，数据包则前往FORWARD链，再经过POSTROUTING（路由后）链，最后由网卡输出。此时，Iptables相当于网络防火墙的作用，作为网关用于数据包的过滤与转发</li></ul><hr><p>Iptables防火墙的的配置工作主要就是通过iptables工具添加、修改和删除规则，且必须注意其顺序，语法为：</p><pre><code class="hljs">iptables [-t table] command [match] [target]iptables -t 表名 &lt;-A/I/D/R&gt; 规则链名 [规则号] &lt;-i/o 网卡名&gt; -p 协议名 &lt;-s 源IP/源子网&gt; --sport 源端口 &lt;-d 目标IP/目标子网&gt; --dport 目标端口 -j 动作</code></pre><ul><li>-t，设置所要操作的表，不指定则默认为filter</li><li>command，设置具体操作动作，如对指定链添加&#x2F;删除规则</li><li>match，设置所要处理的数据包的匹配规则</li><li>target，设置数据包的处理动作</li></ul><h1 id="1-安装iptables"><a href="#1-安装iptables" class="headerlink" title="1.安装iptables"></a>1.安装iptables</h1><pre><code class="hljs">sudo yum install -y iptablessudo apt install -y iptablessudo systemctl start iptablessudo systemctl status iptablessudo systemctl enable iptables</code></pre><h1 id="2-查询规则"><a href="#2-查询规则" class="headerlink" title="2.查询规则"></a>2.查询规则</h1><h2 id="2-1-查询全部规则"><a href="#2-1-查询全部规则" class="headerlink" title="2.1 查询全部规则"></a>2.1 查询全部规则</h2><pre><code class="hljs"># Chain，所属链；policy ACCEPT，该链默认规则；target，对应处理动作；prot，对应协议；opt，规则对应选项；source，对应源IP或网段；destination，对应目的IP或网段iptables -L# 查询规则命中，pkts，命中规则报文个数；bytes，命中规则报文总和；in，规则对应的入向接口；out，规则对应的出向接口iptables -vL# 查询规则显示行号iptables -vL --line-number</code></pre><h2 id="2-2-指定表查询"><a href="#2-2-指定表查询" class="headerlink" title="2.2 指定表查询"></a>2.2 指定表查询</h2><pre><code class="hljs">iptables -t filter -L</code></pre><h2 id="2-3-指定链查询"><a href="#2-3-指定链查询" class="headerlink" title="2.3 指定链查询"></a>2.3 指定链查询</h2><pre><code class="hljs">iptables -L INPUT</code></pre><h1 id="3-新增规则"><a href="#3-新增规则" class="headerlink" title="3.新增规则"></a>3.新增规则</h1><ul><li>-I <CHAIN>，insert，插入规则，即在指定链规则的首位插入，其后加上#则表示插入到指定链的第#号规则的位置</li><li>-A <CHAIN>，append，追加规则，即在指定链规则的末尾插入</li><li>-s <S_IP>，指定源IP，其前加!表示取反</li><li>-j <ACTION>，指定执行的动作</li><li>-d <D_IP>，指定目标IP</li><li>-i <NIC>，指定网卡入口流量</li><li>-o <NIC>，指定网卡出口流量</li><li>-p &lt;TCP|UDP|ICMP&gt;，指定网络协议</li></ul><h2 id="3-1-插入规则到首位"><a href="#3-1-插入规则到首位" class="headerlink" title="3.1 插入规则到首位"></a>3.1 插入规则到首位</h2><pre><code class="hljs"># 查看规则iptables -nvL INPUT# iptables -t filter -I INPUT -s 1.1.1.1 -j DROP</code></pre><h2 id="3-2-插入规则到位置"><a href="#3-2-插入规则到位置" class="headerlink" title="3.2 插入规则到位置"></a>3.2 插入规则到位置</h2><pre><code class="hljs">iptables -I INPUT 3 -s 3.3.3.3 -j ACCEPT</code></pre><h2 id="3-3-插入规则到末尾"><a href="#3-3-插入规则到末尾" class="headerlink" title="3.3 插入规则到末尾"></a>3.3 插入规则到末尾</h2><pre><code class="hljs">iptables -A INPUT -s 255.255.255.255 -j ACCEPT</code></pre><h1 id="4-修改规则"><a href="#4-修改规则" class="headerlink" title="4.修改规则"></a>4.修改规则</h1><ul><li>-R <CHAIN> #，修改指定链中指定序号的规则</li></ul><h2 id="4-1-修改规则"><a href="#4-1-修改规则" class="headerlink" title="4.1 修改规则"></a>4.1 修改规则</h2><pre><code class="hljs"># 将INPUT链filter表编号为1的规则为：-s 10.37.129.3 -j ACCEPTiptables -t filter -R INPUT 1 -s 10.37.129.3 -j ACCEPT</code></pre><h2 id="4-2-修改链的默认策略"><a href="#4-2-修改链的默认策略" class="headerlink" title="4.2 修改链的默认策略"></a>4.2 修改链的默认策略</h2><pre><code class="hljs"># -P，policy，即策略，将FORWARD链的默认规则设置为DROPiptables -P FORWARD DROP</code></pre><ul><li>注：规则的修改操作实质上是将整个规则替换为新规则，而不是只在命令中输入修改部分的内容，非常容易出错，建议将规则删除掉重新编写</li></ul><h1 id="5-删除规则"><a href="#5-删除规则" class="headerlink" title="5.删除规则"></a>5.删除规则</h1><ul><li>-D <CHAIN>，根据规则的具体匹配条件与动作进行删除，其后加上#则表示根据规则的编号进行删除</li><li>-F <CHAIN>，清空指定链上的所有规则</li><li>-t <TABLE> -F，清空某种表在所有链上的规则</li></ul><h2 id="5-1-删除指定规则"><a href="#5-1-删除指定规则" class="headerlink" title="5.1 删除指定规则"></a>5.1 删除指定规则</h2><pre><code class="hljs">iptables -D INPUT -s 1.1.1.1 -j DROP</code></pre><h2 id="5-2-删除指定编号的规则"><a href="#5-2-删除指定编号的规则" class="headerlink" title="5.2 删除指定编号的规则"></a>5.2 删除指定编号的规则</h2><pre><code class="hljs">iptables -D INPUT 5</code></pre><h1 id="6-保存规则"><a href="#6-保存规则" class="headerlink" title="6.保存规则"></a>6.保存规则</h1><p>Iptables系统以命令行设置的规则临时有效，系统重启之后即会丢失，其规则保存文件为&#x2F;etc&#x2F;sysconfig&#x2F;iptables，写入到这个文件的规则才会永久生效</p><h2 id="6-1-当前规则保存到规则文件"><a href="#6-1-当前规则保存到规则文件" class="headerlink" title="6.1 当前规则保存到规则文件"></a>6.1 当前规则保存到规则文件</h2><pre><code class="hljs">service iptables save</code></pre><h2 id="6-2-备份规则文件"><a href="#6-2-备份规则文件" class="headerlink" title="6.2 备份规则文件"></a>6.2 备份规则文件</h2><pre><code class="hljs">iptables-save &gt; iptables.ini</code></pre><h2 id="6-3-从文件恢复规则"><a href="#6-3-从文件恢复规则" class="headerlink" title="6.3 从文件恢复规则"></a>6.3 从文件恢复规则</h2><pre><code class="hljs">iptables-restore &lt; iptables.ini</code></pre><h1 id="7-清空规则"><a href="#7-清空规则" class="headerlink" title="7.清空规则"></a>7.清空规则</h1><pre><code class="hljs"># 清空某个表中所有链上的规则iptables -t filter -F# 清空链内所有规则，不指定链则清空所有链iptables -F INPUT# 删除自定义空链，若链内有规则则无法删除iptables -X# 计数器清零iptables -Z</code></pre><h1 id="8-防火墙配置实例"><a href="#8-防火墙配置实例" class="headerlink" title="8.防火墙配置实例"></a>8.防火墙配置实例</h1><h2 id="8-1-端口开放"><a href="#8-1-端口开放" class="headerlink" title="8.1 端口开放"></a>8.1 端口开放</h2><pre><code class="hljs"># 允许本地回环接口，即允许本机访问本机iptables -A INPUT -s 127.0.0.1 -d 127.0.0.1 -j ACCEPT# 允许已建立的或相关连的通行iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT# 允许所有本机向外的访问iptables -A OUTPUT -j ACCEPT# 允许访问80端口iptables -A INPUT -p tcp --dport 80 -j ACCEPT# 允许192.168.1.0/24网段访问22端口iptables -A INPUT -s 192.168.1.0/24 -p tcp --dport 22 -j ACCEPT</code></pre><h2 id="8-2-封禁所有非开放的策略"><a href="#8-2-封禁所有非开放的策略" class="headerlink" title="8.2 封禁所有非开放的策略"></a>8.2 封禁所有非开放的策略</h2><pre><code class="hljs"># 禁止其他未允许的规则访问iptables -A INPUT -j reject# 禁止其他未允许的规则访问iptables -A FORWARD -j REJECT</code></pre><h2 id="8-3-设置默认规则"><a href="#8-3-设置默认规则" class="headerlink" title="8.3 设置默认规则"></a>8.3 设置默认规则</h2><pre><code class="hljs"># 默认封禁所有入口iptables -P INPUT DROP# 默认封禁所有转发iptables -P FORWARD DROP# 默认开放所有出口iptables -P OUTPUT ACCEPT</code></pre><h2 id="8-4-白名单配置"><a href="#8-4-白名单配置" class="headerlink" title="8.4 白名单配置"></a>8.4 白名单配置</h2><pre><code class="hljs"># 允许内网访问iptables -I INPUT -p all -s 192.168.1.0/24 -j ACCEPT# 允许指定IP访问3306端口iptables -I INPUT -p tcp -s 183.121.3.7 --dport 3306 -j ACCEPT</code></pre><h2 id="8-5-黑名单配置"><a href="#8-5-黑名单配置" class="headerlink" title="8.5 黑名单配置"></a>8.5 黑名单配置</h2><pre><code class="hljs"># 封禁单个IPiptables -I INPUT -s 123.45.6.7 -j DROP# 封禁IP段iptables -I INPUT -s 123.45.6.0/24 -j DROP# 封禁不在指定网段的IP通过网卡ens33的访问iptables -I INPUT -p tcp ! -s 192.168.1.0/24 -i ens33 -j DROP</code></pre><h2 id="8-6-端口映射"><a href="#8-6-端口映射" class="headerlink" title="8.6 端口映射"></a>8.6 端口映射</h2><pre><code class="hljs"># --dport 80，目的IP为公网IP 80端口的流量包；-j DNAT --to-destination 192.168.0.3:80，改写目的IP为内网IP端口，也即将访问公网IP 80端口的流量转发到内网IP的80端口，实现了无公网IP内网服务器的服务发布，解决了流量进不来的问题iptables -t nat -A PREROUTING -i ppp0 -p tcp --dport 80 -j DNAT --to-destination 192.168.0.3:80</code></pre><h2 id="8-7-IP映射"><a href="#8-7-IP映射" class="headerlink" title="8.7 IP映射"></a>8.7 IP映射</h2><pre><code class="hljs"># -s 192.168.100.0/24 -o ens36，经网卡ens36源且IP网段为192.168.100.0/24的出口流量包；-j SNAT --to-source 12.0.0.1，改写源IP为公网IP 12.0.0.1，也即将内网服务器的访问流量路由到公网IP，实现了内部局域网共享公网IP进行外网访问，解决了流量出不去的问题iptables -t nat -A POSTROUTING -s 192.168.100.0/24 -o ens36 -j SNAT --to-source 12.0.0.1</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/fly910905/article/details/123690660">https://blog.csdn.net/fly910905/article/details/123690660</a></li><li><a href="https://blog.csdn.net/chocolee911/article/details/80688200">https://blog.csdn.net/chocolee911/article/details/80688200</a></li><li><a href="https://blog.csdn.net/shujuliu818/article/details/125649998">https://blog.csdn.net/shujuliu818/article/details/125649998</a></li><li><a href="https://blog.csdn.net/qq_42197548/article/details/131461599">https://blog.csdn.net/qq_42197548/article/details/131461599</a></li><li><a href="https://blog.csdn.net/weixin_44431371/article/details/120034719">https://blog.csdn.net/weixin_44431371/article/details/120034719</a></li><li><a href="https://blog.csdn.net/weixin_53139887/article/details/122418822">https://blog.csdn.net/weixin_53139887/article/details/122418822</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>网络</tag>
      
      <tag>Iptables</tag>
      
      <tag>防火墙</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Keepalived配置LVS高可用负载均衡集群</title>
    <link href="/linux/Keepalived-LVS/"/>
    <url>/linux/Keepalived-LVS/</url>
    
    <content type="html"><![CDATA[<h1 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h1><ul><li>172.16.100.100 lvs-server01</li><li>172.16.100.120 lvs-server02</li><li>172.16.100.150 nginx-server01</li><li>172.16.100.160 nginx-server02</li><li>172.16.100.200 vip</li></ul><hr><h1 id="1-安装lvs、keepalived、nginx"><a href="#1-安装lvs、keepalived、nginx" class="headerlink" title="1.安装lvs、keepalived、nginx"></a>1.安装lvs、keepalived、nginx</h1><h1 id="2-创建keepalived配置文件"><a href="#2-创建keepalived配置文件" class="headerlink" title="2.创建keepalived配置文件"></a>2.创建keepalived配置文件</h1><pre><code class="hljs">! Configuration File for keepalivedglobal_defs &#123;  notification_email &#123;  example@example.cn  &#125;notification_email_from example@example.cnsmtp_server 127.0.0.1smtp_connect_timeout 30router_id lvs-ha-001&#125;vrrp_instance lvs-ha &#123;  # 设置为主节点  state MASTER  interface eth1  virtual_router_id 80  # 设置优先级,高于备机  priority 100  advert_int 1  authentication &#123;    auth_type PASS    auth_pass 1111  &#125;  virtual_ipaddress &#123;    172.16.100.200/24  &#125;&#125;virtual_server 172.16.100.120 80 &#123;  delay_loop 6  # 配置lvs调度算法  lb_algo wrr  # 配置lvs调度类型  lb_kind DR  protocol TCP  nat_mask 255.255.255.0  real_server 172.16.100.240 80 &#123;    weight 1    TCP_CHECK &#123;        connect_timeout 8        nb_get_retry 3        delay_before_retry 3        connect_port 80            &#125;&#125;  real_server 172.16.100.248 80 &#123;    weight 1  TCP_CHECK &#123;    connect_timeout 8    nb_get_retry 3    delay_before_retry 3    connect_port 80    &#125;  &#125;&#125;</code></pre><hr><ul><li>注：lvs备机的配置文件基本相同，只需更改router_id、state、priority这三处即可</li></ul><h1 id="3-Nginx后端服务器创建lvs脚本"><a href="#3-Nginx后端服务器创建lvs脚本" class="headerlink" title="3.Nginx后端服务器创建lvs脚本"></a>3.Nginx后端服务器创建lvs脚本</h1><pre><code class="hljs">vi /usr/local/sbin/lvs_dr.sh#! /bin/bashvip=172.16.100.200# RS绑定vip到回环网卡ifconfig lo:0 $vip broadcast $vip netmask 255.255.255.255 up# RS添加路由route add -host $vip lo:0# 配置ARP抑制，也可配置到/etc/sysctl.confecho &quot;1&quot; &gt;/proc/sys/net/ipv4/conf/lo/arp_ignoreecho &quot;2&quot; &gt;/proc/sys/net/ipv4/conf/lo/arp_announceecho &quot;1&quot; &gt;/proc/sys/net/ipv4/conf/all/arp_ignoreecho &quot;2&quot; &gt;/proc/sys/net/ipv4/conf/all/arp_announce</code></pre><h1 id="4-创建LVS规则"><a href="#4-创建LVS规则" class="headerlink" title="4.创建LVS规则"></a>4.创建LVS规则</h1><pre><code class="hljs">chmod +x /usr/local/sbin/lvs_dr.shsh /usr/local/sbin/lvs_dr.sh</code></pre><h1 id="5-启动keepalived、nginx"><a href="#5-启动keepalived、nginx" class="headerlink" title="5.启动keepalived、nginx"></a>5.启动keepalived、nginx</h1><h1 id="6-模拟故障，测试集群高可用"><a href="#6-模拟故障，测试集群高可用" class="headerlink" title="6.模拟故障，测试集群高可用"></a>6.模拟故障，测试集群高可用</h1>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>负载均衡</tag>
      
      <tag>Keepalived</tag>
      
      <tag>高可用</tag>
      
      <tag>LVS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Keepalived配置文件详解</title>
    <link href="/linux/KeepalivedConfig/"/>
    <url>/linux/KeepalivedConfig/</url>
    
    <content type="html"><![CDATA[<pre><code class="hljs">! Configuration File for keepalived# global_defs，全局定义块 global_defs &#123;  notification_email &#123;  # 设置故障切换告警邮箱，多个分行即可  example@example.cn  &#125;# 设置发件人邮箱地址notification_email_from example@example.cn# 设置smtp服务器地址smtp_server 127.0.0.1# 设置smtp连接超时时间smtp_connect_timeout 30# 该节点的标识符router_id web-ha-001&#125;# vrrp_sync_group，实例组配置，可不设置，用于确定失败切换（FailOver）包含的路由实例个数，即在有2个负载均衡器的场景，一旦某个负载均衡器失效，需要自动切换到另外一个负载均衡器的实例是哪些，至少包含一个vrrp实例# vrrp_sync_group VG_1 &#123;     #group &#123;      # VI_1      # VI_2# &#125;# 设置切换master执行的脚本notify_master /path/*.sh# 设置换backup执行的脚本netify_backup /path/*.sh# 设置故障发生执行的脚本notify_fault &quot;/path/*.sh VG_1&quot; notify /path/xx.sh# 使用global_defs中提供的邮件地址和smtp服务器发送邮件通知smtp_alert&#125;# vrrp_instance，vrrp实例定义，实例名出自vrrp_sync_groupvrrp_instance VI_1 &#123;  # 设置主备服务器，若设置nopreempt，则主备由priority决定  state MASTER  # 设置VIP绑定的网卡  interface eth0  # 忽略vrrp的interface错误，默认不设置  dont_track_primary   # 设置额外的监控，其中任一网卡故障都会进行切换  track_interface&#123;    eth0    eth1  &#125;  # 发送多播包的地址，默认使用绑定网卡的primary ip  mcast_src_ip  # 切换master状态后，延迟进行gratuitous ARP请求  garp_master_delay  # VRRP组名，两节点一致，表明节点属同一VRRP组  virtual_router_id 50  # 优先级定义，取值1~254，高优先级竞选为master  priority 100  # 组播信息发送间隔，两节点一致  advert_int 1  # 非抢占模式，设置在备机上，或者优先级低于备机的主机上  # 表示主机或优先级高于主机的备机在故障恢复后不提供服务，即不抢占VIP  nopreempt  # 抢占延时，默认5分钟  preempt_delay  #debug级别  debug  # 设置认证  authentication &#123;    # 认证方式    auth_type PASS    # 认证密码    auth_pass 1234  &#125;  # 设置vip  virtual_ipaddress &#123;    192.168.0.200/24  &#125;&#125;# virtual_server，虚拟服务器定义块# 用于管理LVS，实现与LVS的结合。ipvsadm命令可以实现的管理可以通过参数配置实现，real_server是其子模块# VIP定义，要和vrrp_instance模块中的virtual_ipaddress地址一致virtual_server 192.168.0.200 80 &#123;  # 健康检查时间间隔  delay_loop 6  # lvs调度算法，rr|wrr|lc|wlc|lblc|sh|dh  lb_algo rr  # 负载均衡转发规则,NAT|DR|RUN   lb_kind DR  nat_mask 255.255.255.0  # 会话保持时间  persistence_timeout 5  # 使用的协议,TCP|UDP  protocol TCP  # lvs会话保持粒度  persistence_granularity &lt;NETMASK&gt;  # 检查的web服务器的虚拟主机  virtualhost &lt;string&gt;  # 备用机，所有realserver失效后启用  sorry_server&lt;IPADDR&gt; &lt;port&gt;  # real server，真实服务器定义块  real_server 192.168.0.150 80 &#123;    # 默认为1,0为失效    weight 1    # 服务器健康检查失效时，将其设为0，而不是直接从ipvs中删除     inhibit_on_failure    # 检测到server up后执行的脚本    notify_up &lt;string&gt; | &lt;quoted-string&gt;    # 检测到server down后执行脚本    notify_down &lt;string&gt; | &lt;quoted-string&gt;    # TCP健康检测                TCP_CHECK &#123;      # 连接超时时间      connect_timeout 3      # 重连次数      nb_get_retry 3      # 重连间隔时间      delay_before_retry 3      # 健康检测的端口的端口      connect_port 80    &#125;    # 脚本健康检测    MISC_CHECK&#123;      # 外部脚本路径      misc_path &lt;string&gt; | &lt;quoted-string&gt;      # 脚本执行超时时间      misc_timeout      # 若设置该项，则退出状态码会用来动态调整服务器的权重，返回0 正常，不修改；返回1，检查失败，权重改为0；返回2-255，正常，权重设置为：返回状态码-2      misc_dynamic    &#125;    # HTTP_GET、SSL_GET健康检测    HTTP_GET | SSL_GET &#123;    # 检查url，可以指定多个    url&#123;      path /      # 检查后的摘要信息      digest &lt;string&gt;      # 检查的返回状态码      status_code 200    &#125;    connect_port &lt;port&gt;     bindto &lt;IPADD&gt;    connect_timeout 5    nb_get_retry 3    delay_before_retry 2    &#125;    # SMTP健康检测    SMTP_CHECK&#123;      host&#123;        connect_ip &lt;IP ADDRESS&gt;        # 默认检查25端口        connect_port &lt;port&gt;        bindto &lt;IP ADDRESS&gt;      &#125;      connect_timeout 5      retry 3      delay_before_retry 2      # smtp helo请求命令参数，可选      helo_name &lt;string&gt; | &lt;quoted-string&gt;    &#125;  &#125;&#125;</code></pre>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Keepalived</tag>
      
      <tag>高可用</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Keepalived编译安装</title>
    <link href="/linux/Keepalived/"/>
    <url>/linux/Keepalived/</url>
    
    <content type="html"><![CDATA[<p>Keepalived，由C语言编写的基于VRRP（Vritrual Router Redundancy Protocol，虚拟路由冗余协议)的开源路由软件，用于为基于Linux的基础设施提供简单而稳定的负载均衡和高可用方案。最初是为LVS设计，用于管理与监控LVS集群各个服务节点的状态，根据其健康状况动态地、自适应地维护和管理负载均衡的服务器池，自动检测集群故障并完成转移，避免IP单点故障。Keepalived后来融入了可实现高可用的VRRP功能，也可作为其他服务的高可用解决方案，如Nginx、Haproxy、MySQL等</p><h1 id="1-体系架构"><a href="#1-体系架构" class="headerlink" title="1.体系架构"></a>1.体系架构</h1><h2 id="1-1-core-components"><a href="#1-1-core-components" class="headerlink" title="1.1 core components"></a>1.1 core components</h2><p>keepalived的核心组件，负责主进程的启动及维护，由一系列功能模块组成</p><ul><li>Checkers，基于网络层（IP）、传输层（TCP）、应用层（HTTP、SSL等）对服务器池内真实服务器的健康检查模块</li><li>WatchDog，监控checkers和VRRP进程的状况</li><li>VRRP Stack，VRRP协议的实现，负责负载均衡器之间的切换，从而实现高可用性</li><li>IPVS wrapper，负责设定规则到内核ipvs的接口，即根据配置文件生成IPVS规则并送往内核空间的IPVS使之生效</li><li>Netlink Reflector，负责设定vrrp的vip地址等路由相关的功能</li></ul><h2 id="1-2-Control-Plane"><a href="#1-2-Control-Plane" class="headerlink" title="1.2 Control Plane"></a>1.2 Control Plane</h2><p>keepalived的控制面板，负责配置文件的编译和解析</p><h2 id="1-3-Scheduler-I-x2F-O-Multiplexer"><a href="#1-3-Scheduler-I-x2F-O-Multiplexer" class="headerlink" title="1.3 Scheduler I&#x2F;O Multiplexer"></a>1.3 Scheduler I&#x2F;O Multiplexer</h2><p>I&#x2F;O复用分发调度器，负责keepalived所有内部任务请求的调度</p><h2 id="1-4-Memory-Management"><a href="#1-4-Memory-Management" class="headerlink" title="1.4 Memory Management"></a>1.4 Memory Management</h2><p>内存管理机制，提供内存访问的一些通用方法</p><h2 id="1-5-SMTP"><a href="#1-5-SMTP" class="headerlink" title="1.5 SMTP"></a>1.5 SMTP</h2><p>即SMTP接口，用于VRRP Stack发生地址流动或Checkers发现服务上下线并增删服务节点时，以邮件方式通知管理员</p><h1 id="2-工作原理"><a href="#2-工作原理" class="headerlink" title="2.工作原理"></a>2.工作原理</h1><h2 id="2-1-VRRP工作原理"><a href="#2-1-VRRP工作原理" class="headerlink" title="2.1 VRRP工作原理"></a>2.1 VRRP工作原理</h2><p>VRRP，Virtual Router Redundancy Protocol，即虚拟路由冗余协议，是实现路由器高可用的容错协议，最初是由IETF提出的用于解决局域网静态网关单点故障的路由协议，应用于交换机、路由器等设备。VRRP基本原理是将多台提供相同功能的路由器虚拟成为一个路由设备，虚拟MAC地址为00-00-5e-00-01-xx（xx为唯一VRID，即Virtual Router Identifier），对外提供虚拟IP，内部则由竞选机制根据优先级高低来决定路由器的角色，其中一台被选举为Master的拥有外部虚拟IP的路由器负责路由任务，如ARP请求的解析与响应、转发IP数据包等，其余路由器均为Backup角色。Master角色的路由器将会以IP组播（默认组播地址为224.0.0.18）的方式发送心跳消息给组内的Backup路由器，通知自己的存活状态及优先级。Master路由器若发生故障则会停止发送心跳消息，Backup路由器组检测不到Master的心跳消息将再通过同样的竞选机制选举出优先级最高的一台作为新的Master路由器，并接管路由任务</p><p>VRRP使得外部认为只有一个网关在工作，所有的网络功能都是通过Master处理，突发故障时由Backup立即接替而不影响内外数据通信，且无需手动修改网络配置，实现了路由故障的无缝切换，达到了整个过程对外部而言完全透明无感知的效果</p><h2 id="2-2-Keepalived工作流程"><a href="#2-2-Keepalived工作流程" class="headerlink" title="2.2 Keepalived工作流程"></a>2.2 Keepalived工作流程</h2><p>1.Keepalived高可用节点通过VRRP报文的交互获知各自的优先级即route_id，并通过VRRP的竞选机制确定主、备节点，主节点优先级高于备节点，若优先级相同则通过IP值较大者成为主节点</p><p>2.主节点接管IP资源和服务资源对外提供服务，并不断以组播方式向备节点发送心跳消息用以告知备节点自己的优先级及存活状态</p><p>3.备节点接收到心跳消息，得知自己的优先级低于主节点，处于待机状态</p><p>4.主节点突发故障，不能再发送心跳消息，或人为降低了优先级</p><p>5.备节点接收不到主节点的心跳消息或获知了主节点优先级的变动，则将各自的优先级通告给其余备节点进行协商，即再次进行竞选，最后优先级高的备节点则为新的主节点，由其调用自身的接管程序接管之前主节点的IP资源和服务资源继续对外提供服务。从而，完成了主备切换，保障了业务的连续性</p><p>6.若之前故障的主节点由人工介入完成修复，若工作于抢占模式，则会再次成为主节点，否则将加入备节点</p><h1 id="3-网络场景"><a href="#3-网络场景" class="headerlink" title="3.网络场景"></a>3.网络场景</h1><h2 id="3-1-网络层场景"><a href="#3-1-网络层场景" class="headerlink" title="3.1 网络层场景"></a>3.1 网络层场景</h2><p>工作方式是通过ICMP协议向服务器集群发送ICMP数据包，类似于ping实现的功能，若某节点没有返回响应数据包，则判定为节点故障，Keepalived将报告此节点失效，并从服务器集群中剔除该节点</p><h2 id="3-2-传输层场景"><a href="#3-2-传输层场景" class="headerlink" title="3.2 传输层场景"></a>3.2 传输层场景</h2><p>Keepalived在传输层利用TCP协议的端口连接和扫描技术来判断集群点是否正常。如常见的WEB服务默认的80端口、SSH服务默认的22端口等，若探测到某端口没有响应数据返回，则判定为该端口异常，然后强制将此端口对应的节点从服务器集群组中剔除</p><h2 id="3-3-应用层场景"><a href="#3-3-应用层场景" class="headerlink" title="3.3 应用层场景"></a>3.3 应用层场景</h2><p>Keepalived在应用层的运行方式更加全面化和复杂化，如根据业务场景编写监测程序来判定程序或服务是否运行正常，若Keepalived的检测结果与用户设定不一致，则将对应的服务从服务器剔除</p><h1 id="4-高可用架构"><a href="#4-高可用架构" class="headerlink" title="4.高可用架构"></a>4.高可用架构</h1><h2 id="4-1-主从高可用架构"><a href="#4-1-主从高可用架构" class="headerlink" title="4.1 主从高可用架构"></a>4.1 主从高可用架构</h2><p>一个Master节点和一个Backup节点，其中Master节点对外提供服务，两节点间保持心跳，直到Master节点因宕机服务不可用时，系统会切换到Backup继续提供服务，宕机的Master节点恢复后将作为Backup加入集群</p><h2 id="4-2-双主高可用架构"><a href="#4-2-双主高可用架构" class="headerlink" title="4.2 双主高可用架构"></a>4.2 双主高可用架构</h2><p>两节点均为Master节点，同时对外提供服务，同时保持心跳，某一节点宕机服务不可用时，流量将会全部导向另一节点继续提供服务，宕机节点恢复后重新加入集群提供服务。两节点对外服务的虚拟IP地址不同，可根据业务特点自行衡量</p><h1 id="5-脑裂现象"><a href="#5-脑裂现象" class="headerlink" title="5.脑裂现象"></a>5.脑裂现象</h1><p>高可用（HA）系统中，当联系两个节点的“心跳线”断开时，由于相互失去了联系，双方都会判定对方出现故障。两个节点上的HA软件像“裂脑人”一样争抢共享资源与应用服务，本为一个整体、动作协调的HA系统，分裂成为两个独立的个体。此时，共享资源被瓜分，两边服务都不能运行或都各自独立运行，将会造成业务中断或同时读写共享存储导致数据损坏的严重后果，即为脑裂现象</p><h2 id="5-1-常见原因"><a href="#5-1-常见原因" class="headerlink" title="5.1 常见原因"></a>5.1 常见原因</h2><ul><li>高可用服务器之间心跳线链路故障，导致无法正常通信，如因心跳线断开、老化或连接设备故障（网卡及交换机）等、网卡及驱动故障或心跳网卡地址等信息配置不正确导致的心跳发送失败、IP冲突</li><li>采用仲裁机制的仲裁机器故障</li><li>高可用服务器开启了iptables防火墙阻挡了心跳消息传输</li><li>其他服务配置不当，如心跳方式不同，心跳广插冲突、软件Bug等</li></ul><h2 id="5-2-解决方案"><a href="#5-2-解决方案" class="headerlink" title="5.2 解决方案"></a>5.2 解决方案</h2><ul><li>若开启防火墙，则一定要保障心跳消息的通过，一般以允许IP段的形式解决</li><li>同时使用串行电缆和以太网电缆或同时使用两条心跳线路作为灾备冗余</li><li>开发检测报警程序，通过监控软件检测脑裂</li></ul><hr><h1 id="1-安装依赖包"><a href="#1-安装依赖包" class="headerlink" title="1.安装依赖包"></a>1.安装依赖包</h1><pre><code class="hljs">apt install -y gcc make libssl-dev libnl-3-devyum install -y gcc make openssl-devel libnl-devel</code></pre><h1 id="2-编译安装keepalived"><a href="#2-编译安装keepalived" class="headerlink" title="2.编译安装keepalived"></a>2.编译安装keepalived</h1><pre><code class="hljs">tar -xzvf keepalived-2.2.1.tar.gz &amp;&amp; cd keepalived-2.2.1./configure --prefix=/usr/local/keepalived --sysconfdir=/etc --with-init=systemdmake &amp;&amp; make install</code></pre><h1 id="3-创建keepalived配置文件"><a href="#3-创建keepalived配置文件" class="headerlink" title="3.创建keepalived配置文件"></a>3.创建keepalived配置文件</h1><pre><code class="hljs">mv /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bakvi /etc/keepalived/keepalived.confglobal_defs &#123;  notification_email  &#123;    admin@sword.com  &#125;  notification_email_from  smtp_server 127.0.0.1  smtp_connect_timeout 30  router_id master&#125;vrrp_script check_nginx &#123;  script &quot;/etc/keepalived/nginx_check.sh&quot;  interval 2  weight -20&#125;vrrp_instance NGINX &#123;  state MASTER  interface eth0  virtual_router_id 51  priority 100  advert_int 1  authentication &#123;    auth_type PASS    auth_pass 123456  &#125;  virtual_ipaddress &#123;    172.16.100.150/24  &#125;  track_script &#123;    check_nginx  &#125;&#125;</code></pre><h1 id="4-配置keepalived启动参数"><a href="#4-配置keepalived启动参数" class="headerlink" title="4.配置keepalived启动参数"></a>4.配置keepalived启动参数</h1><pre><code class="hljs">vi /etc/sysconfig/keepalivedKEEPALIVED_OPTIONS=&quot;-D -d -S 0&quot;</code></pre><h1 id="5-创建启动脚本"><a href="#5-创建启动脚本" class="headerlink" title="5.创建启动脚本"></a>5.创建启动脚本</h1><pre><code class="hljs">vi /lib/systemd/system/keepalived.service[Unit]Description=LVS and VRRP High Availability MonitorAfter=network-online.target syslog.targetWants=network-online.target[Service]Type=forkingPIDFile=/run/keepalived.pidKillMode=control-groupEnvironmentFile=-/etc/sysconfig/keepalivedExecStart=/usr/local/keepalived/sbin/keepalived $KEEPALIVED_OPTIONSExecReload=/bin/kill -HUP $MAINPIDExecStop=/bin/kill -TERM $MAINPID[Install]WantedBy=multi-user.target</code></pre><h1 id="6-配置keepalived日志"><a href="#6-配置keepalived日志" class="headerlink" title="6.配置keepalived日志"></a>6.配置keepalived日志</h1><h2 id="6-1-配置系统日志服务rsyslog"><a href="#6-1-配置系统日志服务rsyslog" class="headerlink" title="6.1 配置系统日志服务rsyslog"></a>6.1 配置系统日志服务rsyslog</h2><pre><code class="hljs">vi /etc/rsyslog.conf# 启用UDP端口接收其他设备的日志消息          $ModLoad imudp$UDPServerRun 514# 设置local0级别设备的日志存储路径local0.*    var/log/keepalived.log</code></pre><h2 id="6-2-重启rsyslog"><a href="#6-2-重启rsyslog" class="headerlink" title="6.2 重启rsyslog"></a>6.2 重启rsyslog</h2><pre><code class="hljs">systemctl restart rsyslog.service</code></pre><h1 id="7-启动keepalived服务"><a href="#7-启动keepalived服务" class="headerlink" title="7.启动keepalived服务"></a>7.启动keepalived服务</h1><pre><code class="hljs">systemctl daemon-reloadsystemctl start keepalived.servicesystemctl enable keepalived.service</code></pre><h1 id="8-模拟集群故障，测试VIP漂移"><a href="#8-模拟集群故障，测试VIP漂移" class="headerlink" title="8.模拟集群故障，测试VIP漂移"></a>8.模拟集群故障，测试VIP漂移</h1><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.51cto.com/qiuyue/2364190">https://blog.51cto.com/qiuyue/2364190</a></li><li><a href="https://blog.csdn.net/fduffyyg/article/details/84195323">https://blog.csdn.net/fduffyyg/article/details/84195323</a></li><li><a href="https://blog.csdn.net/qq_22648091/article/details/108519773">https://blog.csdn.net/qq_22648091/article/details/108519773</a></li><li><a href="https://blog.csdn.net/weixin_43740223/article/details/109475495">https://blog.csdn.net/weixin_43740223/article/details/109475495</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Keepalived</tag>
      
      <tag>高可用</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Haproxy配置文件详解</title>
    <link href="/linux/HaproxyConfig/"/>
    <url>/linux/HaproxyConfig/</url>
    
    <content type="html"><![CDATA[<pre><code class="hljs"># 进程级全局配置，系统相关global  # 设置全局日志的设备级别  log    127.0.0.1 local2  # 设置haproxy运行路径  chroot    /usr/local/haproxy  # 设置pid文件路径  pidfile    /var/run/haproxy.pid  # 设置进程运行用户  user    sword  # 设置运行进程用户所属组  group    sword  # 设置haproxy服务为后台运行  daemon  # 设置haproxy进程数，默认为1，建议设为CPU核心数  nbproc    2  # 设置每个进程最大连接数，决定了前端frontend总的连接数  maxconn    1024  # 设置当前节点名称，用于HA集群中多个haproxy节点共享同一IP  node    haproxy001  # 设置统计数据接口      stats socket    /var/lib/haproxy/stats# 默认全局设置，可被frontend，backend，listen等组件引用defaults  # 设置全局日志配置的引用  log    global  # 设置实运行模式，tcp为4层，用于SSL、SSH、SMTP等应用；http为7层，默认模式；health，已废弃  mode    http  # 设置日志类别，记录http请求、session信息  option    httplog  # 设置每次请求完毕后自动关闭http通道  option    httpclose  # 设置获取客户端真实IP，即后端服务器从Http Header中获得客户端IP   option    forwardfor except 127.0.0.0/8                # 设置日志不记录空连接，可剔除健康检查日志，用于上游服务器存在负载均衡器场景  #option    dontlognull  # 设置后端服务器挂掉强制转向其他健康的服务器  option    redispatch  # 设置服务器负载高时自动结束当前队列处理较久的连接  option    abortonclose  # 设置连接重用策略，默认为never，表示禁用  http-reuse    safe  # 设置后端服务器最大重试次数  retries    3  # 设置客户端保持连接的超时时长，高并发场景建议设置为较小值以尽快释放  timeout client    10s  # 设置客户端发送完整请求的最大时长，由于haproxy机制是每次请求或响应全部发送完成再进行转发，为防止洪水攻击建议设为较小值  timeout http-request    2s  # 设置客户端保持长连接的时长，优先级&gt; timeout http-request &gt; timeout client，适用于后端为静态web或静态缓存服务器  # timeout http-keep-alive 10s  # 设置客户请求在等待队列的超时时长  timeout queue    10s  # 设置后端服务器连接超时时长  timeout connect    1s  # 设置后端服务器健康检测超时时长  timeout check    2s  # 设置后端服务器连接的保持时长，高并发场景建议设置为较小值  timeout server    3s  # 设置负载均衡算法，roundrobin，动态权重轮询，默认算法，权重运行时可调整，每个后端服务器最大连接数4128；static-rr，静态权重轮询，权重值运行时调整不生效，后端服务器连接数不限；leastconn，最少连接数，适用于长连接的会话，LDAP、SQL、FTP等，不适合HTTP协议；source，根据请求源IP进行hash运算，将同一IP请求送往同一服务器,实现会话保持功能；first，根据服务器在列表中的位置自上而下进行调度，即前面服务器的连接数达到上限或不可用，新请求才会分配给下一服务；uri，根据请求的URI进行hash运算，用于代理缓存以提高命中率，适用于HTTP后端缓存服务器；url_param，根据请求的URl参数进行hash运算，可将同一个用户ID的请求送往同一特定服务器；hdr(name)，根据HTTP请求头匹配HTTP请求；rdp-cookie(name)，根据据cookie(name)锁定并哈希每一次TCP请求  # balance            roundrobin                           # 监控、统计页面配置，用于监控后端服务器状态# 设置监控组名称，与frontend、backend共同组成完整代理，作用于TCP流量listen monitor  # 设置监控组工作模式  mode    http  # 设置监控组端口  bind    :18088  # 设置启用默认统计报告  stats    enable  # 设置隐藏版本号  stats    hide-version  # 设置统计页面自动刷新时间间隔   stats    refresh 10       　　　　  # 设置监控页面url，即监控访问网址  stats uri    /status  # 设置监控页面密码框提示文本  stats realm    Haproxy Manager  # 设置监控页面用户和密码    stats auth    admin:admin  # 设置监控组是否启用后端服务器管理功能，用于手工启用/禁用后端服务器  stats admin if    TRUE # 设置接收客户端请求的前端虚拟节点，即对外提供服务的接口frontend    http-server  # 设置前端节点模式  mode    http  # 设置前端节点监听端口  bind    0.0.0.0:8088  # acl请求策略配置，用于基于请求报文的首部、响应报文的内容或其它的环境状态信息制定转发决策，从而实现动静分离功能，区分大小写  # 配置静态资源组请求策略，即匹配以/static /images等开头的访问路径，-i表示忽略大小写    # acl url_static path_beg  -i /static /images /img /javascript /stylesheets  # 配置静态资源请求策略，即匹配以.jpg .gif等结尾的访问路径，-i即忽略大小写  # acl url_static path_end  -i .jpg .gif .png .css .js .html  # 配置主机类型静态资源策略，即匹配以.img .ftp等开头的主机访问信息，-i即忽略大小写  # acl host_static hdr_beg(host)  -i img. video. download. ftp. imags. videos.  # 配置php动态资源请求策略  # acl url_php path_end -i .php  # 配置java动态资源请求策略  # acl url_jsp path_end -i .jsp .do  # acl策略请求的匹配规则配置，即分配后端服务器的策略  # url_static或host_static策略匹配  # use_backend web-servers if url_static or host_static  # php请求策略匹配  use_backend php-servers if url_php  # 匹配java请求策略转发匹配  use_backend tomcat-servers if url_jsp  # 后端服务器组配置  # 设置默认后端服务器，即处理所有没有被规则匹配到的请求  default_backend    web-servers    # 设置后端服务器组web-servers  backend web-servers         # 设置负载均衡算法    balance    first    # 设置启用http-keep-alive事务类型，适用于静态后端服务器    option    http-keep-alive    # 设置后端服务器组的实例，check，健康检查；inter，健康检查时间间隔；maxconn，后端服务器最大连接数；rise，2次健康检查正常则判定次数；fall，健康检查失败判定次数；weight，权重；maxqueue，后端服务器请求最大等待队列数    server nginx-server 192.168.0.100:80 check port 80 inter 2000 rise 2 fall 3    server httpd-server 192.168.0.200:80 check port 80 inter 2000 rise 2 fall 3    server tomcat-backup 192.168.0.180:8080 check port 8080 inter 2000 rise 2 fall 3 backup  backend tomcat-servers    # 设置工作方式    mode    http    # 设置启用http-server-close，适用于动态后端服务器    option    http-server-close    # 设置健康检查访问路径    option    httpchk /index.html             　　　　     server tomcat-master 192.168.0.100:8080 check port 8080 inter 2000 rise 2 fall 3 weight 3 maxconn 300 maxqueue 10    server tomcat-slaver 192.168.0.200:8080 check port 8080 inter 2000 rise 2 fall 3 weight 1 maxconn 300 maxqueue 10    # 设置不参与负载的服务器    # server tomcat-servers 127.0.0.1:8080 weight 0    # 设置容灾服务器，即其他所有负载宕机之后再提供服务    # server tomcat-backup 192.168.0.180:8080 check port 8080 inter 2000 rise 2 fall 1 backup  backend php-servers    # 设置会话保持，不建议haproxy配置session共享，可于后端服务器配置    # source方式，将同一IP请求送往同一服务器，若服务不可用，则session消失，不适于小型网络和代理服务器    # balance    source    # server httpd-master 192.168.0.200:80 check port 80 inter 2000 rise 2 fall 3    # server httpd-slaver 192.168.0.100:80 check port 80 inter 2000 rise 2 fall 3    # server nginx-server 192.168.0.180:80 check port 80 inter 2000 rise 2 fall 3 backup    # cookie方式，将服务器端返回给客户端的cookie中插入后端服务器的cookie id，若客户端禁用则无法实现    # cookie SERVERID insert indirect nocache    # server httpd-master 192.168.0.200:80 check port 80 cookie httpd-server inter 2000 rise 2 fall 3    # server httpd-slaver 192.168.0.100:80 check port 80 cookie httpd-server inter 2000 rise 2 fall 3    # server nginx-server 192.168.0.180:80 check port 80 cookie nginx-server inter 2000 rise 2 fall 3 backup</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="http://www.cnblogs.com/moss_tan_jun/p/6616472.html">http://www.cnblogs.com/moss_tan_jun/p/6616472.html</a></li><li><a href="http://blog.51cto.com/jinyudong/1910320">http://blog.51cto.com/jinyudong/1910320</a></li><li><a href="https://blog.csdn.net/li123128/article/details/79510249">https://blog.csdn.net/li123128/article/details/79510249</a></li><li><a href="http://www.ttlsa.com/linux/haproxy-study-tutorial">http://www.ttlsa.com/linux/haproxy-study-tutorial</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>负载均衡</tag>
      
      <tag>Haproxy</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Haproxy搭建负载均衡集群</title>
    <link href="/linux/Haproxy/"/>
    <url>/linux/Haproxy/</url>
    
    <content type="html"><![CDATA[<p>HAProxy，是由C语言编写的提供高可用、负载均衡及基于TCP(第四层)和HTTP(第七层)应用的代理软件，适用于负载特大且需要会话保持或七层处理的web站点，支持虚拟主机、Session保持和后端服务器的url健康检测，其全透明代理的特性已初步具备硬件防火墙的典型特点，特有的连接拒绝功能更是可以抵御小型的DDoS攻击。HAProxy的单进程、事件驱动模型显著降低了上下文切换的开销及内存占用，不受内存、系统调度器及各种锁的限制，从而能支持非常大的并发连接数</p><p>HAProxy的弊端在于多核系统程序的扩展性较差，需要进行优化以使每个CPU时间片(Cycle)能做更多负载，也不支持web缓存功能</p><h1 id="连接模式"><a href="#连接模式" class="headerlink" title="连接模式"></a>连接模式</h1><ul><li>keepalive，默认模式，处理请求和响应连接保持打开，但在响应和新请求之间保持空闲</li><li>tunnel，只有第一个请求和响应被处理，其他都被转发，不建议使用</li><li>passive close，即被动关闭，与隧道模式完全相同，但在两个个方向上添加Connection: close以尝试在第一次交换之后立即关闭连接</li><li>server close，即服务器关闭，服务器连接在收到响应结束后即关闭，但客户端连接保持打开</li><li>forced close，即强制关闭，连接被主动关闭，响应结束</li></ul><hr><h1 id="1-安装依赖包"><a href="#1-安装依赖包" class="headerlink" title="1.安装依赖包"></a>1.安装依赖包</h1><pre><code class="hljs">apt install -y gcc make libssl-dev libsystemd-dev yum install -y gcc make openssl-devel systemd-devel </code></pre><h1 id="2-编译安装haproxy"><a href="#2-编译安装haproxy" class="headerlink" title="2.编译安装haproxy"></a>2.编译安装haproxy</h1><pre><code class="hljs">tar -xzvf haproxy-1.7.9.tar.gz &amp;&amp; cd haproxy-1.7.9make ARCH=x86_64 TARGET=linux-glibc USE_SYSTEMD=1 PREFIX=/usr/local/haproxymake install PREFIX=/usr/local/haproxy SBINDIR=/usr/local/haproxy/sbin</code></pre><h1 id="3-创建haproxy配置文件"><a href="#3-创建haproxy配置文件" class="headerlink" title="3.创建haproxy配置文件"></a>3.创建haproxy配置文件</h1><pre><code class="hljs">mkdir -p /etc/haproxy /var/lib/haproxyvi /etc/haproxy/haproxy.cfgglobal  log         127.0.0.1 local2  chroot      /var/lib/haproxy  pidfile     /var/run/haproxy.pid  maxconn     4096  user        sword  group       sword  daemon  stats socket /var/lib/haproxy/statsdefaults  mode             tcp  log              global  option           tcplog  option           dontlognull  option           redispatch  retries          3  timeout queue    1m  timeout connect  10s  timeout client   1m  timeout server   1m  timeout check    10s  maxconn          3000frontend  nginxbind    0.0.0.0:80mode               tcpmaxconn            2048default_backend    nginx-serversbackend nginx-servers  balance    roundrobin  server master01 172.16.100.100:80  check inter 10000 fall 2 rise 2 weight 1  server master02 172.16.100.120:80  check inter 10000 fall 2 rise 2 weight 1  server master03 172.16.100.160:80  check inter 10000 fall 2 rise 2 weight 1</code></pre><h1 id="4-配置启动脚本"><a href="#4-配置启动脚本" class="headerlink" title="4.配置启动脚本"></a>4.配置启动脚本</h1><pre><code class="hljs">cp contrib/systemd/haproxy.service.in /usr/lib/systemd/system/haproxy.servicesed -i &#39;s#@SBINDIR@#/usr/local/haproxy/sbin#g&#39; /usr/lib/systemd/system/haproxy.service</code></pre><h1 id="5-创建选项配置文件"><a href="#5-创建选项配置文件" class="headerlink" title="5.创建选项配置文件"></a>5.创建选项配置文件</h1><pre><code class="hljs">vi /etc/default/haproxyEXTRAOPTS=&quot;-S /run/haproxy-master.sock&quot;</code></pre><h1 id="6-配置haproxy日志"><a href="#6-配置haproxy日志" class="headerlink" title="6.配置haproxy日志"></a>6.配置haproxy日志</h1><h2 id="6-1-配置系统日志服务rsyslog"><a href="#6-1-配置系统日志服务rsyslog" class="headerlink" title="6.1 配置系统日志服务rsyslog"></a>6.1 配置系统日志服务rsyslog</h2><pre><code class="hljs">vi /etc/rsyslog.conf # 启用UDP端口接收其他设备的日志消息          $ModLoad imudp$UDPServerRun 514# 设置local2级别设备的日志存储路径local2.*    /var/log/haproxy.log</code></pre><h4 id="6-2-重启rsyslog"><a href="#6-2-重启rsyslog" class="headerlink" title="6.2 重启rsyslog"></a>6.2 重启rsyslog</h4><pre><code class="hljs">systemctl restart rsyslog.service</code></pre><h1 id="7-启动haproxy服务"><a href="#7-启动haproxy服务" class="headerlink" title="7.启动haproxy服务"></a>7.启动haproxy服务</h1><pre><code class="hljs">systemctl daemon-reloadsystemctl start haproxy.servicesystemctl enable haproxy.service</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.cmdschool.org/archives/9619">https://www.cmdschool.org/archives/9619</a></li><li><a href="http://www.cnblogs.com/xibei666/p/5877548.html">http://www.cnblogs.com/xibei666/p/5877548.html</a></li><li><a href="https://blog.csdn.net/tantexian/article/details/50056199">https://blog.csdn.net/tantexian/article/details/50056199</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>负载均衡</tag>
      
      <tag>Haproxy</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LVS命令详解</title>
    <link href="/linux/LVS/"/>
    <url>/linux/LVS/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>负载均衡</tag>
      
      <tag>LVS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LVS配置Nginx负载均衡集群</title>
    <link href="/linux/LVS-Nginx/"/>
    <url>/linux/LVS-Nginx/</url>
    
    <content type="html"><![CDATA[<p>LVS，即Linux Virtual Server，Linux虚拟服务器，是由章文嵩博士发起的实现负载均衡集群的开源项目，目前已是Linux内核标准的一部分。LVS架构从逻辑上可分为调度层、Server集群层和共享存储层，LVS工作在TCP&#x2F;IP的第四层，即网络层，适用于1000-2000万PV或并发请求1万以上的Nginx不堪负载的大型网站</p><h1 id="LVS架构"><a href="#LVS架构" class="headerlink" title="LVS架构"></a>LVS架构</h1><ul><li>ipvs，即ip virtual server，工作在内核空间的代码，用于调度的实现</li><li>ipvsadm，工作在用户空间，负责为ipvs内核框架编写规则、定义集群服务的组员及后端真实的服务器(Real Server)的组员</li></ul><h1 id="LVS工作流程"><a href="#LVS工作流程" class="headerlink" title="LVS工作流程"></a>LVS工作流程</h1><ul><li>1.客户端（Client）向负载均衡调度器（Director Server）发起请求，调度器将请求发往至内核空间</li><li>2.调度器的PREROUTING链接收到客户端请求，判断目标IP确定是本机IP，将数据包发往INPUT链</li><li>3.IPVS工作在INPUT链上，当客户端请求到达INPUT时，IPVS会将客户端请求和已经定义好的集群服务进行比对，若客户端请求的就是定义的集群服务，IPVS修改数据包的目标IP地址及端口，并将新数据包发往POSTROUTING链</li><li>4.POSTROUTING链接收数据包，将目标IP地址和已经定义好的后端服务器地址进行比对，通过选路后将数据包最终发送给后端的服务器</li></ul><h1 id="LVS相关术语"><a href="#LVS相关术语" class="headerlink" title="LVS相关术语"></a>LVS相关术语</h1><ul><li>DS，Director Server，调度服务器，即前端负载均衡器节点</li><li>RS，Real Server，即后端真实的工作服务器</li><li>VIP，虚拟IP，即供外部用户请求IP地址</li><li>DIP，Director Server IP，用于和内部主机通讯的IP地址</li><li>RIP，Real Server IP，即后端服务器IP地址</li><li>CIP，Client IP，即客户端IP地址</li></ul><h1 id="LVS工作模式"><a href="#LVS工作模式" class="headerlink" title="LVS工作模式"></a>LVS工作模式</h1><h2 id="1-NAT模式"><a href="#1-NAT模式" class="headerlink" title="1.NAT模式"></a>1.NAT模式</h2><p>即网络地址转换，支持端口映射，LVS通过转换请求报文和响应报文的目标IP实现负载均衡功能。由于其请求和响应的数据报文都需要通过DR进行IP转换，所以当集群规模达到一定程度时DR将成为整个集群的瓶颈</p><h2 id="2-DR模式"><a href="#2-DR模式" class="headerlink" title="2.DR模式"></a>2.DR模式</h2><p>即直接路由，通过为请求报文重新封装一个MAC首部进行转发以实现负载均衡功能。其只有请求报文经由LVS进行转发，而响应报文是由RS直接返回给客户端，所以LVS不会产生流量，只负责分发请求，故整个集群的吞吐量得以大大提高，应用最为广泛。但负载均衡器的网卡必须与物理网卡在一个物理段上，即不能跨地域物理网络调度，也不支持端口映射</p><h2 id="3-TUN模式"><a href="#3-TUN模式" class="headerlink" title="3.TUN模式"></a>3.TUN模式</h2><p>即隧道模式，通过调度算法对请求报文封装一个新IP报头，这个新报头指定了后端服务器的IP，从而实现了负载均衡的功能。此模式负载均衡器只负责将请求包分发给后端服务器，而响应报文直接由后端服务器发送给客户端，故其能处理的请求量极为巨大，单台负载均衡能为超过100台的物理服务器服务。且由于DIP、VIP、 RIP都为公网地址，就具备了跨地域物理网络的调度功能。但需要后端服务器支持IP Tunneling，即IP Encapsulation协议，且通过隧道进行信息传输，增加了部分负载，也不支持端口映射，适用于跨地域或跨机房的场景</p><h1 id="LVS调度算法"><a href="#LVS调度算法" class="headerlink" title="LVS调度算法"></a>LVS调度算法</h1><h1 id="1-RR"><a href="#1-RR" class="headerlink" title="1.RR"></a>1.RR</h1><p>轮询，调度器将外部请求按顺序轮流分配到集群中的真实服务器上，集群中的任意一台服务器都是均等的，不考虑服务器实际连接数和负载</p><h2 id="2-WRR"><a href="#2-WRR" class="headerlink" title="2.WRR"></a>2.WRR</h2><p>加权轮询，调度器根据真实服务器的不同处理能力来调度访问请求，以保证处理能力强的服务器处理更多的访问流量，自动问询负载情况，并动态地调整其权值</p><h2 id="3-DH"><a href="#3-DH" class="headerlink" title="3.DH"></a>3.DH</h2><p>目标地址hash，根据请求的目标IP地址，通过一个散列（Hash）函数将一个目标IP地址映射到某一台服务器，是一种静态映射算法</p><h2 id="4-SH"><a href="#4-SH" class="headerlink" title="4.SH"></a>4.SH</h2><p>源地址hash，根据请求的源IP地址，作为散列键（Hash Key）从静态分配的散列表找出对应的服务器，若该服务器是可用的且未超载，将请求发送到该服务器，否则返回空。实际应用中SH和DH结合使用在防火墙集群，可以保证整个系统的唯一出<br>入口</p><h2 id="5-LC"><a href="#5-LC" class="headerlink" title="5.LC"></a>5.LC</h2><p>最少连接，调度器动态地将网络请求调度到已建立的链接数最少的服务器上，适用于真实服务器具有相近的性能的集群环境</p><h1 id="6-WLC"><a href="#6-WLC" class="headerlink" title="6.WLC"></a>6.WLC</h1><p>加权最少连接，调度器可以自动问询真实服务器的负载情况，并动态地调整其权值，适用于真实服务器性能差异较大的情况，具有较高权值的服务器将承受较大比例的活动连接负载</p><h2 id="7-SED"><a href="#7-SED" class="headerlink" title="7.SED"></a>7.SED</h2><p>最少期望延迟，基于wlc算法，调度器根据权重和当前连接数进行运算决定负载机器</p><h2 id="8-NQ"><a href="#8-NQ" class="headerlink" title="8.NQ"></a>8.NQ</h2><p>从不排队，无需列队，如果有台realserver的连接数&#x3D;0就直接分配过去，不再进行运算</p><h2 id="9-LBLC"><a href="#9-LBLC" class="headerlink" title="9.LBLC"></a>9.LBLC</h2><p>基于本地的最少连接，适用于Cache集群系统。该算法根据请求的目标IP地址找出该目标IP地址最近使用的服务器，若该服务器可用且未超载，将请求发送到该服务器；若不存在或已超载，且有服务器处于一半的工作负载，则用最少链接的原则选出一个可用的服务器，将请求发送到该服务器</p><h2 id="10-LBLCR"><a href="#10-LBLCR" class="headerlink" title="10.LBLCR"></a>10.LBLCR</h2><p>带复制的基于本地的最少连接，适用于Cache集群系统。该算法根据请求的目标IP地址找出该目标IP地址对应的服务器组，按最小连接原则从服务器组中选出一台服务器，若未超载，将请求发送到该服务器；若服务器超载，则按最小连接原则从这个集群中选出一台服务器，将该服务器加入到服务器组中，将请求发送到该服务器。同时，当该服务器组有一段时间没有被修改， 将最忙的服务器从服务器组中删除，以降低复制的程度</p><h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><p>LVS可实现负载均衡，但不能进行健康检查，若某个rs出现故障，LVS仍然会进行请求转发，这样就会导致请求的无效。所以经常会配合keepalived工作，实现后端服务器的健康检查及LVS的高可用功能<br>实际的生产环境中，lvs通常会和nginx配合使用。虽然nginx单独也可实现负载均衡的功能，但请求和响应流量都会经过nginx，当后端的服务器规模庞大时，网络带宽就成为了整个集群的瓶颈。所以，lvs和nginx配合使用，既保障了lvs四层工作的高效性，又避免了nginx流量集中以及lvs映射出错的弊端，且nginx还可以处理静态资源，以及承载业务切换、分流、前置缓存的任务，这样就会大大减轻后端服务器的压力，从而提高整个集群的性能</p><hr><h1 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h1><ul><li>172.16.100.100 ds</li><li>172.16.100.150 rs1</li><li>172.16.100.200 rs2</li><li>172.16.100.120 vip</li></ul><hr><h1 id="1-DR模式"><a href="#1-DR模式" class="headerlink" title="1.DR模式"></a>1.DR模式</h1><h2 id="1-1-安装LVS"><a href="#1-1-安装LVS" class="headerlink" title="1.1 安装LVS"></a>1.1 安装LVS</h2><pre><code class="hljs">yum install -y ipvsadm</code></pre><h2 id="1-2-调度服务器配置LVS脚本"><a href="#1-2-调度服务器配置LVS脚本" class="headerlink" title="1.2 调度服务器配置LVS脚本"></a>1.2 调度服务器配置LVS脚本</h2><pre><code class="hljs">vi /usr/local/sbin/lvs_dr.sh#! /bin/bashipv=/sbin/ipvsadmvip=172.16.100.120rs1=172.16.100.150rs2=172.16.100.200# 配置子网卡ifconfig ens33:0 down# 配置虚拟IPifconfig ens33:0 $vip broadcast $vip netmask 255.255.255.255 up# 配置子网卡路由route add -host $vip dev ens33:0# 开启调度服务器路由转发功能echo 1 &gt; /proc/sys/net/ipv4/ip_forward# 清除当前所有的lvs虚拟服务$ipv -C# 添加tcp协议虚拟服务集群，指定负载均衡算法$ipv -A -t $vip:8080 -s wrr# 配置lvs虚拟服务的后端服务器组# g，设置工作模式为dr；w，设置rs权重$ipv -a -t $vip:80 -r $rs1:80 -g -w 3$ipv -a -t $vip:80 -r $rs2:80 -g -w 1</code></pre><h2 id="1-3-创建LVS规则"><a href="#1-3-创建LVS规则" class="headerlink" title="1.3 创建LVS规则"></a>1.3 创建LVS规则</h2><pre><code class="hljs">chmod +x /usr/local/sbin/lvs_dr.shsh /usr/local/sbin/lvs_dr.sh</code></pre><h2 id="1-4-创建后端服务器配置脚本"><a href="#1-4-创建后端服务器配置脚本" class="headerlink" title="1.4 创建后端服务器配置脚本"></a>1.4 创建后端服务器配置脚本</h2><pre><code class="hljs"> vi /usr/local/sbin/lvs_dr_rs.sh#! /bin/bashvip=172.16.100.120# RS绑定vip到回环网卡ifconfig lo:0 $vip broadcast $vip netmask 255.255.255.255 up# RS添加路由route add -host $vip lo:0# 配置ARP抑制，也可配置到/etc/sysctl.confecho &quot;1&quot; &gt;/proc/sys/net/ipv4/conf/lo/arp_ignoreecho &quot;2&quot; &gt;/proc/sys/net/ipv4/conf/lo/arp_announceecho &quot;1&quot; &gt;/proc/sys/net/ipv4/conf/all/arp_ignoreecho &quot;2&quot; &gt;/proc/sys/net/ipv4/conf/all/arp_announce</code></pre><h2 id="1-5-创建LVS规则"><a href="#1-5-创建LVS规则" class="headerlink" title="1.5 创建LVS规则"></a>1.5 创建LVS规则</h2><pre><code class="hljs">chmod +x /usr/local/sbin/lvs_dr_rs.shsh /usr/local/sbin/lvs_dr_rs.sh</code></pre><h2 id="1-6-查看调度规则，验证集群功能"><a href="#1-6-查看调度规则，验证集群功能" class="headerlink" title="1.6 查看调度规则，验证集群功能"></a>1.6 查看调度规则，验证集群功能</h2><pre><code class="hljs">ipvsadm -ln</code></pre><h1 id="2-NAT模式"><a href="#2-NAT模式" class="headerlink" title="2.NAT模式"></a>2.NAT模式</h1><h1 id="3-TUN模式-1"><a href="#3-TUN模式-1" class="headerlink" title="3.TUN模式"></a>3.TUN模式</h1><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.cnblogs.com/MacoLee/p/5856858.html">https://www.cnblogs.com/MacoLee/p/5856858.html</a></li><li><a href="https://blog.csdn.net/Ki8Qzvka6Gz4n450m/article/details/79119665">https://blog.csdn.net/Ki8Qzvka6Gz4n450m/article/details/79119665</a></li><li><a href="https://www.jianshu.com/p/8a61de3f8be9">https://www.jianshu.com/p/8a61de3f8be9</a></li><li><a href="https://blog.51cto.com/191226139/2089891">https://blog.51cto.com/191226139/2089891</a></li><li><a href="https://www.cnblogs.com/lixigang/p/5371815.html">https://www.cnblogs.com/lixigang/p/5371815.html</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>负载均衡</tag>
      
      <tag>Nginx</tag>
      
      <tag>LVS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Tomcat集群配置基于Redis的会话保持</title>
    <link href="/linux/TomcatRedis/"/>
    <url>/linux/TomcatRedis/</url>
    
    <content type="html"><![CDATA[<h1 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h1><ul><li>172.16.100.100  node01  nginx tomcat redis</li><li>172.16.100.120  node02  tomcat</li><li>172.16.100.200  node03  tomcat</li></ul><hr><h1 id="1-部署tomcat负载均衡集群"><a href="#1-部署tomcat负载均衡集群" class="headerlink" title="1.部署tomcat负载均衡集群"></a>1.部署tomcat负载均衡集群</h1><h1 id="2-部署redis"><a href="#2-部署redis" class="headerlink" title="2.部署redis"></a>2.部署redis</h1><pre><code class="hljs">vi /etc/redis/redis.conf# 设置绑定IPbind 0.0.0.0</code></pre><h1 id="3-tomcat节点添加redis-jar包"><a href="#3-tomcat节点添加redis-jar包" class="headerlink" title="3.tomcat节点添加redis jar包"></a>3.tomcat节点添加redis jar包</h1><pre><code class="hljs">cp *.jar /usr/local/tomcat/lib</code></pre><h1 id="4-tomcat配置文件添加redis节点"><a href="#4-tomcat配置文件添加redis节点" class="headerlink" title="4.tomcat配置文件添加redis节点"></a>4.tomcat配置文件添加redis节点</h1><pre><code class="hljs">vi /usr/local/tomcat/conf/context.xml&lt;Context&gt;  &lt;!-- redis node --&gt;  &lt;Valve className=&quot;com.orangefunction.tomcat.redissessions.RedisSessionHandlerValve&quot; /&gt;  &lt;Manager className=&quot;com.orangefunction.tomcat.redissessions.RedisSessionManager&quot;   host=&quot;172.16.100.100&quot;   port=&quot;6379&quot;  password=&quot;redis&quot;  database=&quot;2&quot;   maxInactiveInterval=&quot;60&quot;/&gt;&lt;/Context&gt;</code></pre><p>5.tomcat节点创建测试文件</p><pre><code class="hljs">vi /usr/local/tomcat/webapps/ROOT/test.jsp&lt;%@ page language=&quot;java&quot; import=&quot;java.util.*&quot; pageEncoding=&quot;UTF-8&quot;%&gt;  &lt;%  String path = request.getContextPath();  String basePath = request.getScheme()+&quot;://&quot;+request.getServerName()+&quot;:&quot;+request.getServerPort()+path+&quot;/&quot;;  %&gt;  &lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot;&gt;  &lt;html&gt;    &lt;head&gt;      &lt;base href=&quot;&lt;%=basePath%&gt;&quot;&gt;      &lt;title&gt;My JSP &#39;index.jsp&#39; starting page&lt;/title&gt;      &lt;meta http-equiv=&quot;pragma&quot; content=&quot;no-cache&quot;&gt;      &lt;meta http-equiv=&quot;cache-control&quot; content=&quot;no-cache&quot;&gt;      &lt;meta http-equiv=&quot;expires&quot; content=&quot;0&quot;&gt;          &lt;meta http-equiv=&quot;keywords&quot; content=&quot;keyword1,keyword2,keyword3&quot;&gt;      &lt;meta http-equiv=&quot;description&quot; content=&quot;This is my page&quot;&gt;      &lt;!--     &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;styles.css&quot;&gt;     --&gt;    &lt;/head&gt;    &lt;body&gt;      SessionID:&lt;%=session.getId()%&gt;      &lt;BR&gt;      SessionIP:&lt;%=request.getServerName()%&gt;      &lt;BR&gt;      SessionPort:&lt;%=request.getServerPort()%&gt;     &lt;%      out.println(&quot;tomcat-node-001&quot;);      %&gt;    &lt;/body&gt;  &lt;/html&gt;</code></pre><h1 id="6-启动redis、tomcat、nginx，验证集群会话保持功能"><a href="#6-启动redis、tomcat、nginx，验证集群会话保持功能" class="headerlink" title="6.启动redis、tomcat、nginx，验证集群会话保持功能"></a>6.启动redis、tomcat、nginx，验证集群会话保持功能</h1><pre><code class="hljs">http://172.16.100.100/test.jsp</code></pre><h1 id="7-停止tomcat节点，模拟集群故障"><a href="#7-停止tomcat节点，模拟集群故障" class="headerlink" title="7.停止tomcat节点，模拟集群故障"></a>7.停止tomcat节点，模拟集群故障</h1><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="http://blog.51cto.com/oceanszf/1752641">http://blog.51cto.com/oceanszf/1752641</a></li><li><a href="http://lyl-zsu.iteye.com/blog/2408292">http://lyl-zsu.iteye.com/blog/2408292</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>负载均衡</tag>
      
      <tag>Redis</tag>
      
      <tag>Java</tag>
      
      <tag>Tomcat</tag>
      
      <tag>会话保持</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Redis部署哨兵模式集群</title>
    <link href="/linux/RedisSentinel/"/>
    <url>/linux/RedisSentinel/</url>
    
    <content type="html"><![CDATA[<h1 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h1><ul><li>172.16.100.100  master</li><li>172.16.100.120  node01</li><li>172.16.100.200  node02</li></ul><hr><p>Redis主从复制集群解决了单点故障，但故障转移还是需要人工干预，手动进行主从切换，并不能第一时间恢复业务。有鉴于此，Redis引入了哨兵模式，即为集群的每个节点部署一个独立的监控服务，以监控主从复制的状态，并在主节点故障后通过投票机制自动进行主从切换</p><h1 id="1-安装redis"><a href="#1-安装redis" class="headerlink" title="1.安装redis"></a>1.安装redis</h1><h1 id="2-配置主节点"><a href="#2-配置主节点" class="headerlink" title="2.配置主节点"></a>2.配置主节点</h1><h2 id="2-1-修改redis配置文件"><a href="#2-1-修改redis配置文件" class="headerlink" title="2.1 修改redis配置文件"></a>2.1 修改redis配置文件</h2><pre><code class="hljs">vi /etc/redis/redis.conf# 设置从节点连接主节点的认证密码masterauth &quot;redis&quot;# 设置从节点权重，主节点失效后权重值最小者将被选为新主节点，为0则表示不参与选举slave-priority 100</code></pre><h2 id="2-2-配置sentinel节点"><a href="#2-2-配置sentinel节点" class="headerlink" title="2.2 配置sentinel节点"></a>2.2 配置sentinel节点</h2><pre><code class="hljs">vi /etc/redis/sentinel.conf# 设置监听IPbind 172.16.100.100# 设置监听端口port 6380# 设置sentinel服务为后台运行daemonize yes# 设置sentinel服务将数据保存到硬盘上的文件名dbfilename dump_sentinel.rdb# 设置sentinel服务硬盘数据文件存储路径dir /usr/local/redis/data# 设置sentinel服务pid文件路径pidfile  /usr/local/redis/data/sentinel.pid# 设置sentinel服务日志路径logfile &quot;/usr/local/redis/logs/sentinel.log&quot;# 设置sentinel所监听的主节点IP及主从切换策略，1表示判断主节点失效至少需要1个Sentinel节点同意sentinel monitor master 172.16.100.100 6379 1# 设置主节点连接密码sentinel auth-pass master redis# 设置监听超时时长，即以ping命令判断Redis数据节点和其余Sentinel节点是否可达，如果超过30000毫秒且没有回复，则判定不可达sentinel down-after-milliseconds master 30000# 当Sentinel节点集合对主节点故障判定达成一致时，Sentinel领导者节点做故障转移操作，选出新主节点，原来的从节点会向新的主节点发起复制操作，限制每次向新的主节点发起复制操作的从节点个数为1sentinel parallel-syncs master 1# 故障转移超时时间为180000毫秒sentinel failover-timeout master 180000</code></pre><h2 id="2-3-启动主节点sentinel"><a href="#2-3-启动主节点sentinel" class="headerlink" title="2.3 启动主节点sentinel"></a>2.3 启动主节点sentinel</h2><pre><code class="hljs">/usr/local/redis/bin/redis-sentinel /etc/redis/sentinel.conf</code></pre><h1 id="3-配置从节点"><a href="#3-配置从节点" class="headerlink" title="3.配置从节点"></a>3.配置从节点</h1><h2 id="3-1-修改redis配置文件"><a href="#3-1-修改redis配置文件" class="headerlink" title="3.1 修改redis配置文件"></a>3.1 修改redis配置文件</h2><pre><code class="hljs">vi /etc/redis/redis.conf# 配置主服务器slaveof 192.168.0.180 6379# 配置主服务器访问密码masterauth &quot;redis&quot;</code></pre><h2 id="3-2-配置sentinel节点"><a href="#3-2-配置sentinel节点" class="headerlink" title="3.2 配置sentinel节点"></a>3.2 配置sentinel节点</h2><pre><code class="hljs">vi /etc/redis/sentinel.confbind 172.16.100.120port 6381daemonize yesdbfilename dump_sentinel.rdbdir /usr/local/redis/datapidfile  /usr/local/redis/data/sentinel.pidlogfile &quot;/usr/local/redis/logs/sentinel.log&quot;sentinel monitor slaver001 192.168.0.100 6379 1sentinel auth-pass slaver001 redissentinel down-after-milliseconds slaver001 30000sentinel parallel-syncs slaver001 1sentinel failover-timeout slaver001 180000</code></pre><h1 id="4-依次启动主从节点redis、sentinel"><a href="#4-依次启动主从节点redis、sentinel" class="headerlink" title="4.依次启动主从节点redis、sentinel"></a>4.依次启动主从节点redis、sentinel</h1><pre><code class="hljs"># 启动redis节点service redis start# 启动sentinel节点/usr/local/redis/bin/redis-sentinel /usr/local/redis/conf/sentinel.conf</code></pre><h1 id="5-查看主服务器的主从信息"><a href="#5-查看主服务器的主从信息" class="headerlink" title="5.查看主服务器的主从信息"></a>5.查看主服务器的主从信息</h1><pre><code class="hljs">redis-cli -a &quot;redis&quot; -h 172.16.100.180 -p 6379 info replication# 连接任意sentinel服务查看当前主redis服务信息redis-cli -a &quot;redis&quot; -h 192.168.0.150 -p 6380 info sentinelredis-cli -a &quot;redis&quot; -h 192.168.0.100 -p 6381 info sentinelredis-cli -a &quot;redis&quot; -h 192.168.0.200 -p 6382 info sentinel</code></pre><h1 id="6-测试集群故障切换"><a href="#6-测试集群故障切换" class="headerlink" title="6.测试集群故障切换"></a>6.测试集群故障切换</h1><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.51cto.com/article/651799.html">https://www.51cto.com/article/651799.html</a></li><li><a href="https://blog.csdn.net/benxiaohai529/article/details/52848414">https://blog.csdn.net/benxiaohai529/article/details/52848414</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Redis</tag>
      
      <tag>中间件</tag>
      
      <tag>NoSQL</tag>
      
      <tag>缓存</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Redis部署主从复制集群</title>
    <link href="/linux/RedisReplication/"/>
    <url>/linux/RedisReplication/</url>
    
    <content type="html"><![CDATA[<p>Redis主从复制是指将一台Redis服务器（主节点）的数据复制到其他的Redis服务器（从节点），该过程是单向进行，即只能从主节点复制到从节点。主节点宕机后即可将业务切换到从节点，尽可能的消弱影响</p><hr><h1 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h1><ul><li>172.16.100.100  master</li><li>172.16.100.200  slaver</li></ul><hr><h1 id="1-部署redis"><a href="#1-部署redis" class="headerlink" title="1.部署redis"></a>1.部署redis</h1><h1 id="2-配置主节点"><a href="#2-配置主节点" class="headerlink" title="2.配置主节点"></a>2.配置主节点</h1><pre><code class="hljs">sudo vi /etc/redis/redis.conf# 设置监听IPbind 0.0.0.0</code></pre><h1 id="3-配置从节点"><a href="#3-配置从节点" class="headerlink" title="3.配置从节点"></a>3.配置从节点</h1><pre><code class="hljs">sudo vi /etc/redis/redis.conf# 设置主节点IP及端口replicaof 172.16.100.100 6379# 设置主节点访问密码masterauth &quot;Redis@2020&quot;# 设置权重值，为0则表示不参与master选举# replica-priority 100</code></pre><h1 id="3-启动主、从redis节点，验证主从功能"><a href="#3-启动主、从redis节点，验证主从功能" class="headerlink" title="3.启动主、从redis节点，验证主从功能"></a>3.启动主、从redis节点，验证主从功能</h1><pre><code class="hljs">redis-cli -a &quot;Redis@2020&quot; -h 172.16.100.200 -p 6379 info replication</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/shianla/article/details/137613665">https://blog.csdn.net/shianla/article/details/137613665</a></li><li><a href="https://blog.csdn.net/qq_36838700/article/details/140637913">https://blog.csdn.net/qq_36838700/article/details/140637913</a></li><li><a href="https://blog.csdn.net/weixin_43412762/article/details/134946683">https://blog.csdn.net/weixin_43412762/article/details/134946683</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Redis</tag>
      
      <tag>中间件</tag>
      
      <tag>NoSQL</tag>
      
      <tag>缓存</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Redis编译安装</title>
    <link href="/linux/Redis/"/>
    <url>/linux/Redis/</url>
    
    <content type="html"><![CDATA[<p>Redis，Remote Dictionary Server，即远程字典服务，由ANSI C语言编写的基于内存的高性能键值对NoSQL数据库，可用作数据库、缓存服务器或消息服务器，支持多种数据结构，如字符串、哈希表、链表、集合、有序集合、位图、Hyperloglogs等，所以又被称为数据结构服务器。Redis特别适用于涉及大数据量的场景，如商品抢购或瞬时访量过高的网站，以缓解成千上万的瞬时请求引发大量磁盘读写操作而导致的数据库压力</p><p>Redis以其超高的性能、完美的文档、简洁易懂的源码和丰富的客户端库支持在开源中间件领域广受好评，国内外很多大型互联网公司都在使用，如Twitter、Github、StackOverflow、腾讯、阿里、京东、华为、新浪微博、暴雪娱乐等等</p><h1 id="1-体系架构"><a href="#1-体系架构" class="headerlink" title="1.体系架构"></a>1.体系架构</h1><p>Redis系统架构分为事件处理模块、数据存储及管理模块、集群管理模块及System扩展模块</p><h2 id="1-1-事件处理模块"><a href="#1-1-事件处理模块" class="headerlink" title="1.1 事件处理模块"></a>1.1 事件处理模块</h2><p>Redis基于事件驱动设计，即是将客户端的连接、读、写和关闭操作转换为各种事件由自研的ae事件驱动模型高效地处理。Redis事件处理模块由文件事件处理器和时间事件处理器两部分组成</p><h3 id="1-1-1-文件事件处理器"><a href="#1-1-1-文件事件处理器" class="headerlink" title="1.1.1 文件事件处理器"></a>1.1.1 文件事件处理器</h3><p>Redis的核心部件，用于处理核心任务，如网络IO读写、命令执行等，原理是基于IO多路复用程序监听多个套接字（socket），并根据套接字执行的任务为其关联不同的事件处理器。Redis单线程模型实质上指的就是因为文件事件处理器被设计为单线程运行，即不需要额外创建监听客户端连接的线程而实现了高并发高性能的网络通信，保持了单线程设计的简洁性，降低了资源消耗。这就是Redis单线程高性能的关键</p><p>文件事件处理器由四部分组成，即套接字、I&#x2F;O多路复用处理程序、文件事件分派器和事件处理器</p><ul><li>多个socket，用于客户端的连接</li><li>IO多路复用程序，用于将多种不同事件类型的客户端sockert置入队列，通过这个队列有序、同步地传送给文件事件分派器</li><li>文件事件分派器，用于将socket关联到相应的事件处理器</li><li>事件处理器，用于执行具体的事件，如连接应答处理器、命令请求处理器、命令回复处理器</li></ul><h3 id="1-1-2-时间事件"><a href="#1-1-2-时间事件" class="headerlink" title="1.1.2 时间事件"></a>1.1.2 时间事件</h3><p>时间事件处理器比较简单，主要由serverCron函数执行做一些统计更新、过期key清理、AOF及RDB持久化等辅助操作</p><h2 id="1-2-数据存储模块"><a href="#1-2-数据存储模块" class="headerlink" title="1.2 数据存储模块"></a>1.2 数据存储模块</h2><p>Redis内存数据存储于redisDB，数据及其相关的辅助信息都以key&#x2F;value格式存储在各个数据库的字典中。此外，数据的写指令还会及时追加到AOF，追加的方式是先实时写入AOF缓冲，再按策略刷缓冲数据到文件。由于AOF记录每个写操作，所以一个key的大量中间状态也会呈现于AOF，从而导致AOF冗余信息过多。因此Redis设计了RDB快照操作，可以通过定期将内存里所有的数据快照落盘到RDB文件，依此记录Redis的所有内存数据。也即是数据持久化</p><h2 id="1-3-集群管理模块"><a href="#1-3-集群管理模块" class="headerlink" title="1.3 集群管理模块"></a>1.3 集群管理模块</h2><p>Redis虽然以高性能著称，但单机模式总还是会达到性能瓶颈，因此主从复制的集群扩展能力也是必不可少。此外，主从复制虽然可以较好的解决单机读写问题，但所有的写操作都集中在Master服务器还是很容易达到写上限，同时主从节点都保存了业务的所有数据，随着业务发展也很容易出现内存不足问题。为此，Redis分区无从避免。虽然业界大多采用在client和proxy端分区，但Redis本身也早早地推出了集群功能，并不断进行优化。Redis cluster预先设定了16384个slot槽，集群启动时通过手动或自动将这些slot分配到不同服务节点上，进行key读写定位时首先对key做hash，并将hash值对16383做按位与运算，确认slot，从而确认服务节点，再对对应的Redis节点进行常规读写。若client发送到错误的Redis分片，则会发送重定向回复。若业务数据大量增加，Redis 集群可以通过数据迁移，来进行在线扩容</p><h2 id="1-4-系统扩展模块"><a href="#1-4-系统扩展模块" class="headerlink" title="1.4 系统扩展模块"></a>1.4 系统扩展模块</h2><p>Redis在4.0版本引入了Module System模块，可以方便地在不修改核心功能的同时进行插件化功能开发，如将新feature封装成动态链接库以便于启动时加载，也可在运行过程中随时按需加载和启用，即以可插拔的方式引入新的数据结构和访问命令</p><h1 id="2-启动流程"><a href="#2-启动流程" class="headerlink" title="2.启动流程"></a>2.启动流程</h1><h2 id="2-1-加载配置"><a href="#2-1-加载配置" class="headerlink" title="2.1 加载配置"></a>2.1 加载配置</h2><p>加载配置文件，接收命令行中传入的参数，替换服务端设置的默认值，如端口号、密码、持久化设置等</p><h2 id="2-2-初始化"><a href="#2-2-初始化" class="headerlink" title="2.2 初始化"></a>2.2 初始化</h2><p>根据配置信息初始化数据结构，如客户端连接、共享对象、事件处理、持久化模块等</p><h2 id="2-3-加载持久化数据"><a href="#2-3-加载持久化数据" class="headerlink" title="2.3 加载持久化数据"></a>2.3 加载持久化数据</h2><p>加载持久化存储于磁盘的RDB或AOF数据文件</p><h2 id="2-4-处理事件"><a href="#2-4-处理事件" class="headerlink" title="2.4 处理事件"></a>2.4 处理事件</h2><p>启动事件监听服务，等待客户端连接请求，接收到连接请求将其放入事件队列，并通过事件处理器进行处理</p><h1 id="3-性能优势"><a href="#3-性能优势" class="headerlink" title="3.性能优势"></a>3.性能优势</h1><ul><li>1.Redis基于内存实现数据存储，没有磁盘IO的损耗，性能极高</li><li>2.Redis内置的数据结构非常高效，大部分操作的时间复杂度为O(1)</li><li>3.Redis的网络IO及数据读写这些核心操作由一个线程完成，也即单线程模型，避免了CPU上下文切换及竞争锁的消耗。Redis 6.0引入了多线程模型，即核心的读写操作仍然由主线程执行，其他功能由额外线程执行，如持久化、异步删除和集群数据同步等</li></ul><h1 id="4-应用场景"><a href="#4-应用场景" class="headerlink" title="4.应用场景"></a>4.应用场景</h1><p>大型电商网站、视频直播和游戏应用等存在大规模数据访问，对数据查询效率要求很高，且数据结构简单，不涉及太多关联查询。此时Redis速度上对传统磁盘数据库就有很大优势，可有效减少数据库磁盘IO，提高数据查询效率，减轻管理维护工作量，降低数据库存储成本。Redis对传统磁盘数据库是一个重要的补充，成为了互联网应用，尤其是支持高并发访问的互联网应用必不可少的基础服务之一</p><ul><li><p>电商网站秒杀抢购，即电商网站的商品类目、推荐系统以及秒杀抢购活动，适宜使用Redis缓存数据库。如秒杀抢购活动，并发高，对于传统关系型数据库来说访问压力大，需要较高的硬件配置（如磁盘IO）支撑。Redis数据库，单节点QPS支撑能达到10万，轻松应对秒杀并发。实现秒杀和数据加锁的命令简单，使用SET、GET、DEL、RPUSH等命令即可</p></li><li><p>视频直播消息弹幕，即直播间的在线用户列表，礼物排行榜，弹幕消息等信息，都适合使用Redis中的SortedSet结构进行存储。如弹幕消息，可使用ZREVRANGEBYSCORE排序返回</p></li><li><p>游戏排行榜，即在线游戏一般涉及排行榜实时展现，如列出当前得分最高的10个用户。使用Redis的有序集合存储用户排行榜非常合适，有序集合使用非常简单，提供多达20个操作集合的命令</p></li><li><p>社交类应用的最新评论&#x2F;回复，即web类应用的“最新评论”之类的查询，如果使用关系型数据库，往往涉及到按评论时间逆排序，随着评论越来越多，排序效率越来越低，且并发频繁。使用Redis的List（链表），例如存储最新1000条评论，当请求的评论数在这个范围，就不需要访问磁盘数据库，直接从缓存中返回，减少数据库压力的同时，提升APP的响应速度</p></li></ul><hr><h1 id="1-安装依赖包"><a href="#1-安装依赖包" class="headerlink" title="1.安装依赖包"></a>1.安装依赖包</h1><pre><code class="hljs">yum install -y gcc gcc-c++ makeapt install -y gcc make pkg-config</code></pre><h1 id="2-编译安装redis"><a href="#2-编译安装redis" class="headerlink" title="2.编译安装redis"></a>2.编译安装redis</h1><pre><code class="hljs">tar -xzvf redis-4.0.8.tar.gz &amp;&amp; cd redis-4.0.8make &amp;&amp; make PREFIX=/usr/local/redis install</code></pre><h1 id="3-创建redis配置文件"><a href="#3-创建redis配置文件" class="headerlink" title="3.创建redis配置文件"></a>3.创建redis配置文件</h1><pre><code class="hljs">mkdir -p /etc/redis /usr/local/redis/data /usr/local/redis/logscp redis.conf /etc/redisvi /etc/redis/redis.conf# 设置IPbind 127.0.0.1# 设置服务为后台启动daemonize yes# 设置进程文件路径pidfile /usr/local/redis/redis-server.pid# 设置持久化到硬盘数据的文件名dbfilename dump.rdb# 设置硬盘数据存储路径，防止redis意外重启数据丢失，启动时搜索该文件，将其数据导入，若无法找到则重新建立，为空表示数据已丢失dir /usr/local/redis/data# 设置最大连接数maxclients 1000# 设置最大内存占用量，若超限则将根据淘汰策略决定新写入请求或移除现有数据maxmemory 1024M# 设置访问密码requirepass Redis@2020# 设置日志存储路径logfile &quot;/usr/local/redis/logs/redis.log&quot;</code></pre><h1 id="5-配置内核参数"><a href="#5-配置内核参数" class="headerlink" title="5.配置内核参数"></a>5.配置内核参数</h1><pre><code class="hljs">echo &quot;vm.overcommit_memory = 1&quot; &gt;&gt; /etc/sysctl.confsysctl vm.overcommit_memory=1echo 511 &gt; /proc/sys/net/core/somaxconn</code></pre><h1 id="6-创建redis启动脚本"><a href="#6-创建redis启动脚本" class="headerlink" title="6.创建redis启动脚本"></a>6.创建redis启动脚本</h1><pre><code class="hljs">vi /lib/systemd/system/redis.service[Unit]Description=RedisAfter=network.target[Service]Type=forkingExecStart=/usr/local/redis/bin/redis-server /etc/redis/redis.confExecReload=/usr/local/redis/bin/redis-cli config rewriteExecStop=/usr/local/redis/bin/redis-cli shutdownPrivateTmp=true[Install]WantedBy=multi-user.target</code></pre><h1 id="7-启动redis"><a href="#7-启动redis" class="headerlink" title="7.启动redis"></a>7.启动redis</h1><pre><code class="hljs">systemctl daemon-reloadsystemctl start redis.servicesystemctl enable redis.service</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="http://blog.51cto.com/oceanszf/1752641">http://blog.51cto.com/oceanszf/1752641</a></li><li><a href="http://lyl-zsu.iteye.com/blog/2408292">http://lyl-zsu.iteye.com/blog/2408292</a></li><li><a href="https://blog.csdn.net/junbaozi/article/details/130632437">https://blog.csdn.net/junbaozi/article/details/130632437</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Redis</tag>
      
      <tag>中间件</tag>
      
      <tag>NoSQL</tag>
      
      <tag>缓存</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Memcached编译安装</title>
    <link href="/linux/Memcached/"/>
    <url>/linux/Memcached/</url>
    
    <content type="html"><![CDATA[<h1 id="1-安装依赖包"><a href="#1-安装依赖包" class="headerlink" title="1.安装依赖包"></a>1.安装依赖包</h1><pre><code class="hljs">yum -y install gcc gcc-c++ make cmake automake autoconf pcre pcre-devel zlib zlib-devel \openssl openssl-devel libjpeg-devel libpng-devel libxml2-devel bzip2 bzip2-devel libcurl-devel</code></pre><h1 id="2-创建memcached用户"><a href="#2-创建memcached用户" class="headerlink" title="2.创建memcached用户"></a>2.创建memcached用户</h1><pre><code class="hljs">groupadd memcached &amp;&amp; useradd -g memcached -s /sbin/nologin memcached -Mmkdir -p /usr/local/memcached/logschown -R memcached.memcached /usr/local/memcached</code></pre><h1 id="3-编译安装libevent"><a href="#3-编译安装libevent" class="headerlink" title="3.编译安装libevent"></a>3.编译安装libevent</h1><pre><code class="hljs">tar -xzvf libevent-2.1.8-stable.tar.gzcd libevent-2.1.8-stable./configure --prefix=/usr/local/libevent -with-libevent=/usr/local/libeventmake &amp;&amp; make install</code></pre><h1 id="4-编译安装memcached"><a href="#4-编译安装memcached" class="headerlink" title="4.编译安装memcached"></a>4.编译安装memcached</h1><pre><code class="hljs">tar -xzvf memcached-1.5.6.tar.gzcd memcached-1.5.6./configure --prefix=/usr/local/memcached --enable-threads --with-libevent=/usr/local/libeventmake &amp;&amp; make install</code></pre><h1 id="5-启动memcached"><a href="#5-启动memcached" class="headerlink" title="5.启动memcached"></a>5.启动memcached</h1><pre><code class="hljs">/usr/local/memcached/bin/memcached -d -u memcached -l 192.168.0.150 -p 11211 -P /usr/local/memcached/memcached.pid</code></pre><hr><h2 id="memcached启动参数"><a href="#memcached启动参数" class="headerlink" title="memcached启动参数"></a>memcached启动参数</h2><ul><li>-d，启动一个守护进程</li><li>-m，设置分配给memcached使用的内存量，单位是MB，设为10MB</li><li>-u，设置memcached运行用户，设为memcached</li><li>-l，设置监听的服务器IP地址，设为192.168.0.150</li><li>-p，设置memcached监听的端口，设为11211，最好是1024以上的端口</li><li>-c，设置最大运行的并发连接数，默认是1024，设为256，按照服务器的负载量设定</li><li>-P，设置memcached的pid文件存储路径，设为&#x2F;usr&#x2F;local&#x2F;memcached&#x2F;memcached.pid</li><li>-v，设置memcached普通的错误或者警告类型的日志信息 </li><li>-vv，设置memcached详细日志信息，包含客户端命令和server端的响应信息 </li><li>-vvv，设置memcached详细日志信息，包含内部的状态信息</li><li>&#x2F;usr&#x2F;local&#x2F;memcached&#x2F;bin&#x2F;memcached -d -vv -m 10 -u memcached -l 192.168.0.150 -p 11211 -c 256 -P &#x2F;usr&#x2F;local&#x2F;memcached&#x2F;logs&#x2F;memcached.pid &gt;&gt; &#x2F;usr&#x2F;local&#x2F;memcached&#x2F;logs&#x2F;memcached.log 2&gt;&amp;1</li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Memcached</tag>
      
      <tag>NoSQL</tag>
      
      <tag>缓存</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Tomcat集群配置基于Memcached的会话保持</title>
    <link href="/linux/TomcatMemcached/"/>
    <url>/linux/TomcatMemcached/</url>
    
    <content type="html"><![CDATA[<h1 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h1><ul><li>192.168.0.180 tomcat nginx </li><li>192.168.0.100 tomcat memcached</li><li>192.168.0.200 tomcat</li></ul><hr><h1 id="1-安装nginx、tomcat、memcached"><a href="#1-安装nginx、tomcat、memcached" class="headerlink" title="1.安装nginx、tomcat、memcached"></a>1.安装nginx、tomcat、memcached</h1><h1 id="2-tomcat节点添加memcached-jar包"><a href="#2-tomcat节点添加memcached-jar包" class="headerlink" title="2.tomcat节点添加memcached jar包"></a>2.tomcat节点添加memcached jar包</h1><pre><code class="hljs">cp *.jar /usr/local/tomcat/libmemcached-session-manager-2.1.1.jarspymemcached-2.11.1.jartomcat-memcached-session-manage-tomcat8.jar</code></pre><h1 id="3-tomcat配置文件添加memcached节点"><a href="#3-tomcat配置文件添加memcached节点" class="headerlink" title="3.tomcat配置文件添加memcached节点"></a>3.tomcat配置文件添加memcached节点</h1><pre><code class="hljs">vi /usr/local/tomcat/conf/context.xml&lt;Context&gt;  # memcached node  &lt;Manager className=&quot;de.javakaffee.web.msm.MemcachedBackupSessionManager  &quot;memcachedNodes=&quot;n:192.168.0.150:11211&quot;  lockingMode=&quot;auto&quot;  sticky=&quot;false&quot;  requestUriIgnorePattern= &quot;.*\.(png|gif|jpg|css|js|html)$&quot;  sessionBackupAsync= &quot;false&quot;  sessionBackupTimeout= &quot;100&quot;  transcoderFactoryClass=&quot;de.javakaffee.web.msm.JavaSerializationTranscoderFactory&quot;/&gt;&lt;/Context&gt;</code></pre><h1 id="4-nginx反向代理负载均衡的配置"><a href="#4-nginx反向代理负载均衡的配置" class="headerlink" title="4.nginx反向代理负载均衡的配置"></a>4.nginx反向代理负载均衡的配置</h1><h1 id="5-tomcat节点配置测试文件"><a href="#5-tomcat节点配置测试文件" class="headerlink" title="5.tomcat节点配置测试文件"></a>5.tomcat节点配置测试文件</h1><pre><code class="hljs">vi /usr/local/tomcat/webapps/ROOT/test.jsp&lt;%@ page language=&quot;java&quot; import=&quot;java.util.*&quot; pageEncoding=&quot;UTF-8&quot;%&gt;  &lt;%  String path = request.getContextPath();  String basePath = request.getScheme()+&quot;://&quot;+request.getServerName()+&quot;:&quot;+request.getServerPort()+path+&quot;/&quot;;  %&gt;  &lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot;&gt;  &lt;html&gt;  </code></pre><p>      <head>  <br>        <base href="<%=basePath%>">  </p><p>        <title>My JSP ‘index.jsp’ starting page</title>  <br>        <meta http-equiv="pragma" content="no-cache">  <br>        <meta http-equiv="cache-control" content="no-cache">  <br>        <meta http-equiv="expires" content="0">      <br>        <meta http-equiv="keywords" content="keyword1,keyword2,keyword3">  <br>        <meta http-equiv="description" content="This is my page">  <br>        <!--         <link rel="stylesheet" type="text/css" href="styles.css">         -->  <br>      </head>  </p><p>      <body>  </p><p>        SessionID:&lt;%&#x3D;session.getId()%&gt;  <br>        <BR>  <br>        SessionIP:&lt;%&#x3D;request.getServerName()%&gt;  <br>        <BR>  <br>        SessionPort:&lt;%&#x3D;request.getServerPort()%&gt; <br>        &lt;%  <br>        out.println(“tomcat-node-001”);  <br>        %&gt;  <br>      </body>  <br>    </html></p><h1 id="6-启动memcached、tomcat、nginx，验证集群功能"><a href="#6-启动memcached、tomcat、nginx，验证集群功能" class="headerlink" title="6.启动memcached、tomcat、nginx，验证集群功能"></a>6.启动memcached、tomcat、nginx，验证集群功能</h1><p><a href="http://192.168.0.180/test.jsp">http://192.168.0.180/test.jsp</a></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.cnblogs.com/jsonhc/p/7344902.html">https://www.cnblogs.com/jsonhc/p/7344902.html</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>负载均衡</tag>
      
      <tag>Java</tag>
      
      <tag>Tomcat</tag>
      
      <tag>Memcached</tag>
      
      <tag>会话保持</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Tomcat集群配置基于组播方式的会话保持</title>
    <link href="/linux/TomcatSession/"/>
    <url>/linux/TomcatSession/</url>
    
    <content type="html"><![CDATA[<h1 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h1><ul><li>172.16.100.100  node01  nginx tomcat</li><li>172.16.100.120  node02  tomcat</li><li>172.16.100.200  node03  tomcat</li></ul><hr><h1 id="1-部署nginx、tomcat，配置负载均衡集群"><a href="#1-部署nginx、tomcat，配置负载均衡集群" class="headerlink" title="1.部署nginx、tomcat，配置负载均衡集群"></a>1.部署nginx、tomcat，配置负载均衡集群</h1><h1 id="2-修改tomcat配置文件，启用session复制和Cluster功能"><a href="#2-修改tomcat配置文件，启用session复制和Cluster功能" class="headerlink" title="2.修改tomcat配置文件，启用session复制和Cluster功能"></a>2.修改tomcat配置文件，启用session复制和Cluster功能</h1><pre><code class="hljs">vi  /usr/local/tomcat/conf/server.xml&lt;Cluster className=&quot;org.apache.catalina.ha.tcp.SimpleTcpCluster&quot;  channelSendOptions=&quot;8&quot;&gt;&lt;Manager className=&quot;org.apache.catalina.ha.session.DeltaManager&quot;               expireSessionsOnShutdown=&quot;false&quot;               notifyListenersOnReplication=&quot;true&quot;/&gt;&lt;Channel className=&quot;org.apache.catalina.tribes.group.GroupChannel&quot;&gt;    &lt;Membership className=&quot;org.apache.catalina.tribes.membership.McastService&quot;        # 设置集群组播地址        address=&quot;228.0.0.4&quot;        port=&quot;45564&quot;        frequency=&quot;500&quot;        dropTime=&quot;3000&quot;/&gt;    &lt;Receiver className=&quot;org.apache.catalina.tribes.transport.nio.NioReceiver&quot;        # 设置本机IP        address=&quot;192.168.0.120&quot;        port=&quot;4000&quot;        autoBind=&quot;100&quot;        selectorTimeout=&quot;5000&quot;        maxThreads=&quot;6&quot;/&gt;    &lt;Sender className=&quot;org.apache.catalina.tribes.transport.ReplicationTransmitter&quot;&gt;    &lt;Transport className=&quot;org.apache.catalina.tribes.transport.nio.PooledParallelSender&quot;/&gt;    &lt;/Sender&gt;    &lt;Interceptor className=&quot;org.apache.catalina.tribes.group.interceptors.TcpFailureDetector&quot;/&gt;    &lt;Interceptor className=&quot;org.apache.catalina.tribes.group.interceptors.MessageDispatch15Interceptor&quot;/&gt;&lt;/Channel&gt;&lt;Valve className=&quot;org.apache.catalina.ha.tcp.ReplicationValve&quot; filter=&quot;&quot;/&gt;&lt;Valve className=&quot;org.apache.catalina.ha.session.JvmRouteBinderValve&quot;/&gt;# &lt;Deployer className=&quot;org.apache.catalina.ha.deploy.FarmWarDeployer&quot;    # tempDir=&quot;/tmp/war-temp/&quot;    # deployDir=&quot;/tmp/war-deploy/&quot;    # watchDir=&quot;/tmp/war-listen/&quot;    # watchEnabled=&quot;false&quot;/&gt;&lt;ClusterListener className=&quot;org.apache.catalina.ha.session.ClusterSessionListener&quot;/&gt;&lt;/Cluster&gt;# 设置路由信息，集群唯一&lt;Engine name=&quot;Catalina&quot; defaultHost=&quot;localhost&quot; jvmRoute=&quot;tomcat1&quot;&gt;</code></pre><h1 id="3-配置项目web-xml文件"><a href="#3-配置项目web-xml文件" class="headerlink" title="3.配置项目web.xml文件"></a>3.配置项目web.xml文件</h1><pre><code class="hljs">vi  /usr/local/tomcat/webapps/ROOT/WEB-INF/web.xml&lt;web-app &lt;display-name&gt;loadbalancer&lt;/display-name&gt;&lt;distributable/&gt;&lt;/web-app&gt;</code></pre><h1 id="4-创建测试页面"><a href="#4-创建测试页面" class="headerlink" title="4.创建测试页面"></a>4.创建测试页面</h1><pre><code class="hljs">vi /usr/local/tomcat/webapps/ROOT/test.jsp&lt;%@ page language=&quot;java&quot; import=&quot;java.util.*&quot; pageEncoding=&quot;UTF-8&quot;%&gt;  &lt;%  String path = request.getContextPath();  String basePath = request.getScheme()+&quot;://&quot;+request.getServerName()+&quot;:&quot;+request.getServerPort()+path+&quot;/&quot;;  %&gt;  &lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot;&gt;  &lt;html&gt;    &lt;head&gt;      &lt;base href=&quot;&lt;%=basePath%&gt;&quot;&gt;      &lt;title&gt;My JSP &#39;index.jsp&#39; starting page&lt;/title&gt;      &lt;meta http-equiv=&quot;pragma&quot; content=&quot;no-cache&quot;&gt;      &lt;meta http-equiv=&quot;cache-control&quot; content=&quot;no-cache&quot;&gt;      &lt;meta http-equiv=&quot;expires&quot; content=&quot;0&quot;&gt;          &lt;meta http-equiv=&quot;keywords&quot; content=&quot;keyword1,keyword2,keyword3&quot;&gt;      &lt;meta http-equiv=&quot;description&quot; content=&quot;This is my page&quot;&gt;      &lt;!--     &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;styles.css&quot;&gt;     --&gt;    &lt;/head&gt;    &lt;body&gt;      SessionID:&lt;%=session.getId()%&gt;      &lt;BR&gt;      SessionIP:&lt;%=request.getServerName()%&gt;      &lt;BR&gt;      SessionPort:&lt;%=request.getServerPort()%&gt;      &lt;%      out.println(&quot;000000&quot;);      %&gt;    &lt;/body&gt;  &lt;/html&gt;</code></pre><h1 id="5-启动tomcat、nginx，验证集群会话保持功能"><a href="#5-启动tomcat、nginx，验证集群会话保持功能" class="headerlink" title="5.启动tomcat、nginx，验证集群会话保持功能"></a>5.启动tomcat、nginx，验证集群会话保持功能</h1><h1 id="6-集群组播相关命令"><a href="#6-集群组播相关命令" class="headerlink" title="6.集群组播相关命令"></a>6.集群组播相关命令</h1><pre><code class="hljs">route add -host 228.0.0.4 dev eth0route add -net 224.0.0.0 netmask 240.0.0.0 dev eth0</code></pre>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>负载均衡</tag>
      
      <tag>Java</tag>
      
      <tag>Tomcat</tag>
      
      <tag>会话保持</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Tomcat的安装与配置</title>
    <link href="/linux/Tomcat/"/>
    <url>/linux/Tomcat/</url>
    
    <content type="html"><![CDATA[<p>Tomcat，用于运行JSP (Java Server Pages，Java服务器网页)和Servlet的基于Java的开源轻量级Web应用服务器。Tomcat是装载Java Web程序的Web容器，主要功能是按照J2EE中的Servlet规范编写好的Java程序解析为动态网页，并处理客户端的请求与响应，从而运行Java网站。Tomcat最初由Sun公司软件架构师詹姆斯·邓肯·戴维森开发，开源后由Sun公司贡献给Apache软件基金会，技术先进，性能稳定、配置简单，特别适用于中小型系统和并发数不是很多的场景，是开发和调试JSP程序的首选</p><hr><h1 id="体系架构"><a href="#体系架构" class="headerlink" title="体系架构"></a>体系架构</h1><p>tomcat由两个核心组件构成，即Connector、Container，两者组合为对外提供WEB服务的Service</p><h2 id="Connector"><a href="#Connector" class="headerlink" title="Connector"></a>Connector</h2><p>即连接器，负责对外接收和响应请求，是Tomcat服务器与外界的交通枢纽，监听端口接收外界请求，并将请求处理后传递给容器做业务处理，最后将容器处理后的结果响应给外部</p><h2 id="Container"><a href="#Container" class="headerlink" title="Container"></a>Container</h2><p>即容器，负责对内处理业务逻辑，内部由Engine、Host、Context和Wrapper四个容器组成，用于管理和调用Servlet相关逻辑</p><p>Container内部包含父子关系的4个子容器，即容器由一个引擎管理多个虚拟主机，每个虚拟主机可以管理多个Web应用，每个Web应用又会有多个Servlet封装器</p><ul><li>Engine，引擎，用于管理多个虚拟主机，一个Service最多只能有一个 Engine</li><li>Host，虚拟主机，又称为站点，通过配置Host可以添加站点</li><li>Context，代表一个Web应用，包含多个 Servlet 封装器</li><li>Wrapper，封装器，容器的最底层，每一Wrapper封装着一个Servlet，负责对象实例的创建、执行和销毁功能</li></ul><h2 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h2><p>即对外提供的Web服务，由Connector和Container两个核心组件及其他功能组件构成，一个Tomcat实例可以管理多个Service，且各个Service之间相互独立</p><h1 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h1><ul><li>connector接收请求，转发给servlet容器里的container，由container里的Engine进行响应</li><li>Engine收到请求，分发给不同的站点，即host，由其调用Context确定相对应的路径，最后到达servlet容器，servlet会通过catalina将其中的Java代码翻译成能识别的servlet代码，最后执行的结果会返回给context</li><li>执行结果重新经过Context，Host，Engine一层一层的返还回去，到达Container，然后再到Connector，最后返回给客户端</li></ul><hr><h1 id="1-配置Java环境"><a href="#1-配置Java环境" class="headerlink" title="1.配置Java环境"></a>1.配置Java环境</h1><h2 id="1-1-安装jdk"><a href="#1-1-安装jdk" class="headerlink" title="1.1 安装jdk"></a>1.1 安装jdk</h2><pre><code class="hljs"># yum install -y java-1.8.0-openjdk java-1.8.0-openjdk-devel# apt install -y tar -zxvf jdk-8u131-linux-i586.tar.gz</code></pre><h2 id="1-2-配置环境变量"><a href="#1-2-配置环境变量" class="headerlink" title="1.2 配置环境变量"></a>1.2 配置环境变量</h2><pre><code class="hljs">vi /etc/profile JAVA_HOME=/usr/local/jdkJAVA_BIN=/usr/local/jdk/binJRE_HOME=/usr/local/jdk/jre PATH=$PATH:/usr/local/jdk/bin:/usr/local/jdk/jre/binCLASSPATH=/usr/local/jdk/jre/lib:/usr/local/jdk/lib:/usr/local/jdk/jre/lib/charsets.jar</code></pre><h2 id="1-3-生效环境变量"><a href="#1-3-生效环境变量" class="headerlink" title="1.3 生效环境变量"></a>1.3 生效环境变量</h2><pre><code class="hljs">  source /etc/profile</code></pre><h2 id="1-4-验证java环境"><a href="#1-4-验证java环境" class="headerlink" title="1.4 验证java环境"></a>1.4 验证java环境</h2><pre><code class="hljs">java --version</code></pre><h1 id="2-安装Tomcat"><a href="#2-安装Tomcat" class="headerlink" title="2.安装Tomcat"></a>2.安装Tomcat</h1><h2 id="2-1-下载Tomcat安装包"><a href="#2-1-下载Tomcat安装包" class="headerlink" title="2.1 下载Tomcat安装包"></a>2.1 下载Tomcat安装包</h2><h2 id="2-2-安装tomcat"><a href="#2-2-安装tomcat" class="headerlink" title="2.2 安装tomcat"></a>2.2 安装tomcat</h2><pre><code class="hljs">tar -zxvf apache-tomcat-8.0.1.tar.gzmv apache-tomcat-8.0.1 /usr/local/tomcat</code></pre><h1 id="3-启动tomcat服务器"><a href="#3-启动tomcat服务器" class="headerlink" title="3.启动tomcat服务器"></a>3.启动tomcat服务器</h1><pre><code class="hljs">/usr/local/tomcat/bin/startup.sh</code></pre><h1 id="4-访问tomcat"><a href="#4-访问tomcat" class="headerlink" title="4.访问tomcat"></a>4.访问tomcat</h1><p><a href="http://ip:8080/">http://ip:8080</a></p>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Java</tag>
      
      <tag>Tomcat</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Git安装与配置</title>
    <link href="/linux/Git/"/>
    <url>/linux/Git/</url>
    
    <content type="html"><![CDATA[<p>Git，由林纳斯于2005年以C语言开发的开源分布式版本控制系统，起初是管理Linux内核源代码，目前可用于敏捷高效地托管各种项目的源代码，简单快速且强大</p><h1 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h1><h2 id="工作区"><a href="#工作区" class="headerlink" title="工作区"></a>工作区</h2><p>工作区，即本地开发环境，管理着本地代码目录，通过更改提交(commit)到本地仓库，之后再推送(push)到远程仓库</p><h2 id="暂存区"><a href="#暂存区" class="headerlink" title="暂存区"></a>暂存区</h2><p>暂存区，即stage或index，一般存储于.git目录下的index文件，也被称之为索引（index），是位于仓库和工作区之间的中间区域</p><h2 id="仓库"><a href="#仓库" class="headerlink" title="仓库"></a>仓库</h2><p>仓库，repository，又称版本库，一般位于工作区下的隐藏目录.git，其中的所有文件都由Git管理，每个文件的修改、删除都被Git跟踪，以便于任何时刻都可以追踪历史或还原</p><h2 id="远程仓库"><a href="#远程仓库" class="headerlink" title="远程仓库"></a>远程仓库</h2><p>远程仓库，Remote Repository，存储在服务器上的Git仓库，用于多人协作和备份。本地的个人开发者可以克隆远程仓库到本地，拉取（pull）远程后本地更改，最后再推送（push）本地更改到远程仓库。远程仓库分为公共仓库和私有仓库两种，如github属于公共仓库，也可用gitlab、gitea搭建私有仓库</p><h1 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h1><ul><li>1.克隆Git资源作为工作目录</li><li>2.在克隆的资源上添加或修改文件，或更新资源其他人修改后的资源</li><li>3.查看修改内容，确认后提交修改</li><li>4.修改完成后，如果发现错误或冲突，可撤回提交并再次修改并提交</li></ul><h1 id="1-安装Git"><a href="#1-安装Git" class="headerlink" title="1.安装Git"></a>1.安装Git</h1><pre><code class="hljs">sudo yum install -y gitsudo apt install -y git</code></pre><h1 id="2-创建Git仓库"><a href="#2-创建Git仓库" class="headerlink" title="2.创建Git仓库"></a>2.创建Git仓库</h1><pre><code class="hljs">mkdir -p /web/gits &amp;&amp; cd /web/gits# 初始化仓库git init</code></pre><h1 id="3-仓库新增文件"><a href="#3-仓库新增文件" class="headerlink" title="3.仓库新增文件"></a>3.仓库新增文件</h1><pre><code class="hljs">echo &quot;Git is simple.&quot; &gt; Readme.md# 将文件添加到仓库git add Readme.md# 表示将目录下所有新增或修改一并添加到仓库# git add .# 将文件提交到仓库git commit</code></pre><ul><li>注：git commit命令，-m参数之后用于输入本次提交的说明，可输入任意内容，最好是有意义的，以便于从历史记录方便地找到改动记录，省略掉-m参数则会提示自由输入改动记录</li></ul><h1 id="4-查看仓库状态"><a href="#4-查看仓库状态" class="headerlink" title="4.查看仓库状态"></a>4.查看仓库状态</h1><pre><code class="hljs"># 将会显示有变更的文件git status</code></pre><h1 id="5-查看文件改动"><a href="#5-查看文件改动" class="headerlink" title="5.查看文件改动"></a>5.查看文件改动</h1><pre><code class="hljs"># 比较文件的不同，即比较文件在暂存区和工作区的差异，将会显示文件具体修改的内容git diff</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://zhuanlan.zhihu.com/p/30044692">https://zhuanlan.zhihu.com/p/30044692</a></li><li><a href="https://www.runoob.com/git/git-tutorial.html">https://www.runoob.com/git/git-tutorial.html</a></li><li><a href="https://mp.weixin.qq.com/s/T3mP2YGzI7WSUr3C9S8ltw">https://mp.weixin.qq.com/s/T3mP2YGzI7WSUr3C9S8ltw</a></li><li><a href="https://blog.csdn.net/mr_zhd/article/details/136320758">https://blog.csdn.net/mr_zhd/article/details/136320758</a></li><li><a href="https://blog.csdn.net/m0_56145255/article/details/127600983">https://blog.csdn.net/m0_56145255/article/details/127600983</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Git</tag>
      
      <tag>CICD</tag>
      
      <tag>DevOps</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Bind搭建DNS服务器</title>
    <link href="/linux/Bind/"/>
    <url>/linux/Bind/</url>
    
    <content type="html"><![CDATA[<p>bind，安全高效的开源域名解析服务程序，发源于1980年代加州大学伯克利分校的由美国国防高级研究项目管理局 (DARPA)资助的研究生项目，故取名为Berkeley Internet Name Domain，以其稳定性、可靠性和灵活性广泛应用于各类操作系统与网络环境，是当今互联网最为流行的域名解析方案</p><h1 id="DNS"><a href="#DNS" class="headerlink" title="DNS"></a>DNS</h1><p>DNS，Domain Name System，即域名系统，是互联网基础设施类的服务，以层次结构的命名系统将域名和IP地址相互映射，从而形成一个分布式数据库系统，最终完成域名与IP地址的相互转换。DNS将网站的域名解析为IP地址，这样通过简单易记的域名即可访问网站，而不必使用复杂的IP地址，通过互联网进行信息检索更加便捷</p><h1 id="1-安装bind"><a href="#1-安装bind" class="headerlink" title="1.安装bind"></a>1.安装bind</h1><pre><code class="hljs"># sudo yum -y install bind bind-utils# sudo mkdir -p /var/named/logsudo useradd named -s /sbin/nologin -Msudo mkdir /var/named &amp;&amp; sudo chown -R named.named /var/namedsudo yum install -y gcc gcc-c++ openssl openssl-devtar -xzvf bind-9.18.26.tar.xzsudo ./configure --prefix=/usr/local/bind --sysconfdir=/etc/bind --enable-threads --enable-largefile --disable-ipv6sudo mkae &amp;&amp; sudo make install  </code></pre><h1 id="2-修改主配置文件"><a href="#2-修改主配置文件" class="headerlink" title="2.修改主配置文件"></a>2.修改主配置文件</h1><pre><code class="hljs">vi /etc/named.confoptions &#123;  # 设置DNS服务器监听端口与IP  listen-on port 53 &#123; 192.168.0.150; &#125;;   # 设置DNS服务器IPv6监听IP  listen-on-v6 port 53 &#123; ::1; &#125;;  # 设置DNS服务器全局配置目录  directory &quot;/var/named&quot;;  # 设置DNS服务器缓存数据文件  dump-file &quot;/var/named/data/cache_dump.db&quot;;  # 设置DNS服务器统计数据文件  statistics-file &quot;/var/named/data/named_stats.txt&quot;;  # 设置DNS服务器内存使用的统计数据文件  memstatistics-file &quot;/var/named/data/named_mem_stats.txt&quot;;  # 设置允许查询的IP地址  allow-query &#123; 192.168.0.0/24; &#125;;  # DNS服务器日志配置  logging &#123;    channel default_debug &#123;    file &quot;/var/named/log/named.log&quot;;    severity dynamic;    &#125;;  &#125;;  # 根区域解析文件配置  zone &quot;.&quot; IN &#123;    type hint;    file &quot;named.ca&quot;;  &#125;;  # 设置区域文件  include &quot;/etc/named.rfc1912.zones&quot;;  include &quot;/etc/named.root.key&quot;;&#125;</code></pre><h1 id="3-配置区域文件"><a href="#3-配置区域文件" class="headerlink" title="3.配置区域文件"></a>3.配置区域文件</h1><pre><code class="hljs">sudo vi /etc/named.rfc1912.zones# 正向解析区域配置# 设置正向解析库名称zone &quot;sxs0618.com&quot; IN &#123;  # 设置为主dns解析库  type master;  # 设置正向解析库文件名  file &quot;sxs0618.com.zone&quot;;  # 设置自动更新解析文件的客户端  allow-update &#123; none; &#125;;&#125;;# 配置反向解析区域  zone &quot;0.168.192.in-addr.arpa&quot; IN &#123;  type master;  file &quot;sxs0618.com.local&quot;;  allow-update &#123; none; &#125;;&#125;;</code></pre><h1 id="4-创建正向区域解析文件"><a href="#4-创建正向区域解析文件" class="headerlink" title="4.创建正向区域解析文件"></a>4.创建正向区域解析文件</h1><pre><code class="hljs">sudo vi /var/named/sxs0618.com.zone$TTL    86400$ORIGIN sxs0618.com.@       IN  SOA     dns.sxs0618.com. admin.sxs0618.com. (  20190518; serial  1H      ; refresh  5M      ; retry  1W      ; expire  3H )    ; minimum         IN         NS          dns  dns      IN         A           192.168.0.150           IN         MX          10  mail. sxs0618.com.  www      IN         A           192.168.0.150  mail     IN         A           192.168.0.120  bbs      IN         A           192.168.0.160  ftp      IN         A           192.168.0.180</code></pre><h1 id="5-创建反向区域解析文件"><a href="#5-创建反向区域解析文件" class="headerlink" title="5.创建反向区域解析文件"></a>5.创建反向区域解析文件</h1><pre><code class="hljs">sudo vi /var/named/sxs0618.com.local$TTL    604800$ORIGIN 0.168.192.in-addr.arpa.@       IN SOA  dns.sxs0618.com. admin.sxs0618.com. (                                    20190518; serial                                    1H      ; refresh                                    5M      ; retry                                    1W      ; expire                                    3H )    ; minimum    IN        NS            dns.sxs0618.com.150     IN        PTR           www.sxs0618.com.120     IN        PTR           mail.sxs0618.com.160     IN        PTR           bbs.sxs0618.com.180     IN        PTR           ftp.sxs0618.com.</code></pre><h1 id="6-启动DNS服务"><a href="#6-启动DNS服务" class="headerlink" title="6.启动DNS服务"></a>6.启动DNS服务</h1><pre><code class="hljs">sudo systemctl start named.servicesudo systemctl enable named.service</code></pre><h1 id="7-配置主机DNS服务器"><a href="#7-配置主机DNS服务器" class="headerlink" title="7.配置主机DNS服务器"></a>7.配置主机DNS服务器</h1><pre><code class="hljs">sudo vi /etc/resolv.conf nameserver 192.168.0.150#nameserver 8.8.8.8</code></pre><h1 id="8-验证DNS服务器解析"><a href="#8-验证DNS服务器解析" class="headerlink" title="8.验证DNS服务器解析"></a>8.验证DNS服务器解析</h1><pre><code class="hljs">nslookup www.sxs0618.comnslookup 192.168.0.150</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="http://www.cnblogs.com/st-jun/p/8022137.html">http://www.cnblogs.com/st-jun/p/8022137.html</a></li><li><a href="https://blog.51cto.com/5165807/2313377?source=dra">https://blog.51cto.com/5165807/2313377?source=dra</a></li><li><a href="https://www.cnblogs.com/heqiuyu/articles/6600326.html">https://www.cnblogs.com/heqiuyu/articles/6600326.html</a></li><li><a href="https://blog.csdn.net/feng271374203/article/details/89920817">https://blog.csdn.net/feng271374203/article/details/89920817</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Bind</tag>
      
      <tag>DNS</tag>
      
      <tag>域名解析</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Zabbix配置邮件告警</title>
    <link href="/linux/ZabbixEmailNotification/"/>
    <url>/linux/ZabbixEmailNotification/</url>
    
    <content type="html"><![CDATA[<h1 id="1-zabbix服务器安装邮件发送程序mailx"><a href="#1-zabbix服务器安装邮件发送程序mailx" class="headerlink" title="1.zabbix服务器安装邮件发送程序mailx"></a>1.zabbix服务器安装邮件发送程序mailx</h1><pre><code class="hljs">sudo yum install -y mailx dos2unix</code></pre><h1 id="2-修改mailx配置文件"><a href="#2-修改mailx配置文件" class="headerlink" title="2.修改mailx配置文件"></a>2.修改mailx配置文件</h1><pre><code class="hljs">sudo vi /etc/mail.rc# for zabbixset from=sxs0618@139.comsmtp=smtp.139.comset smtp-auth-user=sxs0618@139.comsmtp-auth-password=123456set smtp-auth=login</code></pre><h1 id="3-创建邮箱发送脚本文件"><a href="#3-创建邮箱发送脚本文件" class="headerlink" title="3.创建邮箱发送脚本文件"></a>3.创建邮箱发送脚本文件</h1><pre><code class="hljs">sudo vi /usr/local/zabbix/share/zabbix/alertscripts/sendmail.sh#!/bin/bashmessages=`echo $3 | tr &#39;\r\n&#39; &#39;\n&#39;`subject=`echo $2 | tr &#39;\r\n&#39; &#39;\n&#39;`echo &quot;$&#123;messages&#125;&quot; | mail -s &quot;$&#123;subject&#125;&quot; $1 &gt;&gt;/tmp/sendmail.log 2&gt;&amp;1sudo chown zabbix.zabbix /usr/local/zabbix/share/zabbix/alertscripts/sendmail.shsudo chmod +x /usr/local/zabbix/share/zabbix/alertscripts/sendmail.sh</code></pre><h1 id="4-zabbix前端页面创建媒体类型"><a href="#4-zabbix前端页面创建媒体类型" class="headerlink" title="4.zabbix前端页面创建媒体类型"></a>4.zabbix前端页面创建媒体类型</h1><h2 id="管理-gt-报警媒介类型-gt-创建媒体类型"><a href="#管理-gt-报警媒介类型-gt-创建媒体类型" class="headerlink" title="管理 -&gt; 报警媒介类型 -&gt; 创建媒体类型"></a>管理 -&gt; 报警媒介类型 -&gt; 创建媒体类型</h2><ul><li><p>名称：sendmail</p></li><li><p>类型：脚本</p></li><li><p>脚本参数：</p></li></ul><p>{ALERT.SENDTO}<br>{ALERT.SUBJECT}<br>{ALERT.MESSAGE}</p><h1 id="5-添加邮箱收件人"><a href="#5-添加邮箱收件人" class="headerlink" title="5.添加邮箱收件人"></a>5.添加邮箱收件人</h1><h2 id="Admin-gt-报警媒介-gt-添加"><a href="#Admin-gt-报警媒介-gt-添加" class="headerlink" title="Admin -&gt; 报警媒介 -&gt; 添加"></a>Admin -&gt; 报警媒介 -&gt; 添加</h2><ul><li>类型：sendmail</li><li>收件人：<a href="mailto:&#x73;&#120;&#115;&#51;&#48;&#x31;&#51;&#x40;&#111;&#117;&#116;&#x6c;&#111;&#x6f;&#x6b;&#x2e;&#x63;&#x6f;&#x6d;">&#x73;&#120;&#115;&#51;&#48;&#x31;&#51;&#x40;&#111;&#117;&#116;&#x6c;&#111;&#x6f;&#x6b;&#x2e;&#x63;&#x6f;&#x6d;</a></li></ul><h1 id="6-添加邮件发送动作"><a href="#6-添加邮件发送动作" class="headerlink" title="6.添加邮件发送动作"></a>6.添加邮件发送动作</h1><h2 id="配置-gt-动作-gt-创建动作-gt-操作"><a href="#配置-gt-动作-gt-创建动作-gt-操作" class="headerlink" title="配置 -&gt; 动作 -&gt; 创建动作 -&gt; 操作"></a>配置 -&gt; 动作 -&gt; 创建动作 -&gt; 操作</h2><pre><code class="hljs">默认接收人：&#123;TRIGGER.SEVERITY&#125;:&#123;TRIGGER.NAME&#125;默认信息：告警IP:&#123;HOST.CONN&#125;告警主机:&#123;HOSTNAME1&#125;告警时间:&#123;EVENT.DATE&#125; &#123;EVENT.TIME&#125;告警等级:&#123;TRIGGER.SEVERITY&#125;告警信息: &#123;TRIGGER.NAME&#125;告警项目:&#123;TRIGGER.KEY1&#125;问题详情:&#123;ITEM.NAME&#125;:&#123;ITEM.VALUE&#125;当前状态:&#123;TRIGGER.STATUS&#125;:&#123;ITEM.VALUE1&#125;事件ID:&#123;EVENT.ID&#125;</code></pre><h2 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h2><ul><li>步骤1-1</li><li>发送到用户群组添加-&gt; Zabbix administrators</li><li>发送到用户     添加-&gt; Admin</li><li>仅送到    sendmail</li></ul><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="http://www.cnblogs.com/rysinal/p/5834421.html">http://www.cnblogs.com/rysinal/p/5834421.html</a></li><li><a href="http://blog.csdn.net/slovyz/article/details/53100780">http://blog.csdn.net/slovyz/article/details/53100780</a></li><li><a href="http://www.tuicool.com/articles/7jqm2aE">http://www.tuicool.com/articles/7jqm2aE</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>监控告警</tag>
      
      <tag>Zabbix</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Zabbix监控Linux系统SSH远程登录</title>
    <link href="/linux/Zabbix-SSH/"/>
    <url>/linux/Zabbix-SSH/</url>
    
    <content type="html"><![CDATA[<h1 id="1-Linux模板创建系统登录监控项"><a href="#1-Linux模板创建系统登录监控项" class="headerlink" title="1.Linux模板创建系统登录监控项"></a>1.Linux模板创建系统登录监控项</h1><ul><li>名称：Login System</li><li>类型：Zabbix客户端（主动式）</li><li>键值：log[&#x2F;var&#x2F;log&#x2F;secure,”(Accepted|Failed) password”,,,skip,]</li><li>信息类型：日志</li><li>应用集：Security</li></ul><h1 id="2-创建系统登录触发器"><a href="#2-创建系统登录触发器" class="headerlink" title="2.创建系统登录触发器"></a>2.创建系统登录触发器</h1><h2 id="2-1-创建系统登录成功触发器"><a href="#2-1-创建系统登录成功触发器" class="headerlink" title="2.1 创建系统登录成功触发器"></a>2.1 创建系统登录成功触发器</h2><ul><li>名称：Login system has succeed</li><li>表达式：{Template OS Linux:log[&#x2F;var&#x2F;log&#x2F;secure,”(Accepted|Failed) password”,,,skip,].str(Accepted)}&#x3D;1<br>and {Template OS Linux:log[&#x2F;var&#x2F;log&#x2F;secure,”(Accepted|Failed) password”,,,skip,].nodata(60)}&#x3D;0</li><li>严重性：信息</li></ul><p>and之前，表示如果字符串中包含”Accepted”则表达式为真；and之后，表示60秒内有数据产生则表达式为真，即60秒内如果没有新数据了，则表达式为假。and之前匹配登录成功的信息，and之后保证触发器的恢复。即前者保证触发器的触发，后者保证触发器的恢复</p><h2 id="2-2-创建系统登录失败触发器"><a href="#2-2-创建系统登录失败触发器" class="headerlink" title="2.2 创建系统登录失败触发器"></a>2.2 创建系统登录失败触发器</h2><ul><li>名称：Login system has failed</li><li>表达式：{Template OS Linux:log[&#x2F;var&#x2F;log&#x2F;secure,”(Accepted|Failed) password”,,,skip,].str(Failed)}&#x3D;1<br>and {Template OS Linux:log[&#x2F;var&#x2F;log&#x2F;secure,”(Accepted|Failed) password”,,,skip,].nodata(60)}&#x3D;0</li><li>严重性：一般严重</li></ul><p>and之前，表示如果字符串中包含”Failed”则表达式为真；and之后，表示60秒内有数据产生则表达式为真，即60秒内如果没有新数据了，则表达式为假。and之前匹配登录失败的信息，and之后保证触发器的恢复。前者保证触发器的触发，后者保证触发器的恢复</p><h1 id="3-配置Linux系统登录日志文件权限"><a href="#3-配置Linux系统登录日志文件权限" class="headerlink" title="3.配置Linux系统登录日志文件权限"></a>3.配置Linux系统登录日志文件权限</h1><pre><code class="hljs">sudo chmod 755 /var/log/secure</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/weixin_58400622/article/details/127792543">https://blog.csdn.net/weixin_58400622/article/details/127792543</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>SSH</tag>
      
      <tag>监控告警</tag>
      
      <tag>Zabbix</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Zabbix监控MySQL数据库性能</title>
    <link href="/linux/Zabbix-MySQL/"/>
    <url>/linux/Zabbix-MySQL/</url>
    
    <content type="html"><![CDATA[<h1 id="1-创建监控脚本"><a href="#1-创建监控脚本" class="headerlink" title="1.创建监控脚本"></a>1.创建监控脚本</h1><pre><code class="hljs">vi /usr/local/zabbix/share/zabbix/alertscripts/mysql_status.sh#!/bin/bashexport MYSQL_PWD=checkingMYSQL_CONN=&quot;/usr/local/mysql/bin/mysqladmin -hlocalhost -uchecker&quot;if [ $# -ne &quot;1&quot; ];then  echo &quot;arg error!&quot;ficase $1 in  Uptime)    result=`$&#123;MYSQL_CONN&#125; status|cut -f2 -d&quot;:&quot;|cut -f1 -d&quot;T&quot;`    echo $result    ;;  Com_update)    result=`$&#123;MYSQL_CONN&#125; extended-status |grep -w &quot;Com_update&quot;|cut -d&quot;|&quot; -f3`    echo $result    ;;  Slow_queries)    result=`$&#123;MYSQL_CONN&#125; status |cut -f5 -d&quot;:&quot;|cut -f1 -d&quot;O&quot;`    echo $result    ;;  Com_select)    result=`$&#123;MYSQL_CONN&#125; extended-status |grep -w &quot;Com_select&quot;|cut -d&quot;|&quot; -f3`    echo $result            ;;  Com_rollback)    result=`$&#123;MYSQL_CONN&#125; extended-status |grep -w &quot;Com_rollback&quot;|cut -d&quot;|&quot; -f3`            echo $result            ;;  Questions)    result=`$&#123;MYSQL_CONN&#125; status|cut -f4 -d&quot;:&quot;|cut -f1 -d&quot;S&quot;`            echo $result            ;;  Com_insert)    result=`$&#123;MYSQL_CONN&#125; extended-status |grep -w &quot;Com_insert&quot;|cut -d&quot;|&quot; -f3`            echo $result            ;;  Com_delete)    result=`$&#123;MYSQL_CONN&#125; extended-status |grep -w &quot;Com_delete&quot;|cut -d&quot;|&quot; -f3`            echo $result            ;;  Com_commit)    result=`$&#123;MYSQL_CONN&#125; extended-status |grep -w &quot;Com_commit&quot;|cut -d&quot;|&quot; -f3`            echo $result            ;;  Bytes_sent)    result=`$&#123;MYSQL_CONN&#125; extended-status |grep -w &quot;Bytes_sent&quot; |cut -d&quot;|&quot; -f3`            echo $result            ;;  Bytes_received)    result=`$&#123;MYSQL_CONN&#125; extended-status |grep -w &quot;Bytes_received&quot; |cut -d&quot;|&quot; -f3`            echo $result            ;;  Com_begin)    result=`$&#123;MYSQL_CONN&#125; extended-status |grep -w &quot;Com_begin&quot;|cut -d&quot;|&quot; -f3`            echo $result            ;;    *)    echo &quot;Usage:$0(Uptime|Com_update|Slow_queries|Com_select|Com_rollback|Questions|Com_insert|Com_delete|Com_commit|Bytes_sent|Bytes_received|Com_begin)&quot;    ;;esac</code></pre><hr><pre><code class="hljs">chmod +x mysql_status.shchown zabbix.zabbix mysql_status.sh</code></pre><h1 id="2-编辑zabbix-agent配置文件"><a href="#2-编辑zabbix-agent配置文件" class="headerlink" title="2.编辑zabbix_agent配置文件"></a>2.编辑zabbix_agent配置文件</h1><pre><code class="hljs">vi /etc/zabbix/zabbix_agentd.confUnsafeUserParameters=1UserParameter=mysql.version,/usr/local/mysql/bin/mysql -VUserParameter=mysql.status[*],/usr/local/zabbix/share/zabbix/alertscripts/mysql_status.sh $1 $2UserParameter=mysql.ping,/usr/local/mysql/bin/mysqladmin ping|grep -c &quot;alive&quot;</code></pre><h1 id="3-启动zabbix-agent"><a href="#3-启动zabbix-agent" class="headerlink" title="3.启动zabbix agent"></a>3.启动zabbix agent</h1><pre><code class="hljs">systemctl start zabbix_agentd</code></pre><h1 id="4-创建mysql监控账户"><a href="#4-创建mysql监控账户" class="headerlink" title="4.创建mysql监控账户"></a>4.创建mysql监控账户</h1><pre><code class="hljs">MariaDB [(none)]&gt; GRANT SELECT ON *.* TO &#39;checker&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;checking&#39;;MariaDB [(none)]&gt; flush privileges;</code></pre><h1 id="5-Mysql主机配置监控模板"><a href="#5-Mysql主机配置监控模板" class="headerlink" title="5.Mysql主机配置监控模板"></a>5.Mysql主机配置监控模板</h1><p>导入mysql模板 —&gt; 主机链接模板</p><h1 id="6-验证nginx监控"><a href="#6-验证nginx监控" class="headerlink" title="6.验证nginx监控"></a>6.验证nginx监控</h1><pre><code class="hljs">/usr/local/zabbix/bin/zabbix_get -s 192.168.100.188 -k &quot;mysql.status[Bytes_received]&quot;</code></pre>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>监控告警</tag>
      
      <tag>Zabbix</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Zabbix监控Nginx服务器性能</title>
    <link href="/linux/Zabbix-Nginx/"/>
    <url>/linux/Zabbix-Nginx/</url>
    
    <content type="html"><![CDATA[<h1 id="1-Nginx编译安装http-stub-status-module模块"><a href="#1-Nginx编译安装http-stub-status-module模块" class="headerlink" title="1.Nginx编译安装http_stub_status_module模块"></a>1.Nginx编译安装http_stub_status_module模块</h1><pre><code class="hljs">./configure --prefix=/usr/local/nginx --conf-path=/etc/nginx/nginx.conf \--with-http_stub_status_modulemake &amp;&amp; make installs</code></pre><h1 id="2-配置nginx-status"><a href="#2-配置nginx-status" class="headerlink" title="2.配置nginx status"></a>2.配置nginx status</h1><pre><code class="hljs">vi /etc/nginx/nginx.confserver &#123;</code></pre><p>      listen       80;<br>      #access_log  logs&#x2F;host.access.log  main;</p><p>      location &#x2F; {<br>        root   html;<br>        index  index.php index.html index.htm;<br>      }</p><p>      location &#x3D; &#x2F;nginx-status {<br>        stub_status on;<br>        access_log  off;<br>        allow 127.0.0.1;<br>        allow 192.168.100.188;<br>                deny all;<br>      }<br>    }</p><h1 id="3-测试nginx-status"><a href="#3-测试nginx-status" class="headerlink" title="3.测试nginx-status"></a>3.测试nginx-status</h1><pre><code class="hljs">curl 127.0.0.1/nginx-status</code></pre><h1 id="4-编写zabbix监控脚本"><a href="#4-编写zabbix监控脚本" class="headerlink" title="4.编写zabbix监控脚本"></a>4.编写zabbix监控脚本</h1><pre><code class="hljs">vi /usr/local/zabbix/share/zabbix/alertscripts/nginx_status.sh#!/bin/bash HOST=&quot;127.0.0.1&quot; PORT=&quot;80&quot; # Functions to return nginx stats function active &#123;   /usr/bin/curl &quot;http://$HOST:$PORT/nginx-status&quot; 2&gt;/dev/null| grep &#39;Active&#39; | awk &#39;&#123;print $NF&#125;&#39; &#125; function reading &#123;   /usr/bin/curl &quot;http://$HOST:$PORT/nginx-status&quot; 2&gt;/dev/null| grep &#39;Reading&#39; | awk &#39;&#123;print $2&#125;&#39; &#125; function writing &#123;   /usr/bin/curl &quot;http://$HOST:$PORT/nginx-status&quot; 2&gt;/dev/null| grep &#39;Writing&#39; | awk &#39;&#123;print $4&#125;&#39; &#125; function waiting &#123;   /usr/bin/curl &quot;http://$HOST:$PORT/nginx-status&quot; 2&gt;/dev/null| grep &#39;Waiting&#39; | awk &#39;&#123;print $6&#125;&#39; &#125; function accepts &#123;   /usr/bin/curl &quot;http://$HOST:$PORT/nginx-status&quot; 2&gt;/dev/null| awk NR==3 | awk &#39;&#123;print $1&#125;&#39; &#125; function handled &#123;   /usr/bin/curl &quot;http://$HOST:$PORT/nginx-status&quot; 2&gt;/dev/null| awk NR==3 | awk &#39;&#123;print $2&#125;&#39; &#125; function requests &#123;   /usr/bin/curl &quot;http://$HOST:$PORT/nginx-status&quot; 2&gt;/dev/null| awk NR==3 | awk &#39;&#123;print $3&#125;&#39; &#125; $1</code></pre><hr><pre><code class="hljs">chmod +x nginx_status.shchown zabbix.zabbix nginx_status.sh</code></pre><h1 id="5-配置zabbix客户端配置文件"><a href="#5-配置zabbix客户端配置文件" class="headerlink" title="5.配置zabbix客户端配置文件"></a>5.配置zabbix客户端配置文件</h1><pre><code class="hljs">vi /etc/zabbix/zabbix_agentd.confUnsafeUserParameters=1UserParameter=nginx.accepts,/usr/local/zabbix/share/zabbix/alertscripts/nginx_status.sh accepts UserParameter=nginx.handled,/usr/local/zabbix/share/zabbix/alertscripts/nginx_status.sh handled UserParameter=nginx.requests,/usr/local/zabbix/share/zabbix/alertscripts/nginx_status.sh requests UserParameter=nginx.connections.active,/usr/local/zabbix/share/zabbix/alertscripts/nginx_status.sh active UserParameter=nginx.connections.reading,/usr/local/zabbix/share/zabbix/alertscripts/nginx_status.sh reading UserParameter=nginx.connections.writing,/usr/local/zabbix/share/zabbix/alertscripts/nginx_status.sh writing UserParameter=nginx.connections.waiting,/usr/local/zabbix/share/zabbix/alertscripts/nginx_status.sh waiting</code></pre><h1 id="6-启动zabbix-agent"><a href="#6-启动zabbix-agent" class="headerlink" title="6.启动zabbix agent"></a>6.启动zabbix agent</h1><pre><code class="hljs">systemctl start zabbix_agentd</code></pre><h1 id="7-Nginx主机配置监控模版"><a href="#7-Nginx主机配置监控模版" class="headerlink" title="7.Nginx主机配置监控模版"></a>7.Nginx主机配置监控模版</h1><p>导入nginx模板 —&gt; 主机链接模板</p><h1 id="8-验证nginx监控"><a href="#8-验证nginx监控" class="headerlink" title="8.验证nginx监控"></a>8.验证nginx监控</h1>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Nginx</tag>
      
      <tag>监控告警</tag>
      
      <tag>Zabbix</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Zabbix监控Linux系统</title>
    <link href="/linux/Zabbix-Linux/"/>
    <url>/linux/Zabbix-Linux/</url>
    
    <content type="html"><![CDATA[<h1 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h1><ul><li>172.16.100.100 node01 Zabbix server</li><li>172.16.100.120 node02 Zabbix agent</li></ul><hr><h1 id="1-安装依赖包"><a href="#1-安装依赖包" class="headerlink" title="1.安装依赖包"></a>1.安装依赖包</h1><pre><code class="hljs">yum -y install gcc gcc-c++ make automake autoconf \curl curl-devel perl-DBI net-snmp-devel libcurl libxml2 libxml2-devel</code></pre><h1 id="2-创建zabbix用户"><a href="#2-创建zabbix用户" class="headerlink" title="2.创建zabbix用户"></a>2.创建zabbix用户</h1><pre><code class="hljs">groupadd zabbix &amp;&amp; useradd -g zabbix -s /sbin/nologin zabbix</code></pre><h1 id="3-编译安装zabbix"><a href="#3-编译安装zabbix" class="headerlink" title="3.编译安装zabbix"></a>3.编译安装zabbix</h1><pre><code class="hljs">tar -zxvf zabbix-3.0.8.tar.gz &amp;&amp; cd zabbix-3.0.8./configure --prefix=/usr/local/zabbix --sysconfdir=/etc/zabbix --enable-agent \--with-net-snmp --with-libcurl --with-libxml2 --enable-ipv6make &amp;&amp; make install</code></pre><h1 id="4-创建配置文件"><a href="#4-创建配置文件" class="headerlink" title="4.创建配置文件"></a>4.创建配置文件</h1><pre><code class="hljs">sed -i &#39;s@BASEDIR=/usr/local@BASEDIR=/usr/local/zabbix@g&#39; /etc/init.d/zabbix_agentdsed -i &#39;s@LogFile=/tmp/zabbix_agentd.log@LogFile=/var/zabbix/zabbix_agentd.log@g&#39; /etc/zabbix/zabbix_agentd.confsed -i &#39;s@# PidFile=/tmp/zabbix_agentd.pid@PidFile=/var/run/zabbix/zabbix_agentd.pid@g&#39; /etc/zabbix/zabbix_agentd.conf</code></pre><h1 id="5-创建启动脚本"><a href="#5-创建启动脚本" class="headerlink" title="5.创建启动脚本"></a>5.创建启动脚本</h1><pre><code class="hljs">cp /projects/zabbix-3.0.8/misc/init.d/fedora/core/zabbix_agentd /etc/init.dchmod +x /etc/init.d/zabbix_agentdmkdir /var/zabbix /var/run/zabbixchown zabbix:zabbix -R /usr/local/zabbix /etc/zabbix /var/zabbix /var/run/zabbix</code></pre><h1 id="6-启动Zabbix-Agent"><a href="#6-启动Zabbix-Agent" class="headerlink" title="6.启动Zabbix Agent"></a>6.启动Zabbix Agent</h1><pre><code class="hljs">systemctl daemon-reloadsystemctl start zabbix_agentdsystemctl enable zabbix_agentd</code></pre>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>监控告警</tag>
      
      <tag>Zabbix</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Zabbix监控系统的部署与配置</title>
    <link href="/linux/Zabbix/"/>
    <url>/linux/Zabbix/</url>
    
    <content type="html"><![CDATA[<p>Zabbix，由C语言开发的开源分布式企业级监控系统，通过Agent方式采集服务器、应用程序、网络服务、数据库、网站及其他网络硬件的工作状态，最终将监控数据写入数据库。Zabbix具备分布式监控功能，支持复杂架构下的监控解决方案，还提供直观的web页面展现。Zabbix告警通知机制也比较灵活，如邮件、短信和其他报警方式，以便于快速定位、解决各种故障，从而保障业务统的安全与稳定</p><h1 id="体系架构"><a href="#体系架构" class="headerlink" title="体系架构"></a>体系架构</h1><p>Zabbix是典型的C&#x2F;S架构，Zabbix Server作为服务端通过Zabbix Agent或SNMP协议等方式，定期收集被监控端的监控数据并其存储到数据库，最后通过前端UI做展示</p><h2 id="1-Zabbix-Server"><a href="#1-Zabbix-Server" class="headerlink" title="1.Zabbix Server"></a>1.Zabbix Server</h2><p>Zabbix Server，Zabbix监控服务端，核心部分，负责监控数据的采集、计算及存储，并依据监控指标所设置的触发器阈值生成告警信息，进而完成告警动作，如发送告警信息（邮件、微信、短信及钉钉等）、运行命令（如shell命令、reboot、restart、install等），以进行故障处理或通知</p><h2 id="2-Zabbix-Agent"><a href="#2-Zabbix-Agent" class="headerlink" title="2.Zabbix Agent"></a>2.Zabbix Agent</h2><p>Zabbix Agent，部署于被监控端，收集本地资源和应用程序的状态，并将收集到的数据发送给Zabbix Server</p><h2 id="3-Zabbix-Proxy"><a href="#3-Zabbix-Proxy" class="headerlink" title="3.Zabbix Proxy"></a>3.Zabbix Proxy</h2><p>Zabbix Proxy，可选组件，用于分流Zabbix Server的负载，类似于中转站，将收集到的数据转发给Zabbix Server，实现整个监控系统分布式的架构</p><h2 id="4-数据库"><a href="#4-数据库" class="headerlink" title="4.数据库"></a>4.数据库</h2><p>Zabbix配置信息及监控数据的后端存储，支持MySQL、Oracle等主流数据库，官方推荐MySQL</p><h2 id="5-UI"><a href="#5-UI" class="headerlink" title="5.UI"></a>5.UI</h2><p>前端Web界面，根据收集到的数据进行展示和绘图，属于Zabbix Server，可部署于同一服务器</p><h1 id="功能组件"><a href="#功能组件" class="headerlink" title="功能组件"></a>功能组件</h1><h2 id="1-zabbix-server"><a href="#1-zabbix-server" class="headerlink" title="1.zabbix_server"></a>1.zabbix_server</h2><p>Zabbix服务端守护进程，接收其余组件采集到的监控数据，如zabbix_agent、zabbix_get、zabbix_sender、zabbix_proxy、Zabbix_ java_ gateway等</p><h2 id="2-zabbix-agentd"><a href="#2-zabbix-agentd" class="headerlink" title="2.zabbix_agentd"></a>2.zabbix_agentd</h2><p>Zabbix客户端守护进程，负责收集客户端数据，如CPU 负载、内存、硬盘使用情况等</p><h2 id="3-zabbix-proxy"><a href="#3-zabbix-proxy" class="headerlink" title="3.zabbix_proxy"></a>3.zabbix_proxy</h2><p>Zabbix分布式架构代理守护进程，通常监控设备大于500时需要进行分布式监控架构部署</p><h2 id="4-zabbix-get"><a href="#4-zabbix-get" class="headerlink" title="4.zabbix_get"></a>4.zabbix_get</h2><p>Zabbix数据接收工具，单独使用的命令，通常在server或proxy端执行获取远程客户端信息的命令</p><h2 id="5-zabbix-sender"><a href="#5-zabbix-sender" class="headerlink" title="5.zabbix_sender"></a>5.zabbix_sender</h2><p>Zabbix数据发送工具，发送数据给server或proxy端，通常用于耗时比较长的检查</p><h2 id="6-zabbix-java-gataway"><a href="#6-zabbix-java-gataway" class="headerlink" title="6.zabbix_java_gataway"></a>6.zabbix_java_gataway</h2><p>java网关，专用于java的agentd，只能主动取获取数据，而不能被动获取</p><h1 id="监控指标"><a href="#监控指标" class="headerlink" title="监控指标"></a>监控指标</h1><h2 id="1-操作系统状态"><a href="#1-操作系统状态" class="headerlink" title="1.操作系统状态"></a>1.操作系统状态</h2><p>例如CPU使用率、内存使用率、磁盘空间等。Zabbix agent还可以执行命令并返回结果，用于更复杂的监控需求。使用Zabbix agent进行监控的好处是它可以提供更详细和精确的数据，并且对于网络环境有更好的适应性</p><h2 id="2-服务器硬件状态"><a href="#2-服务器硬件状态" class="headerlink" title="2.服务器硬件状态"></a>2.服务器硬件状态</h2><p>IPMI:智能平台管理接口（Intelligent Platform Management Interface）IPMI 能够横跨不同的操作系统、固件和硬件平台，用于监控服务器硬件的状态和性能。Zabbix可以通过IPMI协议直接与服务器进行通信，获取硬件相关的信息，例如温度、风扇转速、电源状态等</p><h2 id="3-应用程序状态"><a href="#3-应用程序状态" class="headerlink" title="3.应用程序状态"></a>3.应用程序状态</h2><h2 id="4-Web网站状态"><a href="#4-Web网站状态" class="headerlink" title="4.Web网站状态"></a>4.Web网站状态</h2><h2 id="5-网络硬件设备状态"><a href="#5-网络硬件设备状态" class="headerlink" title="5.网络硬件设备状态"></a>5.网络硬件设备状态</h2><p>SNMP：网络管理协议（Simple Network Management Protocol）是专门设计用于在 IP 网络管理网络节点（服务器、工作站、路由器、交换机等）的一种标准协议，它是一种应用层协议。通过SNMP协议，Zabbix可以获取网络设备的各种信息，例如接口流量、端口状态等</p><h2 id="6-Java程序状态"><a href="#6-Java程序状态" class="headerlink" title="6.Java程序状态"></a>6.Java程序状态</h2><p>JMX（Java Management Extensions）：JMX是一种Java平台的管理和监控标准。Zabbix可以通过JMX协议与Java应用程序进行通信，获取应用程序运行状态的信息，例如堆内存使用情况、线程数、GC时间等。JMX适用于监控Java应用程序的性能和健康状态，对于Java开发人员和运维人员非常有用</p><h1 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h1><h2 id="1-主机"><a href="#1-主机" class="headerlink" title="1.主机"></a>1.主机</h2><p>host，主机，被监控的网络设备，以IP或域名表示</p><h2 id="2-主机组"><a href="#2-主机组" class="headerlink" title="2.主机组"></a>2.主机组</h2><p>host group，主机组，逻辑概念，包含主机和模板，一个主机组的主机和模板之间并没有任何直接的关联。通常在给不同用户组的主机分配权限时候使用主机组</p><h2 id="3-监控项"><a href="#3-监控项" class="headerlink" title="3.监控项"></a>3.监控项</h2><p>item，监控项，即监控指标，通常是度量数据</p><h2 id="4-触发器"><a href="#4-触发器" class="headerlink" title="4.触发器"></a>4.触发器</h2><p>trigger，触发器，用于定义监控指标阈值和评估监控项接收到的数据的逻辑表达式 当接收到的数据高于阈值时，触发器从“OK”变成“Problem”状态。当接收到的数据低于阈值时，触发器保留&#x2F;返回一个“OK”的状态。</p><p>5、动作 (action)<br>一个对事件做出反应的预定义的操作。 一个动作由操作(例如发出通知)和条件(当时操作正在发生)组成</p><p>6、媒介 (media)<br>发送告警通知的手段；告警通知的途径</p><p>7、远程命令 (remote command)<br>一个预定义好的，满足一些条件的情况下，可以在被监控主机上自动执行的命令</p><p>8、模版 (template)<br>一组可以被应用到一个或多个主机上的实体（监控项，触发器，图形，应用，Web场景等）的集合 模版的任务就是加快对主机监控任务的实施；也可以使监控任务的批量修改更简单。模版是直接关联到每台单独的主机上。</p><p>9、web 场景 (web scenario)<br>利用一个或多个HTTP请求来检查网站的可用性</p><hr><h1 id="1-安装依赖包"><a href="#1-安装依赖包" class="headerlink" title="1.安装依赖包"></a>1.安装依赖包</h1><pre><code class="hljs">apt install -y gcc libsnmp-dev libevent-dev libxml2-dev libghc-curl-devyum instal -y gcc make curl-devel net-snmp-devel libcurl libxml2-devel</code></pre><h1 id="2-安装Nginx、PHP、MySQL"><a href="#2-安装Nginx、PHP、MySQL" class="headerlink" title="2.安装Nginx、PHP、MySQL"></a>2.安装Nginx、PHP、MySQL</h1><h2 id="2-1-配置数据库"><a href="#2-1-配置数据库" class="headerlink" title="2.1 配置数据库"></a>2.1 配置数据库</h2><h3 id="2-1-1-创建zabbix数据库，并授予权限"><a href="#2-1-1-创建zabbix数据库，并授予权限" class="headerlink" title="2.1.1 创建zabbix数据库，并授予权限"></a>2.1.1 创建zabbix数据库，并授予权限</h3><pre><code class="hljs">MariaDB [(none)]&gt; create database zabbix charset utf8 collate utf8_bin;&quot;MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON zabbix.* TO &#39;zabbix&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;zabbix&#39;;MariaDB [(none)]&gt; flush privileges;&quot;</code></pre><h3 id="2-1-2-导入zabbix表结构"><a href="#2-1-2-导入zabbix表结构" class="headerlink" title="2.1.2 导入zabbix表结构"></a>2.1.2 导入zabbix表结构</h3><pre><code class="hljs">MariaDB [(none)]&gt; source database/mysql/schema.sql;MariaDB [(none)]&gt; source database/mysql/images.sql;MariaDB [(none)]&gt; source database/mysql/data.sql;</code></pre><h1 id="3-创建zabbix用户与组"><a href="#3-创建zabbix用户与组" class="headerlink" title="3.创建zabbix用户与组"></a>3.创建zabbix用户与组</h1><pre><code class="hljs">groupadd zabbix &amp;&amp; useradd -g zabbix -s /sbin/nologin zabbixmkdir -p /var/log/zabbix &amp;&amp; chown -R zabbix:zabbix /var/log/zabbix</code></pre><h1 id="4-编译安装zabbix-server"><a href="#4-编译安装zabbix-server" class="headerlink" title="4.编译安装zabbix server"></a>4.编译安装zabbix server</h1><pre><code class="hljs">tar -zxvf zabbix-6.0.18.tar.gz &amp;&amp; cd zabbix-6.0.18./configure --prefix=/usr/local/zabbix --sysconfdir=/etc/zabbix \--enable-server --enable-agent --enable-proxy --with-net-snmp --enable-ipv6 \--with-libcurl --with-libxml2 --with-mysql=/usr/local/mysql/bin/mysql_configmake &amp;&amp; make install</code></pre><h1 id="5-创建配置文件"><a href="#5-创建配置文件" class="headerlink" title="5.创建配置文件"></a>5.创建配置文件</h1><pre><code class="hljs">vi /etc/zabbix/zabbix_server.confLogFile=/var/log/zabbix/zabbix_server.logPidFile=/var/log/zabbix/zabbix_server.pidDBHost=127.0.0.1DBName=zabbixDBUser=zabbixDBPassword=zabbix</code></pre><h3 id="1-5-2-创建zabbix-agent配置文件"><a href="#1-5-2-创建zabbix-agent配置文件" class="headerlink" title="1.5.2 创建zabbix agent配置文件"></a>1.5.2 创建zabbix agent配置文件</h3><pre><code class="hljs">vi /etc/zabbix/zabbix_agentd.confPidFile=/var/log/zabbix/zabbix_agentd.pidLogFile=/var/log/zabbix/zabbix_agentd.logServer=127.0.0.1ServerActive=127.0.0.1Hostname=Zabbix server</code></pre><h1 id="6-创建启动脚本"><a href="#6-创建启动脚本" class="headerlink" title="6.创建启动脚本"></a>6.创建启动脚本</h1><h1 id="7-配置监控web可视化"><a href="#7-配置监控web可视化" class="headerlink" title="7.配置监控web可视化"></a>7.配置监控web可视化</h1><h2 id="7-1-创建nginx配置文件"><a href="#7-1-创建nginx配置文件" class="headerlink" title="7.1 创建nginx配置文件"></a>7.1 创建nginx配置文件</h2><pre><code class="hljs">vi /etc/nginx/conf.d/zabbix.confserver &#123;    listen       80;    server_name  localhost;        location /zabbix &#123;        root   /web;        index  index.html index.htm index.php;        charset utf-8;        access_log  /var/log/nginx/zabbix_access.log main;        error_log  /var/log/nginx/zabbix_error.log;        location ~* \.php$ &#123;        root           /web;        fastcgi_pass   127.0.0.1:9000;        fastcgi_index  index.php;        fastcgi_param  SCRIPT_FILENAME $document_root$fastcgi_script_name;        include        /etc/nginx/fastcgi_params;        &#125;    &#125;&#125;</code></pre><h2 id="7-2-创建UI"><a href="#7-2-创建UI" class="headerlink" title="7.2 创建UI"></a>7.2 创建UI</h2><pre><code class="hljs">cp -r ui /web/zabbix</code></pre><h1 id="8-启动Zabbix-server、Zabbix-agent、nginx、php"><a href="#8-启动Zabbix-server、Zabbix-agent、nginx、php" class="headerlink" title="8.启动Zabbix server、Zabbix agent、nginx、php"></a>8.启动Zabbix server、Zabbix agent、nginx、php</h1><h1 id="9-验证"><a href="#9-验证" class="headerlink" title="9.验证"></a>9.验证</h1><h1 id="2-部署Zabbix-agent"><a href="#2-部署Zabbix-agent" class="headerlink" title="2.部署Zabbix agent"></a>2.部署Zabbix agent</h1><h2 id="2-1-安装依赖包"><a href="#2-1-安装依赖包" class="headerlink" title="2.1 安装依赖包"></a>2.1 安装依赖包</h2><pre><code class="hljs">apt install -y gcc mysql-dev libsnmp-dev libevent-dev libxml2-dev libghc-curl-devyum instal -y gcc make curl-devel net-snmp-devel mysql-devel libcurl libxml2-devel libevent-devel</code></pre><h2 id="2-2-创建zabbix用户与组"><a href="#2-2-创建zabbix用户与组" class="headerlink" title="2.2 创建zabbix用户与组"></a>2.2 创建zabbix用户与组</h2><pre><code class="hljs">groupadd zabbix &amp;&amp; useradd -g zabbix -s /sbin/nologin zabbix</code></pre><h2 id="2-3-编译安装zabbix-server"><a href="#2-3-编译安装zabbix-server" class="headerlink" title="2.3 编译安装zabbix server"></a>2.3 编译安装zabbix server</h2><pre><code class="hljs">tar -zxvf zabbix-6.0.18.tar.gz &amp;&amp; cd zabbix-6.0.18./configure --prefix=/usr/local/zabbix --sysconfdir=/etc/zabbix \--enable-agent --with-net-snmp --enable-ipv6 \--with-libcurl --with-libxml2make &amp;&amp; make install</code></pre><h2 id="2-4-修改zabbix-server配置文件"><a href="#2-4-修改zabbix-server配置文件" class="headerlink" title="2.4 修改zabbix_server配置文件"></a>2.4 修改zabbix_server配置文件</h2><pre><code class="hljs">vi /etc/zabbix/zabbix_agentd.confPidFile=/var/log/zabbix/zabbix_agentd.pidLogFile=/var/log/zabbix/zabbix_agentd.logServer=172.16.100.100ServerActive=172.16.100.100Hostname=node02</code></pre><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/m0_54563444/article/details/141207533">https://blog.csdn.net/m0_54563444/article/details/141207533</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>监控告警</tag>
      
      <tag>Zabbix</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MySQL数据库配置半同步主从复制集群</title>
    <link href="/linux/MySQLSemiSyncReplication/"/>
    <url>/linux/MySQLSemiSyncReplication/</url>
    
    <content type="html"><![CDATA[<p>MySQL数据复制有两种方式，即异步复制、半同步复制、全同步复制，默认为异步方式</p><h1 id="异步方式"><a href="#异步方式" class="headerlink" title="异步方式"></a>异步方式</h1><p>从节点通过IO线程拉取主节点的binlog，这个过程中主节点并不关注从节点是否已经接收、处理数据和数据处理的进度，而是继续处理客户端提出的事务已生成binlog文件工从节点拉取。若主节点此时发生故障，则已经提交的事务就存在并没被从节点拉取的数据，或者从节点拉取到的binlog日志文件不完整，就会造成主从节点的数据不一致，甚至在恢复时造成数据的丢失。但复制速度快，扩展性好，资源占用少</p><h1 id="全同步方式"><a href="#全同步方式" class="headerlink" title="全同步方式"></a>全同步方式</h1><p>主节点等待所有的从节点完成数据同步后再提交事务，再将执行信息返回给客户端，从而保障主从节点数据完整性，但处理速度会大大降低</p><h1 id="半同步方式"><a href="#半同步方式" class="headerlink" title="半同步方式"></a>半同步方式</h1><p>主数据库等待至少一个从节点完成数据同步后再提交事务，既保障了处理速度，又至少保障了一个从节点的数据完整性</p><p>半同步复制模式工作机制：从节点IO线程将binlog日志接收完毕之后，反馈给主节点一个确认消息，通知主节点binlog日志已经接收完毕。然后，主节点再提交下一个事务。若主节点等待反馈超时，则自动切换为异步复制模式，直到接收到至少一个从节点的反馈信息为止。由于增加了这个反馈机制，相对于异步复制就会有一个延迟，性能稍微降低，但至少保障了一个从节点的数据完整性</p><hr><h1 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h1><ul><li>172.16.100.200 master</li><li>172.16.100.100 slaver1</li><li>172.16.100.120 slaver2</li></ul><hr><h1 id="1-安装MySQL数据库服务器"><a href="#1-安装MySQL数据库服务器" class="headerlink" title="1.安装MySQL数据库服务器"></a>1.安装MySQL数据库服务器</h1><h1 id="2-安装MySQL数据库半同步复制插件"><a href="#2-安装MySQL数据库半同步复制插件" class="headerlink" title="2.安装MySQL数据库半同步复制插件"></a>2.安装MySQL数据库半同步复制插件</h1><h2 id="2-1-配置主节点"><a href="#2-1-配置主节点" class="headerlink" title="2.1 配置主节点"></a>2.1 配置主节点</h2><pre><code class="hljs"># 安装半同步复制插件mysql&gt; install plugin rpl_semi_sync_master soname &#39;semisync_master.so&#39;;# 查看半同步复制插件状态mysql&gt; show global variables like &#39;rpl%&#39;;</code></pre><h2 id="2-2-配置从节点"><a href="#2-2-配置从节点" class="headerlink" title="2.2 配置从节点"></a>2.2 配置从节点</h2><pre><code class="hljs"># 安装半同步复制插件mysql&gt; install plugin rpl_semi_sync_slave soname &#39;semisync_slave.so&#39;; # 查看半同步复制插件状态mysql&gt; show global variables like &#39;rpl%&#39;;</code></pre><h1 id="3-主从节点启用半同步复制"><a href="#3-主从节点启用半同步复制" class="headerlink" title="3.主从节点启用半同步复制"></a>3.主从节点启用半同步复制</h1><h2 id="3-1-配置主节点"><a href="#3-1-配置主节点" class="headerlink" title="3.1 配置主节点"></a>3.1 配置主节点</h2><pre><code class="hljs">sudo vi /etc/my.cnf[mysqld]# 启用半同步复制插件rpl_semi_sync_master_enabled=1# 设置半同步复制超时时长rpl_semi_sync_master_timeout=1000</code></pre><h2 id="3-2-配置从节点"><a href="#3-2-配置从节点" class="headerlink" title="3.2 配置从节点"></a>3.2 配置从节点</h2><pre><code class="hljs">sudo vi /etc/my.cnf[mysqld]# 启用半同步复制插件rpl_semi_sync_slave_enabled=1</code></pre><h1 id="5-重启主从节点MySQL数据库服务"><a href="#5-重启主从节点MySQL数据库服务" class="headerlink" title="5.重启主从节点MySQL数据库服务"></a>5.重启主从节点MySQL数据库服务</h1><h1 id="6-测试主从复制功能"><a href="#6-测试主从复制功能" class="headerlink" title="6.测试主从复制功能"></a>6.测试主从复制功能</h1><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/xlgen157387/article/details/69400410">https://blog.csdn.net/xlgen157387/article/details/69400410</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>MySQL</tag>
      
      <tag>SQL</tag>
      
      <tag>数据库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MySQL数据库配置基于GTID机制的主从复制集群</title>
    <link href="/linux/MySQL-GTID/"/>
    <url>/linux/MySQL-GTID/</url>
    
    <content type="html"><![CDATA[<p>GTID，即全局事务标识，是MySQL 5.6新增功能，用于替代通过binlog文件偏移量定位复制位置的传统方式。GTID信息存储于binlog文件，且新增2个binlog事件：Previous_gtids_log_event，位于每个binlog文件的开头且记录了在该binlog文件之前已执行的GTID集合；Gtid_log_event，位于每个事务之前，标明下一事务的GTID</p><p>MySQL服务器启动时，读取binlog文件，初始化gtid_executed和gtid_purged，使之与上次MySQL运行时一致gtid_executed被设置为新binlog文件中Previous_gtids_log_event和所有Gtid_log_event的合集；gtid_purged为最旧的binlog文件中Previous_gtids_log_event</p><p>由于两个重要的变量记录在binlog，所以MySQL 5.6开启gtid_mode须在备库上开启log_slave_updates。MySQL5.7新增了一个系统表mysql.gtid_executed，用于持久化已执行的GTID集合。当主库上没有开启log_bin或在备库上没有开启log_slave_updates时，mysql.gtid_executed跟每次用户事务一起更新，否则只在binlog日志发生rotation时更新mysql.gtid_executed</p><hr><h1 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h1><ul><li>172.16.100.200 master</li><li>172.16.100.100 slaver</li></ul><hr><h1 id="1-安装mysql数据库服务器"><a href="#1-安装mysql数据库服务器" class="headerlink" title="1.安装mysql数据库服务器"></a>1.安装mysql数据库服务器</h1><h1 id="2-配置主节点"><a href="#2-配置主节点" class="headerlink" title="2.配置主节点"></a>2.配置主节点</h1><h2 id="2-1-创建配置文件"><a href="#2-1-创建配置文件" class="headerlink" title="2.1 创建配置文件"></a>2.1 创建配置文件</h2><pre><code class="hljs">sudo vi /etc/my.cnf[mysqld]server-id=100log_bin=mysql-binlog-bin-index=mysql-bin.indexbinlog_format=rowsync_binlog=100expire_logs_days=90# 启用DTID,MariaDB默认启用，不需配置gtid-mode=on# 强制执行GTID一致性,MariaDB默认启用，不需配置enforce-gtid-consistency=on# MariaDB默认启用，不需配置master_info_repository=TABLE# MariaDB默认启用，不需配置relay_log_info_repository=TABLE# 设置开启基于组提交的并行复制，默认为DATABASE，即基于数据库的并行复制slave-parallel-type=LOGICAL_CLOCK# 设置并行复制的SQL线程数，MariaDB为slave-parallel-threadsslave-parallel-workers =8# 设置基于行的复制将SQL语句记录到二进制日志，默认为offbinlog-rows-query-log_events=on# 设置从库是否将主库事务更新到本地二进制文件，用于级联复制架构，默认关闭# log_slave_updates=on# slave-skip-errors=all</code></pre><h2 id="2-2-主节点创建复制账户并授予从节点权限"><a href="#2-2-主节点创建复制账户并授予从节点权限" class="headerlink" title="2.2 主节点创建复制账户并授予从节点权限"></a>2.2 主节点创建复制账户并授予从节点权限</h2><pre><code class="hljs">mysql&gt; GRANT REPLICATION SLAVE ON *.* to &#39;syncer&#39;@&#39;172.16.100.100&#39; identified by &#39;syncer&#39;;mysql&gt; FLUSH PRIVILEGES;</code></pre><h1 id="3-配置从节点"><a href="#3-配置从节点" class="headerlink" title="3.配置从节点"></a>3.配置从节点</h1><h2 id="3-1-创建配置文件"><a href="#3-1-创建配置文件" class="headerlink" title="3.1 创建配置文件"></a>3.1 创建配置文件</h2><pre><code class="hljs">sudo vi /etc/my.cnf[mysqld]server-id=200# 设置中继日志名称和存储位置relay_log=relay-bin# 设置中继日志索引名称和存储位置，用于存储最后一个中继日志的列表relay-log-index=relay-log-bin.index# 设置中继日志写入到磁盘文件的频率sync_relay_log=100# 设置启用中继日志修复功能，即中继日志损坏后重新从主服务器获取，防止其意外损坏造成位点信息读取错误，默认为0relay_log_recovery=1# 设置启用中继日志自动清理功能，配合relay_log_recovery参数防止从库意外崩溃后读取不准确的中继日志relay_log_purge=1# MariaDB默认启用，不需配置slaver_info_repository=TABLE# MariaDB默认启用，不需配置relay_log_info_repository=TABLE# 设置开启基于组提交的并行复制，默认为DATABASE，即基于数据库的并行复制slave-parallel-type=LOGICAL_CLOCK# 设置并行复制的SQL线程数，MariaDB为slave-parallel-workersslave-parallel-threads=4# 设置基于行复制是否启用二进制日志中的信息日志事件，MariaDB不需配置binlog-rows-query-log_events=on# 设置从库是否将主库事务更新到本地二进制文件，用于级联复制架构，默认关闭# log_slave_updates=on# slave-skip-errors=allreplicate_ignore_db=mysqlreplicate_ignore_db=performance_schemareplicate_ignore_db=information_schema</code></pre><h2 id="3-2-配置主从复制"><a href="#3-2-配置主从复制" class="headerlink" title="3.2 配置主从复制"></a>3.2 配置主从复制</h2><pre><code class="hljs"># MySQL数据库mysql&gt;  change master to     master_host=&#39;172.16.100.200&#39;,    master_port=3306,    master_user=&#39;syncer&#39;,    master_password=&#39;syncer&#39;,    master_auto_position=1;# MariaDB数据库mysql&gt;  change master to     master_host=&#39;172.16.100.100&#39;,    master_port=3306,    master_user=&#39;syncer&#39;,    master_password=&#39;syncer&#39;,    master_use_gtid=slave_pos;</code></pre><h2 id="3-3-开启主从复制功能"><a href="#3-3-开启主从复制功能" class="headerlink" title="3.3 开启主从复制功能"></a>3.3 开启主从复制功能</h2><pre><code class="hljs">mysql&gt; start slave;</code></pre><h2 id="3-4-查看从节点复制功能状态，测试主从同步功能"><a href="#3-4-查看从节点复制功能状态，测试主从同步功能" class="headerlink" title="3.4 查看从节点复制功能状态，测试主从同步功能"></a>3.4 查看从节点复制功能状态，测试主从同步功能</h2><pre><code class="hljs">mysql&gt; show slave status\G ; show processlist;*************************** 1. row ***************************           Slave_IO_State: Waiting for node to send event              node_Host: 192.168.0.200              node_User: syncer              node_Port: 3306            Connect_Retry: 60          node_Log_File: node-bin.000001      Read_node_Log_Pos: 859           Relay_Log_File: relay-bin.000002            Relay_Log_Pos: 321    Relay_node_Log_File: node-bin.000001         Slave_IO_Running: Yes        Slave_SQL_Running: Yes          Replicate_Do_DB: data      Replicate_Ignore_DB: test       Replicate_Do_Table: data.test   Replicate_Ignore_Table: test.test  Replicate_Wild_Do_Table:Replicate_Wild_Ignore_Table:               Last_Errno: 0               Last_Error:             Skip_Counter: 0      Exec_node_Log_Pos: 859          Relay_Log_Space: 522          Until_Condition: None           Until_Log_File:            Until_Log_Pos: 0       node_SSL_Allowed: No       node_SSL_CA_File:       node_SSL_CA_Path:          node_SSL_Cert:        node_SSL_Cipher:           node_SSL_Key:    Seconds_Behind_node: 0node_SSL_Verify_Server_Cert: No            Last_IO_Errno: 0            Last_IO_Error:           Last_SQL_Errno: 0           Last_SQL_Error:Replicate_Ignore_Server_Ids:         node_Server_Id: 200              node_UUID: 7b1bbc09-7009-11e8-8487-000c29a5e01f         node_Info_File: mysql.slave_node_info                SQL_Delay: 0      SQL_Remaining_Delay: NULL  Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates       node_Retry_Count: 86400              node_Bind:  Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp:           node_SSL_Crl:       node_SSL_Crlpath:       Retrieved_Gtid_Set:        Executed_Gtid_Set:            Auto_Position: 0     Replicate_Rewrite_DB:             Channel_Name:       node_TLS_Version:1 row in set (0.00 sec)</code></pre><ul><li>注：Slave_IO及Slave_SQL进程必须正常运行，即YES状态，否则表示复制状态不正常</li></ul><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.cnblogs.com/liangshaoye/p/5459421.html">https://www.cnblogs.com/liangshaoye/p/5459421.html</a></li><li><a href="https://www.cnblogs.com/zhoujinyi/p/4717951.html">https://www.cnblogs.com/zhoujinyi/p/4717951.html</a></li><li><a href="https://blog.csdn.net/anzhen0429/article/details/77658663">https://blog.csdn.net/anzhen0429/article/details/77658663</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>MySQL</tag>
      
      <tag>SQL</tag>
      
      <tag>数据库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MySQL数据库配置主从复制集群</title>
    <link href="/linux/MySQLReplication/"/>
    <url>/linux/MySQLReplication/</url>
    
    <content type="html"><![CDATA[<p>MySQL的复制功能是构建大规模、高性能数据库应用的基础，就是将一台数据库服务器的数据和其它服务器保持同步，主库可同步到多台备库，备库也可作为其他服务器的主库，主备之间可以有多种不同的组合方式。复制的基本原理即主库记录DDL和DML操作写入二进制日志，从库连接主库并将获取到的二进制日志重新执行，从而保持主备数据的一致性</p><h1 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h1><h2 id="1-主库记录binlog"><a href="#1-主库记录binlog" class="headerlink" title="1.主库记录binlog"></a>1.主库记录binlog</h2><p>主节点启动binlog dump线程，事务处理完成后将该次更新写入binlog，然后通知存储引擎提交事务，完成该次更新</p><h2 id="2-从库请求读取主库binlog"><a href="#2-从库请求读取主库binlog" class="headerlink" title="2.从库请求读取主库binlog"></a>2.从库请求读取主库binlog</h2><p>从库启动IO线程，连接到主库，请求读取主库的binlog</p><h2 id="3-主库发送binlog到从库"><a href="#3-主库发送binlog到从库" class="headerlink" title="3.主库发送binlog到从库"></a>3.主库发送binlog到从库</h2><p>主库根据从库的请求信息，将本地binlog文件与从库请求的位点信息对比，将binlog文件传送给从库的IO线程。若无请求位点信息，则从第一个日志文件中的第一个事件一个一个传送给从库</p><h2 id="4-从库将binlog写入中继日志Relay-Log"><a href="#4-从库将binlog写入中继日志Relay-Log" class="headerlink" title="4.从库将binlog写入中继日志Relay Log"></a>4.从库将binlog写入中继日志Relay Log</h2><p>从库IO线程将获取到的主库的日志、位点信息写入本地中继日志Relay Log的最末端，并将新的binlog文件名和位点记录到master-info文件，以记录已读取得主库的最新位置信息。若主从节点同步一致，则从库IO线程进入睡眠状态，直到主库有新事件产生后被唤醒，再将新事件更新到中继日志</p><h2 id="5-从库更新数据"><a href="#5-从库更新数据" class="headerlink" title="5.从库更新数据"></a>5.从库更新数据</h2><p>从库SQL线程实时监测到本地Relay Log文件，将其最新更新的日志解析为SQL并执行，重复主库的事务，完成本次复制，最后将从库的中继日志及位点信息写入relay_log.info，以记录下次数据复制的初始位点</p><hr><h1 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h1><ul><li>172.16.100.200 master</li><li>172.16.100.100 slaver</li></ul><hr><h1 id="1-安装Mysql数据库服务器"><a href="#1-安装Mysql数据库服务器" class="headerlink" title="1.安装Mysql数据库服务器"></a>1.安装Mysql数据库服务器</h1><h1 id="2-配置主节点"><a href="#2-配置主节点" class="headerlink" title="2.配置主节点"></a>2.配置主节点</h1><h2 id="2-1-创建配置文件"><a href="#2-1-创建配置文件" class="headerlink" title="2.1 创建配置文件"></a>2.1 创建配置文件</h2><pre><code class="hljs">sudo vi /etc/my.cnf[mysqld]# 设置服务器ID，具有唯一性server-id=100# 设置二进制日志名称和存储位置log-bin=mysql-bin# 设置二进制日志索引名称和存储位置，用于存储最后一个binlog文件的名称log-bin-index=mysql-bin.index# 二进制日志格式，默认为statement，基于SQL语句复制，可能会造成ID重复；row，基于数据行复制，日志量大；mix，混合复制binlog_format=mixed# 设置二进制日志写入到磁盘文件的频率，二进制日志先写入binlog_cache，再根据此参数写入到磁盘文件，复制的关键参数，影响性能与完整性，从库可不启用，默认为0，由操作系统调配，性能最高，安全性低；1，安全性最好，性能最低；n，n次事件提交后执行fsync磁盘同步指令，文件系统再将缓存到内存的binlog数据更新到磁盘sync_binlog=100# 设置每个连接会话所占用缓存量，默认32k# binlog_cache_size=64k# 设置binlog最大缓存内存量，默认1M，可通过binlog_cache_use、binlog_cache_size及binlog_cache_disk_use参数判断设置是否合理，若binlog_cache_disk_use大于1或者binlog_cache_use* binlog_cache_size大于max_binlog_cache_size，则需要调大该值# max_binlog_cache_size=2M# 设置二进制日志文件保存时长，默认为0，即永久保存# expire_logs_days=30</code></pre><h2 id="2-2-创建复制账户并授予权限"><a href="#2-2-创建复制账户并授予权限" class="headerlink" title="2.2 创建复制账户并授予权限"></a>2.2 创建复制账户并授予权限</h2><pre><code class="hljs">mysql&gt;  GRANT REPLICATION SLAVE ON *.* to &#39;syncer&#39;@&#39;172.16.100.100&#39; identified by &#39;syncer&#39;;mysql&gt;  FLUSH PRIVILEGES;</code></pre><h2 id="2-3-查看主节点二进制位置"><a href="#2-3-查看主节点二进制位置" class="headerlink" title="2.3 查看主节点二进制位置"></a>2.3 查看主节点二进制位置</h2><pre><code class="hljs">mysql&gt; show master status;+-------------------+----------+--------------+------------------+-------------------+| File              | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+-------------------+----------+--------------+------------------+-------------------+| log-bin.000001    |    120 |              |                  |                   |+-------------------+----------+--------------+------------------+-------------------+1 row in set (0.00 sec)</code></pre><ul><li>执行完此步骤后不要再操作主数据库，防止主节点二进制日志位置更新</li></ul><hr><h1 id="3-配置从节点"><a href="#3-配置从节点" class="headerlink" title="3.配置从节点"></a>3.配置从节点</h1><h2 id="3-1-创建配置文件"><a href="#3-1-创建配置文件" class="headerlink" title="3.1 创建配置文件"></a>3.1 创建配置文件</h2><pre><code class="hljs">sudo vi /etc/my.cnf[mysqld]# 设置服务器ID，具有唯一性server-id=200# 设置中继日志名称和存储位置relay_log=relay-bin# 设置中继日志索引名称和存储位置，用于存储最后一个中继日志的列表relay-log-index=relay-log-bin.index# 设置中继日志写入到磁盘文件的频率sync_relay_log=100# 设置启用中继日志修复功能，即中继日志损坏后重新从主服务器获取，防止其意外损坏造成位点信息读取错误，默认为0relay_log_recovery=1# 设置启用中继日志自动清理功能，配合relay_log_recovery参数防止从库意外崩溃后读取不准确的中继日志relay_log_purge=1# 设置从库的主节点位点信息写入磁盘的频率# sync_master_info=100# 设置从库的主节点位置信息存储方式，TABLE|FILEmaster_info_repository=TABLE# 设置从库位点信息写入磁盘的频率# sync_relay_log_info=100# 设置从库位点信息存储方式，TABLE|FILErelay_log_info_repository=TABLE# 设置启用基于组提交的并行复制，DATABASE｜LOGICAL_CLOCK，slave-parallel-type=LOGICAL_CLOCK# 设置并行复制的SQL线程数slave-parallel-workers=8# 设置启用链式级联服务，既可为主库又可为从库# log_slave_updates=1# 设置从库故障排除重新启动后不自动复制# skip_slave_start# 设置错误事务的忽略# slave-skip-errors=all# 设置服务器属性为只读，即具有超级用户权限的用户可修改数据，其他用户均不能# read_only=1# 设置binlog日志事件校验，即配置事件校验，保障复制事件完整性，默认none，即不记录checksumbinlog-checksum=CRC32# 设置主库写binlog事件校验，默认为0，不启用master-verify-checksum=1# 设置从库读binlog事件校验，默认为1，启用slave-sql-verify-checksum=1# 设置不参与同步的数据库，默认全部参与replicate_ignore_db=mysqlreplicate_ignore_db=performance_schemareplicate_ignore_db=information_schema# 设置参与同步的数据库，默认全部参与 # replicate_do_db=test# data001库只同步logs表# replicate_do_table=data001.user# data001库不同步以“log”字符串结尾的表# replicate_wild_ignore_table=data001.%log# data002库之同步包含“user”字符串的表# replicate_wild_do_table=data002.%user%# data002库不同步log表# replicate_ignore_table=data002.log</code></pre><h2 id="3-2-配置主从复制"><a href="#3-2-配置主从复制" class="headerlink" title="3.2 配置主从复制"></a>3.2 配置主从复制</h2><pre><code class="hljs">mysql&gt; change master to         master_host=&#39;172.16.100.200&#39;,        master_port=3306,        master_user=&#39;syncer&#39;,        master_password=&#39;syncer&#39;,        master_log_file=&#39;log-bin.000001&#39;,        master_log_pos=120;</code></pre><h2 id="3-3-启动主从复制功能"><a href="#3-3-启动主从复制功能" class="headerlink" title="3.3 启动主从复制功能"></a>3.3 启动主从复制功能</h2><pre><code class="hljs">mysql&gt;  start slave;</code></pre><h1 id="3-4-查看从节点复制功能状态，测试主从同步功能"><a href="#3-4-查看从节点复制功能状态，测试主从同步功能" class="headerlink" title="3.4 查看从节点复制功能状态，测试主从同步功能"></a>3.4 查看从节点复制功能状态，测试主从同步功能</h1><pre><code class="hljs">mysql&gt; show slave status\G*************************** 1. row ***************************           Slave_IO_State: Waiting for master to send event              Master_Host: 172.16.100.200              Master_User: syncer              Master_Port: 3306            Connect_Retry: 60          Master_Log_File: master-bin.000001      Read_Master_Log_Pos: 859           Relay_Log_File: relay-bin.000002            Relay_Log_Pos: 321    Relay_Master_Log_File: master-bin.000001         Slave_IO_Running: Yes        Slave_SQL_Running: Yes          Replicate_Do_DB: data002      Replicate_Ignore_DB: test       Replicate_Do_Table: data002.test   Replicate_Ignore_Table: test.test   Replicate_Wild_Do_Table:Replicate_Wild_Ignore_Table:               Last_Errno: 0               Last_Error:             Skip_Counter: 0      Exec_Master_Log_Pos: 859          Relay_Log_Space: 522          Until_Condition: None           Until_Log_File:            Until_Log_Pos: 0       Master_SSL_Allowed: No       Master_SSL_CA_File:       Master_SSL_CA_Path:          Master_SSL_Cert:        Master_SSL_Cipher:           Master_SSL_Key:    Seconds_Behind_Master: 0Master_SSL_Verify_Server_Cert: No            Last_IO_Errno: 0            Last_IO_Error:           Last_SQL_Errno: 0           Last_SQL_Error:Replicate_Ignore_Server_Ids:         Master_Server_Id: 200              Master_UUID: 7b1bbc09-7009-11e8-8487-000c29a5e01f         Master_Info_File: mysql.slave_master_info                SQL_Delay: 0      SQL_Remaining_Delay: NULL  Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates       Master_Retry_Count: 86400              Master_Bind:  Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp:           Master_SSL_Crl:       Master_SSL_Crlpath:       Retrieved_Gtid_Set:        Executed_Gtid_Set:            Auto_Position: 0     Replicate_Rewrite_DB:             Channel_Name:       Master_TLS_Version:1 row in set (0.00 sec)</code></pre><ul><li>Slave_IO及Slave_SQL进程必须正常运行，即YES状态，否则都是错误的状态</li></ul><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.51cto.com/amyhehe/1699168">https://blog.51cto.com/amyhehe/1699168</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>MySQL</tag>
      
      <tag>SQL</tag>
      
      <tag>数据库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MySQL数据库的备份与恢复</title>
    <link href="/linux/MySQLBackup/"/>
    <url>/linux/MySQLBackup/</url>
    
    <content type="html"><![CDATA[<p>MySQL数据库的备份是确保数据安全的重要措施，是拯救数据库的最后手段，所以生产环境的数据一定要有备份。数据库备份工具和策略的选择直接影响数据的恢复效率，需要根据不同的需求和场景做不同的判断，以适应不同的备份需求和场景。MySQL数据备份方式大致分为两类，即逻辑备份和物理备份</p><h1 id="1-逻辑备份"><a href="#1-逻辑备份" class="headerlink" title="1.逻辑备份"></a>1.逻辑备份</h1><p>逻辑备份，即数据库对象级的备份，具体是指将数据库对象通过SQL查询并转储到文件，该文件包含了用于创建转储对象（数据库，表，触发器、自定义函数、存储过程等）的CREATE语句和用于将数据加载到表中的INSERT语句，数据恢复时执行备份的SQL语句即可实现数据库数据的恢复与重载，备份文件较小，速度较快</p><h2 id="1-1-Mysqldump"><a href="#1-1-Mysqldump" class="headerlink" title="1.1 Mysqldump"></a>1.1 Mysqldump</h2><p>Mysqldump，MySQL数据库自带的备份工具，路径一般为：&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;bin&#x2F;mysqldump，支持基于innodb的热备份。Mysqldump由于其单线程的特性，所以速度不是很快。但作为最早的备份工具，当前应用还是非常广泛，适用于数据量比较小的场景，其完全备份+二进制日志的方式可实现基于时间点的数据恢复</p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://blog.csdn.net/qq_43164571/article/details/113247012">https://blog.csdn.net/qq_43164571/article/details/113247012</a></li><li><a href="https://blog.csdn.net/qq_42768234/article/details/133277583">https://blog.csdn.net/qq_42768234/article/details/133277583</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>MySQL</tag>
      
      <tag>SQL</tag>
      
      <tag>数据库</tag>
      
      <tag>数据灾备</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MySQL数据库SQL命令详解</title>
    <link href="/linux/MySQL-SQL/"/>
    <url>/linux/MySQL-SQL/</url>
    
    <content type="html"><![CDATA[<h1 id="1-设置root账户密码"><a href="#1-设置root账户密码" class="headerlink" title="1.设置root账户密码"></a>1.设置root账户密码</h1><pre><code class="hljs">/usr/local/mysql/bin/mysqladmin -u root password &#39;123456&#39;</code></pre><h1 id="2-登陆数据库"><a href="#2-登陆数据库" class="headerlink" title="2.登陆数据库"></a>2.登陆数据库</h1><pre><code class="hljs"># 格式为：mysql -h 主机地址 -P 端口号 -u 用户名 -p用户密码mysql -u root -p 123456mysql -h127.0.0.1 -P3301 -uroot -p123456</code></pre><h1 id="3-查看所有数据库"><a href="#3-查看所有数据库" class="headerlink" title="3.查看所有数据库"></a>3.查看所有数据库</h1><pre><code class="hljs">MariaDB [(none)]&gt;  show databases;</code></pre><h1 id="4-创建数据库"><a href="#4-创建数据库" class="headerlink" title="4.创建数据库"></a>4.创建数据库</h1><pre><code class="hljs">MariaDB [(none)]&gt;  create database testdb character set utf8mb4 collate utf8mb4_unicode_ci;</code></pre><h1 id="5-切换数据库"><a href="#5-切换数据库" class="headerlink" title="5.切换数据库"></a>5.切换数据库</h1><pre><code class="hljs">MariaDB [(none)]&gt;  use mysql;</code></pre><h1 id="6-创建数据库用户并授予权限"><a href="#6-创建数据库用户并授予权限" class="headerlink" title="6.创建数据库用户并授予权限"></a>6.创建数据库用户并授予权限</h1><pre><code class="hljs"># 格式为：grant 权限 on 数据库.* to 用户名@登录主机 identified by &quot;密码&quot;;MariaDB [(none)]&gt;  GRANT ALL PRIVILEGES ON user01.* TO &#39;testdb&#39;@&#39;172.16.100.200&#39; IDENTIFIED BY &#39;sword&#39;;MariaDB [(none)]&gt;  grant SELECT privileges on user01.* to &#39;testdb&#39;@&#39;172.16.100.200&#39; identified by &#39;123456&#39;;</code></pre><hr><p>权限可取值：</p><ul><li>ALTER，修改表和索引</li><li>CREATE，创建数据库和表</li><li>DELETE，删除表中已有的记录</li><li>DROP，删除数据库和表</li><li>INDEX，创建或删除索引</li><li>INSERT，向表中插入新行</li><li>SELECT，检索表中的记录</li><li>UPDATE，修改现存表记录</li><li>FILE，读或写服务器上的文件</li><li>PROCESS，查看服务器中执行的线程信息或杀死线程</li><li>RELOAD，重载授权表或清空日志、主机缓存或表缓存</li><li>SHUTDOWN，关闭服务器</li><li>ALL，所有权限，同义词ALL PRIVILEGES</li></ul><hr><h1 id="7-刷新权限表"><a href="#7-刷新权限表" class="headerlink" title="7.刷新权限表"></a>7.刷新权限表</h1><pre><code class="hljs">MariaDB [(none)]&gt;  flush privileges;</code></pre><h1 id="8-创建数据表"><a href="#8-创建数据表" class="headerlink" title="8.创建数据表"></a>8.创建数据表</h1><pre><code class="hljs">MariaDB [(none)]&gt;  use testdb;# MariaDB [(none)]&gt;  select DATE_FORMAT(current_timestamp(3),&#39;%Y%m%d%H%i%s%f&#39;);MariaDB [(none)]&gt;  create table test (id varchar(50) not null primary key,name varchar(50),create_time timestamp(3));</code></pre><h1 id="9-显示当前数据库所有数据表"><a href="#9-显示当前数据库所有数据表" class="headerlink" title="9.显示当前数据库所有数据表"></a>9.显示当前数据库所有数据表</h1><pre><code class="hljs">MariaDB [(none)]&gt;  show tables;</code></pre><h1 id="10-显示表结构"><a href="#10-显示表结构" class="headerlink" title="10.显示表结构"></a>10.显示表结构</h1><pre><code class="hljs">MariaDB [(none)]&gt;  desc test;</code></pre><h1 id="11-数据表插入数据"><a href="#11-数据表插入数据" class="headerlink" title="11.数据表插入数据"></a>11.数据表插入数据</h1><pre><code class="hljs">MariaDB [(none)]&gt;  insert into test values (&#39;001&#39;,&#39;test001&#39;);</code></pre><h1 id="12-修改账户密码"><a href="#12-修改账户密码" class="headerlink" title="12.修改账户密码"></a>12.修改账户密码</h1><pre><code class="hljs">MariaDB [(none)]&gt;  use mysql;MariaDB [(none)]&gt;  update mysql.user set password=password(&#39;111111&#39;) where User=&quot;root&quot; and Host=&quot;localhost&quot;;# MariaDB [(none)]&gt;  update user set password=password(&quot;111111&quot;) where user=&quot;root&quot; flush privileges;# MariaDB [(none)]&gt;  alter  user &#39;root&#39;@&#39;localhost&#39; identified by &#39;111111&#39;;</code></pre><h1 id="13-数据库热备份"><a href="#13-数据库热备份" class="headerlink" title="13.数据库热备份"></a>13.数据库热备份</h1><pre><code class="hljs">/usr/local/mysql/bin/mysqldump -uroot -p111111 testdb &gt;/opt/buckups/data/testdb $(date +%Y%m%d_%H%M%S).sql/usr/local/mysql/bin/mysqldump -uroot -p111111--events --all-databases | gzip &gt; /opt/buckups/data/mysql.$(date +%Y%m%d).sql/usr/local/mysql/bin/mysqldump --host=127.0.0.1 --port=3306 --user=root -p111111 --all-databases --events --routines --compress --log-error=/tmp/mysqldump_error.log &gt;  /opt/buckups/data/mysql.sql</code></pre><h1 id="14-数据库导入"><a href="#14-数据库导入" class="headerlink" title="14.数据库导入"></a>14.数据库导入</h1><pre><code class="hljs">/usr/local/mysql/bin/mysql -uroot -p111111 testdb &lt; /opt/buckups/data/mysql.sql</code></pre><h1 id="15-查询数据库中数据量最大的前10个表"><a href="#15-查询数据库中数据量最大的前10个表" class="headerlink" title="15.查询数据库中数据量最大的前10个表"></a>15.查询数据库中数据量最大的前10个表</h1><pre><code class="hljs"> MariaDB [(none)]&gt;  use information_schema; MariaDB [(none)]&gt;  select table_name,table_rows from  tables order by table_rows desc limit 10;</code></pre><h1 id="16-批量插入10000条数据脚本"><a href="#16-批量插入10000条数据脚本" class="headerlink" title="16.批量插入10000条数据脚本"></a>16.批量插入10000条数据脚本</h1><pre><code class="hljs">#!/bin/bashi=1;while [ $i -le 10000 ]  domysql -usword -p111111 testdb -e &quot;insert into test (id,name) values (DATE_FORMAT(current_timestamp(3),&#39;%Y%m%d%H%i%s%f&#39;),substring(MD5(RAND()),1,20));&quot;  let    i=i+1  sleep 0.01doneexit 0</code></pre>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>MySQL</tag>
      
      <tag>SQL</tag>
      
      <tag>数据库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MySQL数据库服务器的安装与配置</title>
    <link href="/linux/MySQL/"/>
    <url>/linux/MySQL/</url>
    
    <content type="html"><![CDATA[<p>MySQL，是由瑞典MySQL AB公司开发的开源的关系型数据库管理系统，目前已被Oracle收购，全球范围内被广泛应用，其使用的SQL语言是用于访问数据库的最常用的标准化语言。MySQL分为社区版和商业版，由于其体积小、速度快、总体拥有成本低，尤其是开放源码这一特点，一般中小型和大型网站的开发都选择MySQL作为网站数据库。</p><h1 id="发展历史"><a href="#发展历史" class="headerlink" title="发展历史"></a>发展历史</h1><ul><li>1979年，MySQL创始人Michael Widenius在开发一个报表工具时设计了一套API，并将mSQL代码集成到存储引擎以支持SQL语句</li><li>1996年，MySQL 1.0发布，Michael Widenius女儿的简称即为MY。当年10月，MySQL 3.11.1发布了Solaris版本。11月，发布linux版本。此后，MySQL逐渐进入大众视野</li><li>1999年，Michael Widenius创立MySQL AB公司，MySQL由个人开发转变为团队开发，社区版个人免费使用，商业版付费使用</li><li>2000年，MySQL遵守GPL协议，将之开源，此举使得MySQL AB收入遭受巨大打击，损失了将近80%的收入，但依然被坚持了下来</li><li>2001年，存储引擎InnoDB诞生，逐渐成为MySQL首选的存储引擎，MySQL用户基数已达到三分之一市场份额的规模</li><li>2005年，Oracle收购InnoDB，InnoDB只能作为第三方插件以供使用</li><li>2008年1月，MySQL AB公司被Sun公司以10亿美金收购，MySQL数据库进入Sun时代。当年11月，MySQL 5.1发布，MySQL成为了最受欢迎的小型数据库。随后，Widenius离开了Sun</li><li>2009年4月，Oracle公司以74亿美元收购Sun公司，MySQL也随之进入Oracle时代</li></ul><hr><p>Oracle公司收购Sun公司的举动大大激怒了Monty Widenius，并极力反对，认为一家独大的Oracle将引起数据库市场的不良竞争，从而导致更高的价格。因此，他发起了著名的“Save MySQL”抗议活动，甚至还差点搅黄了Oracle收购Sun的交易。Oracle也不得不对MySQL许下若干承诺，才使得欧盟最终为收购案亮了绿灯。然而，Oracle的所作所为依然令他大为失望，虽然宣称MySQL依然遵守GPL协议，但却暗地里把开发人员全部换成内部人员，开源社区再也影响不了MySQL发展的脚步，而真正有心做贡献的人也被拒之门外，MySQL随时面临闭源的可能</p><p>有鉴于此，Monty Widenius当即以另外一个女儿Maria命名，创立了MariaDB。MariaDB是一个非商业化的永久免费的产品，用户如果愿意可以为它捐款，目前由MariaDB基金会来管理，发起者正是MySQL AB的三大创始人Monty Widenius、David Axmark和Allan Larsson。MariaDB完全兼容MySQL，包括API和命令行，此外扩展功能、存储引擎以及一些新的改进功能全面超越了MySQL，能够轻松成为MySQL的替代品。业界许多主流厂商也转而拥抱MariaDB，如Google、RedHat、SUSE、维基百科等，其光明的前景已是可以预见的一览无余</p><hr><h1 id="体系架构"><a href="#体系架构" class="headerlink" title="体系架构"></a>体系架构</h1><p>MySQL的逻辑架构分为四层，即连接层、SQL层、存储引擎层</p><h2 id="1-连接层"><a href="#1-连接层" class="headerlink" title="1.连接层"></a>1.连接层</h2><p>由Connection pool组件构成，即连接池组件，主要负责连接处理、用户鉴权、安全管理等工作</p><h3 id="1-1-连接处理"><a href="#1-1-连接处理" class="headerlink" title="1.1 连接处理"></a>1.1 连接处理</h3><p>客户端应用程序通过接口，如ODBC、JDBC等，向MySQL发送连接请求，MySQL将按照客户端连接通信协议接收请求建立TCP连接，并从线程池中分配线程来和客户端进行连接，同时将该连接的用户名、密码、权限校验、线程处理等信息缓存到连接池，该客户端的请求都会被分配到该线程负责的连接通道，即连接复用功能。从而减少了创建线程和释放线程所花费的时间，大大提升了服务器性能</p><h3 id="1-2-用户鉴权"><a href="#1-2-用户鉴权" class="headerlink" title="1.2 用户鉴权"></a>1.2 用户鉴权</h3><p>主要负责客户端连接用户的认证鉴权工作，如用户名、客户端主机地址和用户密码</p><h3 id="1-3-安全管理"><a href="#1-3-安全管理" class="headerlink" title="1.3 安全管理"></a>1.3 安全管理</h3><p>依据客户端连接用户的权限来判断用户具体可执行哪些操作</p><h2 id="2-SQL层"><a href="#2-SQL层" class="headerlink" title="2.SQL层"></a>2.SQL层</h2><p>MySQL核心服务，实现了数据库管理系统的所有逻辑功能，由以下组件构成</p><h3 id="2-1-MySQL-Management-Server-amp-utilities"><a href="#2-1-MySQL-Management-Server-amp-utilities" class="headerlink" title="2.1 MySQL Management Server &amp; utilities"></a>2.1 MySQL Management Server &amp; utilities</h3><p>管理服务器与公用事业组件，具体功能如下：</p><ul><li>数据库备份与还原</li><li>数据库安全管理，如用户及权限管理</li><li>数据库复制管理</li><li>数据库集群管理</li><li>数据库分区、分库、分表管理</li><li>数据库元数据管理</li></ul><h3 id="2-2-SQL-Interface"><a href="#2-2-SQL-Interface" class="headerlink" title="2.2 SQL Interface"></a>2.2 SQL Interface</h3><p>SQL接口组件，用于接收SQL命令及返回查询结果，具体功能如下：</p><ul><li>Data Manipulation Language (DML)</li><li>Data Definition Language (DDL)</li><li>存储过程</li><li>视图</li><li>触发器</li></ul><h3 id="2-3-SQL-Parser"><a href="#2-3-SQL-Parser" class="headerlink" title="2.3 SQL Parser"></a>2.3 SQL Parser</h3><p>SQL解析器组件，用于解析查询语句并最终生成语法树，同时识别SQL语句语法错误，并返回相应的错误信息。若语法检查通过，解析器则优先查询缓存，缓存命中则直接返回结果，不用继续解析与执行</p><h3 id="2-4-Optimizer"><a href="#2-4-Optimizer" class="headerlink" title="2.4 Optimizer"></a>2.4 Optimizer</h3><p>查询优化器组件，用于查询语句的优化，包括选择合适索引、数据读取方式等，查询策略为选取-投影-连接</p><h3 id="2-5-Caches-amp-buffers"><a href="#2-5-Caches-amp-buffers" class="headerlink" title="2.5 Caches &amp; buffers"></a>2.5 Caches &amp; buffers</h3><p>缓存组件，由一系列小缓存组成，如表缓存、记录缓存、key缓存、权限缓存等，提高查询的效率</p><h2 id="3-存储引擎层"><a href="#3-存储引擎层" class="headerlink" title="3.存储引擎层"></a>3.存储引擎层</h2><h3 id="3-1-存储引擎"><a href="#3-1-存储引擎" class="headerlink" title="3.1 存储引擎"></a>3.1 存储引擎</h3><p>存储引擎，即MySQL底层数据文件的组织、处理与存储机制，也就是数据的创建、查询、更新、存储方式，负责数据的存储和提取。存储引擎屏蔽了底层存储的细节，通过插件化的方式配置，可根据具体场景选择不同的存储引擎</p><h2 id="3-1-1-InnoDB"><a href="#3-1-1-InnoDB" class="headerlink" title="3.1.1 InnoDB"></a>3.1.1 InnoDB</h2><p>MySQL数据库的首先存储引擎，如无其他特殊需求建议选择，也是MySQL 5.5及之后的版本默认的存储引擎，之前默认为MyISAM</p><h3 id="3-2-文件系统"><a href="#3-2-文件系统" class="headerlink" title="3.2 文件系统"></a>3.2 文件系统</h3><hr><h1 id="1-安装依赖包"><a href="#1-安装依赖包" class="headerlink" title="1. 安装依赖包"></a>1. 安装依赖包</h1><pre><code class="hljs">yum install -y bison gdb perl ncurses-devel libaio numactlapt install -y libaio1 libaio-dev libncurses5</code></pre><h1 id="2-创建mysql用户"><a href="#2-创建mysql用户" class="headerlink" title="2. 创建mysql用户"></a>2. 创建mysql用户</h1><pre><code class="hljs">groupadd mysql &amp;&amp; useradd -g mysql -s /sbin/nologin mysql -M</code></pre><h1 id="3-安装mysql"><a href="#3-安装mysql" class="headerlink" title="3.安装mysql"></a>3.安装mysql</h1><pre><code class="hljs">tar -zxvf mysql-5.7.22-linux-glibc2.12-x86_64.tar.gzmv mysql-5.7.22-linux-glibc2.12-x86_64 /usr/local/mysql</code></pre><h1 id="4-初始化数据库"><a href="#4-初始化数据库" class="headerlink" title="4.初始化数据库"></a>4.初始化数据库</h1><pre><code class="hljs"># mysql 5.6和mariadb初始化数据库# /usr/local/mysql/scripts/mysql_install_db --basedir=/usr/local/mysql --datadir=/usr/local/mysql/data --user=mysql# mysql 5.7.6之后初始化数据库/usr/local/mysql/bin/mysqld --initialize-insecure --basedir=/usr/local/mysql --datadir=/usr/local/mysql/data --user=mysql</code></pre><h1 id="5-创建配置文件"><a href="#5-创建配置文件" class="headerlink" title="5.创建配置文件"></a>5.创建配置文件</h1><pre><code class="hljs">vi  /etc/my.cnf[client]port = 3306socket = /var/lib/mysql/mysql.sock[mysqld]port = 3306basedir = /usr/local/mysqldatadir = /usr/local/mysql/datasocket = /var/lib/mysql/mysql.sockpid-file = /var/lib/mysql/mysql.pidlog-error = /usr/local/mysql/data/mysql-server.log# mariadb无需配置log_timestamps = systemmax_connections = 4096default-storage-engine = InnoDBcharacter-set-server = utf8collation-server = utf8_general_cisymbolic-links = 0innodb_file_per_table = 1skip-external-lockingwait_timeout = 300interactive_timeout = 600</code></pre><h1 id="6-配置环境变量"><a href="#6-配置环境变量" class="headerlink" title="6. 配置环境变量"></a>6. 配置环境变量</h1><pre><code class="hljs"># echo &#39;PATH=/usr/local/mysql/bin/:$PATH&#39; &gt;&gt;/etc/profile &amp;&amp; source /etc/profileln -s /usr/local/mysql/bin/mysql /usr/bin</code></pre><h1 id="7-配置mysql动态库"><a href="#7-配置mysql动态库" class="headerlink" title="7.配置mysql动态库"></a>7.配置mysql动态库</h1><pre><code class="hljs">echo &quot;/usr/local/mysql/lib&quot; &gt;&gt;/etc/ld.so.conf.d/mysql.confldconfig</code></pre><h1 id="8-配置mysql启动服务"><a href="#8-配置mysql启动服务" class="headerlink" title="8.配置mysql启动服务"></a>8.配置mysql启动服务</h1><pre><code class="hljs">mkdir -p /var/lib/mysqlcp /usr/local/mysql/support-files/mysql.server /etc/init.d/mysqldchown -R mysql.mysql /usr/local/mysql /var/lib/mysqlchmod +x /etc/init.d/mysqld# 启动mysql数据库服务器/etc/init.d/mysqld startchkconfig mysqld on</code></pre><h1 id="9-设置mysql数据库root密码"><a href="#9-设置mysql数据库root密码" class="headerlink" title="9.设置mysql数据库root密码"></a>9.设置mysql数据库root密码</h1><pre><code class="hljs">/usr/local/mysql/bin/mysqladmin -u root password &#39;123456&#39;</code></pre>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>MySQL</tag>
      
      <tag>SQL</tag>
      
      <tag>数据库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux系统部署NFS文件存储服务器</title>
    <link href="/linux/NFS/"/>
    <url>/linux/NFS/</url>
    
    <content type="html"><![CDATA[<h1 id="1-部署NFS服务器"><a href="#1-部署NFS服务器" class="headerlink" title="1. 部署NFS服务器"></a>1. 部署NFS服务器</h1><pre><code class="hljs">yum install -y nfs-utilsapt install -y nfs-kernel-server</code></pre><h2 id="1-2-创建共享目录"><a href="#1-2-创建共享目录" class="headerlink" title="1.2 创建共享目录"></a>1.2 创建共享目录</h2><pre><code class="hljs">mkdir -p /data &amp;&amp; chmod 755 /data</code></pre><h2 id="1-3-创建配置文件"><a href="#1-3-创建配置文件" class="headerlink" title="1.3 创建配置文件"></a>1.3 创建配置文件</h2><pre><code class="hljs">vi /etc/exports/data 172.16.100.0/24(rw,no_root_squash,sync,insecure,no_subtree_check,no_root_squash)</code></pre><h2 id="1-4-配置文件生效"><a href="#1-4-配置文件生效" class="headerlink" title="1.4 配置文件生效"></a>1.4 配置文件生效</h2><pre><code class="hljs">exportfs -r</code></pre><h2 id="1-5-启动NFS服务器"><a href="#1-5-启动NFS服务器" class="headerlink" title="1.5 启动NFS服务器"></a>1.5 启动NFS服务器</h2><pre><code class="hljs">systemctl start nfs.servicesystemctl enable nfs.service</code></pre><h2 id="1-6-测试NFS服务连接"><a href="#1-6-测试NFS服务连接" class="headerlink" title="1.6 测试NFS服务连接"></a>1.6 测试NFS服务连接</h2><pre><code class="hljs">showmount -e localhost </code></pre><h1 id="2-部署NFS客户端"><a href="#2-部署NFS客户端" class="headerlink" title="2.部署NFS客户端"></a>2.部署NFS客户端</h1><h2 id="2-1-安装NFS客户端"><a href="#2-1-安装NFS客户端" class="headerlink" title="2.1 安装NFS客户端"></a>2.1 安装NFS客户端</h2><pre><code class="hljs">yum install -y nfs-utilsapt install -y nfs-common</code></pre><h2 id="2-2-创建NFS的挂载目录"><a href="#2-2-创建NFS的挂载目录" class="headerlink" title="2.2 创建NFS的挂载目录"></a>2.2 创建NFS的挂载目录</h2><pre><code class="hljs">mkdir -p /data</code></pre><h2 id="2-3-挂载共享目录"><a href="#2-3-挂载共享目录" class="headerlink" title="2.3 挂载共享目录"></a>2.3 挂载共享目录</h2><pre><code class="hljs"># NFS默认用UDP协议，使用TCP协议挂载可以提高NFS的稳定性mount -t nfs -o proto=tcp -o nolock 172.16.100.100:/data /data </code></pre><h1 id="3-NFS配置参数详解"><a href="#3-NFS配置参数详解" class="headerlink" title="3.NFS配置参数详解"></a>3.NFS配置参数详解</h1><ul><li><p>rw&#x2F;ro，设置共享目录的权限，读写 (read-write) 或只读 (read-only)</p></li><li><p>sync&#x2F;async，设置数据同步方式，sync数据同步写入到内存与硬盘中，async则先暂存于内存而非直接写入硬盘</p></li><li><p>no_root_squash&#x2F;root_squash，设置客户端压缩所用账号，root_squash，root账号为root；nfsnobody，以保障NFS服务器的安全；no_root_squash，客户端使用 root 身份来操作服务器；all_squash，压缩所有NFS连接账户为匿名账户nobody(nfsnobody)</p></li><li><p>anonuid，anongid，anon，指anonymous (匿名者) ，关于 *_squash 提到的匿名用户的 UID 设定值，通常为 nobody(nfsnobody)<br>anonuid，指的是 UID；anongid则是群组gid，UID 必需要存在于&#x2F;etc&#x2F;passw</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>存储</tag>
      
      <tag>文件存储</tag>
      
      <tag>NFS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PHP工作模式详解</title>
    <link href="/linux/PhpWorkingMode/"/>
    <url>/linux/PhpWorkingMode/</url>
    
    <content type="html"><![CDATA[<p>PHP有三种工作模式，即CLI模式、Module模式和PHP-FPM模式</p><hr><h1 id="1-CLI模式"><a href="#1-CLI模式" class="headerlink" title="1.CLI模式"></a>1.CLI模式</h1><p>CLI，Command Line Interface，即命令行接口，默认安装，通过此接口可在shell环境与PHP交互，就像使用shell、Python一样，不依赖于WEB服务器。如Laravel框架的Artisan命令行工具，其实就是一个PHP脚本，用于快速构建Laravel应用</p><h1 id="2-Module模式"><a href="#2-Module模式" class="headerlink" title="2.Module模式"></a>2.Module模式</h1><p>此模式即是将PHP以模块的形式集成到Apace服务器供Apache加载调用，模块名为php5_module，接收PHP请求并进行处理，然后将处理结果返回给Apache。LAMP组合的PHP即是工作于此模式，但随着Nginx的异军突起，已不再是PHP开发者的第一选择，逐渐被LNMP组合所代替</p><h2 id="2-1-编译安装PHP"><a href="#2-1-编译安装PHP" class="headerlink" title="2.1 编译安装PHP"></a>2.1 编译安装PHP</h2><pre><code class="hljs">./configure --prefix=/usr/local/php --with-config-file-path=/etc --with-apxs2=/usr/local/httpd/bin/apxs \--with-mysqli=mysqlnd --with-pdo-mysql=mysqlnd --enable-opcache --with-openssl --with-pcre --enable-inline-optimization make &amp;&amp; make install</code></pre><h2 id="2-2-配置Apache加载PHP模块"><a href="#2-2-配置Apache加载PHP模块" class="headerlink" title="2.2 配置Apache加载PHP模块"></a>2.2 配置Apache加载PHP模块</h2><pre><code class="hljs">vi /etc/httpd/conf/httpd.conf# 设置启用mod_php5模块LoadModule php5_module modules/libphp5.so# 设置服务器解析PHP文件&lt;FilesMatch \.php$&gt;  SetHandler application/x-httpd-php&lt;/FilesMatch&gt;</code></pre><h1 id="3-PHP-FPM模式"><a href="#3-PHP-FPM模式" class="headerlink" title="3. PHP-FPM模式"></a>3. PHP-FPM模式</h1><p>PHP-FPM，PHP FastCGI Process Manager，即PHP FastCGI进程管理器，是PHP对FastCGI（Fast Common Gateway Interface，快速通用网关接口）通信协议的具体实现，用于PHP脚本与WEB服务器之间的通信，致力于减少网页服务器与CGI程序之间交互的开销，从而使WEB服务器得以支持高更的并发量</p><p>PHP-FPM负责管理维护一个进程池，用来处理来自WEB服务器的HTTP动态请求。其中，master主进程负责与web服务器进行通信，接收HTTP请求，再将请求转发给worker进程进行处理；worker进程主要负责动态执行PHP代码，处理完成后，将处理结果返回给web服务器，再由web服务器将结果发送给客户端</p><p>PHP-FPM监听方式有两种，即TCP socket(IP和port)，监听端口为9000；Unix Domain Socket（UDS)，需指明socket位置</p><h2 id="3-1-编译安装PHP"><a href="#3-1-编译安装PHP" class="headerlink" title="3.1 编译安装PHP"></a>3.1 编译安装PHP</h2><pre><code class="hljs">./configure --prefix=/usr/local/php --with-config-file-path=/etc --enable-fpm --with-fpm-user=fpm --with-fpm-group=fpm \--with-mysqli=mysqlnd --with-pdo-mysql=mysqlnd --enable-opcache --with-openssl --with-pcre --enable-inline-optimization make &amp;&amp; make install</code></pre><h2 id="3-2-配置php-fpm服务"><a href="#3-2-配置php-fpm服务" class="headerlink" title="3.2 配置php-fpm服务"></a>3.2 配置php-fpm服务</h2><pre><code class="hljs">cp /usr/local/php/etc/php-fpm.conf.default /usr/local/php/etc/php-fpm.confcp php.ini-production /etc/php.inicp sapi/fpm/init.d.php-fpm /etc/init.d/php-fpmchmod +x /etc/init.d/php-fpm</code></pre><h2 id="3-1-配置Apache对PHP-FPM的支持"><a href="#3-1-配置Apache对PHP-FPM的支持" class="headerlink" title="3.1 配置Apache对PHP-FPM的支持"></a>3.1 配置Apache对PHP-FPM的支持</h2><pre><code class="hljs">vi /etc/httpd/conf/httpd.conf# 设置启用代理模块LoadModule proxy_module modules/mod_proxy.so# 设置启用fcgi代理模块LoadModule proxy_fcgi_module modules/mod_proxy_fcgi.so# 设置启用tcp socket监听方式&lt;FilesMatch \.php$&gt;  SetHandler &quot;proxy:fcgi://127.0.0.1:9000&quot;&lt;/FilesMatch&gt;# 设置启用Unix Domain Socket监听方式# &lt;Proxy &quot;unix:/dev/shm/php-fpm.sock|fcgi://php-fpm&quot;&gt;  # ProxySet disablereuse=off# &lt;/Proxy&gt;</code></pre><h2 id="3-2-配置Nginx对PHP-FPM的支持"><a href="#3-2-配置Nginx对PHP-FPM的支持" class="headerlink" title="3.2 配置Nginx对PHP-FPM的支持"></a>3.2 配置Nginx对PHP-FPM的支持</h2><pre><code class="hljs">vi /etc/nginx/nginx.confserver &#123;    listen       80;    server_name  localhost;    #charset koi8-r;    #access_log  logs/host.access.log  main;    location / &#123;        root   html;        index  index.html index.htm;    &#125;    #error_page  404              /404.html;    # redirect server error pages to the static page /50x.html    #    error_page   500 502 503 504  /50x.html;    location = /50x.html &#123;        root   html;    &#125;    # proxy the PHP scripts to Apache listening on 127.0.0.1:80      #location ~ \.php$ &#123;    #    proxy_pass   http://127.0.0.1;    #&#125;    # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000        location ~ \.php$ &#123;        root           html;        fastcgi_pass   127.0.0.1:9000;        fastcgi_index  index.php;        fastcgi_param  SCRIPT_FILENAME  /scripts$fastcgi_script_name;        include        fastcgi_params;    &#125;&#125;</code></pre>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>PHP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PHP-FPM编译安装</title>
    <link href="/linux/PhpFpm/"/>
    <url>/linux/PhpFpm/</url>
    
    <content type="html"><![CDATA[<h1 id="1-安装依赖包"><a href="#1-安装依赖包" class="headerlink" title="1.安装依赖包"></a>1.安装依赖包</h1><pre><code class="hljs">yum install -y gcc gcc-c++ make autoconf epel-release pcre-devel openssl-devel perl-devel \libzip-devel bzip2-devel sqlite-devel  libxml2-devel libcurl-devel libicu-devel libxslt-devel \libsodium-devel oniguruma-devel libjpeg-devel libpng-devel freetype-develapt install -y gcc g++ make autoconf libssl-dev libpcre3-dev libsqlite3-dev libzip-dev \libbz2-dev zlib1g-dev libxml2-dev libcurl4-openssl-dev libonig-dev libxslt1-dev libpng-dev \libsodium-dev libjpeg-dev libfreetype6-dev pkg-config</code></pre><h1 id="2-编译安装php-fpm"><a href="#2-编译安装php-fpm" class="headerlink" title="2.编译安装php-fpm"></a>2.编译安装php-fpm</h1><pre><code class="hljs">tar -zxvf php-8.1.8.tar.gz &amp;&amp; cd php-8.1.8./configure --prefix=/usr/local/php --with-config-file-path=/etc --enable-fpm --with-fpm-user=www --with-fpm-group=www \--with-mysqli --with-pdo-mysql --with-bz2 --with-zip --disable-ipv6 --enable-xml --with-curl --enable-calendar --enable-gd --with-xsl \--with-openssl --with-zlib --enable-mbstring --with-gettext --enable-bcmath --enable-sockets --with-libdir=lib64 --without-pear --disable-rpath  \--with-sodium --enable-opcache --enable-dom --with-libxml --disable-debug --enable-intl --with-jpeg --with-freetypemake &amp;&amp; make install</code></pre><h1 id="3-创建配置文件"><a href="#3-创建配置文件" class="headerlink" title="3.创建配置文件"></a>3.创建配置文件</h1><pre><code class="hljs">cp /usr/local/php/etc/php-fpm.conf.default /usr/local/php/etc/php-fpm.confcp /usr/local/php/etc/php-fpm.d/www.conf.default /usr/local/php/etc/php-fpm.d/www.confcp php.ini-production /etc/php.ini</code></pre><h1 id="4-配置php-fpm服务"><a href="#4-配置php-fpm服务" class="headerlink" title="4.配置php-fpm服务"></a>4.配置php-fpm服务</h1><pre><code class="hljs">cp sapi/fpm/init.d.php-fpm /etc/init.d/php-fpmchmod +x /etc/init.d/php-fpm</code></pre><h1 id="5-修改配置文件"><a href="#5-修改配置文件" class="headerlink" title="5.修改配置文件"></a>5.修改配置文件</h1><pre><code class="hljs">sed -i &#39;s@;date.timezone =@date.timezone = Asia/Shanghai@g&#39; /etc/php.inised -i &#39;s@max_execution_time = 30@max_execution_time = 300@g&#39; /etc/php.inised -i &#39;s@post_max_size = 8M@post_max_size = 32M@g&#39; /etc/php.inised -i &#39;s@max_input_time = 60@max_input_time = 300@g&#39; /etc/php.inised -i &#39;s@memory_limit = 128M@memory_limit = 128M@g&#39; /etc/php.inised -i &#39;s@;always_populate_raw_post_data = -1@always_populate_raw_post_data = -1@g&#39; /etc/php.ini</code></pre><h1 id="6-配置opcache缓存"><a href="#6-配置opcache缓存" class="headerlink" title="6.配置opcache缓存"></a>6.配置opcache缓存</h1><pre><code class="hljs">opcahce=`find /usr/local/php -name &#39;opcache.so&#39;` &amp;&amp; echo zend_extension=$opcahce &gt;&gt; /etc/php.ini</code></pre><h1 id="7-启动php-fpm服务"><a href="#7-启动php-fpm服务" class="headerlink" title="7.启动php-fpm服务"></a>7.启动php-fpm服务</h1><pre><code class="hljs">/etc/init.d/php-fpm start</code></pre>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>PHP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PHP编译安装</title>
    <link href="/linux/PHP/"/>
    <url>/linux/PHP/</url>
    
    <content type="html"><![CDATA[<p>PHP，Hypertext Preprocessor，即超文本预处理器，广泛应用于服务端的开源脚本程序语言，尤其适用于Web开发并可嵌入到HTM。PHP于1994年由Rasmus Lerdorf开发 ，最初只是一个简单的用Perl语言编写的统计自己网站访问量的程序。其后重用C语言编写，1995年发布的PHP2加入了对MySQL数据库的支持。此后，越来越多的网站开始使用PHP，不断有其他开发者对其进行重构，使得PHP性能不断提高，已经可以应用在TCP&#x2F;UDP服务、高性能Web、WebSocket服务、物联网、实时通讯、游戏、微服务等非Web领域的系统研发</p><p>目前，业界主流网站服务器架构为LAMP或LNMP，即linux操作系统、Apache&#x2F;Nginx web服务器、MySQL数据服务器、PHP网页编程语言。这些软件全部是开源免费的，大大的节省了企业成本，所以得到了广泛的应用。根据W3Techs2019年12月6号发布的统计数据，PHP在WEB网站服务器端使用的编程语言所占份额高达78.9%，在内容管理系统的网站中，有58.7%的网站使用WordPress（PHP开发的CMS系统），在所有网站的占比为25.0%</p><hr><h1 id="体系架构"><a href="#体系架构" class="headerlink" title="体系架构"></a>体系架构</h1><p>PHP主要由四层体系构成，从下到上依次是Zend引擎、Extensions、SAPI、Application</p><h2 id="Zend引擎"><a href="#Zend引擎" class="headerlink" title="Zend引擎"></a>Zend引擎</h2><p>php的内核，由C语言实现，将php代码翻译（词法、语法解析等一系列编译过程）为可执行的中间语言opcode，从而实现PHP代码所表达的功能。此外，还实现了基本的数据结构（如hashtable、oo）、内存分配及管理，提供了相应的api方法供外部调用，所有的外围功能均围绕其实现</p><h2 id="Extensions"><a href="#Extensions" class="headerlink" title="Extensions"></a>Extensions</h2><p>围绕zend引擎，通过组件方式提供各种基础服务，如各种内置函数（array系列）、标准库等都通过其实现，用户也可以根据需要实现自己的extension以达到功能扩展、性能优化等目的，如贴吧正在使用的php中间层、富文本解析就是extension典型应用</p><h2 id="Sapi"><a href="#Sapi" class="headerlink" title="Sapi"></a>Sapi</h2><p>Server Application Programming Interface，即服务端应用编程接口，通过一系列钩子函数使得php可以和外围交互数据，可以看作是PHP和外部环境的代理器，是非常优雅和成功的一个设计。Sapi把外部环境抽象后，为内部的PHP提供一套固定的，统一的接口，使得 PHP 自身实现能够不受错综复杂的外部环境影响，保持一定的独立性。通过将php本身和上层应用解耦隔离，可以不再考虑如何针对不同应用进行兼容，而应用本身也可以针对自己的特点实现不同的处理方式</p><h2 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h2><p>即所编写的php程序，通过不同的sapi方式得到各种各样的应用模式，如通过webserver实现web应用、在命令行下以脚本方式运行等</p><hr><h1 id="1-安装依赖包"><a href="#1-安装依赖包" class="headerlink" title="1.安装依赖包"></a>1.安装依赖包</h1><pre><code class="hljs">yum install -y gcc gcc-c++ make cmake pcre-devel openssl-devel libzip-devel \bzip2-devel sqlite-devel perl-devel libxml2-devel libcurl-devel libicu-devel libxslt-devel \libjpeg-devel libpng-devel freetype-devel oniguruma-develapt install -y gcc g++ make libssl-dev libpcre3-dev libsqlite3-dev libzip-dev \libbz2-dev zlib1g-dev libxml2-dev libcurl4-openssl-dev libonig-dev libxslt1-dev libpng-dev \libjpeg-dev libfreetype6-dev </code></pre><h1 id="2-编译安装php"><a href="#2-编译安装php" class="headerlink" title="2.编译安装php"></a>2.编译安装php</h1><pre><code class="hljs">tar -zxvf php-8.1.8.tar.gz &amp;&amp; cd php-8.1.8./configure --prefix=/usr/local/php --with-apxs2=/usr/bin/apxs --with-config-file-path=/etc \--with-mysqli --with-pdo-mysql --with-bz2 --with-zip --enable-xml --with-curl --with-zlib --with-openssl \--enable-mbstring --with-gettext --enable-bcmath --enable-sockets --enable-opcache --with-libxml --enable-intl \--enable-dom --enable-calendar --enable-gd --with-jpeg --with-freetype --with-xsl --with-libdir=lib64 --disable-debug \--without-pear --disable-rpath --disable-ipv6make &amp;&amp; make install</code></pre><h1 id="3-创建配置文件"><a href="#3-创建配置文件" class="headerlink" title="3.创建配置文件"></a>3.创建配置文件</h1><pre><code class="hljs">cp php.ini-production /etc/php.ini -r</code></pre><h1 id="4-修改配置文件"><a href="#4-修改配置文件" class="headerlink" title="4.修改配置文件"></a>4.修改配置文件</h1><pre><code class="hljs">sed -i &#39;s@;date.timezone =@date.timezone = Asia/Shanghai@g&#39; /etc/php.inised -i &#39;s@max_execution_time = 30@max_execution_time = 300@g&#39; /etc/php.inised -i &#39;s@post_max_size = 8M@post_max_size = 32M@g&#39; /etc/php.inised -i &#39;s@max_input_time = 60@max_input_time = 300@g&#39; /etc/php.inised -i &#39;s@memory_limit = 128M@memory_limit = 128M@g&#39; /etc/php.inised -i &#39;s@;always_populate_raw_post_data = -1@always_populate_raw_post_data = -1@g&#39; /etc/php.ini</code></pre><h1 id="5-配置opcache缓存"><a href="#5-配置opcache缓存" class="headerlink" title="5.配置opcache缓存"></a>5.配置opcache缓存</h1><pre><code class="hljs">opcahce=`find /usr/local/php -name &#39;opcache.so&#39;`echo zend_extension=$opcahce &gt;&gt; /etc/php.ini</code></pre>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>PHP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Nginx日志详解</title>
    <link href="/linux/NginxLogs/"/>
    <url>/linux/NginxLogs/</url>
    
    <content type="html"><![CDATA[<p>nginx日志分为两种，即访问日志和错误日志</p><h1 id="1-访问日志"><a href="#1-访问日志" class="headerlink" title="1.访问日志"></a>1.访问日志</h1><p>访问日志，access_log，用于记录客户端的请求，如客户端IP地址、浏览器信息、请求信息等等，对于统计分析作用巨大</p><h2 id="1-1-格式设置"><a href="#1-1-格式设置" class="headerlink" title="1.1 格式设置"></a>1.1 格式设置</h2><p>访问日志由log_format指令定义，用于自定义日志格式，如存放路径、类型、缓存大小等，默认参数格式为：</p><pre><code class="hljs">‘$remote_addr – $remote_user [$time_local] “$request” ‘‘$status $body_bytes_sent “$http_referer” ‘‘”$http_user_agent” “$http_x_forwarded_for”‘;</code></pre><hr><p>设置参数：</p><ul><li><p>$remote_addr，http客户端的IP地址，如 112.10.24.36</p></li><li><p>$remote_user，http客户端用户名称，一般为空</p></li><li><p>$time_local，http客户端访问时间和时区，如 03&#x2F;Jan&#x2F;2020:17:15:35 +0800</p></li><li><p>$request，http请求的URI和HTTP协议，如 “GET &#x2F;jpress&#x2F;static&#x2F;components&#x2F;layer&#x2F;theme&#x2F;jpress&#x2F;style.css HTTP&#x2F;1.1”</p></li><li><p>$status，http请求的响应状态码，如200、404</p></li><li><p>$bytes_sent，nginx返回给客户端数据的字节数，即响应报文，包括响应头和响应体</p></li><li><p>$body_bytes_sent，nginx返回给客户端数据的响应体的字节数，不包含响应头</p></li><li><p>$http_referer，url跳转来源，如 <a href="https://49.232.97.179/jpress/admin">https://49.232.97.179/jpress/admin</a></p></li><li><p>$http_user_agent，客户端浏览器等信息，如”Mozilla&#x2F;5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;79.0.3945.88 Safari&#x2F;537.36”</p></li><li><p>$http_x_forwarded_for，web服务器记录的客户端地址，适用于前端有代理服务器，且代理服务器http_x_forwarded_for也需配置</p></li><li><p>$ssl_protocol，nginx SSL协议版本，如TLSv1</p></li><li><p>$ssl_cipher，nginx SSL交换数据中的算法，如RC4-SHA</p></li><li><p>$upstream_addr，后端web服务器地址，即真正提供服务的主机地址，如 172.21.0.7:8088</p></li><li><p>$upstream_status，后端web服务器的响应状态码，如200、500</p></li><li><p>$request_time，nginx完成本次请求的时长，包括接收http请求时间、后端web服务器响应时间、将响应数据传给客户端的时间</p></li><li><p>$upstream_response_time，后端服务器的响应时长，包括nginx与后端web服务器建立连接的时间、nginx接收响应数据的时间、关闭连接的时间</p></li><li><p>$request_length，http客户端请求报文的字节数，包括请求行、请求头和请求体。其值在请求解析过程中不断累加，若解析请求时出现异常或提前完成，则$request_length只是已经累加部分的长度，而不是nginx从客户端收到的完整请求的总字节数</p></li><li><p>$http_host，http客户端的请求地址，即浏览器输入的地址（IP或域名），如 49.232.97.179</p></li><li><p>$connection              TCP连接的序列号</p></li><li><p>$connection_requests     TCP连接当前的请求数量</p></li><li><p>$content_length          “Content-Length” 请求头字段</p></li><li><p>$content_type            “Content-Type” 请求头字段</p></li><li><p>$cookie_name             cookie名称</p></li><li><p>$nginx_version           nginx版本</p></li><li><p>$proxy_protocol_addr     获取代理访问服务器的客户端地址，如果是直接访问，该值为空字符串</p></li><li><p>$remote_port             客户端端口</p></li><li><p>$scheme                  请求使用的Web协议，”http” 或 “https”</p></li><li><p>$server_addr             服务器端地址，需要注意的是：为了避免访问linux系统内核，应将ip地址提前设置在配置文件中</p></li><li><p>$server_name             服务器名</p></li><li><p>$server_port             服务器端口</p></li><li><p>$server_protocol         服务器的HTTP版本，通常为 “HTTP&#x2F;1.0” 或 “HTTP&#x2F;1.1”</p></li><li><p>$cookie_NAME             客户端请求Header头中的cookie变量，前缀”$cookie_”加上cookie名称的变量，该变量的值即为cookie名称的值</p></li><li><p>$http_NAME               匹配任意请求头字段；变量名中的后半部分NAME可以替换成任意请求头字段，如在配置文件中需要获取http请求头：”Accept-Language”,使用$http_accept_language即可</p></li><li><p>$sent_http_NAME          可以设置任意http响应头字段；变量名中的后半部分NAME可以替换成任意响应头字段，如需要设置响应头Content-length，$sent_http_content_length即可</p></li><li><p>$sent_http_cache_control</p></li><li><p>$sent_http_connection</p></li><li><p>$sent_http_content_type</p></li><li><p>$sent_http_keep_alive</p></li><li><p>$sent_http_last_modified</p></li><li><p>$sent_http_location</p></li><li><p>$sent_http_transfer_encoding</p><p>  log_format  main ‘$remote_addr - $remote_user [$time_local] “$request” ‘<br>               ‘$status $bytes_sent $body_bytes_sent “$http_referer” ‘<br>               ‘“$http_user_agent” “$http_x_forwarded_for” ‘<br>               ‘“$request_time” “$upstream_response_time” “$upstream_status” “$upstream_addr” $http_host’;</p></li></ul><h2 id="1-2-nginx访问日志分析"><a href="#1-2-nginx访问日志分析" class="headerlink" title="1.2 nginx访问日志分析"></a>1.2 nginx访问日志分析</h2><p>分析、统计网站的流量数据是非常重要的工作，可以了解网站当前的访问效果和访问用户行为并发现当前网络营销活动中存在的问题，并为进一步修正或重新制定网络营销策略提供依据。nginx通常作为业务系统的前端入口，其访问日志就包含了大量的值得分析的数据指标</p><h3 id="1-2-1-常用网站流量指标"><a href="#1-2-1-常用网站流量指标" class="headerlink" title="1.2.1 常用网站流量指标"></a>1.2.1 常用网站流量指标</h3><h4 id="网站并发连接数"><a href="#网站并发连接数" class="headerlink" title="网站并发连接数"></a>网站并发连接数</h4><p>网站服务器单位时间内能够处理的最大连接数，如某网站的并发为5000，则意味着单位时间内（1秒或数秒）正在<br>处理的连接数与正在建立的连接数之和</p><h4 id="IP"><a href="#IP" class="headerlink" title="IP"></a>IP</h4><p>即客户端IP地址，一般指独立IP数，即不同IP地址的计算机访问网站的总次数，一般24小时内（00:00-24:00）相同IP地址只被计入一次</p><h4 id="PV"><a href="#PV" class="headerlink" title="PV"></a>PV</h4><p>Page View，即页面浏览量或点击量，无论IP、浏览器、网站页面是否相同，只要访问一次网站页面就会被计入一次PV</p><h4 id="UV"><a href="#UV" class="headerlink" title="UV"></a>UV</h4><p>Unique Visitor，同一客户端（PC或移动端）访问网站被计为一个访客，24小时内（00:00-24:00）相同的客户端访问同一个网站只统计一次UV。UV一般是以客户端Cookie等技术作为统计依据，实际统计会有误差</p><h3 id="1-2-2-常用分析命令"><a href="#1-2-2-常用分析命令" class="headerlink" title="1.2.2 常用分析命令"></a>1.2.2 常用分析命令</h3><h4 id="总请求数"><a href="#总请求数" class="headerlink" title="总请求数"></a>总请求数</h4><pre><code class="hljs">wc -l  access.log |awk &#39;&#123;print $1&#125;&#39;</code></pre><h4 id="独立IP数"><a href="#独立IP数" class="headerlink" title="独立IP数"></a>独立IP数</h4><pre><code class="hljs">awk &#39;&#123;print $1&#125;&#39; access.log|sort |uniq |wc -l</code></pre><h4 id="每秒客户端请求数-TOP5"><a href="#每秒客户端请求数-TOP5" class="headerlink" title="每秒客户端请求数 TOP5"></a>每秒客户端请求数 TOP5</h4><pre><code class="hljs">awk &#39;&#123;print $6&#125;&#39; access.log|sort|uniq -c|sort -rn|head -5</code></pre><h4 id="访问最频繁IP-Top5"><a href="#访问最频繁IP-Top5" class="headerlink" title="访问最频繁IP Top5"></a>访问最频繁IP Top5</h4><pre><code class="hljs">awk &#39;&#123;print $1&#125;&#39; access.log|sort |uniq -c |sort -nr |head -5</code></pre><h4 id="访问最频繁的URL-TOP5"><a href="#访问最频繁的URL-TOP5" class="headerlink" title="访问最频繁的URL TOP5"></a>访问最频繁的URL TOP5</h4><pre><code class="hljs">awk &#39;&#123;print $7&#125;&#39; access.log|sort |uniq -c |sort -nr |head -5</code></pre><h4 id="响应大于5秒的URL-TOP5"><a href="#响应大于5秒的URL-TOP5" class="headerlink" title="响应大于5秒的URL TOP5"></a>响应大于5秒的URL TOP5</h4><pre><code class="hljs">awk &#39;&#123;if ($7 &gt; 5)&#123;print $6&#125;&#125;&#39; access.log|sort|uniq -c|sort -rn |head -5</code></pre><h4 id="HTTP状态码-非200-统计-Top5"><a href="#HTTP状态码-非200-统计-Top5" class="headerlink" title="HTTP状态码(非200)统计 Top5"></a>HTTP状态码(非200)统计 Top5</h4><pre><code class="hljs">awk &#39;&#123;if ($11 != 200)&#123;print $11&#125;&#125;&#39; access.log|sort|uniq -c|sort -rn|head -5</code></pre><h4 id="请求数大于50000的源IP"><a href="#请求数大于50000的源IP" class="headerlink" title="请求数大于50000的源IP"></a>请求数大于50000的源IP</h4><pre><code class="hljs">cat access.log|awk &#39;&#123;print $NF&#125;&#39;|sort |uniq -c |sort -nr|awk &#39;&#123;if ($1 &gt;50000)&#123;print $2&#125;&#125;&#39;</code></pre><h1 id="2-错误日志"><a href="#2-错误日志" class="headerlink" title="2.错误日志"></a>2.错误日志</h1><p>错误日志，error_log，用于记录服务器和请求处理过程中的错误信息，对于错误原因定位意义重大。错误日志格式由error_log指令定义，不支持自定义，只能定义错误日志记录的等级及其存储路径，等级分为debug、info、notice、warn、error、crit几种，日志详细程度逐级递减，默认级别为error</p>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>负载均衡</tag>
      
      <tag>日志分析</tag>
      
      <tag>Nginx</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Nginx基于自签名SSL证书实现https</title>
    <link href="/linux/NginxSSL/"/>
    <url>/linux/NginxSSL/</url>
    
    <content type="html"><![CDATA[<p>HTTP，HyperText Transfer Protocol，即超文本传输协议，是一种用于分布式、协作式和超媒体信息系统的应用层协议。简单来说就是一种发布和接收HTML页面的方法，被用于在Web浏览器和网站服务器之间传递信息。HTTP默认工作在TCP协议80端口，通过http:&#x2F;&#x2F;打头访问的都是标准HTTP服务。HTTP协议缺陷在于以明文方式发送内容，没有提供任何方式的数据加密，若攻击者截取了Web浏览器和网站服务器之间的传输报文，就可以直接截取其中的信息。所以，HTTP协议不适合传输一些敏感信息，如信用卡号、密码、支付信息等</p><p>HTTPS，Hypertext Transfer Protocol Secure，即超文本传输安全协议，是HTTP的安全版，实质上就是在HTTP通信的基础上集成SSL&#x2F;TLS以加密数据包，即HTTPS&#x3D;HTTP+SSL，主要目的是提供对网站服务器的身份认证，以保护交换数据的隐私与完整性。HTTPS默认工作在TCP协议443端口，https:&#x2F;&#x2F;打头的网站都是HTTPS服务</p><h1 id="SSL原理"><a href="#SSL原理" class="headerlink" title="SSL原理"></a>SSL原理</h1><p>SSL&#x2F;TLS协议采用公钥加密法（最著名的是RSA加密算法），具体流程为：客户端向服务器索要公钥，然后用公钥加密信息，服务器收到密文，用自己的私钥解密即可</p><p>SSL证书，即遵守SSL协议的数字证书，因部署在服务器上也称服务器SSL证书，服务器通过证书管理公钥与私钥，一般由全球信任的证书颁发机构(CA)验证服务器身份后颁发，通过证书启动SSL安全通道以实现加密传输。向权威机构申请证书需要额外的费用，也可自己制作，即自签名证书，但不受信任，客户端访问的时候会弹出告警信息，需要客户端验证通过才能继续访问</p><h1 id="HTTPS工作流程"><a href="#HTTPS工作流程" class="headerlink" title="HTTPS工作流程"></a>HTTPS工作流程</h1><h2 id="1-客户端发起HTTPS请求"><a href="#1-客户端发起HTTPS请求" class="headerlink" title="1.客户端发起HTTPS请求"></a>1.客户端发起HTTPS请求</h2><p>用户在浏览器里输入一个https网址，经过TCP三次同步握手后，与服务器的443端口建立TCP连接</p><h2 id="2-服务端传送证书"><a href="#2-服务端传送证书" class="headerlink" title="2.服务端传送证书"></a>2.服务端传送证书</h2><p>服务端将证书发送给客户端，实质上就是公钥，包含证书颁发机构、过期时间等信息</p><h2 id="3-客户端验证证书"><a href="#3-客户端验证证书" class="headerlink" title="3.客户端验证证书"></a>3.客户端验证证书</h2><p>客户端的TLS验证公钥是否有效，如颁发机构是否可信、是否过期等，若发现异常，则会发出证书异常的警告；若证书没有问题，将生成随机值，再用证书对其加密，形成私钥，完成双方通信的加密算法与密钥的协商</p><h2 id="4-客户端传送私钥"><a href="#4-客户端传送私钥" class="headerlink" title="4.客户端传送私钥"></a>4.客户端传送私钥</h2><p>客户端将私钥发送到服务端，此后客户端和服务端双方通信就基于此进行加密与解密</p><h2 id="5-服务端加密响应数据并发送给客户端"><a href="#5-服务端加密响应数据并发送给客户端" class="headerlink" title="5.服务端加密响应数据并发送给客户端"></a>5.服务端加密响应数据并发送给客户端</h2><p>服务端通过客户端发送的密钥与加密算法，将响应数据进行运算完成加密，以保障数据的安全性和隐私性，最后发送给客户端</p><h2 id="6-客户端解密信息"><a href="#6-客户端解密信息" class="headerlink" title="6.客户端解密信息"></a>6.客户端解密信息</h2><p>客户端依据之前的加密算法与密钥解密响应数据，渲染成网页呈现给用户</p><hr><h1 id="1-编译安装http-ssl-module模块"><a href="#1-编译安装http-ssl-module模块" class="headerlink" title="1.编译安装http_ssl_module模块"></a>1.编译安装http_ssl_module模块</h1><p>–with-http_ssl_module</p><h1 id="2-创建证书"><a href="#2-创建证书" class="headerlink" title="2.创建证书"></a>2.创建证书</h1><pre><code class="hljs">mkdir /etc/nginx/sslcd /etc/pki/tls/certs</code></pre><h2 id="2-1-创建RSA密钥"><a href="#2-1-创建RSA密钥" class="headerlink" title="2.1 创建RSA密钥"></a>2.1 创建RSA密钥</h2><pre><code class="hljs">openssl genrsa -des3 -out nginx.key 2048</code></pre><h2 id="2-2-创建CSR，即证书签名请求文件"><a href="#2-2-创建CSR，即证书签名请求文件" class="headerlink" title="2.2 创建CSR，即证书签名请求文件"></a>2.2 创建CSR，即证书签名请求文件</h2><pre><code class="hljs">openssl req -new -key nginx.key \-subj &quot;/C=CN/ST=HeNan/L=ShangQiu/O=Sword/OU=Opt/CN=registry.sword.org&quot; \-out nginx.csr</code></pre><h2 id="2-3-生成自签名证书"><a href="#2-3-生成自签名证书" class="headerlink" title="2.3 生成自签名证书"></a>2.3 生成自签名证书</h2><pre><code class="hljs">openssl x509 -req -days 3650 -in nginx.csr -signkey nginx.key -out nginx.crt</code></pre><h2 id="2-4-退掉密码"><a href="#2-4-退掉密码" class="headerlink" title="2.4 退掉密码"></a>2.4 退掉密码</h2><pre><code class="hljs">mv nginx.key nginx.bak.keyopenssl rsa -in nginx.bak.key -out nginx.key</code></pre><h1 id="3-配置SSL证书"><a href="#3-配置SSL证书" class="headerlink" title="3.配置SSL证书"></a>3.配置SSL证书</h1><pre><code class="hljs">vi /etc/nginx/nginx.confserver &#123;  listen       80;  server_name  localhost;  location / &#123;    root   html;    index  index.html index.htm;  &#125;  # 配置http自动跳转https，实现全站加密  # rewrite ^(.*) https://$host$1 permanent;&#125;server &#123;  listen       443 ssl;  server_name  localhost;  root         /web;  ssl on;  ssl_certificate      /etc/nginx/ssl/nginx.crt;  ssl_certificate_key  /etc/nginx/ssl/nginx.key;  ssl_session_cache    shared:SSL:1m;  ssl_session_timeout  5m;  ssl_ciphers  ECDHE-RSA-AES128-GCM-SHA256:HIGH:!aNULL:!MD5:!RC4:!DHE;  ssl_prefer_server_ciphers  on;  ssl_protocols TLSv1 TLSv1.1 TLSv1.2;</code></pre><p>}</p><h1 id="4-测试https访问"><a href="#4-测试https访问" class="headerlink" title="4.测试https访问"></a>4.测试https访问</h1><p><a href="https://ip/">https://ip</a></p><hr><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://www.cnblogs.com/lan1x/p/5872915.html">https://www.cnblogs.com/lan1x/p/5872915.html</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>负载均衡</tag>
      
      <tag>Nginx</tag>
      
      <tag>SSL</tag>
      
      <tag>Https</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Nginx配置身份认证</title>
    <link href="/linux/NginxAuth/"/>
    <url>/linux/NginxAuth/</url>
    
    <content type="html"><![CDATA[<p>nginx的ngx_http_auth_basic_module模块允许使用HTTP基本身份验证，即通过用户名及其密码限制资源的访问</p><h1 id="1-安装htpasswd"><a href="#1-安装htpasswd" class="headerlink" title="1.安装htpasswd"></a>1.安装htpasswd</h1><pre><code class="hljs">yum install -y httpd-tools</code></pre><h1 id="2-添加认证用户"><a href="#2-添加认证用户" class="headerlink" title="2.添加认证用户"></a>2.添加认证用户</h1><pre><code class="hljs"># 创建认证文件，新增认证用户admin，并设置密码htpasswd -c /etc/nginx/.auth_list admin 123456# 新增认证用户sword，并设置密码htpasswd /etc/nginx/.auth_list sword 123456</code></pre><h1 id="3-配置nginx服务器的用户认证"><a href="#3-配置nginx服务器的用户认证" class="headerlink" title="3.配置nginx服务器的用户认证"></a>3.配置nginx服务器的用户认证</h1><pre><code class="hljs">vi /etc/nginx/nginx.confserver &#123;  listen       80;  server_name  localhost;  # 设置访问认证提示  auth_basic &quot;Server Auth&quot;;  # 设置认证文件  auth_basic_user_file /etc/nginx/.auth_list;  location / &#123;    root   html;    index  index.html;  &#125;&#125;</code></pre><h1 id="4-启动nginx，验证用户认证"><a href="#4-启动nginx，验证用户认证" class="headerlink" title="4.启动nginx，验证用户认证"></a>4.启动nginx，验证用户认证</h1><p><a href="http://ip/">http://ip</a></p>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>负载均衡</tag>
      
      <tag>Nginx</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Nginx配置Tomcat动静分离负载均衡集群</title>
    <link href="/linux/Nginx-Tomcat/"/>
    <url>/linux/Nginx-Tomcat/</url>
    
    <content type="html"><![CDATA[<h1 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h1><ul><li>172.16.100.100  node01  nginx tomcat</li><li>172.16.100.120  node02  tomcat</li><li>172.16.100.200  node03  tomcat</li></ul><hr><h1 id="1-安装Nginx、Tomcat"><a href="#1-安装Nginx、Tomcat" class="headerlink" title="1.安装Nginx、Tomcat"></a>1.安装Nginx、Tomcat</h1><h1 id="2-创建Nginx配置文件"><a href="#2-创建Nginx配置文件" class="headerlink" title="2.创建Nginx配置文件"></a>2.创建Nginx配置文件</h1><pre><code class="hljs">vi /etc/nginx/conf.d/tomcat.conf# 配置负载均衡集群upstream tomcat-servers &#123;  # 设置后端服务器组  server 172.16.100.120:8080 weight=2 max_fails=2 fail_timeout=30s;  server 172.16.100.200:8080 weight=2 max_fails=2 fail_timeout=30s;  # 设置备用服务器，即所有非备用服务器down或忙时再承担负载  server 172.16.100.100:8080 weight=2 max_fails=2 fail_timeout=30s;  # 设置不承担负载的服务器  # server 172.16.100.100:8080 weight=2 max_fails=2 fail_timeout=30s;&#125;server &#123;  listen       80;  server_name  localhost;  charset utf-8;  location / &#123;    proxy_pass http://tomcat-servers;    access_log  /var/log/nginx/tomcat_access.log  main;    error_log  /var/log/nginx/tomcat_error.log;  &#125;    # 配置动静分离  location ~ \.(jsp|jspx|do)$ &#123;    proxy_pass http://tomcat-servers;  &#125;&#125;</code></pre><h1 id="3-Tomcat节点配置测试页面"><a href="#3-Tomcat节点配置测试页面" class="headerlink" title="3.Tomcat节点配置测试页面"></a>3.Tomcat节点配置测试页面</h1><pre><code class="hljs">vi /usr/local/tomcat/webapps/ROOT/test.jspnode01</code></pre><hr><ul><li>注：其余两个节点的测试页面名称相同，内容分别为node02、node03</li></ul><h1 id="4-启动Tomcat、Nginx，验证集群功能"><a href="#4-启动Tomcat、Nginx，验证集群功能" class="headerlink" title="4.启动Tomcat、Nginx，验证集群功能"></a>4.启动Tomcat、Nginx，验证集群功能</h1><p><a href="http://172.16.100.100/test.jsp">http://172.16.100.100/test.jsp</a></p>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>负载均衡</tag>
      
      <tag>Nginx</tag>
      
      <tag>Tomcat</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Nginx配置文件详解</title>
    <link href="/linux/NginxConfig/"/>
    <url>/linux/NginxConfig/</url>
    
    <content type="html"><![CDATA[<pre><code class="hljs">### 核心模块的设置# 设置worker进程运行用户及用户组，默认为nobody        user  sword sword;# 设置worker进程数，默认为cpu数量     worker_processes  1;       # 设置错误日志路径及级别，debug|info|notice|warn|error|crit，默认为error，debug日志最为详细，crit日志数量最少error_log  /var/log/nginx/error.log error;# 设置pid文件路径pid  /var/log/nginx/nginx.pid;# 设置worker进程打开文件最大数，用于增大限制文件数worker_rlimit_nofile 51200;### 设置事件模块events &#123;# 设置nginx连接请求的处理方式，即事件驱动模型，select|poll|kqueue|epoll|resig|/dev/poll|eventport，默认使用最高效的方式，kqueue适用于freebsd；epoll适用于linux 2.6以上内核use epoll;# 设置单个worker进程最大并发连接数，默认为512worker_connections  512; &#125;### http模块的设置http &#123;  # 设置隐藏nginx版本号  server_tokens off;   # 设置扩展名与文件类型映射表  include       mime.types;  # 设置响应的默认MIME类型，默认为text/plain;  default_type  application/octet-stream;  # 设置http客户端发起的连接在服务端保持的超时时长，默认为75s，设为0则关闭长连接方式  keepalive_timeout  60s;  # 设置是否开启tcp_nodelay功能，开启时将尽量发送小数据块，前提需开启keep-alive，与tcp_nopush互斥，默认开启，适用于及时性高的通信场景  tcp_nodelay    on;  # 设置是否开启sendfile功能，即不通过CPU进行数据传输，而是通过DMA的方式，提高传输效率，默认关闭  # sendfile       on;  # 设置是否开启tcp_nopush功能，前提需开启sendfile功能，数据包不会马上传送出去，而是等到数据包最大时一次性的传输，适用于高延时、数据量大、磁盘IO负载大的通信场景，能平衡磁盘与网络I/O处理速度，降低系统负载，有效解决网络堵塞问题，默认关闭  # tcp_nopush     on;  # 设置gzip压缩功能，用于压缩响应数据，降低传输流量通常可达一半以上，默认关闭  gzip  on;  # 设置用于压缩响应数据的缓冲区大小和数量，默认大小为一页内存，4k或8k取，决于平台，即32 4k或16 8k  # gzip_buffers    32 4k;  # 设置压缩等级，默认为1，取值范围1-9，其值与处理速度成反比  # gzip_comp_level  2;  # 设置压缩响应最小长度，默认为20，0表示都压缩，其值决定了响应头Content-Length  # gzip_min_length    1k;  # 设置压缩功能的最小http版本，取值为1.0或1.1，低于该值的版本号将出现乱码  # gzip_http_version  1.1;  # 设置不压缩的请求，以正则表达式来匹配User-Agent（浏览器标识）  # gzip_disable &quot;msie6&quot;;  # 设置是否添加头域“Vary: Accept-Encoding”到响应头的功能，默认关闭  # gzip_vary on;  # 设置需要压缩的数据格式，默认为text/html，即这两类数据格式的响应被压缩  gzip_types text/plain text/css application/json application/x-javascript text/xml application/xml application/xml+rss text/javascript;  # 客户端请求缓冲区的设置，用于处理POST提交的数据，如文件上传等  # 设置请求行/请求头缓冲值，先处理请求行，再处理请求头，两者缓冲策略相同client_header_buffer_size默认为1K，先根据此值分配一个内存buffer，若  # 其无法容纳line/header，则再根据large_client_header_buffers值再分配  # 磁盘buffer，即临时文件，即产生了磁盘IO。若还不能容纳，就会报400错误。  # line/header都很大的情况设置client_header_buffer_size参数即可，只有  # 少量大line/header，则设置large_client_header_buffers，避免资源浪费  ############################################################### client_header_buffer_size   2K;# large_client_header_buffers  4 2K;# 设置http客户端请求头的超时时间，若在此时间内没有完成header传输，则返回# 408状态码，默认值为60s# client_header_timeout 60s;# 设置请求体缓冲值，默认8K(32b)/16k(64b)，若请求体大于此值，则整个请求或请求的一部分会# 被写入磁盘的临时文件，即产生了磁盘IO。而当磁盘空间不足时，则会断开此链接client_body_buffer_size 128K;# 设置请求的临时文件目录，默认为/tmp，最多支持三级目录# client_body_temp_path /tmp/nginx;# 设置请求体最大值，超出值返回413错误码。由于浏览器不会正确的显示这个错误# 建议将其设为0，以禁止检查请求体的大小client_max_body_size 0;# 设置proxy服务响应数据缓冲区，处理后端服务器的响应数据，如文件下载等，实现后# 端服务器的响应数据和http客户端的传输# nginx作为代理服务器时，接收http请求，然后从后端服务器取得响应数据。若proxy# 缓冲关闭，则Nginx会按将响应数据立刻同步到客户端；若proxy缓冲开启，则响应数据# 先存于由proxy_buffer_size和proxy_buffers指令设置的内存缓冲区。若缓冲区无法# 容纳整个响应，则部分数据会被写入磁盘的临时文件，即产生了磁盘IO# 设置是否开启代理缓冲，默认开启# proxy_buffering   on;# 设置代理缓冲值，默认一页内存，4k或8K# proxy_buffer_size 4K;# 设置代理缓冲区的数量与大小，数量默认为8个# proxy_buffers 8 4K;# 设置busy状态缓冲区的值，只有处于该状态buffer中的响应数据才会被发送给客户端，其余# buffer处于等待状态，默认值为8K或16K# proxy_busy_buffers_size 8K;# 设置proxy服务器响应数据临时文件目录# proxy_temp_path   /tmp/nginx# 设置proxy服务器响应数据临时文件目录最大值，默认为1024M,设为0则关闭临时文件缓存功# 能，若磁盘溢出，则会返回502错误码# proxy_max_temp_file_size    1024M;# 设置同时写入临时文件数据量的总大小，默认为8K或16K# proxy_temp_file_write_size 8K; # 设置proxy服务器响应数据硬盘缓存功能，具体是将http客户端请求数据的URL按照定义的规则与其他信息组合作为Key，# 再对其进行md5编码哈希计算后，于内存中生成索引，同时生成硬盘缓存目录。然后proxy服务器将该转发后端服务器，返回# 的响应数据存储于缓存目录。后续客户端发起的请求先与内存中的数据进行匹配，若能匹配到，即缓存命中且未过期，直接将# 缓存目录中的数据发给客户端，不用经过后端服务器。若不能匹配，即缓存未命中，则将请求转发到后端服务器，后端服务器# 发来的响应数据再由proxy服务器转发给客户端，同时形成新的缓存。这种缓存机制能大大提高静态资源的响应速度，当然缓# 存过程也需要消耗计算资源# 设置用于存放缓存索引数据的共享内存区域名称，即开启负责定时检查过期数据、检索等的缓存管理进程，默认为关闭# 缓存管理进程会检查proxy服务器响应数据HTTP头中的“Cache-Control”头域、“Expires”头域，若“Cache-Control”# 头域中的值为“no-cache”、“no-store”、“private”或者“max-age”赋值为0或者无意义，或者“Expires”头域包含# 一个过期的时间时，该响应数据不被Nginx服务器缓存，以避免私有数据被其他客户端获取# proxy_cache cache_one;# 设置缓存索引的关键字，默认为$scheme$proxy_host$request_uri# proxy_cache_key  &quot;$scheme$host$request_uri$cookie_user&quot;;# 设置缓存数据的硬盘路径及索引参数，缓存数据先写入临时文件，再以key值做md5计算后重命名，最后再存放到缓存目录# 若缓存目录忽略或设为on，则临时文件目录将被使用，由proxy_temp_path参数设置，这样临时文件和缓存文件就处在了# 同一文件系统，避免了不必要的复制操作，提高了效率。levels，缓存数据在相对path路径的第几级hash目录；keys_zone，# 缓存名称和内存大小，保存活动的key及其元数据，可以快速判断一个request是否命中或者未命中缓存，1M大约可存储# 8000个键字；inactive，超出此时间内缓存数据没有被访问将被删除，默认为10分钟max_size，最大硬盘缓存空间，由# cache manager进程监控，超出此值则覆盖掉缓存时间最长的数据proxy_cache_path  /tmp/nginx levels=1:2 keys_zone=cache_one:100m inactive=3d max_size=50g;# 设置请求方法的缓存，默认值为GET HEAD，即GET和HEAD方式的请求响应数据总是会被缓存proxy_cache_methods GET HEAD;# 设置响应不从缓存中读取的场景，如至少有一个字符串指令不为空或者不等于0# proxy_cache_bypass $cookie_nocache $arg_nocache $arg_comment $http_pragma $http_authorization# 设置响应缓存的最低访问次数，即超出此值才会进行缓存，默认为1# proxy_cache_min_uses 1;# 设置缓存并发锁，当同时有多个相同请求key时，只响应一个生成缓存，其余请求尝试从缓存获取数据，或等待超时，默认关闭# proxy_cache_lock on;# 设置并发锁生命周期，若一个缓存未在此时间内完成，则其余请求将被发送给proxy服务器，默认5s# proxy_cache_lock_age 10s;# 设置并发锁超时时长，超时后由proxy服务器将请求转发给后端服务器，默认为5s# proxy_cache_lock_timeout 10s;# 设置后端服务器返回哪些错误码时直接调用旧的缓存，默认关闭proxy_cache_use_stale error timeout updating http_502 http_504;# 设置状态码为200 302过期时间为10分钟# proxy_cache_valid  200 302  10m;# 设置状态码404的过期时间为1分钟# proxy_cache_valid  404      1m;# 设置所有状态码的过期时间为1分钟# proxy_cache_valid  any      1m;# 设置proxy服务器连接后端服务器的的超时时长，默认为60s，但不能超过75sproxy_connect_timeout 65;# 设置proxy服务器发送请求的超时时长，即将请转发给后端服务器的超时时长，若两次send操作期间超过该时长后端服务器还# 没收到新数据，则nginx将关闭连接，默认为60s，但不能超过75sproxy_send_timeout 65;# 设置proxy服务器接收响应的超时时长，即接收来自后端服务器响应的超时时长，若两次read操作期间超过该时长后端服务器# 还没有传输新数据，则nginx将关闭连接，默认为60s，但不能超过75sproxy_read_timeout 65;# 设置由proxy服务器所转发的请求头数据的处理方式，即对http客户端的请求头数据进行更改# 或添加操作，然后再转发给后端服务器，可以获取客户端IP、端口及代理服务器IP、端口，用于# IP反查、端口范围确认等业务# 设置获取http请求数据主机名、端口字段，一般为客户端直接访问的那台服务器的信息proxy_set_header Host $host:$server_port;# 设置后端服务器获取http客户端IPproxy_set_header X-Real-IP $remote_addr;# 设置添加X-Forwarded-For变量，用于后端服务器获取经过proxy服务器或负载均衡器的http客# 户端IP，否则获取的将是代理服务器或负载均衡器的IP，该变量不在默认配置中proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;# 设置过滤掉空请求头的数据，即空请求头数据不予转发proxy_set_header Accept-Encoding &quot;&quot;;# 设置不做处理的后端服务器返回的响应头proxy_ignore_headers X-Accel-Expires Expires Cache-Control Set-Cookie;# 设置不返回给客户端的响应头，Date、Server、X-Pad和X-Accel-…一般不会返回proxy_hide_header X-Powered-By;# 设置后端服务器获取http客户端的传输协议，即http或httpsproxy_set_header X-Forwarded-Proto $scheme;# 设置由proxy服务器所转发的响应头数据的处理方式,即对转发给http客户端数据响应头的Location# 和Refresh字段进行更改，用于后端服务器IP与端口的屏蔽，默认为default，响应头的Location# 即为proxy_pass定义的值，但当proxy_pass为变量时不能设为default# proxy_redirect http:// https://;# 设置后端服务器故障转移，即剔除掉返回502、504、执行超时等错误的后端服务器proxy_next_upstream http_502 http_504 error timeout invalid_header;# 设置nginx日志格式log_format  main  &#39;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#39;              &#39;$status $bytes_sent $body_bytes_sent &quot;$http_referer&quot; &#39;              &#39;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot; &#39;              &#39;&quot;$request_time&quot; &quot;$upstream_response_time&quot; &quot;$upstream_status&quot; &quot;$upstream_addr&quot; $http_host&#39;;# 设置负载均衡集群，支持多组upstream app-slb-tomcat&#123;    # 设置负载均衡算法    # 1.轮询（默认），每个请求按时间顺序逐一分配到不同的后端服务器，若后端服务器down掉，则自动剔除。weight指定权重，即轮    # 询几率，和访问比率成正比，用于后端服务器性能不均的情况    # 2.ip_hash，每个请求按访问ip的hash结果分配，即每个访问固定访问一个后端服务器，解决了session问题    # 3.fair（第三方），按后端服务器的响应时间来分配请求，响应时间短的优先分配    # 4.url_hash（第三方），按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效     server 192.168.0.188:8080 weight=2 max_fails=2 fail_timeout=30s;    server 192.168.0.189:8080 weight=1 max_fails=2 fail_timeout=30s;    # 设置不参与负载的服务器组    # server 192.168.0.200:8080 down;    # 设置备用服务器组，所有非备用服务down或者忙时，再响应请求    # server 192.168.0.100:8080 backup;    &#125;# 设置静态资源负载均衡服务器组upstream static &#123;    server 127.0.0.1:8080 weight=2 max_fails=2 fail_timeout=30s;    server 127.0.0.1:8088 weight=2 max_fails=2 fail_timeout=30s;    &#125;### 虚拟主机模块server &#123;    listen       80;    server_name  localhost;    access_log  /var/log/nginx/access.log main;    error_log  /var/log/nginx/error.log;    location / &#123;        root   html;        index  index.html index.htm;        &#125;    # 设置错误页面的跳转    # error_page  404              /404.html;    # error_page   500 502 503 504  /50x.html;    # location = /50x.html &#123;        # root   html;    # &#125;    # 设置jsp动态页面反向代理    location ~ \.(jsp|jspx|do)$ &#123;    proxy_pass http://app-slb-tomcat;    &#125;    # 设置静态页面反向代理    location ~ .*\.(js|css|ico|png|eot|svg|ttf|woff|htm|html|gif|jpg|jpeg|bmp|swf|ioc|rar|zip|txt|flv|mid|doc|ppt|pdf|xls|mp3|wma)     &#123;    root /web/static;    proxy_pass http://static;    # 所有静态文件直接读取硬盘,缓存30天    expires 30d;     &#125;    # 设置nginx状态监控，需http_stub_status_module模块支持    location = /nginx-status &#123;    stub_status on;    access_log  off;    allow 127.0.0.1;    # allow 192.168.0.180;    deny all;    &#125;    # 设置proxy服务器缓存的清理，需安装ngx_cache_purge，且只支持静态编译    # location ~ /purge(/.*) &#123;    # allow 127.0.0.1;    # deny all;    # 设置缓存清理    # proxy_cache_purge cache_one $scheme$host$request_uri$cookie_user;    # &#125;</code></pre>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>负载均衡</tag>
      
      <tag>Nginx</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Nginx编译安装</title>
    <link href="/linux/Nginx/"/>
    <url>/linux/Nginx/</url>
    
    <content type="html"><![CDATA[<p>Nginx，由俄罗斯工程师Igor Sysoev（戈尔·西索耶夫）于2004年所开发的高性能轻量级HTTP、反向代理、负载均衡及电子邮件（IMAP&#x2F;POP3）服务器，以资源消耗少、并发能力强、配置简单、模块库丰富、性能稳定等特点闻名于业界，目前在web服务器市场份额已超越Apache成为全球第一</p><h3 id="体系架构"><a href="#体系架构" class="headerlink" title="体系架构"></a>体系架构</h3><p>nginx服务器由一个master进程和多个worker进程所构成，整体上表现为依赖事件驱动、异步、非阻塞的模式</p><h4 id="1-master进程"><a href="#1-master进程" class="headerlink" title="1.master进程"></a>1.master进程</h4><p>master进程负责创建、管理、调度和监控worker进程的工作，具体如下：</p><ul><li>加载、解析配置文件，并验证其有效性和正确性</li><li>建立、绑定和关闭socket连接</li><li>启动、管理worker进程，并持续监控，如woker进程异常退出则自动重新启动新woker进程</li><li>接收外部信号，如QUIT信号表示关闭服务</li><li>开启日志文件，获取文件描述符</li><li>程序及配置文件的热更新，保持业务运行中的平滑更新</li></ul><h4 id="2-worker进程"><a href="#2-worker进程" class="headerlink" title="2.worker进程"></a>2.worker进程</h4><p>worker进程主要负责处理网络请求及响应，多个worker进程相互独立处理客户端请求，每个worker进程以异步非阻塞方式处理客户端请求，接收到客户端的请求之后调用IO，若不能立即得到响应，则非阻塞地处理其他请求，而客户端在此期间也无需等待响应，可异步处理其他事情。而当IO返回数据时，则将通知此worker进程，之后将暂时挂起当前处理的事务而去响应客户端请求。此外，worker进程构建于Linux基于事件驱动机制的epoll模型，监控多个事件是否准备完毕，如果OK，那么放入epoll队列中，这个过程是异步的。worker只需要从epoll队列循环处理即可。</p><p>worker进程具体功能如下：</p><ul><li>接受客户端请求，并将其依次送入各个功能模块进行处理</li><li>I&#x2F;O调用，获取响应数据</li><li>与后端服务器通信，接收后端服务器的处理结果</li><li>缓存数据，访问缓存索引，查询和调用缓存数据</li><li>发送请求结果，响应客户的请求</li><li>接收主程序指令，如重启、升级和退出</li></ul><p>worker进程依靠epoll模型管理并发连接</p><h3 id="功能模块"><a href="#功能模块" class="headerlink" title="功能模块"></a>功能模块</h3><p>Nginx服务器由内核与功能模块组成，内核负责底层通讯协议的实现、运行环境的创建及功能模块的调用与协调，其余功能均由各种功能模块完成，模块之间严格遵循高内聚、低耦合的原则，这就是Nginx高度模块化的设计</p><p>Nginx将各功能模块组织成链，客户端请求依次经过这条链上的部分或者全部模块进行处理，每个模块只负责实现各自的功能</p><h2 id="1-核心模块"><a href="#1-核心模块" class="headerlink" title="1.核心模块"></a>1.核心模块</h2><p>提供错误日志记录、配置文件解析、事件驱动机制、进程管理等核心功能</p><h2 id="2-标准HTTP模块"><a href="#2-标准HTTP模块" class="headerlink" title="2.标准HTTP模块"></a>2.标准HTTP模块</h2><p>提供HTTP协议解析相关功能，如端口配置、网页编码设置、HTTP响应头设置等</p><h2 id="3-扩展HTTP模块"><a href="#3-扩展HTTP模块" class="headerlink" title="3.扩展HTTP模块"></a>3.扩展HTTP模块</h2><p>用于扩展标准的HTTP功能，可处理一些特殊的服务，如Flash多媒体传输、解析GeoIP请求、 网络传输压缩、安全协议SSL支持等</p><h2 id="4-邮件服务模块"><a href="#4-邮件服务模块" class="headerlink" title="4.邮件服务模块"></a>4.邮件服务模块</h2><p>用于支持邮件服务，包括对POP3协议、IMAP协议和SMTP协议的支持</p><h2 id="5-第三方模块"><a href="#5-第三方模块" class="headerlink" title="5.第三方模块"></a>5.第三方模块</h2><p>用于扩展Nginx服务器应用，完成开发者自定义功能，如Json支持、Lua支持等</p><hr><h1 id="1-安装依赖包"><a href="#1-安装依赖包" class="headerlink" title="1.安装依赖包"></a>1.安装依赖包</h1><pre><code class="hljs"># apt install -y nginxyum install -y gcc make pcre-devel openssl-devel zlib-develapt install -y gcc make libssl-dev libpcre3-dev zlib1g-dev</code></pre><h1 id="2-编译安装nginx"><a href="#2-编译安装nginx" class="headerlink" title="2.编译安装nginx"></a>2.编译安装nginx</h1><pre><code class="hljs">tar -zxvf nginx-1.20.2.tar.gz &amp;&amp; cd nginx-1.20.2./configure --prefix=/usr/local/nginx --conf-path=/etc/nginx/nginx.conf \--with-http_ssl_module --with-http_stub_status_module --with-http_gunzip_module \--with-http_realip_module --with-http_sub_module --with-pcre --with-stream --with-threads \--http-log-path=/var/log/nginx/access.log --error-log-path=/var/log/nginx/error.logmake &amp;&amp;  make install</code></pre><h1 id="3-修改配置文件"><a href="#3-修改配置文件" class="headerlink" title="3.修改配置文件"></a>3.修改配置文件</h1><pre><code class="hljs">sed -i &#39;/nobody/a user sword sword;&#39; /etc/nginx/nginx.confsed -i &#39;/SCRIPT_FILENAME/a fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;&#39; /etc/nginx/nginx.confsed -i &#39;/default_type/a client_max_body_size 20m;&#39; /etc/nginx/nginx.conf</code></pre><h1 id="4-创建启动脚本"><a href="#4-创建启动脚本" class="headerlink" title="4.创建启动脚本"></a>4.创建启动脚本</h1><pre><code class="hljs">vi /lib/systemd/system/nginx.service[Unit]Description=Nginx ServerAfter=network.target[Service]Type=forkingExecStart=/usr/local/nginx/sbin/nginxExecReload=/usr/local/nginx/sbin/nginx -s reloadExecStop=/usr/local/nginx/sbin/nginx -s quitPrivateTmp=true[Install]WantedBy=multi-user.target</code></pre><h1 id="5-启动nginx"><a href="#5-启动nginx" class="headerlink" title="5.启动nginx"></a>5.启动nginx</h1><pre><code class="hljs">systemctl daemon-reloadsystemctl start nginx.servicesystemctl enable nginx.service</code></pre><h1 id="6-访问nginx"><a href="#6-访问nginx" class="headerlink" title="6.访问nginx"></a>6.访问nginx</h1><pre><code class="hljs"># curl 127.0.0.1http://IP</code></pre>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>负载均衡</tag>
      
      <tag>Nginx</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Apache配置Tomcat负载均衡集群</title>
    <link href="/linux/ApacheLB/"/>
    <url>/linux/ApacheLB/</url>
    
    <content type="html"><![CDATA[<h1 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h1><ul><li>172.16.100.100  node01  apache</li><li>172.16.100.120  node02  tomcat</li><li>172.16.100.200  node03  tomcat</li></ul><hr><p>Apache配置Tomcat的负载均衡集群有三种方式，即mod_jk、http_proxy及ajp_proxy</p><hr><h1 id="1-mod-jk方式"><a href="#1-mod-jk方式" class="headerlink" title="1.mod_jk方式"></a>1.mod_jk方式</h1><p>mod_jk通过AJP协议与Tomcat通信，Tomcat默认的AJP Connector端口为8009，此外还提供了一个监控器jkstatus，用于监控JK的工作状态</p><h2 id="1-1-编译安装mod-jk"><a href="#1-1-编译安装mod-jk" class="headerlink" title="1.1 编译安装mod_jk"></a>1.1 编译安装mod_jk</h2><pre><code class="hljs">tar -xzvf tomcat-connectors-1.2.42-src.tar.gz &amp;&amp; cd tomcat-connectors-1.2.42-src/native./configure --with-apxs=/usr/local/httpd/bin/apxsmake &amp;&amp; make install</code></pre><h2 id="1-2-Apache配置文件加载mod-jk模块"><a href="#1-2-Apache配置文件加载mod-jk模块" class="headerlink" title="1.2 Apache配置文件加载mod_jk模块"></a>1.2 Apache配置文件加载mod_jk模块</h2><pre><code class="hljs">vi /etc/httpd/conf/httpd.conf# 设置加载mod_jk模块配置文件Include /etc/httpd/conf/extra/mod_jk.conf</code></pre><h2 id="1-3-创建mod-jk模块配置文件"><a href="#1-3-创建mod-jk模块配置文件" class="headerlink" title="1.3 创建mod_jk模块配置文件"></a>1.3 创建mod_jk模块配置文件</h2><pre><code class="hljs">vi /etc/httpd/conf/extra/mod_jk.conf# 设置加载mod_jk模块 LoadModule jk_module /usr/local/httpd/modules/mod_jk.so# 设置加载负载均衡控制器配置文件，用于定义转发主机和监听端口JkWorkersFile /etc/httpd/conf/extra/workers.properties# 设置日志文件存储路径JkLogFile /usr/local/httpd/logs/mod_jk.log# 设置日志级别[debug/error/info]JkLogLevel info# 设置日志格式JkLogStampFormat &quot;[%a %b %d %H:%M:%S %Y] &quot;JkOptions +ForwardKeySize +ForwardURICompat -ForwardDirectoriesJkRequestLogFormat &quot;%w %V %T&quot;# 设置URL规则，实现动静分离JkMount /*.do loadbalancerJkMount /*.jsp loadbalancerJkMount /servlet/* loadbalancer# 设置监控器访问地址JkMount /jk_status status</code></pre><h2 id="1-4-设置负载均衡控制器配置文件"><a href="#1-4-设置负载均衡控制器配置文件" class="headerlink" title="1.4 设置负载均衡控制器配置文件"></a>1.4 设置负载均衡控制器配置文件</h2><pre><code class="hljs">vi /etc/httpd/conf/extra/workers.properties# 设置Tomcat服务器工作组，对应于mod_jk.conf文件worker.list=loadbalancer,status,tomcat1,tomcat2# 设置Tomcat服务器JK端口worker.tomcat1.port=8009 # 设置Tomcat服务器IPworker.tomcat1.host=172.16.100.100.120# 设置通信协议，与Tomcat配置文件server.xml protocol保持一致worker.tomcat1.type=ajp13# 设置Tomcat服务器负载权重worker.tomcat1.lbfactor=1worker.tomcat2.port=8009worker.tomcat2.host=172.16.100.200worker.tomcat2.type=ajp13worker.tomcat2.lbfactor=1# 设置loadbalancer负载均衡控制器worker.loadbalancer.type=lb# 设置负载列表worker.loadbalancer.balance_workers=tomcat1,tomcat2# 设置启用会话保持worker.loadbalancer.sticky_session=ture# 设置启用JK模块监控器worker.status.type=status</code></pre><h2 id="1-5-启动Apache、Tomcat服务验证负载均衡"><a href="#1-5-启动Apache、Tomcat服务验证负载均衡" class="headerlink" title="1.5 启动Apache、Tomcat服务验证负载均衡"></a>1.5 启动Apache、Tomcat服务验证负载均衡</h2><h1 id="2-http-proxy方式"><a href="#2-http-proxy方式" class="headerlink" title="2.http_proxy方式"></a>2.http_proxy方式</h1><p>mod_proxy模块自Apache 2.2开始正式启用，是基于HTTP协议的代理，Tomcat默认的HTTP Connector端口为8080</p><h2 id="2-1-安装Apache、Tomcat"><a href="#2-1-安装Apache、Tomcat" class="headerlink" title="2.1 安装Apache、Tomcat"></a>2.1 安装Apache、Tomcat</h2><h2 id="2-2-修改Apache配置文件"><a href="#2-2-修改Apache配置文件" class="headerlink" title="2.2 修改Apache配置文件"></a>2.2 修改Apache配置文件</h2><pre><code class="hljs">vi /etc/httpd/conf/httpd.conf# 启用反向代理模块LoadModule proxy_module modules/mod_proxy.so# 启用代理http协议模块LoadModule proxy_http_module modules/mod_proxy_http.so# 启用代理ajp协议模块LoadModule proxy_ajp_module modules/mod_proxy_ajp.soLoadModule proxy_connect_module modules/mod_proxy_connect.so# 启用负载均衡模块LoadModule proxy_balancer_module modules/mod_proxy_balancer.so# 启用负载均衡管理器模块LoadModule status_module modules/mod_status.so # 启用负载均衡算法模块LoadModule lbmethod_byrequests_module modules/mod_lbmethod_byrequests.soLoadModule lbmethod_bytraffic_module modules/mod_lbmethod_bytraffic.soLoadModule lbmethod_bybusyness_module modules/mod_lbmethod_bybusyness.so LoadModule slotmem_shm_module modules/mod_slotmem_shm.so# 反向代理负载均衡配置# 关闭正向代理ProxyRequests Off# 反向代理支持虚拟主机ProxyPreserveHost On# session粘性，为JSESSIONID是浏览器cookie方式session处理，为jsessionid则是客户端采用URL# ProxyPass / balancer://tomcat-cluster/ stickysession=JSESSIONID nofailover=On# session非粘性，需后端Tomcat服务器配置session复制功能，ProxyPass / balancer://app-slb-tomcat/ nofailover=On# 负载均衡管理器，仅调试用，生产环境禁用，访问路径为 http://localhost/balancer-manager# &lt;Location /balancer-manager&gt;   # SetHandler balancer-manager   # Order Deny,Allow   # Allow from all   # Allow from localhost # &lt;/Location&gt; # 配置动静分离ProxyPassMatch /*.gif$ ! ProxyPassMatch /*.jpg$ !ProxyPassMatch /*.png$ ! ProxyPassMatch /*.css$ ! ProxyPassMatch /*.js$ ! ProxyPassMatch /*.htm$ ! ProxyPassMatch /*.html$ !# 配置负载均衡器日志路径ErrorLog &quot;/usr/local/httpd/logs/app-slb-tomcat-error.log&quot;CustomLog &quot;/usr/local/httpd/logs/app-slb-tomcat-access.log&quot; common&lt;Proxy balancer://app-slb-tomcat&gt;        BalancerMember http://172.16.100.100.100:8080  loadfactor=1 route=tomcat1  BalancerMember http://172.16.100.100.200:8080  loadfactor=1 route=tomcat2  # 热备服务器  # BalancerMember http://172.16.100.100.120:8080 status=+H  # 设置负载均衡算法  # 按照请求次数均衡(默认)  # ProxySet lbmethod=byrequests  # 按照流量均衡  # ProxySetlbmethod=bytraffic  # 最少请求数  # ProxySet lbmethod=bybusyness&lt;/Proxy&gt; # 配置双机热备高可用集群# ProxyRequests Off # ProxyPass / balancer://app-ha-tomcat/ # &lt;Proxy balancer://app-ha-tomcat&gt;   # BalancerMember http://172.16.100.100.120:8080   # BalancerMember http://172.16.100.100.200:8080 status=+H # &lt;/Proxy&gt; </code></pre><h2 id="2-3-启动Apache与Tomcat服务"><a href="#2-3-启动Apache与Tomcat服务" class="headerlink" title="2.3 启动Apache与Tomcat服务"></a>2.3 启动Apache与Tomcat服务</h2><h1 id="3-ajp-proxy方式"><a href="#3-ajp-proxy方式" class="headerlink" title="3.ajp_proxy方式"></a>3.ajp_proxy方式</h1><p>类似于http_proxy，由mod_proxy模块提供代理功能，只需将http:&#x2F;&#x2F;换成ajp:&#x2F;&#x2F;，也是连接Tomcat的AJP Connector端口，默认为8009</p><pre><code class="hljs">&lt;Proxy balancer://app-slb-tomcat&gt;        BalancerMember ajp://172.16.100.100.120:8009  loadfactor=1 route=tomcat1  BalancerMember ajp://172.16.100.100.200:8009  loadfactor=1 route=tomcat2&lt;/Proxy&gt; </code></pre>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Apache</tag>
      
      <tag>负载均衡</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Apache配置文件详解</title>
    <link href="/linux/ApacheConfig/"/>
    <url>/linux/ApacheConfig/</url>
    
    <content type="html"><![CDATA[<pre><code class="hljs"># 设置安装目录ServerRoot &quot;/usr/local/httpd&quot;# 设置HTTP服务器响应头，值为major|minor|minimal|productonly|os|full，显示Apache版本和操作系统名称ServerTokens OS # Mutex default:logs# 设置监听端口，默认监听所有网卡#Listen 12.34.56.78:80Listen 80# 设置启用的模块LoadModule authn_file_module modules/mod_authn_file.so#LoadModule authn_dbm_module modules/mod_authn_dbm.so#LoadModule authn_anon_module modules/mod_authn_anon.so#LoadModule authn_dbd_module modules/mod_authn_dbd.so#LoadModule authn_socache_module modules/mod_authn_socache.soLoadModule authn_core_module modules/mod_authn_core.soLoadModule authz_host_module modules/mod_authz_host.soLoadModule authz_groupfile_module modules/mod_authz_groupfile.soLoadModule authz_user_module modules/mod_authz_user.so#LoadModule authz_dbm_module modules/mod_authz_dbm.so#LoadModule authz_owner_module modules/mod_authz_owner.so#LoadModule authz_dbd_module modules/mod_authz_dbd.soLoadModule authz_core_module modules/mod_authz_core.soLoadModule access_compat_module modules/mod_access_compat.soLoadModule auth_basic_module modules/mod_auth_basic.so#LoadModule auth_form_module modules/mod_auth_form.so#LoadModule auth_digest_module modules/mod_auth_digest.so#LoadModule allowmethods_module modules/mod_allowmethods.so#LoadModule file_cache_module modules/mod_file_cache.so#LoadModule cache_module modules/mod_cache.so#LoadModule mem_cache_module modules/mod_mem_cache.so#LoadModule cache_disk_module modules/mod_cache_disk.so#LoadModule cache_socache_module modules/mod_cache_socache.so#LoadModule socache_shmcb_module modules/mod_socache_shmcb.so#LoadModule socache_dbm_module modules/mod_socache_dbm.so#LoadModule socache_memcache_modulemodules/mod_socache_memcache.so#LoadModule watchdog_module modules/mod_watchdog.so#LoadModule macro_module modules/mod_macro.so#LoadModule dbd_module modules/mod_dbd.so#LoadModule dumpio_module modules/mod_dumpio.so#LoadModule buffer_module modules/mod_buffer.so#LoadModule ratelimit_module modules/mod_ratelimit.soLoadModule reqtimeout_module modules/mod_reqtimeout.so#LoadModule ext_filter_module modules/mod_ext_filter.so#LoadModule request_module modules/mod_request.so#LoadModule include_module modules/mod_include.soLoadModule filter_module modules/mod_filter.so#LoadModule substitute_module modules/mod_substitute.so#LoadModule sed_module modules/mod_sed.soLoadModule mime_module modules/mod_mime.soLoadModule log_config_module modules/mod_log_config.so#LoadModule log_debug_module modules/mod_log_debug.so#LoadModule logio_module modules/mod_logio.soLoadModule env_module modules/mod_env.so#LoadModule unique_id_module modules/mod_unique_id.soLoadModule setenvif_module modules/mod_setenvif.soLoadModule version_module modules/mod_version.so#LoadModule remoteip_module modules/mod_remoteip.so#LoadModule proxy_module modules/mod_proxy.so#LoadModule proxy_connect_module modules/mod_proxy_connect.so#LoadModule proxy_ftp_module modules/mod_proxy_ftp.so#LoadModule proxy_http_module modules/mod_proxy_http.so#LoadModule proxy_fcgi_module modules/mod_proxy_fcgi.so#LoadModule proxy_scgi_module modules/mod_proxy_scgi.so#LoadModule proxy_fdpass_module modules/mod_proxy_fdpass.so#LoadModule proxy_wstunnel_module modules/mod_proxy_wstunnel.so#LoadModule proxy_ajp_module modules/mod_proxy_ajp.so#LoadModule proxy_balancer_module modules/mod_proxy_balancer.so#LoadModule proxy_express_module modules/mod_proxy_express.so#LoadModule proxy_hcheck_module modules/mod_proxy_hcheck.so#LoadModule session_module modules/mod_session.so#LoadModule session_cookie_module modules/mod_session_cookie.so#LoadModule session_dbd_module modules/mod_session_dbd.so#LoadModule slotmem_shm_module modules/mod_slotmem_shm.so#LoadModule lbmethod_byrequests_module modules/mod_lbmethod_byrequests.so#LoadModule lbmethod_bytraffic_module modules/mod_lbmethod_bytraffic.so#LoadModule lbmethod_bybusyness_module modules/mod_lbmethod_bybusyness.so#LoadModule lbmethod_heartbeat_module modules/mod_lbmethod_heartbeat.so#LoadModule mpm_event_module modules/mod_mpm_event.so#LoadModule mpm_prefork_module modules/mod_mpm_prefork.soLoadModule mpm_worker_module modules/mod_mpm_worker.soLoadModule unixd_module modules/mod_unixd.so#LoadModule dav_module modules/mod_dav.soLoadModule status_module modules/mod_status.soLoadModule autoindex_module modules/mod_autoindex.so#LoadModule info_module modules/mod_info.so#LoadModule dav_fs_module modules/mod_dav_fs.so#LoadModule vhost_alias_module modules/mod_vhost_alias.so#LoadModule negotiation_module modules/mod_negotiation.soLoadModule dir_module modules/mod_dir.so#LoadModule actions_module modules/mod_actions.so#LoadModule speling_module modules/mod_speling.so#LoadModule userdir_module modules/mod_userdir.soLoadModule alias_module modules/mod_alias.so&lt;IfModule unixd_module&gt;  # 设置运行用户及用户组          User daemon  Group daemon&lt;/IfModule&gt;# 设置服务进程数# StartServers    8# 设置服务器保持的最少空闲进程数，若空闲子进程小于此值，就创建一个新的子进程作准备#MinSpareServers    5   # 设置服务器保持的最大空闲进程数，若空闲子进程大于此值，就逐一删除子进程以提高系统性能#MaxSpareServers    20# 设置服务器最大进程数#ServerLimit    256# 设置服务器最大并发连接数#MaxClients    256# 设置子进程在结束处理请求之前最大能处理的连接请求数，超过此值就释放重新建立，为0表示永不释放#MaxRequestsPerChild  4000  # 设置pid文件路径PidFile /var/run/httpd.pid# 设置TCP连接超时时长，超过则主动关闭，接受下一个连接Timeout 60# 设置开启持久连接功能，实现一个连接处理多个请求，以提高响应速度，降低服务器开销KeepAlive On# 设置持久连接时长，即一个HTTP请求响应结束后不关闭连接，等待下一个请求的超时时长# KeepAliveTimeout 30# 设置一个连接所处理的最大请求数，默认为100，为0则表示无限制# MaxKeepAliveRequests 100# 设置管理员邮箱ServerAdmin you@example.com# 设置服务器域名和端口ServerName localhost:80# 设置服务器根目录访问控制权限&lt;Directory /&gt;  AllowOverride none  Require all denied&lt;/Directory&gt;# 设置网站根目录DocumentRoot &quot;/usr/local/httpd/htdocs&quot;# 设置网站根目录访问控制权限&lt;Directory &quot;/usr/local/httpd/htdocs&quot;&gt;  Options Indexes FollowSymLinks  AllowOverride None  Require all granted&lt;/Directory&gt;# 设置默认索引页&lt;IfModule dir_module&gt;  DirectoryIndex index.html&lt;/IfModule&gt;&lt;Files &quot;.ht*&quot;&gt;  # 设置.ht文件拒绝访问  Require all denied&lt;/Files&gt;# 设置错误日志存储路径ErrorLog &quot;logs/error_log&quot;# 设置错误日志级别LogLevel warn&lt;IfModule log_config_module&gt;  # 设置访问日志文件格式  LogFormat &quot;%h %l %u %t \&quot;%r\&quot; %&gt;s %b \&quot;%&#123;Referer&#125;i\&quot; \&quot;%&#123;User-Agent&#125;i\&quot;&quot; combined  LogFormat &quot;%h %l %u %t \&quot;%r\&quot; %&gt;s %b&quot; common  &lt;IfModule logio_module&gt;    # 设置访问日志记录请求的输入/输出字节数，包括请求头与响应头正文之和，能正确反映SSL/TLS加密所造成的影响    LogFormat &quot;%h %l %u %t \&quot;%r\&quot; %&gt;s %b \&quot;%&#123;Referer&#125;i\&quot; \&quot;%&#123;User-Agent&#125;i\&quot; %I %O&quot; combinedio  &lt;/IfModule&gt;  # 设置访问日志存储路径  CustomLog &quot;logs/access_log&quot; common&lt;/IfModule&gt;# 设置访问日志不记录客户端主机名，避免客户端域名反向解析造成服务器性能下降HostnameLookups Off &lt;IfModule alias_module&gt;  # 设置URL重定向  ScriptAlias /cgi-bin/ &quot;/usr/local/httpd/cgi-bin/&quot;&lt;/IfModule&gt;&lt;IfModule cgid_module&gt;&lt;/IfModule&gt;&lt;Directory &quot;/usr/local/httpd/cgi-bin&quot;&gt;  AllowOverride None  Options None  Require all granted&lt;/Directory&gt;&lt;IfModule headers_module&gt;&lt;/IfModule&gt;&lt;IfModule mime_module&gt;  TypesConfig /etc/httpd/conf/mime.types  #AddType application/x-gzip .tgz  #AddEncoding x-compress .Z  #AddEncoding x-gzip .gz .tgz  AddType application/x-compress .Z  AddType application/x-gzip .gz .tgz  AddType application/x-php .php  #AddHandler cgi-script .cgi  #AddHandler type-map var  #AddType text/html .shtml  #AddOutputFilter INCLUDES .shtml  AddDefaultCharset UTF-8&lt;/IfModule&gt;#MIMEMagicFile /etc/httpd/conf/magic#ErrorDocument 500 &quot;The server made a boo boo.&quot;#ErrorDocument 404 /missing.html#ErrorDocument 404 &quot;/cgi-bin/missing_handler.pl&quot;#ErrorDocument 402 http://www.example.com/subscription_info.html#MaxRanges unlimited#EnableMMAP off#EnableSendfile on# Server-pool management (MPM specific)#Include /etc/httpd/conf/extra/httpd-mpm.conf# Multi-language error messages#Include /etc/httpd/conf/extra/httpd-multilang-errordoc.conf# Fancy directory listings#Include /etc/httpd/conf/extra/httpd-autoindex.conf# Language settings#Include /etc/httpd/conf/extra/httpd-languages.conf# User home directories#Include /etc/httpd/conf/extra/httpd-userdir.conf# Real-time info on requests and configuration#Include /etc/httpd/conf/extra/httpd-info.conf# 设置虚拟主机#Include /etc/httpd/conf/extra/httpd-vhosts.conf# Local access to the Apache HTTP Server Manual#Include /etc/httpd/conf/extra/httpd-manual.conf# Distributed authoring and versioning (WebDAV)#Include /etc/httpd/conf/extra/httpd-dav.conf# Various default settings#Include /etc/httpd/conf/extra/httpd-default.conf# Configure mod_proxy_html to understand HTML4/XHTML1&lt;IfModule proxy_html_module&gt;  Include /etc/httpd/conf/extra/proxy-html.conf&lt;/IfModule&gt;# 设置SSL安全连接#Include /etc/httpd/conf/extra/httpd-ssl.conf&lt;IfModule ssl_module&gt;  SSLRandomSeed startup builtin  SSLRandomSeed connect builtin&lt;/IfModule&gt;</code></pre>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Apache</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Apache编译安装</title>
    <link href="/linux/Apache/"/>
    <url>/linux/Apache/</url>
    
    <content type="html"><![CDATA[<p>Apache HTTP Server，是最流行的开源免费的HTTP服务器软件之一，功能强大，配置简单，速度快，应用广泛，性能稳定可靠，曾是世界使用排名第一的Web服务器软件，广泛应用于世界著名网站，如Amazon、Yahoo!、W3 Consortium、Financial Times等</p><p>Apache服务器的发轫颇具戏剧性。其最初发源于NCSA（伊利诺伊大学香槟分校的国家超级电脑应用中心）的httpd服务器，当时只用于小型或试验Internet性网络。该项目停顿之后，鉴于其强大的功能，一些爱好者与用户开始自发维护并不断改善其功能。为了更好进行沟通，Brian Behlendorf 自己建立了一个邮件列表，将其作为这个群体（或者社区）交流技术、维护软件的一个媒介，把代码重写与维护的工作有效组织起来。这些开发者们群体自称为“Apache 组织”，将这个经过不断修正并改善的服务器软件命名为 Apache 服务器（Apache Server）。该组织就是现今赫赫有名的Apache软件基金会的雏形</p><h1 id="体系架构"><a href="#体系架构" class="headerlink" title="体系架构"></a>体系架构</h1><p>Apache服务器基于模块化设计，大部分功能可被分割成相互独立的模块，通过增加和删除模块就可以扩展和修改功能。依据其功能模块，可将其架构划分为五层</p><h2 id="1-操作系统平台功能层"><a href="#1-操作系统平台功能层" class="headerlink" title="1. 操作系统平台功能层"></a>1. 操作系统平台功能层</h2><p>Apache服务器操作系统本身提供的底层功能，比如进程和线程、进程和线程的通信，网络套接字通信和文件操作等</p><h2 id="2-可移植运行库"><a href="#2-可移植运行库" class="headerlink" title="2.可移植运行库"></a>2.可移植运行库</h2><p>apr，即Apache portable runtime，操作系统适配层，实现了Apache服务器跨平台的功能。由于不同的操作系统提供的底层API不同，即实现同一个操作所用的函数方法不同，位于Apache和操作系统之间的APR根据不同的操作系统分别实现这一功能，直接用APR的统一API接口即可实现跨操作系统操作</p><h2 id="3-核心功能层"><a href="#3-核心功能层" class="headerlink" title="3.核心功能层"></a>3.核心功能层</h2><p>由核心程序和核心模块组成，用于实现基本功能和核心功能</p><ul><li><p>核心程序由核心组件构成，即配置文件组件(http_config)、进程并发组件(MPM)、连接处理组件(http_connection)、HTTP协议处理组件(http_protocol)、HTTP请求处理组件(http_request)、HTTP核心组件(http_core)、核心模块组件(mod_core) 、日志处理组件、虚拟主机处理组件、过滤模块组件，各个组件之间可相互调用，用于实现基本的功能，如启动和终止apache、处理配置文件(config.c)、接受和处理HTTP连接、读取和响应HTTP请求，处理HTTP协议。此外，还提供其余模块可调用的API，用于实现非核心模块的功能</p></li><li><p>核心模块有两个，即mod_core和mod_so，其中mod_core负责处理配置文件中的大部分配置指令，并根据这些指令运行apache；mod_so模块负责在需要的时候动态加载其余模块</p></li></ul><h2 id="4-可选功能层"><a href="#4-可选功能层" class="headerlink" title="4.可选功能层"></a>4.可选功能层</h2><p>即其余非核心的模块</p><h2 id="5-第三方支持库"><a href="#5-第三方支持库" class="headerlink" title="5.第三方支持库"></a>5.第三方支持库</h2><p>用于实现第三方的附加功能，如mod_perl、mod_ssl等</p><h1 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h1><h2 id="1-启动流程"><a href="#1-启动流程" class="headerlink" title="1.启动流程"></a>1.启动流程</h2><p>apache服务器的启动分为两个阶段，即高权限启动阶段和低权限运行阶段，通常称之为两阶段启动方式</p><p>apache服务器启动时首先初始化内存池资源，然后读取和解析apache的配置文件(httpd.conf) ，在启动的最后阶段将将控制权交给MPM，只有当MPM执行失败或结束后才把控制权交还给主程序，MPM在处理HTTP连接时使用普通用户权限，以避免root权限的攻击</p><h2 id="2-链接处理流程"><a href="#2-链接处理流程" class="headerlink" title="2.链接处理流程"></a>2.链接处理流程</h2><p>收到client端的HTTP请求并建立socket连接后，apache将读取请求信息，并调用HTTP_PROTOCOL模块将开始对该报文进行解析</p><h2 id="3-请求处理流程"><a href="#3-请求处理流程" class="headerlink" title="3.请求处理流程"></a>3.请求处理流程</h2><p>处理HTTP请求报文，分为三个阶段，即请求解析阶段、安全处理阶段、请求准备阶段</p><ul><li>请求解析阶段，主要是包含URL的一些处理，如字符转义、名称转换、重定向等</li><li>安全处理，即访问控制、身份认证、用户授权</li><li>请求准备阶段，确定客户端请求资源类型(html txt gif ）及补丁修复，如优化操作等</li></ul><h2 id="4-响应生成流程"><a href="#4-响应生成流程" class="headerlink" title="4.响应生成流程"></a>4.响应生成流程</h2><p>对于静态的HTML文件直接读取文件返回给客户端，对于动态文件，如脚本CGI、js、数据库文件等，将调用对应的处理器生成请求的内容，生成内容再进入过滤器进行内容过滤，通过最后一个过滤器(网络过滤器)后，将响应内容发送到网络，最后传输到客户端并在浏览器中显示</p><hr><h1 id="1-安装依赖包"><a href="#1-安装依赖包" class="headerlink" title="1.安装依赖包"></a>1.安装依赖包</h1><pre><code class="hljs">yum install -y gcc make openssl-devel pcre-devel zlib-devel perl-devel expat-devel</code></pre><h1 id="2-编译安装apr"><a href="#2-编译安装apr" class="headerlink" title="2.编译安装apr"></a>2.编译安装apr</h1><pre><code class="hljs">tar -zxvf apr-1.5.2.tar.gz &amp;&amp; cd apr-1.5.2./configure --prefix=/usr/local/aprmake &amp;&amp;  make install </code></pre><h1 id="3-编译安装apr-util"><a href="#3-编译安装apr-util" class="headerlink" title="3.编译安装apr-util"></a>3.编译安装apr-util</h1><pre><code class="hljs">tar -zxvf apr-util-1.5.4.tar.gz &amp;&amp; cd apr-util-1.5.4./configure --prefix=/usr/local/apr-util --with-apr=/usr/local/aprmake &amp;&amp;  make install</code></pre><h1 id="4-编译安装apache"><a href="#4-编译安装apache" class="headerlink" title="4.编译安装apache"></a>4.编译安装apache</h1><pre><code class="hljs">tar -zxvf httpd-2.4.25.tar.gz &amp;&amp; cd httpd-2.4.25./configure --prefix=/usr/local/httpd --sysconfdir=/etc/httpd/conf \--enable-cache --enable-disk-cache --enable-mem-cache --enable-file-cache \--enable-proxy --enable-proxy-http --enable-proxy-balancer --enable-proxy-ftp --enable-proxy-connect --enable-proxy-ajp \--enable-deflate=static --enable-headers=static --enable-rewrite=static --enable-expires=static --enable-ssl=static --enable-cgid=static \--with-zlib --with-pcre --enable-mods-shared=all --enable-mpms-shared=all --with-mpm=worker --with-apr=/usr/local/apr --with-apr-util=/usr/local/apr-utilmake &amp;&amp;  make install</code></pre><h1 id="5-修改配置文件"><a href="#5-修改配置文件" class="headerlink" title="5.修改配置文件"></a>5.修改配置文件</h1><pre><code class="hljs">sed -i &#39;s@#ServerName www.example.com:80@ServerName localhost:80@g&#39; /etc/httpd/conf/httpd.conf</code></pre><h1 id="6-配置Apache启动脚本"><a href="#6-配置Apache启动脚本" class="headerlink" title="6.配置Apache启动脚本"></a>6.配置Apache启动脚本</h1><pre><code class="hljs">cp /usr/local/httpd/bin/apachectl /etc/init.d/httpd</code></pre><h1 id="7-启动Apache服务器"><a href="#7-启动Apache服务器" class="headerlink" title="7.启动Apache服务器"></a>7.启动Apache服务器</h1><pre><code class="hljs">/etc/init.d/httpd start</code></pre><h1 id="8-访问Apache服务器"><a href="#8-访问Apache服务器" class="headerlink" title="8.访问Apache服务器"></a>8.访问Apache服务器</h1><p><a href="http://ip/">http://ip</a></p>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>Apache</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux系统磁盘分区与文件系统</title>
    <link href="/linux/Partition/"/>
    <url>/linux/Partition/</url>
    
    <content type="html"><![CDATA[<p>磁盘，即Disk，可反复擦写的计算机外部存储器设备，是计算机的信息载体。目前一般为硬盘，分为固态硬盘机械硬盘HDD和固态硬盘SSD两种，HDD以磁性碟片存储信息，SSD则是闪存颗粒。此外，还有一种混合硬盘，即HHD，也就是磁性硬盘和闪存的集成</p><h1 id="1-硬盘接口类型"><a href="#1-硬盘接口类型" class="headerlink" title="1.硬盘接口类型"></a>1.硬盘接口类型</h1><p>硬盘接口是硬盘与主机系统间的连接部件，作用是在硬盘缓存和主机内存之间传输数据。不同的硬盘接口决定了硬盘与计算机之间的连接速度，在整个系统中硬盘接口的优劣直接影响着程序运行快慢和系统性能好坏，主要分为IDE、SATA、SCSI、光纤通道和SAS</p><h2 id="1-IDE"><a href="#1-IDE" class="headerlink" title="1.IDE"></a>1.IDE</h2><p>IDE，Integrated Drive Electronics，即电子集成驱动器，也就是将硬盘控制器与盘体集成的硬盘驱动器，以缩减硬盘接口的电缆数目与长度，使得制造更轻易，不必再考虑与其他厂商控制器的兼容性，且保障了数据传输的可靠性。目前已被淘汰</p><h1 id="2-SATA"><a href="#2-SATA" class="headerlink" title="2.SATA"></a>2.SATA</h1><p>SATA，Serial ATA，又被称为串口硬盘，是PC机硬盘的主流趋势。2001年，由Intel、APT、Dell、IBM、希捷、迈拓几大厂商组成的Serial ATA委员会正式确立了Serial ATA 1.0规范。Serial ATA采用串行连接方式，串行ATA总线使用嵌入式时钟信号，具备更强的纠错能力，可对传输指令与数据进行检查，发现错误将会自动矫正，大大提高了数据传输的可靠性</p><h1 id="3-SCSI"><a href="#3-SCSI" class="headerlink" title="3.SCSI"></a>3.SCSI</h1><p>SCSI，Small Computer System Interface，即小型计算机系统接口，是同IDE（ATA）完全不同的接口，广泛应用于小型机上的高速数据传输技术。SCSI接口具有应用范围广、多任务、带宽大、CPU占用率低及热插拔等优点，但较高的价格使得它很难如IDE硬盘般普及，因此主要应用于中高端服务器和高档工作站中</p><h1 id="4-光纤通道"><a href="#4-光纤通道" class="headerlink" title="4.光纤通道"></a>4.光纤通道</h1><p>光纤通道，即Fibre Channel，起初专为网络系统设计，但随着存储系统对速度的需求，逐渐应用到硬盘系统。光纤通道的主要特性有：热插拔性、高速带宽、远程连接、连接设备数量大等，适用于服务器这样的多硬盘系统环境，能满足高端工作站、服务器、海量存储子网络、外设间通过集线器、交换机和点对点连接进行双向、串行数据通讯等对高数据传输率的要求</p><h1 id="5-5SAS"><a href="#5-5SAS" class="headerlink" title="5.5SAS"></a>5.5SAS</h1><p>SAS，Serial Attached SCSI，即串行连接SCSI，新一代的SCSI技术，兼容SATA硬盘</p><h1 id="2-硬盘分区"><a href="#2-硬盘分区" class="headerlink" title="2.硬盘分区"></a>2.硬盘分区</h1><p>硬盘分区，即Disk Partition，是将硬盘的整体存储空间划分成多个独立的逻辑区域，分别用于安装操作系统、应用程序及存储数据文件等，使得数据存放和管理更为方便与快捷。分区的规划非常必要，如一块硬盘划分为几个分区，每个分区多大容量，以及每个分区准备使用什么文件系统等。对于某些操作系统而言，硬盘必须分区后才能使用，否则不能被识别</p><h2 id="2-1-分区类型"><a href="#2-1-分区类型" class="headerlink" title="2.1 分区类型"></a>2.1 分区类型</h2><h3 id="2-1-1-非DOS分区"><a href="#2-1-1-非DOS分区" class="headerlink" title="2.1.1 非DOS分区"></a>2.1.1 非DOS分区</h3><p>非DOS分区，即Non-DOS Partition，一种特殊的分区形式，是将硬盘中的一块区域单独划分出来供另一个操作系统使用，对主分区的操作系统来讲是被划分出去的存储空间，且只有非DOS分区的操作系统才能管理和使用这块存储区域</p><h3 id="2-1-2-主分区"><a href="#2-1-2-主分区" class="headerlink" title="2.1.2 主分区"></a>2.1.2 主分区</h3><p>主分区通常位于硬盘最前面的区域，用于存储主引导程序，负责检测硬盘分区的正确性及确定活动分区，并将引导权移交给活动分区的操作系统。主分区损坏将无法从硬盘引导系统，可从软驱或光驱引导之后再对硬盘读写</p><h3 id="2-1-3-扩展分区"><a href="#2-1-3-扩展分区" class="headerlink" title="2.1.3 扩展分区"></a>2.1.3 扩展分区</h3><p>扩展分区，严格地讲并不是一个实际意义的分区，仅仅是一个指向下一个分区的指针，从而将形成一个标明分区起始位置的单向链表。这样，在主引导扇区中除了主分区外，就逐个类推找到所有的分区。无论系统中建立多少个逻辑磁盘，在主引导扇区中通过一个扩展分区的参数就可以逐个找到每一个逻辑磁盘。扩展分区不能直接用，实际上是以若干逻辑分区作为单独的分区使用，也即所有逻辑分区都属于扩展分区的一部分</p><h2 id="2-2-文件系统"><a href="#2-2-文件系统" class="headerlink" title="2.2 文件系统"></a>2.2 文件系统</h2><p>文件系统，file system，即存储系统组织文件的方式，随磁盘分区格式化而建立。文件系统负责管理和存储文件信息，属于操作系统软件，是操作系统用于明确磁盘或分区文件的方法和数据结构，常见的文件系统如下：</p><h3 id="2-2-1-FAT16-x2F-FAT32"><a href="#2-2-1-FAT16-x2F-FAT32" class="headerlink" title="2.2.1 FAT16&#x2F;FAT32"></a>2.2.1 FAT16&#x2F;FAT32</h3><p>早期MS-DOS和Windows操作系统常见的磁盘分区格式，采用16&#x2F;32位文件分配表，磁盘利用效率低，运行速度慢</p><h3 id="2-2-2-NTFS"><a href="#2-2-2-NTFS" class="headerlink" title="2.2.2 NTFS"></a>2.2.2 NTFS</h3><p>优点是安全性和稳定性极其出色，使用中不易产生文件碎片，且能对用户的操作进行记录，权限管理严格，使每个用户只能按照系统赋予的权限进行操作，充分保护了系统与数据的安全，常见于Windows NT、Windows 2000&#x2F;Vista&#x2F;7&#x2F;8&#x2F;10等</p><h3 id="2-2-3-EXT2-x2F-EXT3-x2F-EXT4"><a href="#2-2-3-EXT2-x2F-EXT3-x2F-EXT4" class="headerlink" title="2.2.3 EXT2&#x2F;EXT3&#x2F;EXT4"></a>2.2.3 EXT2&#x2F;EXT3&#x2F;EXT4</h3><p>EXT，Linux extended file system，即Linux扩展文件系统，是Linux操作系统的磁盘格式，使用索引节点记录文件信息，类似于Windows文件分配表。索引节点包含文件长度、创建及修改时间、权限、数据区块等信息，由文件系统维护成为数组，每个文件或目录都与索引节点数组中的唯一一个元素对应，并分配了一个索引节点号。Linux文件系统将文件索引节点号和文件名同时保存在目录，也即是说目录是将文件名及其索引节点号结合的一张表，目录中每一对文件名和索引节点号称为一个连接。一个文件对应一个索引节点号，一个索引节点号可有多个文件名与之对应。因此，在磁盘上的同一个文件可以通过不同的路径去访问</p><p>EXT2是Linux早期的文件系统，EXT3支持日志文件系统，EXT4是EXT3的升级版，支持更大的文件系统和更高的性能</p><h3 id="2-2-4-XFS"><a href="#2-2-4-XFS" class="headerlink" title="2.2.4 XFS"></a>2.2.4 XFS</h3><p>XFS是由SGI公司设计的被称为业界最先进最具可升级性的高性能文件系统，可以支持大容量的数据存储，并具有良好的IO性能，扩展性优于EXT4。Centos默认的文件系统即为XFS</p><h2 id="2-3-挂载点"><a href="#2-3-挂载点" class="headerlink" title="2.3 挂载点"></a>2.3 挂载点</h2><p>挂载点，即Linux系统将文件系统（磁盘分区、磁盘或网络共享等）连接到本地的目录，用于文件的访问和操作，从而将不同的文件系统整合为一个统一的文件结构。建议将不同的硬盘分区、外部存储设备、网络共享等挂载到不同的挂载点，以实现数据的组织和管理</p><h1 id="3-目录结构"><a href="#3-目录结构" class="headerlink" title="3.目录结构"></a>3.目录结构</h1><p>Linux操作系统所有的文件和目录被组织成以一个根节点开始的倒置的树状结构，文件系统的最顶层由根目录开始，使用&#x2F;来表示根目录，根目录之下既可以是目录也可以是文件，每一个目录又可以包含子目录文件，如此反复</p><h2 id="3-1-x2F-bin"><a href="#3-1-x2F-bin" class="headerlink" title="3.1 &#x2F;bin"></a>3.1 &#x2F;bin</h2><p>bin，Binaries，即二进制文件，用于存储经常使用的命令</p><h2 id="3-2-x2F-boot"><a href="#3-2-x2F-boot" class="headerlink" title="3.2 &#x2F;boot"></a>3.2 &#x2F;boot</h2><p>引导分区，用于存储Linux系统启动时的核心文件，包括一些连接文件及镜像文件</p><h2 id="3-3-x2F-dev"><a href="#3-3-x2F-dev" class="headerlink" title="3.3 &#x2F;dev"></a>3.3 &#x2F;dev</h2><p>dev，Device，即设备，Linux外部设备的挂载点，访问方式和文件一致</p><h2 id="3-4-x2F-etc"><a href="#3-4-x2F-etc" class="headerlink" title="3.4 &#x2F;etc"></a>3.4 &#x2F;etc</h2><p>etc，Etcetera，用于存储系统管理所需要的配置文件和子目录</p><h2 id="3-5-x2F-home"><a href="#3-5-x2F-home" class="headerlink" title="3.5 &#x2F;home"></a>3.5 &#x2F;home</h2><p>用户家目录的挂载点，Linux系统每个用户都有自己的主目录，一般以用户账号命名，如alice、bob和eve</p><h2 id="3-6-x2F-lib"><a href="#3-6-x2F-lib" class="headerlink" title="3.6 &#x2F;lib"></a>3.6 &#x2F;lib</h2><p>lib，Library，即库，用于存储Linux系统最基本的动态连接共享库，作用类似于Windows的DLL文件，几乎所有应用程序都要用到这些共享库</p><h2 id="3-7-x2F-lost-found"><a href="#3-7-x2F-lost-found" class="headerlink" title="3.7 &#x2F;lost+found"></a>3.7 &#x2F;lost+found</h2><p>一般情况下为空，用于存储系统非法关机后生成的文件</p><h2 id="3-8-x2F-media"><a href="#3-8-x2F-media" class="headerlink" title="3.8 &#x2F;media"></a>3.8 &#x2F;media</h2><p>通常用于临时挂载设备，如USB驱动器、光盘等</p><h2 id="3-9-x2F-mnt"><a href="#3-9-x2F-mnt" class="headerlink" title="3.9 &#x2F;mnt"></a>3.9 &#x2F;mnt</h2><p>用于临时挂载文件系统，如光驱等</p><h2 id="3-10-x2F-opt"><a href="#3-10-x2F-opt" class="headerlink" title="3.10 &#x2F;opt"></a>3.10 &#x2F;opt</h2><p>opt，optional，即可选，用于额外安装软件的目录，如Oracle数据库安装目录</p><h2 id="3-11-x2F-proc"><a href="#3-11-x2F-proc" class="headerlink" title="3.11 &#x2F;proc"></a>3.11 &#x2F;proc</h2><p>proc，Processes，即进程，伪文件系统，是一个虚拟目录，也称虚拟文件系统，用于存储当前内核运行状态的一系列特殊文件，是系统内存的映射，也即是说这个目录的信息不实存储于硬盘而是内存，可以直接访问这个目录以获取系统信息，也可以直接修改某些文件，如屏蔽主机的ping命令而主机无法被ping：</p><pre><code class="hljs">echo 1 &gt; /proc/sys/net/ipv4/icmp_echo_ignore_all</code></pre><h2 id="3-12-x2F-root"><a href="#3-12-x2F-root" class="headerlink" title="3.12 &#x2F;root"></a>3.12 &#x2F;root</h2><p>该目录为系统管理员，也称作超级管理员的主目录</p><h2 id="3-13-x2F-sbin"><a href="#3-13-x2F-sbin" class="headerlink" title="3.13 &#x2F;sbin"></a>3.13 &#x2F;sbin</h2><p>Super User Binaries，即超级用户的二进制文件，存储系统管理员使用的系统管理程序</p><h2 id="3-14-x2F-selinux"><a href="#3-14-x2F-selinux" class="headerlink" title="3.14 &#x2F;selinux"></a>3.14 &#x2F;selinux</h2><p>Redhat&#x2F;CentOS特有目录，用于存储Selinux安全机制的相关文件，类似于Windows防火墙，但这套机制比较复杂，不易操控，一般都建议关闭</p><h2 id="3-15-x2F-srv"><a href="#3-15-x2F-srv" class="headerlink" title="3.15 &#x2F;srv"></a>3.15 &#x2F;srv</h2><p>用于存储一些服务启动之后需要提取的数据</p><h2 id="3-16-x2F-sys"><a href="#3-16-x2F-sys" class="headerlink" title="3.16 &#x2F;sys"></a>3.16 &#x2F;sys</h2><p>sysfs文件系统的挂载点，用于存储内核对象所对应的文件和目录，是内核设备树的直观反映，集成于Linux2.6内核，包含三种文件系统的信息：针对进程信息的 proc文件系统、针对设备的devfs文件系统及针对伪终端的devpts文件系统</p><h2 id="3-17-x2F-tmp"><a href="#3-17-x2F-tmp" class="headerlink" title="3.17 &#x2F;tmp"></a>3.17 &#x2F;tmp</h2><p>tmp，temporary，即临时的，用于存放临时文件，对于一个大型多用户的系统或者网络服务器，建议创建单独的分区</p><h2 id="3-18-x2F-usr"><a href="#3-18-x2F-usr" class="headerlink" title="3.18 &#x2F;usr"></a>3.18 &#x2F;usr</h2><p> usr，unix shared resources，即共享资源，用于存储应用程序和文件，类似于Windows的program files目录</p><h2 id="3-19-x2F-usr-x2F-bin"><a href="#3-19-x2F-usr-x2F-bin" class="headerlink" title="3.19 &#x2F;usr&#x2F;bin"></a>3.19 &#x2F;usr&#x2F;bin</h2><p>用于存储系统用户所使用的应用程序</p><h2 id="3-20-x2F-usr-x2F-sbin"><a href="#3-20-x2F-usr-x2F-sbin" class="headerlink" title="3.20 &#x2F;usr&#x2F;sbin"></a>3.20 &#x2F;usr&#x2F;sbin</h2><p>用于存储超级用户所使用的比较高级的管理程序和系统守护程序</p><h2 id="3-21-x2F-usr-x2F-src"><a href="#3-21-x2F-usr-x2F-src" class="headerlink" title="3.21 &#x2F;usr&#x2F;src"></a>3.21 &#x2F;usr&#x2F;src</h2><p>用于存储Linux系统内核源程序和RPM&#x2F;DEB包的源程序，以及内核所有源程序与新版本内核（主机可能有保存几个不同版本内核的需求）</p><h2 id="3-22-x2F-var"><a href="#3-22-x2F-var" class="headerlink" title="3.22 &#x2F;var"></a>3.22 &#x2F;var</h2><p>var，variable，即变量，用于存储持续扩充的文件，如日志文件，一般写入&#x2F;var&#x2F;log，打印队列文件通常写入&#x2F;var&#x2F;spool</p><h2 id="3-23-x2F-run"><a href="#3-23-x2F-run" class="headerlink" title="3.23 &#x2F;run"></a>3.23 &#x2F;run</h2><p>用于存储系统启动之后的信息，临时文件系统，系统重启将被清空，建议将&#x2F;var&#x2F;run目录指向&#x2F;run</p><h2 id="3-24-swap"><a href="#3-24-swap" class="headerlink" title="3.24 swap"></a>3.24 swap</h2><p>交换分区，用于创建虚拟内存，物理内存不足时数据将被写入</p><h1 id="4-分区方案"><a href="#4-分区方案" class="headerlink" title="4.分区方案"></a>4.分区方案</h1><h2 id="4-1-普通分区"><a href="#4-1-普通分区" class="headerlink" title="4.1 普通分区"></a>4.1 普通分区</h2><ul><li><p>swap，建议为物理内存小于8G时设为物理内存的2倍，小于64G设为物理内存的1.5倍，大于64G设为大于4G</p></li><li><p>&#x2F;boot，设为1G</p></li><li><p>&#x2F;(root)，剩余所有空间</p></li></ul><h2 id="4-2-数据库服务器"><a href="#4-2-数据库服务器" class="headerlink" title="4.2 数据库服务器"></a>4.2 数据库服务器</h2><p>普通分区方案之外，创建数据挂载点用于挂载额外的磁盘，并将数据库的数据目录映射到该挂载点，如MySQL数据库数据文件目录设为&#x2F;var&#x2F;lib&#x2F;mysql，以便于升级或重装系统之后数据库的数据无需再进行恢复</p><h2 id="4-3-文件服务器"><a href="#4-3-文件服务器" class="headerlink" title="4.3 文件服务器"></a>4.3 文件服务器</h2><p>普通分区方案之外，创建独立的分区用于文件的存储</p><h2 id="4-4-网站服务器"><a href="#4-4-网站服务器" class="headerlink" title="4.4 网站服务器"></a>4.4 网站服务器</h2><p>普通分区之外，创建独立的分区用于存储网站目录，如&#x2F;var&#x2F;www</p>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>存储</tag>
      
      <tag>文件系统</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CentOS7配置国内Yum源</title>
    <link href="/linux/CentOSYum/"/>
    <url>/linux/CentOSYum/</url>
    
    <content type="html"><![CDATA[<p>Yum，“Yellow dog Updater, Modified”，RedHat系列发行版的RPM软件包管理器，即从指定的服务器自动下载、安装RPM包，并自动的、一次性的解决软件包的依赖关系，无需编译安装那样繁琐地一次次下载、编译及安装，节省了大量的查找、安装依赖包的时间。Yum由软件源仓库和客户端两部分组成，官方源由于防火墙的原因下载速度受限，一般都要更改为国内源，以提高下载速度</p><h1 id="1-备份原yum源配置文件"><a href="#1-备份原yum源配置文件" class="headerlink" title="1.备份原yum源配置文件"></a>1.备份原yum源配置文件</h1><pre><code class="hljs">cd /etc/yum.repos.dmv CentOS-Base.repo CentOS-Base.repo.bak</code></pre><h1 id="2-创建yum源配置文件"><a href="#2-创建yum源配置文件" class="headerlink" title="2.创建yum源配置文件"></a>2.创建yum源配置文件</h1><pre><code class="hljs">vi CentOS-Base.repo# CentOS-Base.repo## The mirror system uses the connecting IP address of the client and the# update status of each mirror to pick mirrors that are updated to and# geographically close to the client.  You should use this for CentOS updates# unless you are manually picking other mirrors.## If the mirrorlist= does not work for you, as a fall back you can try the# remarked out baseurl= line instead.##[os]name=Qcloud centos os - $basearchbaseurl=http://mirrors.cloud.tencent.com/centos/$releasever/os/$basearch/enabled=1gpgcheck=1gpgkey=http://mirrors.cloud.tencent.com/centos/RPM-GPG-KEY-CentOS-7[updates]name=Qcloud centos updates - $basearchbaseurl=http://mirrors.cloud.tencent.com/centos/$releasever/updates/$basearch/enabled=1gpgcheck=1gpgkey=http://mirrors.cloud.tencent.com/centos/RPM-GPG-KEY-CentOS-7[centosplus]name=Qcloud centosplus - $basearch baseurl=http://mirrors.cloud.tencent.com/centos/$releasever/centosplus/$basearch/enabled=0gpgcheck=1gpgkey=http://mirrors.cloud.tencent.com/centos/RPM-GPG-KEY-CentOS-7[cr]name=Qcloud centos cr - $basearchbaseurl=http://mirrors.cloud.tencent.com/centos/$releasever/cr/$basearch/enabled=0gpgcheck=1gpgkey=http://mirrors.cloud.tencent.com/centos/RPM-GPG-KEY-CentOS-7[extras]name=Qcloud centos extras - $basearchbaseurl=http://mirrors.cloud.tencent.com/centos/$releasever/extras/$basearch/enabled=1gpgcheck=1gpgkey=http://mirrors.cloud.tencent.com/centos/RPM-GPG-KEY-CentOS-7[fasttrack]name=Qcloud centos fasttrack - $basearchbaseurl=http://mirrors.cloud.tencent.com/centos/$releasever/fasttrack/$basearch/enabled=0gpgcheck=1gpgkey=http://mirrors.cloud.tencent.com/centos/RPM-GPG-KEY-CentOS-7</code></pre><h1 id="3-清理缓存"><a href="#3-清理缓存" class="headerlink" title="3.清理缓存"></a>3.清理缓存</h1><pre><code class="hljs">yum clean allyum makecache</code></pre><h1 id="4-安装常用工具软件"><a href="#4-安装常用工具软件" class="headerlink" title="4.安装常用工具软件"></a>4.安装常用工具软件</h1><pre><code class="hljs">yum install -y bash-completion epel-release net-tools</code></pre>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>CentOS</tag>
      
      <tag>Yum</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CentOS配置静态IP</title>
    <link href="/linux/CentOSIP/"/>
    <url>/linux/CentOSIP/</url>
    
    <content type="html"><![CDATA[<h1 id="1-备份网卡配置文件"><a href="#1-备份网卡配置文件" class="headerlink" title="1.备份网卡配置文件"></a>1.备份网卡配置文件</h1><pre><code class="hljs">cd /etc/sysconfig/network-scriptscp ifcfg-enp3s0 ifcfg-enp3s0.bak</code></pre><h1 id="2-修改网卡配置"><a href="#2-修改网卡配置" class="headerlink" title="2.修改网卡配置"></a>2.修改网卡配置</h1><pre><code class="hljs">vi ifcfg-enp3s0TYPE=&quot;Ethernet&quot;PROXY_METHOD=&quot;none&quot;BROWSER_ONLY=&quot;no&quot;BOOTPROTO=&quot;static&quot;DEFROUTE=&quot;yes&quot;IPV4_FAILURE_FATAL=&quot;no&quot;IPV6INIT=&quot;yes&quot;IPV6_AUTOCONF=&quot;yes&quot;IPV6_DEFROUTE=&quot;yes&quot;IPV6_FAILURE_FATAL=&quot;no&quot;IPV6_ADDR_GEN_MODE=&quot;stable-privacy&quot;NAME=&quot;enp3s0&quot;UUID=&quot;fbc1994d-a800-4026-8188-765cb2747088&quot;DEVICE=&quot;enp3s0&quot;ONBOOT=&quot;yes&quot;IPADDR=172.16.100.100NETMASK=255.255.255.0GATEWAY=172.16.100.1DNS1=172.16.100.1DNS2=8.8.8.8</code></pre><h1 id="3-重启网络服务"><a href="#3-重启网络服务" class="headerlink" title="3.重启网络服务"></a>3.重启网络服务</h1><pre><code class="hljs">systemctl restart network.service</code></pre>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>CentOS</tag>
      
      <tag>网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux系统</title>
    <link href="/linux/Linux/"/>
    <url>/linux/Linux/</url>
    
    <content type="html"><![CDATA[<p>Linux，自由与开源的基于POSIX和UNI多用户、多任务、支持多线程和多CPU的类UNIX操作系统，能运行主要的UNIX工具软件、应用程序和网络协议，支持32位和64位硬件，继承了Unix以网络为核心的设计思想，是一个性能稳定的多用户网络操作系统</p><h1 id="Linux之父-林纳斯"><a href="#Linux之父-林纳斯" class="headerlink" title="Linux之父-林纳斯"></a>Linux之父-林纳斯</h1><p>Linux内核最初是由由芬兰人林纳斯·托瓦兹（Linus Torvalds）1991年于赫尔辛基大学读书时出于个人爱好而编写，最初目的就是建立不受任何商业化软件版权制约的、全世界都能自由使用的类 Unix 操作系统兼容产品。其后在自由软件之父理查德·斯托曼（Richard Stallman）某些精神的感召下，林纳斯将这款类Unix的操作系统命名为Linux，并加入到自由软件基金（FSF）的GNU计划中，即任何人通过GPL的通用性授权之后都可销售、拷贝及改动程序，但必须将同样的自由传递下去，且须免费公开修改后的代码。林纳斯的这一举措给他自己以及Linux赢得了巨大的成功和极高的声誉，短短几年间，Linux就聚集了成千上万的狂热分子不计得失的为Linux增补、修改，并随之将开源运动的自由主义精神传扬下去，人们几乎像看待神明一样对林纳斯顶礼膜拜。</p><ul><li>有些人生来就具有统率百万人的领袖风范，另一些人则是为写出颠覆世界的软件而生。唯一一个能同时做到这两者的人，就是托瓦兹</li></ul><p>这是美国《时代》周刊给林纳斯的评价，而在《时代》周刊根据读者投票评选出的二十世纪100位最重要人物中，林纳斯排到15位，而从20世纪的最后几年就开始霸占全球首富称号的盖茨是17位</p><p>事实上，与做出的贡献相比，林纳斯所获赞誉远不止此。他创造的“优秀的开源软件吸引更多人对开源进行贡献”这一机制，所引领的浪潮极其快速的席卷了全球，从而引导了这个世界上绝大多数电子设备的诞生，如路由器、交换机、智能手机、服务器等</p><p>林纳斯，创造了开源世界，真正的云上信息时代由此而开启</p><h1 id="Linux系统架构"><a href="#Linux系统架构" class="headerlink" title="Linux系统架构"></a>Linux系统架构</h1><p>Linux系统由四部分组成，即内核、shell、文件系统和应用程序，其中内核、shell和文件系统一起形成了基本的操作系统结构，使得用户可以运行程序、管理文件并使用系统</p><h2 id="1-内核"><a href="#1-内核" class="headerlink" title="1.内核"></a>1.内核</h2><p>内核，操作系统的核心，负责管理系统的进程、内存、设备驱动程序、文件和网络系统等操作系统的基本功能，决定系统的性能和稳定性</p><h2 id="2-Shell"><a href="#2-Shell" class="headerlink" title="2.Shell"></a>2.Shell</h2><p>shell，系统用户界面，提供用户与内核进行交互操作的接口，接收用户输入的命令并把它送入内核去执行，是一个命令解释器</p><p>shell版本</p><ul><li>Bourne Shell，由贝尔实验室开发</li><li>BASH，GNU的Bourne Again Shell，是GNU操作系统上默认的shell，为大部分linux发行版所使用</li><li>Korn Shell，Bourne SHell的延伸版，大部分与Bourne Shell兼容</li><li>C Shell，SUN公司Shell的BSD版本</li></ul><h2 id="3-文件系统"><a href="#3-文件系统" class="headerlink" title="3.文件系统"></a>3.文件系统</h2><p>文件系统，即文件存放在磁盘等存储设备上的组织方法，Linux系统能支持多种目前流行的文件系统，如EXT2、 EXT3、 FAT、 FAT32、 VFAT和ISO9660等</p><h2 id="4-应用程序"><a href="#4-应用程序" class="headerlink" title="4.应用程序"></a>4.应用程序</h2><p>Linux系统一般都有一套称为应用程序的程序集，包括文本编辑器、编程语言、X Window、办公套件、Internet工具和数据库等</p><h1 id="Linux发行版"><a href="#Linux发行版" class="headerlink" title="Linux发行版"></a>Linux发行版</h1><p>Linux发行版，是指一些组织或厂商将林纳斯所开发的Linux内核与各种软件和文档包装起来，并提供系统安装界面和系统配置、设定与管理工具所构成的一套完整的Linux操作系统</p><p>Linux发行版大体分为两类，即商业公司维护的发行版，如RedHat；社区组织维护的发行版，如Debian</p><h2 id="1-Red-Hat-Linux"><a href="#1-Red-Hat-Linux" class="headerlink" title="1.Red Hat Linux"></a>1.Red Hat Linux</h2><p>Red Hat，红帽公司，创建于1993年，是目前世界上资深的Linux厂商，产品主要包括RHEL（Red Hat Enterprise Linux，收费）和CentOS（RHEL的社区克隆版本，免费）、Fedora Core（由RedHat桌面版发展而来，免费版本）。特别是CentOS，国内使用人群最多、资料丰富，且大多数Linux教程都以之作为基础，推荐初学者使用</p>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>云计算</title>
    <link href="/linux/CloudComputing/"/>
    <url>/linux/CloudComputing/</url>
    
    <content type="html"><![CDATA[<p>云计算，即Cloud Computing，是当今最热门的IT技术之一，是继1980年代大型计算机到客户端-服务器的大转变之后的又一巨变。云计算使高性能并行计算走近普通用户，意味着计算能力也可以作为一种商品进行流通，就像煤气、水电一样，取用方便，费用低廉</p><h1 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h1><p>云计算是分布式计算的一种，最基本的原理是通过网络，将运行巨量数据的计算程序拆分成无数个小程序，再交由大批量服务器所组成的大型的计算和存储资源池系统平台，最后经过分析、计算及汇总之后将处理结果反馈给用户。这个网络，即抽象为云</p><p>云计算的核心理念是按需服务，即为用户提供可扩缩容的随取随用的计算、存储、网络资源。类似于自来水，只需安装水龙头接上自来水厂的管道随用随取，而无需新建自来水厂。即自来水厂提供水资源服务，云计算提供计算资源服务</p><h1 id="服务模式"><a href="#服务模式" class="headerlink" title="服务模式"></a>服务模式</h1><ul><li>IaaS，Infrastructure as a server，基础设施即服务，提供计算资源，包括服务器、网络带宽、存储和数据中心空间。无须再架构任何硬件设施，配置软件运行环境后直接部署，且基础架构支持按需扩展以支持动态负载，可根据需要提供灵活的服务</li><li>PaaS，Platform as a Server，平台即服务，自带运行环境，直接进行部署，操作系统、数据库、中间件和运行库等平台自带，只需专注于具体承载业务的那部分应用软件即可</li><li>SaaS，Software as a Server，软件即服务，应用软件已在云端服务器运行，通过浏览器直接登录即可进行业务流转，最大程度地降低技术门槛</li></ul><h1 id="部署模式"><a href="#部署模式" class="headerlink" title="部署模式"></a>部署模式</h1><ul><li>公有云，第三方提供商提供给用户进行使用的云，通过互联网访问，所有硬件、软件及其他支持基础架构均由云提供商拥有和管理</li><li>私有云，为企业单独使用而构建的云，对基础设施、应用程序、数据、安全性和服务质量有完全的控制权</li><li>混合云，公有云和私有云两种方式的结合</li></ul><h1 id="云资源"><a href="#云资源" class="headerlink" title="云资源"></a>云资源</h1><ul><li>计算资源，即CPU和内存，由虚拟化技术实现</li><li>存储资源，即磁盘，由分布式存储技术实现</li><li>网络，即网络带宽，由负载均衡、高可用及网关等技术实现</li></ul><h1 id="技术路线"><a href="#技术路线" class="headerlink" title="技术路线"></a>技术路线</h1><h2 id="1-Linux"><a href="#1-Linux" class="headerlink" title="1.Linux"></a>1.Linux</h2><p>Linux操作系统的安装、配置与维护，如系统盘制作、系统安装、网络配置、内核参数、定时任务、磁盘分区与扩容等</p><h2 id="2-基础服务"><a href="#2-基础服务" class="headerlink" title="2.基础服务"></a>2.基础服务</h2><p>操作系统基础服务的安装与配置，如FTP、NTP、NFS、DNS、Email、Git、SVN等</p><h2 id="3-Web服务"><a href="#3-Web服务" class="headerlink" title="3.Web服务"></a>3.Web服务</h2><p>Web服务器的部署与配置及手工发布Web平台，如Tomcat&#x2F;Jboss、PHP、Websphere、Weblogic等</p><h2 id="4-负载均衡"><a href="#4-负载均衡" class="headerlink" title="4.负载均衡"></a>4.负载均衡</h2><p>负载均衡集群的部署与配置，如Nginx&#x2F;Apache、Haproxy、LVS等</p><h2 id="5-高可用"><a href="#5-高可用" class="headerlink" title="5.高可用"></a>5.高可用</h2><p>高可用集群搭建与维护，如Keepalived&#x2F;Heartbeat等</p><h2 id="6-中间件"><a href="#6-中间件" class="headerlink" title="6.中间件"></a>6.中间件</h2><p>中间件部署、配置与维护，如Zookeeper、Kafka、Activemq、Fastdfs等</p><h2 id="7-数据库"><a href="#7-数据库" class="headerlink" title="7.数据库"></a>7.数据库</h2><p>数据库服务器及集群的部署、管理、扩容、调优、灾备及故障处理，如MySQL&#x2F;Mariadb的读写分离、Mycat&#x2F;Atlas&#x2F;MySQL-Proxy&#x2F;MySQL-Router&#x2F;Amoeba代理配置及MySQL Galera Cluster集群配置；MongoDB、Oracle数据库集群搭建；SQL语句及其调优；数据库存储过程的编写；LDAP的配置与管理</p><h2 id="8-缓存服务"><a href="#8-缓存服务" class="headerlink" title="8.缓存服务"></a>8.缓存服务</h2><p>缓存服务的部署与配置，如Redis、Memcatched、Squid、Varnish等</p><h2 id="9-监控告警"><a href="#9-监控告警" class="headerlink" title="9.监控告警"></a>9.监控告警</h2><p>监控告警系统平台的建立，将所有资源都纳入其中，如Zabbix&#x2F;Prometheus监控系统和ELK日志分析平台的部署与维护</p><h2 id="10-计算机网络"><a href="#10-计算机网络" class="headerlink" title="10.计算机网络"></a>10.计算机网络</h2><p>网络配置及故障排查，如TCP&#x2F;IP网络协议、网络流量分析及抓包分析等</p><h2 id="11-脚本语言"><a href="#11-脚本语言" class="headerlink" title="11.脚本语言"></a>11.脚本语言</h2><p>编写脚本批量执行重复任务，如Shell、Python等</p><h2 id="12-自动化运维"><a href="#12-自动化运维" class="headerlink" title="12.自动化运维"></a>12.自动化运维</h2><p>自动化运维工具的运用及平台的建立，如Ansible&#x2F;Puppet等</p><h2 id="13-容器技术"><a href="#13-容器技术" class="headerlink" title="13.容器技术"></a>13.容器技术</h2><p>容器及云原生，如Docker容器、镜像、仓库及网络；kubernetes容器编排集群的部署与维护</p><h2 id="14-虚拟化及云平台"><a href="#14-虚拟化及云平台" class="headerlink" title="14.虚拟化及云平台"></a>14.虚拟化及云平台</h2><p>虚拟化技术，如KVM&#x2F;Xen&#x2F;VMware虚拟化技术；开源云平台架构OpenStack&#x2F;CloudStack；超融合平台Ovirt&#x2F;Proxmox VE</p><h2 id="15-DevOps"><a href="#15-DevOps" class="headerlink" title="15.DevOps"></a>15.DevOps</h2><p>CICD平台技术，如Git、Jenkins等</p><h2 id="16-大数据"><a href="#16-大数据" class="headerlink" title="16.大数据"></a>16.大数据</h2><p>Hadoop分布式计算平台</p><h2 id="17-硬件相关"><a href="#17-硬件相关" class="headerlink" title="17.硬件相关"></a>17.硬件相关</h2><p>路由器、交换机、防火墙的配置；磁盘阵列的安装与配置</p><h2 id="18-计算机安全"><a href="#18-计算机安全" class="headerlink" title="18.计算机安全"></a>18.计算机安全</h2><p>计算机等保评测；Ddos攻击防护；WAF防攻击工具</p>]]></content>
    
    
    <categories>
      
      <category>工作</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>云计算</tag>
      
      <tag>大数据</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
